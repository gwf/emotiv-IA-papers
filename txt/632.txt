ISSN 1870-4069

Building a Corpus of Facial Expressions
for Learning-Centered Emotions
María Lucía Barrón-Estrada, Ramón Zatarain-Cabada,
Bianca Giovanna Aispuro-Medina, Elvia Minerva Valencia-Rodríguez,
Ana Cecilia Lara-Barrera
Instituto Tecnológico de Culiacán, Culiacán, Sinaloa,
Mexico
{lbarron, rzatarain, m06170904, m95170906, m15171452} @itculiacan.edu.mx

Abstract. Recognizing emotions in software interfaces is very important
today and even more in the educational field where the students can, through
their emotions, reveal affective states related or not to the learning process.
An educational software with the capability to recognize emotions will allow
the students to receive appropriate feedback to their personal cognitive needs.
In this paper a method is presented for the construction of an image database
of facial expressions, in which, through the detection of EGG signals
corresponding to affective states focused on education (engagement, bored,
frustrated and meditation), detects and registers spontaneous facial
expressions. We also show experiments with the obtained results.
Keywords: Face expression recognition, affective computing, EEG
recognition, face expression database.

1

Introduction

The emotional state of a person is important because it enables or restricts the
performance of its actions to achieve different goals. The study of emotions and the
affective states had become increasingly popular over the last 20 years including
them on the computing systems that we use nowadays. In the educational area, the
emotions of the individuals are transcendental to the learning process. Several
research works [1, 2, 3] had been developed to study the existing relation between
the affective state of the student and the cognitive processes through the use of
intelligent systems of learning. Some of the most important affective states focused
on education are bored, confused, frustrated and engagement [4].
The purpose of this work was to build an image database of spontaneous facial
expressions that corresponds to affective states focused on education. The main
purpose was to use the corpus in different Intelligent Tutoring Systems (ITS). The
main motivation of this work emerges from the need of having an image database
of spontaneous facial expressions corresponding to affective states focused on
education, since the existent data bases have issues such as: they only contain

pp. 45–52; rec. 2016-07-21; acc. 2016-09-15

45

Research in Computing Science 129 (2016)

María Lucía Barrón-Estrada, Ramón Zatarain-Cabada, Bianca Giovanna Aispuro-Medina, et al.

images from people with posed facial expressions to represent an emotion and the
images are from people with physiological features different from our community.
For the creation of the corpus, we needed to capture the images and label them
with the emotion that they represent. The Emotiv EPOC headset, was used to
perform the reading of EGG signals in persons participating in the experiment; this
device allows to detect affective states focused on education using the affective
suite. Additionally, to capture the image of the facial expression corresponding to
the affective state of the person, a camera C920 Logitech HD Pro Webcam was
used.
This paper is organized as follows: in the second section the related work is
described; a description of the technology used to build the corpus is presented in
the third section; in the fourth section the methodology for the development of the
corpus is shown; the experiment and the obtained results are described in the fifth
section; finally in the sixth section we present the conclusions.

2

Related Work

In this section some important works related to databases of facial expressions are
presented and discussed.
Radboud Faces Database (RaFD) is a database that contains photographs of a
group of 67 models, formed by Caucasians adults and children. Each model was
trained to show 8 different facial expressions (anger, disgust, fear, happiness,
sadness, contempt and neutral) and each emotion is shown with three different gaze
directions where all photographs were taken from 5 different camera angles
simultaneously in a highly controlled environment [5].
JAFFE Database is a database of facial expressions acted by Japanese women.
The database contains facial expressions of over 60 women and each one of them
presents 3 to 4 expressions of the following emotions: neutral, happy, anger, disgust,
fear, sadness and surprise. The database contains a total of 219 images on grey scale
[5]. In figure 2 two participants can be observed showing the seven expressions
previously mentioned.
Database for Emotion Analysis using Physiological Signals (DEAP), is a
multimodal database which presents a data set for the analysis of human affective
states based on the basic emotions proposed by Ekman (fear, anger, happy, sadness,
disgust and surprise). In this database EGG and physiological signals of 32
participants were recorded as each watched 40 one-minute long excerpts of music
videos. Participants rated each video in terms of levels of arousal, valence,
like/dislike, dominance and familiarity. For 22 of the 32 participants frontal face
video was also recorded. The correlation between EGG signals and the participant
ratings was investigated. The data set is publicly available so it can be used on
investigations regarding estimation methods of affective states [6].
NVIE Database is a Natural Visible and Infrared Facial Expression Database
that contains both posed and spontaneous expressions performed by 215 students
(157 men and 58 women). The evaluated emotions were: happiness, sadness,
surprise, fear, anger and disgust. The spontaneous emotions were induced by

Research in Computing Science 129 (2016)

46

ISSN 1870-4069

Building a Corpus of Facial Expressions for Learning-Centered Emotions

screening deliberately emotional videos. Besides of capturing the facial expressions
the goal was to analyze the relation between facial temperature and the emotion
through a statistical analysis [7].
Cohn-Kanade dataset is a database that contains images of 100 college students
between 18 and 35 years old. From this population, 65% are women, 15% are
Africa-Americans, and only 3% are Asian or Spanish. All of the subjects were
recorded acting a total of 23 facial expressions corresponding to seven emotions
(neutral, anger, disgust, fear, joy, sadness and surprise) [8] using a video camera.
This database was released in order to promote investigation on the individual
automatic recognition of facial expressions, since then it has become the database
of facial expressions more used in the development of algorithms and assessments
of this kind [9].
The database of facial expressions that is presented in this paper distinguishes
itself from the previous mentioned works in that the registered emotions are focused
specifically in the learning process.

3

Technology

Emotiv EPOC is a brain-computer interface, created by Emotiv Systems. It is used
in researching applications and contains three components named “suites” to
process the signals: Expressive suite, Cognitive suite, and Affective suite.

Fig. 1. Emotiv EPOC suite’s interfaces.

The expressive Suite determines the facial expressions in real time according to
the signals received by the interface. An avatar appears in the computer’s screen

ISSN 1870-4069

47

Research in Computing Science 129 (2016)

María Lucía Barrón-Estrada, Ramón Zatarain-Cabada, Bianca Giovanna Aispuro-Medina, et al.

imitating the user’s facial expressions. This action make possible a natural
interaction. The cognitive suite interprets the user’s thinking and intentions. The
affective suite monitors the user’s affective states in real time. Figure 1 shows the
Emotiv interfaces.
The Emotiv EPOC headset has two support electrodes. They are set behind each
ear in the skull’s protuberant zone and are considered as points of reference to guide
the correct placement of the rest of the electrodes. With the Control panel included
in the kit, it is possible a graphic visualization of the electrodes’ status to be able to
know if the electrodes are positioned correctly or the electrodes need an adjustment.
There is also the TestBench application to visualize the EGG signals in real time.

4

Application Development to Build the Database

The affective states detected by Emotiv EPOC considered in the creation of the
facial expressions database are: engagement, boring, frustration and meditation. A
Java application was implemented in order to create the facial image database
considering the affective state when using the Emotiv headset. Emocomposer was
required to emulate the headset operation. Figure 2 shows the environment and
components of the application. The environment of the application consist of
different components around the Emotiv’s SDK and the Emotiv EPOC headset. The
Java program running in the PC was implemented with the NetBeans IDE
development environment. A jna.jar library was needed to make possible the
interaction of the application and Emotiv EPOC headset. The open source OpenCV
library was incorporated to perform the capture of the images using a Web Cam.

Fig. 2. Environment to create the facial expression database.

Research in Computing Science 129 (2016)

48

ISSN 1870-4069

Building a Corpus of Facial Expressions for Learning-Centered Emotions

4.1

Testing the Environment

Some situations were detected and solved: lighting troubles which were solved
using two extra white light lamps in addition to the experimentation room
illumination. Figure 3 shows the general experiment environment, where the
participant, as can be appreciated, is in front of the monitor, the position of the
illumination lamps, a web cam, and the Emotiv EPOC headset.
4.2

Important Points in the Final Experiment

In the Final Experiment phase, the captured images were stored to feed the facial
expressions database, using a partially controlled environment in which aspects like
illumination, background, and acoustic were considered, to allow the participant
express his/her emotions in a natural manner. Next, each experiment phase is
described.

Fig. 3. Experiment environment.

A group of Master degree students from Instituto Tecnológico de Culiacán was
chosen to participate in the final experiment. It was conformed of 8 students (5 men
and 3 women), ages from 24 to 47. Intending to homologate the initial states of the
subjects, they were instructed to follow the next recommendations: Sleep at least 8
hours the day before the experiment, had a good breakfast, do not consume coffee
or energy drinks, do not consume substances that affect the nervous system, do not
use gel or hair spray the day of experiment, do not wear glasses and wear dark
clothes the day of the experiment.

ISSN 1870-4069

49

Research in Computing Science 129 (2016)

María Lucía Barrón-Estrada, Ramón Zatarain-Cabada, Bianca Giovanna Aispuro-Medina, et al.

The materials used in the experiment were: The Emotiv SDK Kit; a Laptop with
Windows 7 Professional 64-bit operating system, Intel (R) Core (TM) i3-2310M
and 4.00 GB RAM; the development environment NetBeans 8.1; Open CV library
to capture the images; Java platform: Standard Edition Development Kit (JDK ™)
to 32 bits; Logitech Webcam C920 HD Pro; Java application EmoStateLog Emotiv
SDK and application implemented for the experiment; the Emotiv EPOC headset.
Some examples of programming exercises on Java that were provided to the
subject participant to perform cognitive activities for 30 minutes were:
a) Hello World program
b) A program that reads name and date of birth of a person and get the
number of days he has lived
c) A program that accepts as input 10 numbers and outputs their sum and
average.
Experiment Execution Protocol
Our application captures an image every four seconds; to classify the image we take
the amplitude value of emotions with greater intensity of the EEG signals and store
it in a specific directory in the appropriate folder according to the student emotion.
The directory contains four folders, one for each emotion (Engagement, Boredom,
Frustration and Excitement). Additionally we recorded the name of the image and
the amplitude values of the signals in a text file.
This process is performed individually with each of the eight participants, during
the morning using the Master of Science laboratory from the Instituto Tecnológico
de Culiacán, registering an average of 450 photographs by participant.
Debugging of Facial Expressions Database
We eliminate the photographs that had some type of facial obstruction, closed eyes,
or have no frontal posture. The images were cut massively to reduce the margin of
the image and emphasize the subject's face. At the end we had a total of 730
photographs. Figure 4 shows some photographs stored in our corpus.

5

Evaluation, Results, and Conclusions

Next to the creation of the corpus, we evaluate it with an application for emotion
recognition for facial expressions using the technique local binary patterns (LBP).
The recognizer working with 118 photographs had an accuracy of 86.95%. Due to
the positive results in the evaluation, the use of this method for the preparation of a
spontaneous learning-centered facial expressions database is considered feasible.
For future work, we will perform the final experiment in a highly controlled
environment and with improved lighting equipment, increasing the number of
subjects, and taking into account the emotion meditation.

Research in Computing Science 129 (2016)

50

ISSN 1870-4069

Building a Corpus of Facial Expressions for Learning-Centered Emotions

Fig. 4. Examples of facial expressions stored in the corpus.

References
1.

2.
3.

4.

5.

6.

7.

8.

Barrón-Estrada, M. L., Zatarain-Cabada, R., Beltrán, J. A., Cibrian, F. L., HernándezPérez, Y.: An Intelligent and Affective Tutoring System within a Social Network for
Learning Mathematics. Lecture Notes in Computer Science, 7637, pp. 651‒661 (2012)
Hickey, T. J., Tarimo, W. T.: The affective tutor. Journal of Computing Sciences in
Colleges, 29(6), pp. 50‒56 (2014)
Petrovica, S., Pudane, M.: Simulation of Affective Student-Tutor Interaction for
Affective Tutoring Systems: Design of Knowledge Structure. International Journal of
Education and Learning Systems, 1 (2016)
Bosch, N., Mello, S. D., Hall, F., Baker, R., Shute, V., Wang, L.: Automatic Detection
of Learning - Centered Affective States in the Wild. In: Proc. 20th Int. Conf. Intell.
User Interfaces, pp. 379–388 (2015)
Anitha, C., Venkatesha, M., Adiga, B.: A survey on facial expression databases.
International Journal of Engineering Science and Technology, 2(10), pp. 5158–5174
(2010)
Koelstra, S., Muhl, C., Soleymani, M., Yazdani, A., Ebrahimi, T., Pun, T., Nijholt, A.,
Patras, I.: DEAP: A Database for Emotion Analysis ;Using Physiological Signals.
IEEE Trans. Affect. Comput., 3(1), pp. 18–31 (2012)
Wang, S., Liu, Z., Lv, S., Lv, Y., Wu, G., Peng, P., Chen, F., Wang, X.: A natural
visible and infrared facial expression database for expression recognition and emotion
inference. IEEE Trans. Multimed., 12(7), pp. 682–691 (2010)
Bartlett, M. S., Littlewort, G., Frank, M., Lainscsek, C., Fasel, I., Movellan, J.:
Recognizing facial expression: Machine learning and application to spontaneous
behavior. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit., 2, pp. 568–
573 (2005)

ISSN 1870-4069

51

Research in Computing Science 129 (2016)

María Lucía Barrón-Estrada, Ramón Zatarain-Cabada, Bianca Giovanna Aispuro-Medina, et al.

9.

10.

Lucey, P., Cohn, J. F., Kanade, T., Saragih, J., Ambadar, Z., Matthews, I.: The extended
Cohn-Kanade dataset (CK+): A complete dataset for action unit and emotion-specified
expression. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. - Work, CVPRW
2010, no.July, pp. 94–101 (2010)
Cohn, J. F., Ambadar, Z., Ekman, P.: Observer-based measurement of facial expression
with the Facial Action Coding System. Handb. Emot. Elicitation Assessment, pp. 203–
221 (2007)

Research in Computing Science 129 (2016)

52

ISSN 1870-4069

