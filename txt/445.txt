ISSN 1392â€“124X (print), ISSN 2335â€“884X (online) INFORMATION TECHNOLOGY AND CONTROL, 2014, T. 43, Nr. 1

Identification of Human Response to Virtual 3D Face Stimuli
Egidijus VaÅ¡keviÄius1, AuÅ¡ra VidugirienÄ—2, Vytautas Kaminskas3
Department of Systemsâ€™ Analysis, Vytautas Magnus University
Vileikos st. 8, LT â€“ 44404, Kaunas Lithuania
e-mail: 1e.vaskevicius@if.vdu.lt, 2a.vidugiriene@if.vdu.lt,
3
v.kaminskas@if.vdu.lt
http://dx.doi.org/10.5755/j01.itc.43.1.5927
Abstract. This paper introduces identification results of human response to virtual 3D face stimuli. Observations
of human reactions are done using preprocessed EEG (electroencephalogram) signals: excitement, meditation,
frustration, engagement/boredom. Virtual 3D face features â€“ distance between eyes, nose width, and chin width â€“ are
used as stimuli. Cross-correlation analysis demonstrated that dynamical relations between human reactions and stimuli
exist. Input-output models describing relations between stimuli and corresponding human reactions are built. A new
input-output model building method is proposed that allows building stable models with the least output prediction
error. Modelsâ€™ validation results demonstrate relatively high prediction accuracy of human reactions.
Keywords: 3D face stimuli; human reaction; cross-correlation analysis; input-output model; model parameter
estimation; model validation.

measure and analyze the signals [23]. We have chosen
to use EEG-based signals for human affective state
monitoring because of the reliability and quick
response [18], [6].
In this research, a virtual 3D face was used as
a stimulus for eliciting human reaction. It is known
that the majority of information to another person
when communicating is transferred by face features
[13]. A person is used to react to the smallest
face feature changes during a very short time [21].
We have created and used a realistic virtual 3D
face model. Human reactions when he/she is
observing visual stimuli (3D face changes) were
investigated.
Our previous investigations on the dependencies
between virtual stimuli and human affective responses
demonstrated that in certain circumstances the
dependencies exist and there is a need of further
research [20].
Three types of stimuli in virtual 3D face (distance
between eyes, nose width and chin width) were used
for human reaction elicitation and four response
signals (excitement, meditation, engagement/boredom,
and frustration) were observed. The aim of the
investigation is to identify the relationships between
stimuli and reactions to them and to describe those
using mathematical models that can predict human
reactions in high accuracy and can be used for human
state control systems design.

1. Introduction
Virtual environments are becoming a part of our
daily life including computer games, work tasks,
various mental and physical training programs,
e-learning, online shopping, and specialized software.
These environments affect the users in different
extents and ways. The influence can be both
positive and negative. Various studies introduce
investigations, where human state observation is an
important task. Several of them concentrates on
stress evaluation and training when using virtual
environments for military purposes and curing
post-traumatic stress disorder [15]. Other focuse on
neuro-marketing [11], [12], adaptive virtual mediator
services [14], social networks [22], and learning
applications [1], [4].
In order to protect people from various harmful
effects to their health (e.g. stress) that virtual
environment can cause [17], to create training
applications, or to implement/use virtual mediator
services [7], control mechanisms of virtual
environment object features and the interactions have
to be modeled [5], [10].
The most effective way to observe a state of a
person in a real time is to monitor human bio-signals
[19]. A variety of human bio-signals such as galvanic
skin response, pulse, magnetoencephalography
(MEG), heart rate, EEG, etc. are used for this
purpose and plenty of methods and techniques help to

47

E. VaÅ¡keviÄius, A. VidugirienÄ—, V. Kaminskas

largest/smallest value and zero (normal face) was 10 s.
The features were changing continuously: from
normal to extremely wide, then back to normal and to
extremely thin, then again to normal and to extremely
wide and again back to normal. The animated tests
were prepared for every of three features: distance
between eyes, nose width and chin width.

2. Observations and data
Features of virtual 3D face (distance between eyes,
nose width, and chin width) were used for input and
EEG-based human reaction signals (excitement,
meditation, frustration, engagement/boredom) of a
person were measured as output (Fig. 1). These output
signals were chosen as important features in learning,
and stress regulation. The output signals were
recorded using Emotive Epoc device.The device
records EEG inputs from 14 channels (according to
international 10-20 locations): AF3, F7, F3, FC5, T7,
P7, O1, O2, P8, T8, FC6, F4, F8, AF4 [3].

TYPE 1
2
0
-2
0

10

20

30
time, s
TYPE 2

40

50

10

20

30
time, s

40

50

2
0
-2
0

Figure 3. TYPE 1 and TYPE 2 input signals: values
variation of corresponding face feature modifications

Figure 1. Input-Output scheme for the experiments

TYPE 2 input signals (Fig. 3, bottom) were formed
when changing the 3D face features suddenly. â€œSmall
distance-between-eyesâ€, â€œthin noseâ€ or â€œthin chinâ€
was shown for some period of time, then the picture
was suddenly changed to â€œnormalâ€ and after some
time to â€œlarge distance-between-eyesâ€, â€œwide noseâ€ or
â€œwide chinâ€ and again to â€œnormalâ€. It was repeated
two times with different time steps between each
change. â€œNeutralâ€ face was equal to 0, â€œlargest distance-between-eyesâ€, â€œwidest noseâ€ or â€œwidest chinâ€
were equal to 1.25 and the smallest features â€“ to -1.25.
Output signals â€“ excitement, meditation,
frustration and engagement/boredom â€“ varied from 0
to 1. If excitement, meditation, frustration and engagement were low, the value was 0 or close to 0 and if
they were high, the values of parameters were 1 or
close to 1. The signals were recorded with the
sampling period of T0=0.5 s.
Five volunteers (volunteers no. 1-3 were female,
and volunteers no. 4-5 were male) were tested. A
volunteer was watching three animated scenes of
approximately 1 minute one after another, and
EEG-based signals were measured simultaneously
using Emotiv Epoc device. The recorded preprocessed
signals were stored for later analysis.

Seven 3D woman faces were created using
Autodesk MAYA. One 3D woman face was used as a
â€œneutralâ€ one (Fig. 2, middle). The other faces were
formed by changing face features in an extreme
manner: large and small distance between eyes (Fig. 2,
left and right, top), wide and thin nose (Fig. 2, left and
right, middle), wide and thin chin (Fig. 2, left and
right, bottom). After importing the 3D faces into Unity
3D engine, the transitions between the same kind of
features in the face were programmed using morph
targetâ€™s method.

Figure 2. A â€œneutralâ€ 3D woman face (middle) and limit
cases of features: the largest (left top) and the smallest (right
top) distance between eyes, the widest (left middle) and the
thinnest (right middle) nose, the widest (left bottom) and the
thinnest (right bottom) chin

3. Correlation analysis
To estimate the possible relations between virtual
stimuli and human reaction signals, a cross-correlation
analysis between stimuli (3D virtual face features) and
response (EEG-based reaction) signals was performed.
Cross-covariation functions between input and
output signals and auto-covariation functions of input
and output signals are used for this purpose [16]:

There were two types of input signals used.
TYPE 1 input signals (Fig. 3, top) were formed when
neutral face was equal to 0, the largest (widest) feature
was equal to 1.8 and the smallest (thinnest) feature
was equal to -1.8. The values in-between were
changed
linearly.
Time
interval
between

48

Identification of Human Response to Virtual 3D Face Stimuli
ğ‘âˆ’ğœ

Excitement

1
ğ‘…ğ‘¦ğ‘¥ [ğœ] = ï¿½(ğ‘¦ğ‘¡ âˆ’ ğ‘¦ï¿½)(ğ‘¥ğ‘¡+ğœ âˆ’ ğ‘¥Ì… )
ğ‘
1
ğ‘…ğ‘¥ğ‘¥ [ğœ] = ï¿½(ğ‘¥ğ‘¡ âˆ’ ğ‘¥Ì… )(ğ‘¥ğ‘¡+ğœ âˆ’ ğ‘¥Ì… )
ğ‘

(1)

ğ‘âˆ’ğœ

1
ğ‘…ğ‘¦ğ‘¦ [ğœ] = ï¿½(ğ‘¦ğ‘¡ âˆ’ ğ‘¦ï¿½)(ğ‘¦ğ‘¡+ğœ âˆ’ ğ‘¦ï¿½)
ğ‘
ğ‘

1
ï¿½ ğ‘¥ğ‘¡ ,
ğ‘
ğ‘¡=1

ğ‘¦ï¿½ =

yx

0
time, s
Frustration

0
50
time, s
Engagement/Boredom
1

-1
-50

(2)

ğ‘¡=1

yx

r

ryx

ğ‘

1
ï¿½ ğ‘¦ğ‘¡
ğ‘

0

0
time, s

50

0
-1
-50

50

1

ğ‘¡=1

ğ‘¥Ì… =

0
-1
-50

ğ‘¡=1

1

r

ryx

ğ‘¡=1

ğ‘âˆ’ğœ

where

Meditation

1

0
-1
-50

0
time, s

50

Figure 6. Cross-correlation functions between chin width
input feature and all output features for volunteer
no.1. Solid line denotes input Type 1,
dotted line denotes input Type 2

are the averages of input and output, Ï„ = 0, Â±1, â€¦ and
N=115.
Cross-correlation functions between both types of
input signals (TYPE 1 and TYPE 2) and all output signals for volunteer no. 1 are demonstrated in Figs. 4âˆ’6.

Maximum cross-correlation function values
Meditation
1

0
50
time, s
Engagement/Boredom
1

-1
-50

0
time, s

50

0
time, s

50

Figure 4. Cross-correlation functions between distance
between eyes input feature and all output features for
volunteer no.1. Solid line denotes input Type 1,
dotted line denotes input Type 2
Excitement

-1
-50

yx

r

ryx

1

0

0
time, s
Frustration

50
0
time, s
Engagement/Boredom
1

-1
-50

yx

r

ryx

1
0

0
time, s

50

0
-1
-50

50

0
-1
-50

0
time, s

Output (reaction)
Input (stimulus)

Distance between eyes
1 Nose width
Chin width
Distance between eyes
2 Nose width
Chin width
Distance between eyes
3 Nose width
Chin width
Distance between eyes
4 Nose width
Chin width
Distance between eyes
5 Nose width
Chin width

Meditation

1

(3)

Table 1. Maximum cross-correlation function values when
Type 1 input signal is used

0
-1
-50

ï¿½
ï¿½ğ‘…ğ‘¦ğ‘¦ [0]ğ‘…ğ‘¥ğ‘¥ [0]

50

Engagement/
Boredom

ryx

r

yx

1

ğ‘…ğ‘¦ğ‘¥ [ğœ]

were estimated for each stimulus and the
corresponding response signal pair. The estimates of
the maximum cross-correlation function values are
shown in Table 1 and Table 2.

-1
-50

50

0

0

ğœ

Frustration

0
time, s
Frustration

ğœ

Volunteer no.

-1
-50

maxï¿½ğ‘Ÿğ‘¦ğ‘¥ [0]ï¿½ = max ï¿½

Meditation

yx

0

r

r

yx

1

Excitement

Excitement

0.55
0.51
0.73
0.47
0.73
0.35
0.54
0.30
0.38
0.76
0.47
0.40
0.30
0.71
0.70

0.75
0.55
0.57
0.30
0.51
0.75
0.52
0.67
0.74
0.69
0.65
0.64
0.57
0.68
0.66

0.88
0.53
0.59
0.44
0.47
0.43
0.58
0.80
0.43
0.64
0.59
0.43
0.54
0.75
0.54

0.81
0.55
0.63
0.38
0.66
0.37
0.46
0.62
0.38
0.44
0.66
0.55
0.69
0.82
0.62

The shift of the maximal values of crosscorrelation functions in relation to ğ‘Ÿğ‘¦ğ‘¥ [0] allow stating
that there exist dynamic relations between virtual

Figure 5. Cross-correlation functions between nose width
input feature and all output features for volunteer no.1. Solid
line denotes input Type 1, dotted line denotes input Type 2

49

E. VaÅ¡keviÄius, A. VidugirienÄ—, V. Kaminskas

stimuli and human reactions to them. High crosscorrelation values justify a possibility to use linear
dynamic models.

according to the observations obtained during the
experiments with the volunteers.
It is not difficult to show that in the case of using
the model, the following relationship exists between
covariation functions:

Engagement/
Boredom

Distance between eyes
1 Nose width
Chin width
Distance between eyes
2 Nose width
Chin width
Distance between eyes
3 Nose width
Chin width
Distance between eyes
4 Nose width
Chin width
Distance between eyes
5 Nose width
Chin width

Frustration

Input (stimulus)

Meditation

Output (reaction)
Excitement

Volunteer no.

Table 2. Maximum cross-correlation function values when
Type 2 input signal is used

0.45

0.57

0.49

0.56

0.54

0.47

0.45

0.68

0.53

0.56

0.47

0.38

0.46

0.54

0.56

0.43

0.49

0.41

0.43

0.61

0.63

0.56

0.57

0.38

0.48

0.45

0.33

0.63

0.47

0.49

0.47

0.68

0.55

0.40

0.42

0.57

0.61

0.59

0.43

0.44

0.50

0.44

0.46

0.41

0.53

0.63

0.64

0.54

0.36

0.37

0.45

0.60

0.43

0.50

0.45

0.52

0.46

0.56

0.36

0.48

ğ‘š

ğ‘—=0

âˆ’ğ‘…ğ‘¦ğ‘¥ [ğœ âˆ’ 1], â€¦ , âˆ’ğ‘…ğ‘¦ğ‘¥ [ğœ âˆ’ ğ‘›] ]

ğ’„ğ‘‡ = ï¿½ğ‘0 , ğ‘1 , â€¦ , ğ‘ğ‘š, ğ‘1 , ğ‘2 , â€¦ , ğ‘ğ‘› ï¿½

ğ’„ï¿½ = ğ‘¸âˆ’1 ğ’’

where M is the number of covariation function values
used.
Model parameter estimatesâ€™ vector ğ’„ï¿½ is calculated
when M=0, 1, â€¦ and model stability condition is
verified [8]. It means that the following polynomial
ğ‘›

ğ´Ì‚ğ‘€ (ğ‘§) = ğ‘§ ğ‘› ğ´Ì‚ğ‘€ (ğ‘§ âˆ’1 ) = ğ‘§ ğ‘› + ï¿½ ğ‘ï¿½ğ‘– ğ‘§ ğ‘›âˆ’ğ‘–

ğ‘§ğ‘–ğ´ :

ğ‘–=1

(14)

ğ‘–=1

roots

ğ´Ì‚ğ‘€ (ğ‘§) = 0, ğ‘– = 1, 2, â€¦ , ğ‘›

(15)

have to be in the unit disk
ï¿½ğ‘§ğ‘–ğ´ ï¿½ â‰¤ 1

(16)

ï¿½:
ğ‘€

(17)

In that way, we get a subset ğ›ºğ‘€ with M values
where the models are stable. From this subset we
choose

(6)

with sampling period ğ‘‡0 , ğœ€ğ‘¡ corresponds to noise
signal and z-1 is the backward-shift operator (z âˆ’1 xt =
xtâˆ’1 ).
Eq. (4) can be expressed in the following form:
ğ‘—=0

ğ‘€

ğ‘¸ = ï¿½ğœ=âˆ’ğ‘€ ğœ·ğœ ğœ·ğ‘‡ğœ , ğ’’ = ï¿½ğœ=âˆ’ğ‘€ ğ‘…ğ‘¦ğ‘¥ [ğœ]ğœ·ğœ (13)

ğ‘›

.

(11)

(12)

ğ‘€

where ğ‘¦ğ‘¡ is an output (excitement, engagement/
boredom meditation, frustration or) and ğ‘¥ğ‘¡ is an input
(distance between eyes, nose width, chin width)
signals respectively calculated as

ğ‘¦ğ‘¡ = ï¿½ ğ‘ğ‘— ğ‘¥ğ‘¡âˆ’ğ‘— âˆ’ ï¿½ ğ‘ğ‘– ğ‘¦ğ‘¡âˆ’ğ‘– + ğœ€ğ‘¡

(10)

and T is a vector transpose sign.
For the estimation of unknown parameter vector c
we use a method of least squares [8]:

ğ‘–=1

ğ‘›

(9)

ğœ·ğ‘‡ğœ = [ ğ‘…ğ‘¥ğ‘¥ [ğœ], ğ‘…ğ‘¥ğ‘¥ [ğœ âˆ’ 1], â€¦ , ğ‘…ğ‘¥ğ‘¥ [ğœ âˆ’ ğ‘š],

ğ´(ğ‘§ âˆ’1 ) = 1 + ï¿½ ğ‘ğ‘– ğ‘§ âˆ’ğ‘– (5)

ğ‘š

ğ‘–=1

where

(4)

ğ‘¦ğ‘¡ = ğ‘¦(ğ‘¡ğ‘‡0 ), ğ‘¥ğ‘¡ = ğ‘¥(ğ‘¡ğ‘‡0 )

ğ‘—=0

ğ‘…ğ‘¦ğ‘¥ [ğœ] = ğœ·ğ‘‡ğœ ğ’„

All dependencies between virtual object stimuli
and human reactions are described by input-output
structure models [8]:

ğµ(ğ‘§ âˆ’1 ) = ï¿½ ğ‘ğ‘— ğ‘§ âˆ’ğ‘— ,

ğ‘›

Equation (8) can be expressed as a linear
regression equation:

4. Mathematical model building

ğ´(ğ‘§ âˆ’1 )ğ‘¦ğ‘¡ = ğµ(ğ‘§ âˆ’1 )ğ‘¥ğ‘¡ + ğœ€ğ‘¡

ğ‘š

ğ‘…ğ‘¦ğ‘¥ [ğœ] = ï¿½ ğ‘ğ‘— ğ‘…ğ‘¥ğ‘¥ [ğœ âˆ’ ğ‘—] âˆ’ ï¿½ ğ‘ğ‘– ğ‘…ğ‘¦ğ‘¥ [ğœ âˆ’ ğ‘–] .(8)

where

ğ‘šğ‘šğ‘š{ğœğœ€ [ğ‘€|ğ‘š, ğ‘›], ğ‘€ğ‘€ğ›ºğ‘€ }
ğ‘

1
ğœğœ€ [ğ‘€|ğ‘š, ğ‘›] = ï¿½ ï¿½ ğœ€Ì‚ğ‘¡2 [ğ‘€|ğ‘š, ğ‘›]
ğ‘

(18)

ğ‘¡=1

(7)

is one step output prediction error standard deviation,
ğœ€Ì‚ğ‘¡ [ğ‘€|ğ‘š, ğ‘›] = ğ‘¦ğ‘¡ âˆ’ ğ‘¦ï¿½ğ‘¡|ğ‘¡âˆ’1 [ğ‘€|ğ‘š, ğ‘›]

Model parameters (coefficients of the polynomials
(5)) and model order (degrees m and n of polynomials
(5)) are unknown. They have to be estimated

is one step output prediction error,

50

(19)

Identification of Human Response to Virtual 3D Face Stimuli

ğ‘¦ï¿½ğ‘¡|ğ‘¡âˆ’1 [ğ‘€|ğ‘š, ğ‘›] =

= ğ‘§ï¿½1 âˆ’ ğ´Ì‚ğ‘€ (ğ‘§ âˆ’1 )ï¿½ğ‘¦ğ‘¡âˆ’1 + ğµï¿½ğ‘€ (ğ‘§ âˆ’1 )ğ‘¥ğ‘¡

corresponds to a relative variation of prediction error
standard deviation from 0,1% to 1%.
This way a stable input-output model is built that
ensures the best one step output signal prediction.
Figs. 9-10 demonstrate prediction error standard
deviations for an input-output pair when n=1, 2 and
m=0, 1, 2 for each model. Fig. 9. shows error standard
deviation when distance-between-eyes input (Type 1)
and engagement/boredom output is used for prediction
for every of five models. Fig. 10. shows the same with
chin width input (Type 2) and excitement output
signal.

(20)

is one step output prediction [9] and z is the forwardshift operator (zyt = yt+1 ).
Figs. 7-8 demonstrate examples of prediction error
standard deviations with every M value for each
output signal (when the same input signal is used).

Ïƒ

Îµ

0.1

0.05

0
0

10

20
M

30

40

Figure 7. Prediction error standard deviation, nose width
input (Type 2), volunteer no. 4, model order m=1, n=1.
Lines: solid thickâ€“ excitement, dottedâ€“ meditation,
solid thinâ€“ frustration, dashedâ€“ engagement/boredom

Figure 9. Prediction error standard deviation, volunteer
no. 2, distance-between-eyes input signal (Type 1),
engagement/boredom output signal

Ïƒ

Îµ

0.1

0.05

0
0

10

20
M

30

40

Figure 8. Prediction error standard deviation, distancebetween-eyes input (Type 2), volunteer no. 1,
model order m=0, n=2. Lines: solid thickâ€“ excitement,
dottedâ€“ meditation, solid thinâ€“ frustration,
dashedâ€“ engagement/boredom

Figure 10. Prediction error standard deviation, volunteer
no. 3, chin width input signal (Type 2),
excitement output signal

The circles indicate M values where the error
standard deviations were the smallest in each output
case. If models are not stable with certain M values,
the error standard deviations are not estimated and
there are discontinuities in the error standard
deviations signal.
Estimates of the model orders â€“ ğ‘š
ï¿½ and ğ‘›ï¿½ â€“ are
defined from the following conditions:
ğ‘›ï¿½ :

ï¿½ |ğ‘š, ğ‘› + 1ï¿½ âˆ’ ğœğœ€ ï¿½ğ‘€
ï¿½ |ğ‘š, ğ‘›ï¿½
ğœğœ€ ï¿½ğ‘€
ï¿½
ï¿½ â‰¤ ğ›¿,
ï¿½
ğœğœ€ ï¿½ğ‘€|ğ‘š, ğ‘›ï¿½

ğ‘š
ï¿½:

ï¿½ |ğ‘š + 1, ğ‘›ï¿½ âˆ’ ğœğœ€ ï¿½ğ‘€
ï¿½ |ğ‘š, ğ‘›ï¿½
ğœğœ€ ï¿½ğ‘€
ï¿½ â‰¤ ğ›¿,
ï¿½
ï¿½ |ğ‘š, ğ‘›ï¿½
ğœğœ€ ï¿½ğ‘€

ğ‘› = 1,2, â€¦

5. Models validation
Every stimuli type requires 12 input-output models
for each volunteer. Each model is selected from five
possible models (when n=1, 2; m=0, â€¦, n) using the
rule (21). Two types of stimuli for each of five
volunteers were used, so 120 models were analyzed.
The analysis of five volunteersâ€™ data showed that
relations between each of three stimuli (distance
between eyes, nose width, and chin width) and
excitement output signal can be modeled when model
order is ğ‘š
ï¿½ = 0 and ğ‘›ï¿½ = 1. Relations between each of
three stimuli and meditation, frustration and
engagement/boredom output signals are best modeled
when model order is ğ‘š
ï¿½ = 0 and ğ‘›ï¿½ = 2.
Every model was validated through the output
signal (excitement, meditation, engagement/boredom,
frustration) prediction analysis. The predicted output
signals have the following expression [8]:

(21)

ğ‘š = 0,1, â€¦ , ğ‘›

where ğ›¿ > 0 is a chosen constant value. Usually in the
practice of identification ğ›¿ âˆˆ [0,001 Ã· 0,01] what

51

E. VaÅ¡keviÄius, A. VidugirienÄ—, V. Kaminskas
Table 5. Prediction accuracy measures, volunteer no. 3

ğ‘¦ï¿½ğ‘¡+1|ğ‘¡ = ğ‘§ï¿½1 âˆ’ ğ´Ì‚(ğ‘§ âˆ’1 )ï¿½ğ‘¦ğ‘¡ + ğµï¿½ (ğ‘§ âˆ’1 )ğ‘¥ğ‘¡+1 =
ğ‘›ï¿½

ï¿½
ğ‘š

ğ‘–=1

ğ‘—=0

Input
stimulus

(22)

= âˆ’ ï¿½ ğ‘ï¿½ğ‘– ğ‘¦ğ‘¡+1âˆ’ğ‘– + ï¿½ ğ‘ï¿½ğ‘— ğ‘¥ğ‘¡+1âˆ’ğ‘—

Excitement
Distance
Meditation
between
Frustration
eyes
Eng/Bor
Excitement
Nose Meditation
width Frustration
Eng/Bor
Excitement
Chin Meditation
width Frustration
Eng/Bor

Prediction accuracies were evaluated using the
following measures:
â€¢ prediction error standard deviation
ğ‘âˆ’1

1

ğœğœ€ = ï¿½ ï¿½
ğ‘

ğ‘¡=0

2

ï¿½ğ‘¦ğ‘¡+1 âˆ’ ğ‘¦ï¿½ğ‘¡+1|ğ‘¡ ï¿½

(23)

â€¢ relative prediction error standard deviation
1

ğœï¿½ğœ€ = ï¿½ ï¿½
ğ‘

ğ‘âˆ’1
ğ‘¡=0

ï¿½

ğ‘¦ğ‘¡+1 âˆ’ğ‘¦ï¿½ğ‘¡+1|ğ‘¡ 2
ğ‘¦ğ‘¡+1

ï¿½ âˆ— 100%

Output
reaction

|ğœ€Ì…| =

1

ğ‘

ï¿½

ğ‘¡=0

ï¿½

ğ‘¦ğ‘¡+1

ï¿½ âˆ— 100%

Input
stimulus

(25)

Excitement
Distance
Meditation
between
Frustration
eyes
Eng/Bor
Excitement
Nose Meditation
width Frustration
Eng/Bor
Excitement
Chin Meditation
width Frustration
Eng/Bor

Input TYPE 1
ğˆğœº

0.044
0.006
0.011
0.010
0.038
0.005
0.017
0.003
0.026
0.007
0.017
0.004

ğˆ
ï¿½ğœº,
%

|ğœºï¿½|,
%

11.5 7.79
1.9 1.5
2.1 1.4
1.6 1.5
16.0 10.5
2.0 1.4
4.9 3.3
0.6 0.5
10.9 7.0
2.7 1.7
4.1 2.9
0.8 0.5

Input TYPE 2
ğˆğœº

0.054
0.010
0.023
0.010
0.031
0.007
0.019
0.013
0.017
0.005
0.016
0.014

ğˆ
ï¿½ğœº,
%

13.8
3.0
4.0
1.7
16.1
2.2
7.4
2.4
8.2
2.6
4.8
2.9

|ğœºï¿½|,
%
8.6
2.6
2.8
1.7
9.4
1.7
5.0
2.2
5.2
1.7
3.6
2.8

Output
reaction

Excitement
Distance
Meditation
between
Frustration
eyes
Eng/Bor
Excitement
Nose Meditation
width Frustration
Eng/Bor
Excitement
Chin Meditation
width Frustration
Eng/Bor

Input TYPE 1
ğˆğœº

0.057
0.037
0.016
0.005
0.062
0.002
0.016
0.005
0.040
0.002
0.015
0.006

ğˆ
ï¿½ğœº,
%

|ğœºï¿½|,
%

Input
stimulus

10.8 8.3
1.1 1.0
3.4 2.5
0.7 0.5
18.3 12.3
0.8 0.7
4.7 3.4
0.8 0.6
18.7 11.8
0.8 0.6
4.5 3.3
1.0 0.8

0.041
0.005
0.023
0.008
0.055
0.008
0.024
0.007
0.077
0.005
0.020
0.006

ğˆ
ï¿½ğœº,
%

Output
reaction

Excitement
Distance
Meditation
between
Frustration
eyes
Eng/Bor
Excitement
Nose Meditation
width Frustration
Eng/Bor
Excitement
Chin Meditation
width Frustration
Eng/Bor

Input TYPE 2
ğˆğœº

|ğœºï¿½|,
%

10.6 8.7
4.1 2.9
2.0 1.4
1.5 1.2
9.1 7.3
2.8 1.7
1.9 1.5
1.2 0.9
18.7 11.8
0.8 0.6
4.5 3.3
1.0 0.8

ğˆğœº

0.025
0.010
0.014
0.015
0.033
0.006
0.021
0.011
0.040
0.006
0.025
0.010

ğˆ
ï¿½ğœº,
%

9.2
5.2
3.3
1.7
11.0
3.2
3.3
1.5
11.8
3.3
4.4
1.4

|ğœºï¿½|,
%
7.4
4.2
2.8
1.0
8.2
2.2
2.4
1.2
9.7
2.3
3.6
1.0

Input TYPE 1
ğˆğœº

0.052
0.002
0.021
0.008
0.057
0.003
0.022
0.010
0.066
0.004
0.021
0.008

ğˆ
ï¿½ğœº,
%

|ğœºï¿½|,
%

11.5 8.7
0.7 0.5
4.2 2.8
1.2 0.9
17.0 12.4
0.9 0.7
5.8 4.1
1.7 1.1
13.4 10.6
1.2 0.9
5.7 3.5
1.4 1.0

Input TYPE 2
ğˆğœº

0.060
0.011
0.026
0.011
0.064
0.006
0.033
0.012
0.063
0.005
0.033
0.010

ğˆ
ï¿½ğœº,
%

|ğœºï¿½|,
%

17.1 12.7
3.5 3.3
7.3 4.8
2.0 1.3
12.4 10.1
1.6 0.9
5.7 3.8
2.9 1.8
16.6 11.6
1.8 1.1
7.9 5.6
2.0 1.6

Table 7. Prediction accuracy measures, volunteer no. 5

Table 4. Prediction accuracy measures, volunteer no. 2
Input
stimulus

Output
reaction

Excitement
Distance
Meditation
between
Frustration
eyes
Eng/Bor
Excitement
Nose Meditation
width Frustration
Eng/Bor
Excitement
Chin Meditation
width Frustration
Eng/Bor

Table 3. Prediction accuracy measures, volunteer no. 1
Output
reaction

0.027
0.007
0.009
0.013
0.026
0.006
0.010
0.010
0.040
0.002
0.015
0.006

ğˆ
ï¿½ğœº,
%

Table 6. Prediction accuracy measures, volunteer no. 4

Measures of prediction accuracy (23) - (25) are
given in Tables 3-7.

Input
stimulus

ğˆğœº

Input TYPE 2

(24)

â€¢ and average absolute relative prediction error
ğ‘âˆ’1 ğ‘¦
ï¿½ ğ‘¡+1|ğ‘¡
ğ‘¡+1 âˆ’ğ‘¦

Input TYPE 1

|ğœºï¿½|,
%

20.3 15.4
1.4 1.1
7.4 5.2
1.4 1.1
12.2 8.0
2.5 2.4
3.8 2.6
1.8 1.2
20.6 14.4
1.6 1.2
3.9 2.9
1.3 0.9

Input TYPE 1
ğˆğœº

0.053
0.002
0.015
0.010
0.062
0.003
0.015
0.012
0.059
0.005
0.009
0.008

ğˆ
ï¿½ğœº,
%

|ğœºï¿½|,
%

16.9 12.4
0.9 0.6
5.2 3.4
2.8 1.6
15.6 10.2
1.0 0.8
4.4 2.7
3.0 2.0
18.3 10.8
1.6 1.2
2.7 1.9
1.5 1.1

Input TYPE 2
ğˆğœº

0.068
0.005
0.012
0.012
0.048
0.006
0.014
0.014
0.054
0.005
0.018
0.012

ğˆ
ï¿½ğœº,
%

|ğœºï¿½|,
%

10.2 7.9
1.4 1.2
3.1 2.2
2.7 2.3
14.6 9.9
1.9 1.6
3.0 2.2
3.6 2.6
18.8 13.5
1.5 1.0
4.1 2.7
3.0 2.1

Figs. 11-22 demonstrate observed output signals,
their predictions and prediction errors.
Figs. 11-16 demonstrate observed output signals,
their predictions and prediction errors of female
volunteer when Type 1 and Type 2 inputs are used.
Figs. 17-22 demonstrate the same signals of a male
volunteer when Type 1 and Type 2 inputs are used.

52

Identification of Human Response to Virtual 3D Face Stimuli
Excitement

Meditation

Excitement

Meditation

1

1

1

1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Engagement/Boredom
1

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Engagement/Boredom
1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 11. Volunteer no. 1 (female), distance between eyes
TYPE 1 input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction error
between observed and predicted signal
Excitement

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 14. Volunteer no. 1 (female), distance between eyes
TYPE 2 input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal

Meditation

Meditation

Excitement

1

1

1

1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Engagement/Boredom
1

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Engagement/Boredom
1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 12. Volunteer no. 1 (female), nose width TYPE 1
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal
Excitement

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 15. Volunteer no. 1 (female), nose width TYPE 2
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal

Meditation

Excitement

Meditation

1

1

1

1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Engagement/Boredom
1

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Engagement/Boredom
1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 13. Volunteer no. 1 (female), chin width TYPE 1
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal

10 20 30 40 50
time, s

Figure 16. Volunteer no. 1 (female), chin width TYPE 2
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal

53

E. VaÅ¡keviÄius, A. VidugirienÄ—, V. Kaminskas
Excitement

1

0.5

0.5

0.5

0

0

0

0.5
0

1

Meditation

1

1

10 20 30 40 50
time, s
Frustration

Excitement

Meditation

1

10 20 30 40 50
time, s
Frustration

10 20 30 40 50
time, s
Engagement/Boredom
1

1

10 20 30 40 50
time, s
Engagement/Boredom
1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 17. Volunteer no. 4 (male), distance between eyes
TYPE 1 input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal
Excitement

10 20 30 40 50
time, s

Figure 20. Volunteer no. 4 (male), distance between eyes
TYPE 2 input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal
Meditation

Excitement

Meditation

1

1

1

1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Frustration

10 20 30 40 50
time, s
Engagement/Boredom
1

1

10 20 30 40 50
time, s
Engagement/Boredom
1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 18. Volunteer no. 4, (male) nose width TYPE 1
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal
Excitement

10 20 30 40 50
time, s

Figure 21. Volunteer no. 4 (male), nose width TYPE 2
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal
Excitement

Meditation

Meditation

1

1

1

1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s
Frustration
1

10 20 30 40 50
time, s
Frustration

10 20 30 40 50
time, s
Engagement/Boredom
1

1

10 20 30 40 50
time, s
Engagement/Boredom
1

0.5

0.5

0.5

0.5

0

0

0

0

10 20 30 40 50
time, s

10 20 30 40 50
time, s

10 20 30 40 50
time, s

10 20 30 40 50
time, s

Figure 22. Volunteer no. 4(male), chin width TYPE 2 input.
Solid thick line â€“ observed signal, dotted line â€“ predicted
signal, solid thin line â€“ prediction error between
observed and predicted signal

Figure 19. Volunteer no. 4 (male), chin width TYPE 1
input. Solid thick line â€“ observed signal, dotted
line â€“ predicted signal, solid thin line â€“ prediction
error between observed and predicted signal

54

Identification of Human Response to Virtual 3D Face Stimuli

Results of model validation demonstrate that
average absolute relative prediction errors of
meditation and engagement/boredom signals in all
five volunteer cases were less than 1.5 %, frustration â€“
less than 3.5 %, and excitement â€“ less than 10 %. The
corresponding relative error standard deviation
averages are less than 2 % for meditation and
engagement/boredom, less than 4.5 % for frustration
and less than 15 % for excitement output signals.
Predictive models are necessary in the design of
predictor-based self-tuning control systems [2], [9].
High prediction accuracies of the built models allow
expecting that they can be a basis for this type control
systems of human reactions to virtual stimuli. Such
systems could serve as a mean for regulation of
human attention level in learning applications as well
as in computerized workplaces for dispatchers.

[2]
[3]

[4]

[5]

[6]

6. Conclusions
Cross-correlation analysis demonstrated that there
is a relatively high correlation between stimuli (input)
and human response (output) and that evidences the
existence of linear dynamic relations between them.
When using cross- and auto-correlation functions,
a new method for building an input-output model was
proposed. It allows building a stable model for output
signal prediction with the least prediction error. The
models allow describing the dynamic dependencies
between virtual 3D face that changes during time and
the reactions to it.
Validation results of the models demonstrate that
every volunteer reacted to the stimuli individually and
the reactions are described by input-output models
with different estimates of polynomial coefficient.
The models describe the dependencies between
every input signal (distance between eyes, nose width
and chin width) and every corresponding output signal
(excitement, meditation, engagement/boredom, and
frustration) in high accuracy: averages of absolute
relative prediction errors of engagement/boredom and
meditation signals in five volunteer cases were less
than 1.5 %, frustration â€“ less than 3.5 %, and
excitement â€“ less than 10 %.

[7]

[8]

[9]

[10]

[11]

Acknowledgments
Postdoctoral fellowship of Ausra Vidugiriene is
funded by European Union Structural Funds project
â€Postdoctoral
Fellowship
Implementation
in
Lithuaniaâ€ within the framework of the Measure for
Enhancing Mobility of Scholars and Other
Researchers and the Promotion of Student Research
(VP1-3.1-Å MM-01) of the Program of Human
Resources Development Action Plan.

[12]

[13]

References
[1]

[14]

R. Ciceri, S. Balzarotti. From Signals to Emotions:
Applying Emotion Models to HM Affective

55

Interactions. In: Affective Computing: Emotion
Modeling, Synthesis and Recognition, ITech,
Education and Publishing, 2008, pp. 271-296.
Available at: http://dx.doi.org/10.5772/6176.
D. W. Clarke. Advances in Model Predictive Control.
Oxford Science Publications, UK, 1994.
Emotiv Epoc specifications. Brain-computer interface
technology. Available at: http://www.emotiv.com/
upload/manual/sdk/EPOCSpecifications.pdf.
S. Fatahi,
N. Ghasem-Aghaee.
An
Effective
Intelligent Educational Model Using Agent with
Personality and Emotional Filters. In: Proceedings of
the World Congress on Engineering, Vol. 1, 2010,
pp. 142-147. Available at: http://www.iaeng.org/
publication/WCE2010/WCE2010_pp142-147.pdf.
J. Hoey, T. Schroeder, A. Alhothali. Bayesian Affect
Control Theory. In: IEEE 2013 Humaine Association
Conference on Affective Computing and Intelligent
Interaction, Geneva, Switzerland, 2013, pp. 166-172.
Available at: http://dx.doi.org/10.1109/ACII.2013.34.
C. Hondrou, G. Caridakis. Affective, Natural
Interaction Using EEG: Sensors, Application and
Future Directions. In: Artificial Intelligence: Theories
and Applications. LNCS Vol. 7297, Springer-Verlag
Berlin Heidelberg, 2012, pp. 331-338. Available at:
http://dx.doi.org/10.1007/978-3-642-30448-4_42.
M. E. Hoque, M. Courgeon, J.-C. Martin, B. Mutlu,
R. W. Picard. MACH: My Automated Conversation
coacH. In: 15th International Conference on
Ubiquitous
Computing
(Ubicomp),
Zurich,
Switzerland, September, 2013, pp. 697-706. Available
at: http://dx.doi.org/10.1145/2493432.2493502.
V. Kaminskas. Dynamic Systems Identification via
Discrete-Time Observations, Part1 â€“ Statistical
Method Foundations. Estimation in Linear Systems,
Vilnius: Mokslas, 1982, (in Russian).
V. Kaminskas. Predictor-based self tuning control
with constraints. Model and Algorithms for Global
Optimization, Optimization and Its Applications,
Vol. 4, Springer, 2007, pp. 333-341. Available at:
http://dx.doi.org/10.1007/978-0-387-36721-7_20.
R. N. Khushaba,
L. Greenacre,
S. Kodagoda,
J. Louviere, S. Burke, G. Dissanayake. Choice
modeling and the brain: A study on the
Electroencephalogram (EEG) of preferences. Expert
Systems with Applications, Vol. 39, Is. 16, 2012,
pp. 12378â€“12388. Available at: http://dx.doi.org/10.
1016/j.eswa.2012.04.084.
R. N. Khushaba,
Ch. Wise,
S. Kodagoda,
J. Louviere, B. E. Kahn, C. Townsend. Consumer
neuroscience: Assessing the brain response to
marketing stimuli using electroencephalogram (EEG)
and eye tracking. Expert Systems with Applications,
Vol. 40,
Is. 9,
pp. 3803-3812.
Available
at
http://dx.doi.org/10.1016/j.eswa.2012.12.095.
Y. Liu, O. Sourina, M. Rizqi Hafiyyandi. EEG-based
Emotion-adaptive Advertising. In: IEEE 2013
Humaine Association Conference on Affective
Computing and Intelligent Interaction, Geneva,
Switzerland, 2013, pp. 843-848. Available at:
http://dx.doi.org/10.1109/ ACII.2013.158.
A. Mehrabian.
Silent
messages:
Implicit
communication of emotions and attitudes. Belmont,
CA: Wadsworth Publishing Company, 1981.
J. Renny Octavia, K. Luyten, J. Vermeulen,
B. Mommen,
K. Coninx.
Exploring
Psycho-

E. VaÅ¡keviÄius, A. VidugirienÄ—, V. Kaminskas

[15]

[16]

[17]

[18]

[19]

physiological Measures for the Design and Behavior of
Socially-Aware Avatars in Ubicomp Environments. In:
CHI 2010, April, Atlanta, GA, USA pp. 1-4.
A. Rizzo, J. G. Buckwalter, B. John, B. Newman,
T. Parsons, P. Kenny, J. Williams. STRIVE: Stress
Resilience In Virtual Environments: A PreDeployment VR System for Training Emotional
Coping Skills and Assessing Chronic and Acute Stress
Responses. In: Medicine Meets Virtual Reality 19, IOS
Press,
2012,
pp. 379-385.
Available
at:
http://dx.doi.org/10.3233/978-1-61499-022-2-379.
J. G. Proakis, D. G. Manolakis. Digital signal
Processing: Principles, Algorithms, and Application,
4th edition, Pearson Prentice Hall, 2007.
A. Sano, R. W. Picard. Stress recognition using
wearable sensors and mobile phones. In: IEEE 2013
Humaine Association Conference on Affective
Computing and Intelligent Interaction, Geneva,
Switzerland, 2013, pp. 671-676. Available at:
http://dx.doi.org/10.1109/ ACII.2013.117.
O. Sourina, Y. Liu. A Fractal-based Algorithm of
Emotion Recognition from EEG using Arousal-valence
model. In: Proc. Biosignals, Rome,2011, pp. 209-214.
Suprijanto, L. Sari, V. Nadhira, IGN. Merthayasa,
I. M. Farida. Development System for Emotion

[20]

[21]

[22]

[23]

Detection Based on Brain Signals and Facial Images.
In: World Academy of Science, Engineering and
Technology 26, 2009, pp. 931-938. Available at:
http://waset.org/ publications/8769.
E. Vaskevicius, A. Vidugiriene, V. Kaminskas.
Investigation of dependencies between virtual 3d face
stimuli and emotion-based human responses. In: Proc.
of the 8th International Conference on Electrical and
Control Technologies, Kaunas, 2013, pp. 64-69.
J. Willis, A. Todorov. First Impressions: Making Up
Your Mind After a 100-Ms Exposure to a Face.
In: Psychological science, Vol. 17, No. 7, 2006,
pp. 592âˆ’598.
Available
at:
http://dx.doi.org/10.1111/j.1467-9280.2006. 01750.x.
Y. Zhang, J. Tang, J. Sun, Y. Chen, J. Rao.
MoodCast: Emotion Prediction via Dynamic
Continuous Factor Graph Model. In: IEEE 10th
International Conference on Data Mining, Sydney,
2010,
pp. 1193-1198.
Available
at:
http://dx.doi.org/10.1109/ICDM.2010.105.
M. Zisook,
J. Hernandez,
M. S. Goodwin,
R. W. Picard. Enabling Visual Exploration of Longterm Physiological Data. In: Proceedings of the 2013
IEEE Conference on Visual Analytics Science and
Technology, Altanta, Georgia, USA, October 2013.

Received December 2013.

56

