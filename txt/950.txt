Brain-controlled NXT Robot: Tele-operating a robot through brain electrical
activity
Athanasios Vourvopoulos

Fotis Liarokapis

Interactive Worlds Applied
Research Group
Coventry University
Coventry, UK
vourvopa@uni.coventry.ac.uk

Interactive Worlds Applied
Research Group
Coventry University
Coventry, UK
F.Liarokapis@coventry.ac.uk

Abstract— This paper focuses on the research of humanrobot interaction through tele-operation with the help of
brain-computer interfaces (BCIs). To accomplish that, a
working system has been created based on off-the-shelf
components. The experimental prototype uses the basic
movement operations and obstacle detection of a Lego
Mindstroms NXT Robot. There are two versions of this
prototype, taking readings from the users' brain electrical
activity in real-time performance. The first version is made
by using a Neurosky Mindset, and is based on the attention
levels of the user as the robot accelerates or decelerates.
The second version is using an Emotiv Epoc headset taking
readings from 14 sensors, being able to control fully the
robot.
Keywords – serious games, brain-computer interfaces,
human-robot interaction, tele-operation.

I.

INTRODUCTION

Robotics is an increasingly developed field with
elements from different scientific principles including
electronics,
mechatronics,
software engineering,
physiology, human-machine interaction and psychology.
On the other hand, Brain-Computer Interface (BCI)
technology is a rapidly growing field of research with
various applications in modern computer games [1],
prosthetics and control systems [2] through to medical
diagnostics. Although research on BCI started during the
1970’s only the last few years we have been able to
introduce brain-computer interfacing to simple users
mostly through serious games with commercial products.
In the past, a number of research groups have
investigated various ways in controlling robotic
platforms mainly electrical wheelchairs or robotic arms
for people suffering from a diverse range of impairments.
A very good example that combines both platforms is the
9-degree-of-freedom (DoF) Wheelchar-Mounted Robotic
Arm System from the University of South Florida,
Tampa [3]. This kind of research had a profound impact
on the person’s independence; social-activity; and self
esteem [4]. This technology launched for commercial use
with headsets that use simplified version of an
electroencephalograph. Examples include the mindcontrolled electric carts using NeuroSky’s technology
with the drivers wearing the MindSet headsets [5]
demonstrated on Courtesy of “The Doctors” show on

CBS [6]. Another relevant project originated from the
BCI research group of The University of Essex and
demonstrated the NXT robot as the main control object
on Channel 4 TV in 2007 [7]. Another example of a nonmedical EEG for controlling a remote device is the
Emotiv Epoc headset moving a robotic arm [8]. Using a
series of 16 electrodes, the EPOC can measure levels of
attention and facial expressions [9]. Emotiv Epoc was
launched in the same period as the Neurosky’s Headset.
The aim of this research is to introduce the field of
human-robot interaction using BCIs to tele-operate a
robotic unit. In particular this research focuses on how a
robotic unit operated through brainwaves can overcome
the kinetic constraints of the user. The prototypes use the
basic movement operations and obstacle detection of a
Lego Mindstroms NXT Robot [10]. The robot is
controlled through a Neurosky Mindset for the first
prototype and Emotiv Epoc for the second taking
readings from the users' brain electrical activity in realtime performance. This activity produced by firing
neurons of the brain is recorded through
electroencephalography (EEG) [11] and it can be used as
the basis for the development of serious applications
ranging from computer games to computer simulations.
II.

SYSTEM ARCHITECTURE

For the development of the experimental prototype,
a Desktop PC running Windows XP was used as the
main development system and a Netbook PC running
Windows 7 for demonstration purposes. The basic
hardware components are a Lego Mindstorms NXT
robot, the Neurosky Headset and the Emotiv Epoc
headset as illustrated in Figure 1.

Figure 1 System Architecture

The Neurosky headset was used to get raw
brainwave signals from the users. The raw EEG has
usually been described in terms of frequency bands:
GAMMA greater than 30 (Hz) BETA (13-30 Hz),
ALPHA (8-12 Hz), THETA (4-8 Hz), and DELTA (less
than 4 Hz) [12]. The headset is calculating through a chip
the Raw EEG to produce the eSense Meters (Attention
and Meditation) and output it to the PC [5]. On the other
hand, the Emotiv EPOC Headset is a neuro-signal
acquisition and processing wireless neuroheadset with 14
saline sensors being able not only to detect brain signals
but also users facial expressions. As for the robot, the
Lego Mindstorms NXT kit has been used being a crossplatform programmable robotics unit released by Lego in
2006. The version that was used includes a 32-bit
AT91SAM7S256 (ARM7TDMI) main microprocessor at
48 MHz (256 KB flash memory, 64 KB RAM). It
includes 4 different types of sensors such as: servo
motors, touch, light, sound and ultrasonic.
Three identical servo motors that have built-in
reduction gear assemblies with internal optical rotary
encoders that sense their rotations within one degree of
accuracy. The touch sensor detects whether it is currently
pressed, has been bumped, or released. This has been
implemented in the prototype to allow for collision
detection. The light sensor detects the light level in one
direction, and also includes an LED for illuminating an
object. The light sensor can sense reflected light values
(using the built-in red LED), or ambient light on a scale
of ‘0’ to ‘100’, ‘100’ being very bright and ‘0’ being
dark. If calibrated, the sensor can also be used as a
distance sensor. The light sensor is used as a height
detector on this model [10].
As far as the software components are concerned,
the programming language that was used is JAVA with
the use of Eclipse IDE. LeJOS was installed to the NXT
robot as the main OS so it can be programmed in JAVA.
LeJOS is a tiny Java Virtual Machine ported to the NXT
brick in 2006. The main advantage of LeJOS NXJ is that
it provides the extensibility and adaptability of JAVA
[13]. Furthermore, the pccomm and bluecove libraries
were used for the Bluetooth communication between the
computer and the robot. Finally, for the computerheadset communication in the Neurosky set the thinkgear
library has been used from the Mindset development
tools and for the Emotiv Epoc headset the Emotiv
Development Kit.
III.

FIRST PROTOTYPE (NEUROSKY SYSTEM)

To initialise the application, the steps that need to be
followed are briefly described here. Initially, the new
firmware (LeJOS) must to be installed to the NXT robot
with the drivers as well as the MindSet Development
Tools. This will allow to send raw data through sockets
from the PC to the robot and vice versa in order to
establish a two way communication. As a result,
‘attention’ and ‘meditation’ readings from the mindset
can be recorded in real-time performance. There is no
need for user training as it is very simple for the user to
control. After the connection is established between the

main program and the robot, the values from the headset
will be parsed to the robot through the dedicated
computer. The user interacts directly with the robot and
there is no need calibrating or using the computer, only
to be concentrated to the robot. Both the raw brainwaves
and the eSense Meters (Attention and Meditation) are
calculated on the ThinkGear chip. The calculated values
are output by the ThinkGear chip, through the headset, to
the dedicated PC [5].
The software application that was created establishes
a bluetooth connection with the robot and the mindset.
Readings from the mindset are processed continuously
passing instructions to the robot. If the user’s ‘attention’
level reaches a threshold level, the program instructs the
robot to move forwards (Figure 2).

Figure 2 Operation of the system on a navigation test surface

On the other hand, when the users attention drops
from that threshold level, the program will instruct the
robot again to stop. Also based on the attention level, the
robot will accelerate or descelerate. If the robot hits an
obstacle with the bumper sensor the robot will send a
notification to the PC and both programs (on the
computer and the robot) will be terminated. An overview
of this is illustrated in Figure 3.
Moreover, to monitor the attention levels real time,
the LiveGraph real-time plotter application and API were
used by reading a CSV file that is exported by the main
application (class) in which the values were recorded
from the mindset. The advantage of using LiveGraph is
that it is an open-source tool that can automatically
update graphs of real-time data while it is still being
computed by the application. It also allows the
comparison of data series even in applications that output
over 1000 series simultaneously [14].

Figure 4 Attention levels when the user is calm, in normal state and
concentrated to the object

Next, the second set of recordings measured the
same states (calm, normal and concentrated) but this
time in terms of the meditation levels. Results are
illustrated in Figure 5.

Figure 3 Operation of the system illustrating how the robot is
controlled through the BCI

To decide on the threshold level (which level we
must set the limit for the robot to move) readings were
recorded from the mindset on different stages of
concentration. For this system there is no need for any
kind of user training and the threshold level is the same
for all potential users based on the sample that was
gathered from 5 users.
State
Calm
Normal
Concentrated

Max
44
59
67

Min
20
34
43

Median
27
44
57

Mean
30
45.7
56.6

Table 1 Descriptive statistics of attention level taken from 5 users

Table 1 illustrates the gathered statistic information
we collected from 5 users of different backgrounds to
decide the threshold level of the system in different
stages of concentration. The range of age is between 1932, three males and 2 females with only one of them to
have familiarity with robotics.
The first set of recordings examined the attention
level for three different states: calm, normal and
concentrated. The results were plotted in a graph ranging
between 0-100% and are illustrated in Figure 4.

Figure 5 Meditation levels when the user is calm, in normal state and
concentrated to the object

Based on those measurements it was decided the
values that were used as a threshold value for the
movement of the robot. Finally, one of the main
characteristics of the system is its use of operation as
well as the adaptability. This adaptability refers to the
need of the device to be trained for new users. In this
case it can be worn by different users producing different
outputs [1].
IV.

SECOND PROTOTYPE (THE EMOTIV SYSTEM)

A more advanced version is under development with
the robot performing basic maneuvering (moving
forwards, backwards, turn left and turn right) with the
help of Emotiv Epoc headset. The system is currently
operational [15] and user testing will be the next step of
the project. The Emotiv EPOC Headset is a neuro-signal
acquisition and processing wireless neuroheadset with 14
saline sensors being able not only to detect brain signals
but also user’s facial expressions [9].

REFERENCES
[1]

[2]
[3]

Figure 6 The Emotiv Epoc system

Figure 6 illustrates the Emotiv Epoc system in
operation. With this it is possible to obtain and analyse
more accurate user data to produce a more sufficient and
robust software system to control the robot. For this
research the Emotiv Development Kit was used
connecting the headset to the Emotiv control panel to
create and train a new user profile. By using the Emotiv
Emokey tool (that emulates keyboard keystrokes) it is
possible to parse inputs into the robot through a Java
application. Finally, a basic graphics user interface (GUI)
is made for the user to interact sufficiently with the
system.
V.

[4]

[5]
[6]
[7]
[8]

CONCLUSIONS AND FUTURE WORK

This project presented an affordable human-robot
interaction system based on tele-operation with the help
of brain-computer interfaces. The experimental prototype
allows a robot to be controlled successfully through
users' brain electrical activity in real-time performance,
allowing for the generation of serious applications. The
aim of this research focuses in the ways that a robotic
platform is controlled for future applications and studies
the way the human brain works (in terms of attention and
medidation levels). This kind of research can transform
the way we live today enhancing human potential.
Future work will include the combination of
different readings with the use of more sophisticated
device equipped with more electrodes. Moreover, a large
sample of users will be tested to qualitatively measure
the effectiveness of the system. Furthermore, a third
prototype will be developed with the robot performing
more functionality with the help of custom BCI devices
[16]. Finally, other physiological data will be extracted
including body posture and facial expressions so that it
will be possible to determine user’s visual attention.
ACKNOWLEDGEMENTS
The authors would like to thank the Interactive
Worlds Applied Research Group (iWARG) members for
their support and inspiration. Two videos that illustrate
the operation of the system can be found at:
http://www.youtube.com/watch?v=zxRxHVQ9Wds and
http://www.youtube.com/watch?v=D9-2xQbki14.

[9]
[10]
[11]

[12]
[13]
[14]
[15]
[16]

Rebolledo-Mendez, G., Dunwell, I., et al. “Assessing the
Usability of a Brain-Computer Interface (BCI) that
Detects Attention Levels in an Assessment Exercise”,
Proc. of the 13th Int’l Conference on Human-Computer
Interaction, Springer Berlin/Heidelberg Lecture Notes In
Computer Science, Volume 5610/2009, San Diego,
California, USA, 19-24 July, 149-158, (2009).
Loudin, J.D., et al. “Optoelectronic retinal prosthesis:
system design and performance”, Journal of Neural
Engineering, 4, 72-84, (2007).
Palankar, M. De Laurentis, K. Dubey, R. “Using
biological approaches for the control of a 9-DoF
wheelchair-mounted robotic arm system: Initial
experiments”, Proc. of the IEEE International
Conference on Robotics and Biomimetics, Bangkok,
1704, (2009).
Vanacker, G., del R. Millán, J., et al. “Context-based
filtering for assisted brain-actuated wheelchair driving”,
Computational Intelligence and Neuroscience EEG/MEG Signal Processing, Hindawi Publishing Corp.,
(2007).
Neurosky, Available at: [http://www.neurosky.com/],
Accessed at: 10/11/2010.
Karlin, S. “Mind Over Matter”, IEEE Spectrum, May,
(2007).
BCI
demo
on
Channel
4,
Available
at:
[http://csee.essex.ac.uk/Research/BCIs/ch4TheFarm.wm
v], Accessed at: 05/11/2010.
Ranky, G.N. Adamovich, S. “Analysis of a commercial
EEG device for the control of a robot arm”, Proc. of the
IEEE 36th Annual Northeast Bioengineering Conference,
New York, USA, 1-2, (2010).
Emotiv EPOC Software Development Kit, Available at:
[http://www.emotiv.com/store/hardware/299/], Accessed
at: 19/11/2010.
Lego
Mindstorm
NXT,
Available
at:
[http://www.legomindstormsnxt.co.uk/lego-nxt.html],
Accessed at: 10/11/2010.
Crespel,
A.,
Gelisse,
P.
“Atlas
of
Electroencephalography: EEG Awake and Sleep EEG
Activation Procedures and Artifacts”, Volume 1, John
Libbey Eurotext, (2005).
The Science of Brainwaves - The language of the brain,
Available at: [http://www.nhahealth.com/science.htm],
Accessed at: 10/11/2010.
LeJOS, Java for Lego Mindstorms, Available at:
[http://lejos.sourceforge.net/], Accessed at: 10/11/2010.
LiveGraph: The real-time data graph plotter, Available
at:
[http://www.live-graph.org/],
Accessed
at:
10/11/2010.
Brain Controlled NXT Robot, Available at:
[http://bci.vourvopoulos.com/], Accessed at: 10/12/2010.
Guger Technologies, Available at: [http://www.gtec.at/],
Accessed at: 12/12/2010.

