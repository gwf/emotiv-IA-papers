Pre-print: Krishnaswamy, K. & Kuber, R., 2012. Toward the Development of a BCI and Gestural Interface to Support Individuals with Physical
Disabilities. In proceedings of the 14th International ACM Conference on Computers and Accessibility – ASSETS’12, Boulder, USA, 229-230.

Toward the Development of a BCI and Gestural Interface
to Support Individuals with Physical Disabilities
Kavita Krishnaswamy and Ravi Kuber
UMBC

{ kavi1, rkuber } @umbc.edu
ABSTRACT

2. RELATED WORK

In this paper, we describe a first step towards the development of
a solution to support the movement and repositioning of an
individual’s limbs. Limb repositioning is particularly valuable for
individuals with physical disabilities who are either bed or chairbound, to help reduce the occurrence of contractures and pressure
ulcers. A data gathering study has been performed examining
attitudes towards using BCI and gestural devices to control a
robotic aid to assist with the repositioning process. Findings from
a preliminary study evaluating a controller interface prototype
suggest that while BCI and gestural technologies may play a
valuable role in limiting fatigue from interacting with a mouse or
other input device, challenges are faced accurately identifying
specific facial expressions (e.g. blinks). Future work would aim
to refine algorithms to detect gestures, with a view to augmenting
the experience when using a BCI and gestural device to control a
robotic aid.

Assistive robotic interfaces have been developed to aid
individuals with physical disabilities to perform tasks which
others take for granted. Examples include Robovie R3, a robot
designed to behave as a guide in a grocery store, carry shopping
items, and locate nearby products [3]. In contrast with other
solutions, it is able to assist the user to express emotions or
sentiments. For example, the robot is able to grasp the hand and
hug others. In terms of BCI-controlled interfaces, the FRIEND
robot provides individuals with severe physical disabilities with
90 minutes of temporary independence from caregivers [1]. EEG
signals can be used to generate the task sequences necessary to
perform robotic operations. To gain a deeper understanding of
the ways in which BCI and gestural technologies would benefit
physically-disabled users, a data gathering study was performed.

Categories and Subject Descriptors
H.5.2 User Interfaces – Input devices and strategies

General Terms
Human Factors.

Keywords
BCI, Gestural interfaces, Physical disabilities.

1. INTRODUCTION
Robotic interfaces offer considerable promise to individuals with
physical disabilities. Examples include solutions designed to
support the retrieval of objects [2] and to assist with household
chores [5]. However, research has yet to extensively focus upon
the ways in which physically-disabled users can independently
control robotic support aids, and the ways in which interaction
potential can be maximized during this process. Brain Computer
Interfaces (BCI) have been developed to support individuals with
neuromuscular disorders, when other methods of interaction (e.g.
eye gaze, speech input) may not be feasible for use. In this paper,
we describe an exploratory study examining attitudes to the use of
non-invasive BCI and gestural technologies to support tasks such
as repositioning limbs. A prototype of a controller interface has
been evaluated. The long term goal of the research would be to
interface a robotic aid with the controller, to assist the process of
repositioning limbs, thereby reducing the likelihood of developing
stiffness in the muscles and pressure ulcers.

3. DATA GATHERING STUDY
Due to the difficulties identifying individuals with severe physical
disabilities, three participants were recruited for the study.
Participants were asked to describe the ways in which they
currently interact with computing technologies, and their attitudes
towards using BCI and gestural interfaces to support limb
repositioning. Their case studies are presented below.
Case Study #1: Alex is a 43 year old computer teacher with
Limb-Girdle Muscular Dystrophy and Polio. Alex needs help with
typing to reduce fatigue because he can only use the keyboard for
half-an-hour per day. He has never tried using BCI and gestural
technologies, but envisions these technologies could assist him
when navigating through a graphical interface or when entering
data. As he has the ability to make facial gestures and head
movements, he is keen to use BCI and gestural technologies to aid
him when interacting with the Web. He suggests that mapping
mouse movements to facial gestures (e.g. by turning the head
slightly either left or right to move the cursor in the respective
direction) would reduce the physical strain on him. Alex limits his
sitting time to 2 to 3 hours because he is unable to find a caregiver
to help him to reposition himself, or to assist him with his
physical therapy.
He would be interested in using BCI
technologies to gain assistance on demand for repositioning his
arms to stretch above the head, and aiding him to perform leg
stretches, when a caregiver is not available.
Case Study #2: Betty is 28 years old and has been diagnosed with
Spinal Muscular Atrophy. She has full control of facial muscles,
but has difﬁculty turning her head to the left side. She often sits in
a reclined position to maximize neck control and reduce the
occurrence of pressure ulcers. As her range of movement is
limited, she requires assistance for personal care (e.g. bathing,
feeding). She is able to interact with a laptop and touchpad
mouse, using two fingers from each hand. While Betty is excited

Pre-print: Krishnaswamy, K. & Kuber, R., 2012. Toward the Development of a BCI and Gestural Interface to Support Individuals with Physical
Disabilities. In proceedings of the 14th International ACM Conference on Computers and Accessibility – ASSETS’12, Boulder, USA, 229-230.

that BCI may provide an alternative to the slow process of typing
and traversing hierarchical menus, she is eager to use this
technology to gain more independence, by controlling a robot to
assist with limb repositioning and personal care. She hopes that
the same technologies can be used to non-invasively monitor her
physiological condition and alert caregivers when a problem is
detected. BCI offers her more potential compared to using an eyegaze system, as she finds it difficult to visually focus for long
periods of time. Her poor level of neck control can make the
process of keeping her head upright challenging. Breathing issues
have caused her difficulty to use speech-input solutions.
However, a solution able to monitor the user’s EEG activity or
facial/lip movements when silently mouthing words is thought to
offer considerable potential to improving her communication with
caregivers and medical professionals.
Case Study #3: Tim is a 31 year old entrepreneur with Duchenne
Muscular Dystrophy. He has full control of facial movements but
finds tilting his head difﬁcult. He uses voice-recognition to access
his computer. He can move his hands once they are situated in
speciﬁc locations, but often experiences muscle stiffness. He
relies on caregivers to help reposition his limbs, but can only do
this during the day, when they are around to assist him. He limits
trips to the bathroom because of difﬁculty with obtaining
assistance transferring himself to the patient hoist lift. Tim has not
used BCI and gestural technologies before, but hopes these will
assist him to control a robotic aid to perform personal care tasks.
He suggested that performing a gesture, such as looking up or
down, could convey to a robotic aid to help him transfer to the
lift, while looking left or right would execute the necessary series
of actions to assist him when using the bathroom. Actions could
be confirmed by performing a nodding gesture.
Findings from the interviews have revealed that BCI and gestural
interfaces could offer considerable potential to support individuals
with physical disabilities to achieve greater levels of
independence. This would provide an alternative when caregivers
are unavailable or when respite is needed.

1 female, aged 24-58) were selected to execute ten randomlypresented arm tasks using the keyboard and performing facial
gestures with the headset, to determine the feasibility of the
solution.
Preliminary results have suggested that while all participants were
able to interact with the avatar using both methods of input,
participants spent 90.3s (SD: 123.1s) longer performing tasks
using BCI and gestural technologies compared with the keyboard
condition. Certain facial gestures were detected within a shorter
time period compared with others. Examples including looking to
the left or right to raise the avatar’s arms (Left Arm: Keystroke:
1.8s (SD: 1.0s), BCI: 9.3s (SD: 6.2s); Right Arm: Keystroke: 3.8s
(SD: 1.5s), BCI: 33.5s (SD: 24.8s)). The greatest difficulties were
experienced when detecting blinks to lower the arms (M: 136.4s,
SD: 188.3s). Findings contrasted with those of Lievesley et al.
[4], whose participants found the blink action easiest to perform
during their training process. The researchers also suggest the
headset itself can be cumbersome to use, and is not recommended
for individuals who retain a small amount of head movement. Our
future work would examine ways in which these algorithms could
be refined to ensure that gestures are more accurately detected.

Figure 1: Emotiv Epoc
Headset

Figure 2: Robot Controller
interface accessed using BCI

We also aim to develop the system to cater to the needs of
individuals who are not able to make fully expressive facial
gestures (e.g. small raises of the brow which may not be visually
detectable), prior to more extensive evaluations with target users.
Further study is also needed to identify ways to maintain levels of
comfort, particularly for longer periods of device usage.

4. PROTOTYPE DESIGN & EVALUATION
A controller interface prototype was developed for limb
repositioning using the Emotiv Epoc headset1 (Figure 1). The
device is able to detect emotional information, facial expressions
and conscious thoughts. The user is able to control the on-screen
avatar’s limbs (Figure 2) by issuing a set of specified facial
gestures that are detected using EEG sensors embedded within the
device (Expressiv suite). A future robotic aid will be able to
reposition the appropriate limb based on these commands.
Examples of gestures created include looking left or right to raise
the left or right arm, or blinking to lower either arm. The solution
is customizable, enabling users with limited physical capabilities,
to perform the gestures available to them. Commands are also
accessible through keystrokes, for users with some level of
movement in one or both arms.
An exploratory study was performed to determine the feasibility
of using BCI technologies to control the on-screen avatar’s limbs.
We aimed to determine the effectiveness of detecting the
following facial gestures: smiling, winking, blinking, looking left
and looking right. Three participants without disabilities (2 male,

1

Emotiv Epoc - http://www.emotiv.com

5. REFERENCES
[1] Grigorescu, S.M., Lüth, T., Fragkopoulos, C., Cyriacks, M.
and Gräser, A., 2012. A BCI-controlled robotic assistant for
quadriplegic people in domestic and professional life.
Robotica, 30 (03), 419–431.
[2] Jain, A. and Kemp, CC., 2010. EL-E: An assistive mobile
manipulator that autonomously fetches objects from flat
surfaces. Autonomous Robots, 28, 45–64.
[3] Hornyak, T., 2012. Robovie R3 robot wants to hold your
hand.
http://news.cnet.com/8301-17938_105-200029811.html
[4] Lievesley, R., Wozencroft, M. and Ewins, D., 2011. The
Emotiv Epoc neuroheadset: An inexpensive method of
controlling assistive technologies using facial expressions
and thoughts? Journal of Assistive Technologies, 5 (2), 67–
82.
[5] Liu, K., Sakamato, D., Inami, M. and Igarashi, T., 2011.
Roboshop: multi-layered sketching interface for robot
housework assignment and management. In Proceedings of
CHI ’11, 647–656.

