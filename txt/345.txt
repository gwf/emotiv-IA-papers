ConVRgence

3 Days Scientific Conference

2019

VRIC 2019

VIRTUAL REALITY
INTERNATIONAL CONFERENCE
PROCEEDINGS

Editor: Simon RICHIR
Publisher: Laval Virtual
2019, April 20-22
www.laval-virtual.com
ISBN
978-2-9566251-2-4

EAN
9782956625124

VIRTUAL REALITY INTERNATIONAL CONFERENCE
VRIC 2019 PROCEEDINGS
TABLE OF CONTENTS
Paper

3
5

Authors

Title

Sanaa Sharaf, Ebtehal Alsaggaf, Salwa
Almalki, Amany Alattas, Amna Omar and
Mawadh Sait
Juan Sebastian Munoz-Arango, Dirk
Reiners and Carolina Cruz-Neira

6

Alcinia Zita Sampaio

12

Rémy Sohier

15
16
17
18
20
21
24
26
>>>

8
11
14
22

Environmental adaptation for Autistic children
using Virtual reality
Maximizing lenticular lens performance for Multi
User VR displays
Using VR technology to improve BIM: maintenance
and construction
New collaborative game experiences, the example
of "Game Jockey"

Martin Viktor, Anders Hansen, Kirstine
Effects of Physical Props on Torso Movement in a
Bundgaard Larsen, Helene Høgh Nielsen
Virtual Reality Shooter
and Martin Kraus
MITMI Man In The Middle Interaction. The human
Cedric Plessiet and Jean François Jego
back in the loop
Ifigeneia Mavridou, Maya Perry, Ellen
Emerging Affect Detection Methodologies in VR
Seiss, Theodoros Kostoulas and Emili
and future directions
Balaguer-Ballester
Windtherm: A Wearable VR Device That Provides
Masatoshi Suzuki and Akihiro Matsuura
Temperature-Controlled Wind
VR Conferencing: communicating and
Simon Gunkel, Hans Stokking, Rick
collaborating in photo-realistic social immersive
Hindriks and Tom de Koninck
environments
Virtuality of Virtual Reality: Indiscernibility or
Suzanne Beer
Ontological Model?
François Garnier, Fabienne TsaÏ, Dionysis TAMED CLOUD: sensible interaction with a swarm
Zamplaras, Florent Levillain, David Bihanic of data.
Exploratory research on the gamification of
Katherine Hoolahan
exercise for Fibromyalgia using virtual reality
PAPERS to be Published on IJVR.org

Page

3
7
17
21
23
30
35
39
45
47
51
59

VRIC Laval Virtual 2019 Special Issue

Julia Santl, Youssef Shiban, Andreas Plab,
Gender Differences in Stress Responses during a
Stefan Wüst, Brigitte M. Kudielka and
Virtual Reality Trier Social Stress Test
Andreas Mühlberger
3D Objects based Security of Virtual and
Haorun Dja ghloul and Jean-Pierre Jessel
Augmented Reality Systems
Abdul Rahman Abdel Razek, Marc Pallot, Comparing Conventional versus Immersive
Christian van Husen and Simon Richir
Service Prototypes: An Empirical Study
Violette Abergel, Kévin Jacquot, Livio De Towards a SLAM-based augmented reality
Luca and Philippe Veron
application for the 3D annotation of rock art

>>> The articles included in these proceedings should be cited as follows:
Official reference of the proceedings:
Richir, S. (2019) Eds. ConVRgence (VRIC) Virtual Reality International Conference Proceedings. 20-22 March, Laval, France.
International Journal of Virtual Reality. https://doi.org/10.20870/IJVR.2019.0.2920.
Official reference of a paper included in that proceedings:
K. Hoolahan, 2019, Gamification of exercise for Fibromyalgia using virtual reality, in Proceedings oConVRgence (VRIC) Virtual Reality
International Conference Proceedings, S. Richir (ed), 20-22 March, Laval, France. International Journal of Virtual Reality.
https://doi.org/10.20870/IJVR.2019.0.2920.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 2

Environmental adaptation for Autistic children using
Virtual reality
Amany A. Alattas

Mawadh Sait

Amnah S. Omar

Information Technology Department, Information Technology Department,
Information Technology Department, Faculty Of Computing And Information Faculty Of Computing And Information
Faculty of Computing And Information
Technology
Technology
Technology
University of Jeddah
University of Jeddah
University of Jeddah
Jeedah, Saudi Arabia
Jeedah, Saudi Arabia
Jeedah, Saudi Arabia
mawadh.a.sait@gmail.com
aomar0035@stu.kau.edu.sa
Amanyalattass@gmail.com

Salwa Z. Almalki
Information Technology Department,
Faculty Of Computing And Information
Technology
University of Jeddah
Jeedah, Saudi Arabia
salmalki0355@stu.kau.edu.sa

Sanaa A. Sharaf
Technology
King Abdulaziz University
Jeedah, Saudi Arabia
ssharaf@kau.edu.sa

ABSTRACT

This work in progress designs scenarios in a virtual environment
for autism spectrum disorder (VE4ASD) in order to reduce the
time and costs associated with taking autistic children to real
environments as each needs to be accompanied by a mentor. It
presents the user's interaction with the VR system and shows the
relationship among the child, the other user (specialist or parent)
and VR content. The VR prototype of VE4ASD is implemented
as a proof of concept using MiddleVR for Unity3D. The VR
database will be supported by SQL and Python. It will provide an
effective educational tool for autistic individuals, helping them to
adapt to new environments. Most of the previous studies have
used the English language but this proposal will use the Arabic
language, which will provide a beneficial contribution to the field
of Arabic language research. We expect this study will reduce
time, costs and human errors. Making the tool available to more
people who want to assist children with autism and their families
will lead to the development of early intervention strategies.

Keywords

Autism spectrum disorders; virtual reality; VR technology; social
adaptation

1. INTRODUCTION

Ebtehal A. Alsaggaf

Computer Science Department, Faculty
Computer Science Department,
Of Computing And Information
Faculty Of Computing And Information

In Saudi Arabia, the integration of children with special
needs with typically developing students is one of the Ministry of
Education’s objectives. It will help them to attend mainstream
schools. Children with autism spectrum disorders (ASD) have
difficulty adapting to new and unfamiliar environments, which
may cause them anxiety and stress. Technology provides solutions
for many problems in our daily lives and makes life easier, but
there is a limitation on how technology can assist autistic children
and help them in the problems they face every day in their lives.
The purpose of this study is to design a school environment that
helps children with ASD to prepare for attending an integrated
school with students with autism and typically developing
students.

Technology
University of Jeddah
Jeedah, Saudi Arabia
eaalsaggaf@kau.edu.sa

To cope with daily life, autistic individuals and their
educators need help in developing early intervention strategies.
Therefore, some studies have investigated the use of virtual reality
(VR) in autism. They found that a simulation of the real world
based on computer graphics, can be useful as an educational tool
during learning[1]. A key reason for using VR is that some
scenarios are difficult to create with physical objects. It is costly
in time to get the autistic children to real environments because
each needs a mentor. "VR makes it possible at a low cost" [2]. For
this reason, VR technology will undoubtedly be an effective
educational tool for individuals with ASD in helping them to
adapt to new environments.
Using VR in providing several selected scenarios will help
them to adapt to new environments and feel less anxiety.
Although there is no known treatment, the role of early
intervention programmes is to modify autistic traits, teaching
appropriate behaviour and integration within the community[3]. It
also can help them gain self-care, social and communication
skills[4].
This study aims to help children with ASD to adapt to new and
unfamiliar environments by providing realistic scenarios to
prepare the children for the real environment, providing a safe
space to help them to explore the surroundings without feeling
afraid. Having early training for children with ASD will improve
their skills and get them used to real environments. Consequently,
VE4ASD is an educational tool especially for autistic children. It
will provide new environments by using VR to help the children
to cope, as many autistic children have difficulty adapting to new
situations.

2. LITERATURE REVIEW

Researchers agree that VR technology is an effective tool for
education. After conducting many types of research that proved
that VR is a technology that improves education and gives better
results than other methods, it has been found to be better in
education than traditional teaching tools especially for children [5,
6].

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 3

One study was applied to children with autism in the
framework of Virtual Dolphin-Assisted Intervention (VDAI)
including four core educational directions such as development of
cognitive
skills,
personal
relevance
and
societal
adaptation/reconstruction. The authors discussed the need to reach
an effective balance that is to be derived from practical use with
the spectrum of diverse needs evident in autistic individuals. They
concluded the use of VR technology is still in its infancy phase
[3].
A study of VR-SCT was focused on improving social
aspects, such as social skills and social cognition. The scenarios
were designed as meeting new people, handling a roommate
conflict, negotiating financial or social decisions and interviewing
for a job. Inside the system – VR – the coach guided the
participant to interact in each social situation. At the end of each
scenario, the coach asked the participant questions about the scene
and gave instructions. Then the next scene started, and the
participant could benefit from the feedback. All ten of the VRSCT training sessions were completed by all participants.
Improvements were noted in social cognitive measurements of
theory of emotion and mind recognition[7].
Another study focused on cognitive behavioural therapy
(CBT) to reduce anxiety. The immersive technology, 'Blue Room'
was used, which means participants did not need to wear a headset
because they can see 360° around spanning walls and ceiling and
can move about in the room. The psychology assistant helped in
scenario steps with 4 session treatments of 20–30 minutes for each
participant. Beginning with a relaxing scene, the scenario then
commenced (e.g. taking the bus, shopping) ranging from low level
to high level, depending on the participant’s comfort level. Using
this immersive virtual environment (VE), the study concluded that
8 out of 9 participants showed improvement and positive
outcomes and 7 out of 9 reported increased confidence[8].
In a further case study, participants were concerned with
getting a job. The case comprised two phases: the first phase
invited the participant to try three scenarios using VR and then the
second phase presented two longer and more intense scenarios. It
included: Apollo 11 mission and Tuscan house. All participants in
phase one agreed to wear the VR head-mounted display (HMD);
86% of them finished the three scenarios. After phase one, 79%
agreed to return for phase two and 11 participants were selected.
All the selected participants completed the two scenarios in phase
two. They used impressive VR and equipment (PC, HMD, input
device and headphones)[9].
Based on the previous research using VR with autistic
people, VR technology has been shown to be an effective
educational tool for individuals with ASD when helping them to
adapt to new environments. Most of the previous studies used the
English language. Our project will use the Arabic language which
will provide a beneficial contribution to the field of Arabic
language research. The purpose of our study is to design a school
environment that helps children with ASD to prepare for an
integrated school setting with both typically developing students
with those with autism. It is one of the Ministry of Education’s
objectives to help children with special needs to attend integrated
schools.

3. MATERIAL AND METHODS

Figure 1. The illustration of the proposed VR system (activity
diagram)

3.1 Hardware and Software Tools

To facilitate the software implemented process, MiddleVR for
Unity3D[10], visual studio for coding by C#[10] and Blender for
designing the models[11] are used. A major reason for this is to
create a powerful and interactive fast VR application using a
head-mounted display (HMD) system.
In addition, mobile HMDs can provide a portable and inexpensive
experience, compared to the HMDs connected to a powerful
computer. The Oculus Go device is portable, light-weight and
easy-to-use. It is applicable for reliable assessment for our
proposed study[12]. For this, all modules will be implemented
using Unity 3D, Blender and PHP MyAdmin. Then they will be
developed and tested in Unity3D 5.1 with C# as the programming
language. The virtual reality database will be supported by the
programming languages SQL and Python, the environment setting
required to support our system by VR glasses.

3.2 Example Scenario

A VR environment with high accuracy is required. The VR
scenarios should comprise real school situations faced by school
children in their life. While giving the child appropriate freedom
to interact in the scenario, the specialist or parent should still be
able to lead and facilitate the education proceeding and intervene
when necessary. Considering these basics principles, a HMD VR
system will be chosen to facilitate the designated programme.

3.3 Virtual Environment for Autism
Spectrum Disorder (VE4ASD)

The current VR system offerings will be described in this
section. The details regarding hardware environment, software
environment and VR content design and development will be
presented. Figure 1 illustrates our elementary concepts on the
current VR system. It presents the user's interaction with the
system and shows the relationship among the child, the other user
(specialist or parent) and the VR system.
 The specialist or parent should create an account for the
child successfully.
 A parent or specialist logs in to the VR system by
username and password to access their child’s account.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 4

 To start a new scenario, the child wears the HMD glasses
as appropriate and presses the start button to view the
scenario.
 To watch the scenario with 360°, the scenario will be
displayed at 360° so that the child can move their head to
see the virtual environment around them.
 Take a test. When the scenario display is finished, a
simple test is displayed.
 Get the score. After completing the test, the child is
given a score based on the correct answers.
 Choose a reward. The child chooses a reward from a list
of rewards.
 The child, parents or specialist can press the exit button
at any time.

Figure 4. Register interface (Unity3D)

The proposed VR system can analyse the results of the score and
provide some recommendations to add a new environment and to
the upgrade the child’s level (see Figure 2).

Figure 3. School from outside (Unity3D)

Figure 1. The illustration of the proposed VR system (use case
diagram)

3.4 Entity Relationship Diagram

The entity relationship diagram is a technique for data models that
graphically describes the entities of the information system and
the relationships between those entities.
.

Figure 6. Classroom (Unity3D)

4. DISCUSSION

Figure 2. Entity relationship diagram

3.5 Prototype Unity3D

The VR prototype of VE4ASD is implemented as a proof of
concept using Unity3D, C# and Blender. Of course, this is a
simple design, which is obviously expected to improve as the
research develops.
Some examples of school environment are in the following
figures:

The proposed VR system will provide a new educational tool
especially for autistic children that will facilitate improved social
skills in individuals using VR capabilities. A prototype is
illustrative of an original product which shows simple
representation of the system and essential functions and how it
will become after completing the implementation. The prototype
is designed to gather feedback from users while the developers
can modify the planning and design. In addition, the prototype
helps to increase accuracy and remove ambiguity to make the
product clear and easily comprehensible for users.
Most of the previous studies have used the English language
but this proposal will use the Arabic language, which will be a
beneficial contribution to the field of Arabic language research.
Importantly, the current study will reduce time, costs and the
number of human errors and it will develop early intervention
strategies by making it available to more people who support the
autistic children and their families.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 5

4.1 Features

This paper addresses the use of VR for autistic children,
which is an interesting and potentially effective application. It
makes a pertinent contribution in the area of educational tools for
people with ASD. The description of the VE4ASD system is too
technical to include here. When describing the ongoing work, the
following is expected:


To study the performance of typically developing
children in a VE that was designed to support the
adaptation of autistic individuals in real school
environments.



A comparison can be made to distinguish actual
contributions from work for identifying the difference
between the proposed VE and the VEs for typically
developing children.



It would be relevant to distinguish the results of the use
of VR as an educational tool for autistic children. In
addition, it would be of interest to explore in what way
multidisciplinary teams were used for development.



Analysing the results and providing recommendations.

Consequently, VE4ASD is an educational tool especially for
autistic children. It will provide new environments by using VR to
help the children to adapt, because many autistic children find
new environments stressful. Analysing the results and providing
recommendations will improve educational progress, particularly
the development of cognitive skills and societal adaptation.

5. CONCLUSION AND FUTURE WORK

In this paper, a virtual environment for autism spectrum
disorder (VE4ASD) is proposed. A notable point of this study is
the use of the Arabic language, which makes a beneficial
contribution to the field of Arabic language research. The
prototype illustrates an original product that shows a simple
representation of the system and the essential functions and how it
will develop upon completion. It aims to accomplish the
following: to reduce the time of the traditional educational process
where the speed of reaching a goal is very important; to reduce the
number of human errors; and to make this educational tool
available to all people.
At present, only the integrated school scenario is implemented in
the study. The prototype is designed to modify the planning and
design. In future, VE4ASD will add new scenarios. Thus, it will
be an effective educational tool for autistic individuals in helping
them to adapt to new environments. In addition, it will help to
increase accuracy and remove ambiguity to make the product
clear and comprehensible for users.

6. REFERENCES

[1] Bellani, M., et al., Virtual reality in autism: state of the art.
Epidemiology and psychiatric sciences, 2011. 20(3): p. 235238.
[2] Wenting, Z. VR provides training for autistic kids. 2018;
Available from: https://www.thedailystar.net/health/virtualreality-technology-vr-provides-training-autistic-childrenchina-shanghai-interactive-games-1563691.
[3] Stahmer, A.C., N.M. Collings, and L.A. Palinkas, Early
intervention practices for children with autism: Descriptions
from community providers. Focus on Autism and Other
Developmental Disabilities, 2005. 20(2): p. 66-79.
[4] Scott, M.M. and P. Chris, Management of children with
autism spectrum disorders. Pediatrics, 2007. 120(5): p. 11621182.
[5] Greenwald, S., et al., Technology and applications for
collaborative learning in virtual reality. 2017.
[6] Fernández-Herrero, J., G. Lorenzo-Lledó, and A.L. Carreres,
A Bibliometric Study on the Use of Virtual Reality (VR) as
an Educational Tool for High-Functioning Autism Spectrum
Disorder (ASD) Children, in Contemporary Perspective on
Child Psychology and Education. 2018, InTech.
[7] Kandalaft, M.R., et al., Virtual reality social cognition
training for young adults with high-functioning autism.
Journal of autism and developmental disorders, 2013. 43(1):
p. 34-44.
[8] Maskey, M., et al., Reducing specific phobia/fear in young
people with autism spectrum disorders (ASDs) through a
virtual reality environment intervention. PloS one, 2014.
9(7): p. e100374.
[9] Newbutt, N., et al., The acceptance, challenges, and future
applications of wearable technology and virtual reality to
support people with autism spectrum disorders, in Recent
Advances in Technologies for Inclusive Well-Being. 2017,
Springer. p. 221-241.
[10] Bond, J.G., Introduction to Game Design, Prototyping, and
Development: From Concept to Playable Game with Unity
and C. 2014: Addison-Wesley Professional.
[11] Dovramadjiev, T., Modern accessible application of the
system blender in 3d design practice. International scientific
on-line journal" SCIENCE & TECHNOLOGIES".
Publishing House" Union of Scientists-Stara Zagora", ISSN,
2015: p. 1314-4111.
[12] Pope, H., Virtual and Augmented Reality Applications.
Library Technology Reports, 2018. 54(6): p. 12-17.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 6

Maximizing lenticular lens performance for Multi User VR displays
Juan Sebastian Munoz-Arango 1 , Dirk Reiners 1 , Carolina Cruz-Neria 1
1

Emerging Analytics Center - UA Little Rock, AR.

Abstract - One of the ongoing issues of Virtual Reality
systems regardless if they are head-mounted or projection based is that they can only provide a correct perspective to one user. This limitation reduces the usability
of such systems for collaborative work which is nowadays the standard in any industry. One of the approaches
for multiplexing images to different users from a single
screen is through optical routing; this approach relies on
bending the light from pixels to generate perspective correct images to different users. Lenticular lenses do exactly that; bend the light so different persons can see different images from different positions. On this paper we
devise a lenticular lens simulator that compares and assesses different lenticular lens/screen combinations and
finally present and discuss the performance of lenticular lenses for multiplexing images from a single screen to
different users.
Index Terms - Lenticular lens; Multi user VR; Ghosting;
pixel refraction.

I. I NTRODUCTION
Several methods for aiding in designing and interacting
through VR have been proposed across the years, head
mounted displays (HMDs), volumetric screens, projectionbased virtual reality systems and several different devices
help us interacting with computer generated worlds.
HMDs work really well when the person who is interacting with the simulation is working alone. But this is rarely
the case in any industry. There have been attempts to work
cooperatively with HMDs (Szalavári et al. 1998), these systems exhaust users because of weight and eyesight hindering
among other issues. Although one may have different opinions on how much a see through HMD might occlude one’s
face, it makes facial expressions less visible to other participants removing an important nonverbal communication
channel.
When a group of domain experts get together, they surround and gesture toward a common dataset hoping they
achieve a consensus. This engagement is persistent across
disciplines and insists for a need of a VR system that accommodates small groups of people working close to each
other with correct perspective viewpoints. What looks like
a sphere for a user cannot look like an egg to another. Studies like the ones proposed by Pollock et al (Pollock et al.
2012) back up this idea by demonstrating that even when
perspective correct 3D is not needed for each user, collaboration times get significantly longer when participants stay
E-mail:

eacinfo@ualr.edu

at different locations compared to the same location relative
to the center of perception (CoP).
Different approaches have been attempted to multiplex
images from a single display to different users providing a
correct perspective for each one; In (Bolas, McDowall, and
Corr 2004), Mark Bolas presents a great classification on
the different approaches for doing multiuser immersive display systems. He proposes in his research that all these attempts convey into a “Solution Framework” which fall into
four general categories: Spatial Barriers, Optical Filtering,
Optical Routing and Time Multiplexing.
Lenticular lenses fall in the optical routing approach and
are widely used for lenticular printing, corrective lenses for
improving vision, 3D TV and lenticular screens among other
uses (Wikipedia 2018).
On this paper we are going to present and discuss an assessment of the performance of lenticular lenses for multiplexing images from a single screen to different users.

II. BACKGROUND
A. A NATOMY OF A L ENTICULAR L ENS
A lenticular lens is basically a set of cylinders (lenticules)
placed next to each other overlapping with one side flat and
the other side with the protubing lenses (fig 1), these lenses
are designed so that when viewed from different angles different slices of an image are displayed (fig 2).

Figure 1: Top: Diagram of a lenticular lens, Bottom: Real
lenticular lens
The most common example of lenticular lenses are the
ones used in lenticular printing, where the technology is
used to give an illusion of depth, or to make images that
appear to change or move as the image is viewed from different angles (Wikipedia 2018). Depending on the lens used
the effect achieved changes.

B. T YPES OF LENTICULAR LENSES
Lenticular lenses are normally classified by their Lenses Per
Inch (LPI), the higher the LPI the smaller each lenticule is.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 7

Figure 2: Image looked from different angles in a lenticular
lens
Normally manufacturers offer lenses in 10, 15, 20, 30, 50,
60, 75, 100LPI lenses; However, with enough amount of
funds one can get a lens with custom specifications manufactured.
Different types of lenticular lenses offer different effects
(see fig 3); these effects can be just about anything you can
do with video; some of the most popular effects include 3D,
flip, animation and morph. (Microlens 2018) (LenstarLenticular 2018).

Figure 3: Flip to 3D lenticular lenses.
1. 3D effect: This effect provides an illusion of depth and
perspective by layering objects within an image (fig. 4
A).
2. Flip effect: In this effect, a dramatic swap of two images occurs vanishing and then reappearing from one to
another (fig. 4 B).
3. Animation effect: This effect generates the illusion of
motion coming from a set of video frames or sequential
images (fig. 4 C).
4. Morph effect: This effect is commonly used to create the
illusion of transformation (fig. 4 D).
With the same principle on how lenticular printing works;
lenticular lenses have also been used for multiplexing images to different users in VR as an optical routing approach.

III. P REVIOUS W ORK
Optical routing uses the angle-sensitive optical characteristics of certain materials to direct or occlude images based on
the user’s position.(Bolas, McDowall, and Corr 2004).
In 1994, Little et al talk about a design for an autostereoscopic, multiperspective raster-filled display (Little,
Gustafson, and Nikolaou 1994). Here, they propose a time
multiplexed approach and an optical routing approach. The
optical routing approach features video cameras and LCTV
projectors. Here, they use an array of video cameras to capture multiple perspective views of the scene and then they

Figure 4: Lenticular lens effects. A: 3D, B: Flip, C: Animation, D: Morph

fed these to an array of LCTVs and simultaneously project
the images to a special pupil-forming viewing screen. The
viewing screen is fabricated by either a holographic optical
element or a Fresnel lens and a pair of crossed lenticular
arrays. Their resolution is limited by the LCTV projectors
and they use a lot of projectors / cameras to provide multiple views. For the time-multiplexed approach they investigated the use of a DMD projector that provided 20 sequentially perspective views, each one separated by one degree
in viewing angle by a spinning disk. The DMD approach is
monochromatic.
Van Berkel et al in (Van Berkel and Clarke 1997)
(Van Berkel 1999) built a prototype display using a LCD
and a lenticular lens from Philips Optics to display 3D images; they slanted the lenticular lens with respect to the LCD
panel in order to reduce the ”picket fence” effect. Their display has a resolution of 342x200.
Later in the same year, Matsumoto et al in (Matsumoto
and Honda 1997) proposes a system that consists of combination of cylindrical lenses with different focal lengths, a
diffuser screen and several projectors to create a 3D image.
They had issues with one of the lenses causing a dark stripe
in the 3D image affecting the stereoscopic vision by reducing the sense of depth.
Omura presents a system that uses double lenticular
lenses with moving projectors that move according to the
tracked user’s position to extend the viewable area (Omura,
Shiwa, and Miyasato 1998), their system needs a pair of projectors per person and their projectors move to adjust each
user’s position. Their system suffers from latency due to the
mechanical movement.
Lipton proposed the Synthagram (Lipton and Feldman
2002), a system that consists of an LCD Screen with a lenticular screen that overlays the LCD display. They angled the
lenticular screen in order to reduce the moiré pattern and
their system uses nine progressive perspective views from
a single image. They sample these views into a program
called the Interzig where they process the images and assign
each pixel to a specific position in the screen. Synthagram
files are stored as 24bit bmp files and it’s not able to process
images in real-time according to the authors.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 8

Matusik proposes a system that consists of an array of
cameras, clusters of network connected PCs and a multiprojector 3D display with the purpose to transmit autostereoscopic realistic 3D TV (Matusik and Pfister 2004). They
record the imagery with a small cluster of cameras that are
connected to PCs. The PCs broadcast the recorded video
which later on is decoded by another cluster of consumer
PCs and projectors. Their 3D Display consists of 16 NEC
LT-170 projectors that are used for front or rear projection. The rear projection approach consists for two lenticular sheets mounted back to back with an optical diffuser
material in the center and the front projection system uses
one lenticular sheet with a retro reflective front projection
screen material. They mention their system runs only at
12fps and the rear projection system exhibits moiré artifacts
along some visible vertical lines due to the lenticular sheet
used.
Another way of optical routing approach use is the display proposed by Nguyen et al (Nguyen and Canny 2005)
(Nguyen and Canny 2007). Here, they propose a special
display which consists of a screen with 3 layers that has directional reflections for projectors so each participant sees a
customized image from their perspective; their system supports up to 5 viewing zones but doesn’t support tracking and
it needs a projector per participant.
Takaki et al proposes a system that can produce 72
views(Takaki 2005). Their system consists of a light source
array, a micro lens and a vertical diffuser (a lenticular sheet).
They mention that as the horizontal positions of all light
sources are different, rays from different light sources proceed to different horizontal directions after passing through
the micro lenses thus generating different views. They also
mention that it’s difficult to fabricate a large micro lens array
and also say that unused pixels remain at the corners of the
LCD panel.
Later on. In (Nakanuma, Kamei, and Takaki 2005)
(Kikuta and Takaki 2007) followed by (Takaki 2009), Takaki
discusses a multiple projection system that is modified to
work as a super multiview display. Here, they attach a lens
to the display screen of a HDD projector and by combining
the screen lens and the common lens, they project an aperture array. This aperture array is placed on the focal plane of
the common lens, and the display screen (a vertical diffuser)
is placed on the other focal plane. Hence, the image of the
aperture array is produced on the focal plane of the screen
lens. With this, the image of an aperture array gets enlarged
generating enlarged images that become viewpoints. The
authors comment that there is some discontinuity between
the different generated views when the observation distance
is different from the distance to the viewpoints.
In 2009. Takaki and his team introduce a prototype panel
that can produce 16 views (Takaki, Yokoyama, and Hamagishi 2009). They do this by building a LCD with slanted
subpixels and a lenticular screen. They place a diffusion
material between the lenticular sheet and the LCD screen in
order to defocus the moiré pattern but increase the crosstalk
among viewpoints. They mention that by slanting the subpixel arrangement instead of the lenticular sheet, they can
increase the number of views but the optical transmittance

of the display decreases. They conclude that by slanting the
subpixels in the screen instead in the lenticular sheet, they
can reduce significantly the crosstalk and moire compared
to the normal approaches. Their approach requires to build
a LCD display which is a quite a complex task.
Finally, in 2010 Takaki and his team combine several 16-view flat-panels that have slanted subpixels(Takaki,
Yokoyama, and Hamagishi 2009) and creates a system with
256 views(Takaki and Nago 2010). They superimpose the
different projected output of the panels to a single vertical
diffuser. The multiple viewing zones for each flat panel are
generated on an incident pupil plane of its corresponding
projection lens. Each projection lens projects to the display surface of its corresponding flat panel system on the
common screen and finally a screen lens is located on the
common screen so the lens generates viewing zones for observers. They mention that their prototype display has the
possibility of producing 3D images on which the human
eye can focus but also they report that there is considerable
crosstalk between the viewing zones and the resolution of
the prototype is not very high.
Another system that takes advantage of the optical routing approach is the Free2C display, a project proposed by
Surman in (Surman et al. 2006). Here, they created a single
viewer autostereoscopic display using a head tracker. The
display accommodates the head movement of the viewer by
continually re-adjusting the position of the lenticular lens in
relation to the LCD to steer the stereoscopic views onto the
eyes of the viewer. Their display resolution is 1200x1600,
the viewing distance goes from 40cm to 110cm and side to
side movements range of approximately +-25 degrees from
the center of the screen. They also attempted a multi-user
display that steers the LCD instead of the lenses to produce
image regions for the users but they mention the display performance was really poor.
Similarly to Free2C, Brar et al use image recognition to
track users’ heads to produce multiple steerable exit pupils
for left and right eyes (Brar et al. 2010a) (Brar et al. 2010b).
Here, they describe the design and construction of a stereoscopic display that doesnt require wearing special eye wear.
A stereo par is produced on a single LCD by simultaneously
displaying left and right images on alternate rows of pixels.
They propose steering optics controlled by the output the
aforementioned head tracker to direct regions, referred as
exit pupils to the appropriate viewers’ eyes. Their prototype
is not optimal due to insufficient brightness and instability in
the holographic projector and their current research doesn’t
support multiple users.
Kooima et al (Kooima et al. 2010) uses 24” and 42”
3DHD Alioscopy displays which come with integrated
lenticular lenses. They propose a system that consists of
scalable tiled displays for large field of views and use a generalization of a GPU based autostereoscopic algorithm for
rendering in lenticular barriers.They tried different methods
for rendering but they had issues where they perceived repeated discontinuities, exaggerated perspectives and as the
displays pixels cannot be moved smoothly but in discrete
steps. The tracked viewer moves into transition between
channels, the user begins to see the adjacent view before the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 9

channel perspective is updated to follow the user’s head.
Zang et al propose a frontal multi-projection autostereoscopic display (Zang et al. 2014). Their approach consists of
8 staggered projectors and a 3D image guided screen. The
3D image screen is mainly composed of a single lenticular sheet, a retro-reflective diffusion screen and a transparent layer that is filled between them to control the pitch of
the rearranged pixel stripe in interlaced images. Their system is space efficient compared to previous approaches that
produce light from the back of the screen, but the loss of intensity and crosstalk are seriously increased out of the system boundaries and besides being complex it doesn’t provide perspective perfect views for each user.
We have mentioned here some research that has been done
throughout the years that use an optical routing approach;
specifically, lenticular lenses to separate users. Still, none of
these projects mention an objective approach for the correct
lens values for each of the setups used.

IV. S IMULATING

LENTICULAR LENSES

Different lenses work better with different screens due to the
different variables each lens / screen marriage have. Ideally,
factors like ghosting (being able to see images from other
users), number of pixels perceived for each user and color
banding are things that need to be taken into account when
pursuing an optical routing approach for multiplexing images.
To assess the performance of any given lens/screen marriage, a lenticular lens simulator was developed to determine
how good or bad a specific lens will work with a specific
screen, see how much can the lens specifications be pushed
with several users and theoretically come up with a lens that
can work great with a given screen and a number of users.

Mathematical model for the simulator
A lenticular lens has several variables that can be modified to
produce different effects; manufacturers sell lenses based on
their LPI but at the end, each of these lenses comes with a set
of specifications like the refraction index of the material the
lens is made of, substrate thickness, viewing angle, lenticule
radius, etc.(fig. 5) (Wikipedia 2018). To simulate how a lens

the lens in three steps: Substrate contact, lens contact and
lens refraction.

Step 1: Substrate contact.
In this phase n number of rays are calculated for each pixel
with a spread S (in deg) in order to get n contact points
P0 , P1 . . . Pn from a horizontal line (parallel to the screen)
that defines the substrate thickness of the lens (fig. 6). To

Figure 6: Step 1: Find where rays intersect substrate line.
find the points P0 , P1 . . . Pn where the rays intersect the end
of the lens substrate we represent the rays from the pixel
and the substrate with line equations and find their respective
intersection points.
Given the points R1 and R2 from the line L1 that represents the ray that gets generated from the pixel and points
S1 and S2 from the line L2 that defines the substrate of the
lens, we can generate two standard line equations with the
form y = mx + b, make them equal on the y axis (as it is
the substrate thickness that the manufacturer gives) and find
the intersection point Pi on x (fig. 6).
L1 → y = m1 x + b1
L2 → y = m2 x + b2
x=

b2 − b1
m1 − m2

intersection X axis (Px ).

Again, finding the Py becomes trivial as is given by the lens
manufacturer and is the lens substrate thickness.

Step 2: Lens contact.
After finding where the rays of light intersect in the substrate thickness line, we proceed to find which lens the ray
“belongs” to in order to apply the corresponding refraction
in step three.
To do so, we find the center Ci of the lenticule li that
is closest to the intersection Pj in order to know which lens
refracts each ray from the pixel (fig. 7). To find these centers

Figure 5: Detailed lenticular lens
works; a discretization of the light emitted from each pixel
is represented with several rays that start from each subpixel
along the lens substrate that get refracted to the air from the
lenticular lens. To achieve this, the simulator generates a
number of rays for each pixel and calculates ray trajectories
from when they start in the pixel until they get refracted by

Figure 7: Step 2: Find points Pj closest to lens center Ci .
we just need to find Cx for each lens because Cy in all the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 10

lenticule centers remain the same and can be easily deduced
from figure 7 as Cy = lensT hickness − lenticuleRadius.
Getting Cx is also pretty straight forward as the pixel
number (P ixN um) the ray comes from is known, the physical pixel size (P ps) has already been pre-calculated from the
screen density PPI and we can know each lenticule size (Ls)
by just dividing the LPI of the lens by 1 inch (Ls = LP1 I ).
First, we calculate the lenticule center that stays on top of
the current pixel (Lct) with:
Lct =

j P ixN um ∗ P ps k
Ls

  Ls 
∗ Ls +
2

(1)

After calculating Lct (eq. 1), we proceed to check if the distance with Px i is lesser than half of an individual lenticule
size (|Lct − Px | < Ls
2 ), if it is we have found the lenticule center Cx the ray belongs to, else we carry checking for neighboor lenticule centers with Cx ± Ls
2 until the condition gets
satisfied.
In conclusion, the algorithm for finding the lenticule center Cj that belongs to the ray intersection Pi we are calculating in this phase can be seen in Alg. 1.

Figure 8: Step 3: Ray intersection with lens and refraction.
ray of light that comes from a given pixel:
Ray → y = mx + b Lens → (x − cx )2 + (y − cy )2 = r 2
2

2

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:

. Return closest C to P
Cy ← lensT hickness − lenticuleRadius
j P ixN um ∗ P ps k
  Ls 
Lct ←
∗ Ls +
Ls
2
if |Lct − Px | < Ls/2 then
return C(Lct, Cy )
end if
counter ← 1
while true do
. Search neighboor lens centers
centerL ← Lct − (counter ∗ Ls)
if |centerL − Px | < Ls/2 then
return C(centerL, Cy )
end if
centerR ← Lct + (counter ∗ Ls)
if |centerR − Px | < Ls/2 then
return C(centerR, Cy )
end if
counter ← counter + 1
end while
end procedure

2

2

2

2

2

2

0 = x (1 + m ) + x(2mb − 2cx − 2mcy ) + (cx + b − 2bcy + cy − r )

Solving the quadratic form of the resulting equation we end
up with:

Algorithm 1 Get closest lens center from ray intersection Pi
1: procedure C LOSEST C ENTER(P ixN um, P ps, Ls, P )

2

0 = (x − cx ) + (mx + (b − cy )) − r Replace ray in lens equation

x1,2 =

=

−mb + cx + mcy ±
1 + m2
q
−2mbcx + 2mcx cy − b2 + 2bcy − c2y + r 2 + r 2 m2 − m2 c2x
1 + m2

(2)
After finding the x component in eq. (2), we can see we
have three possible values that the quadratic equation gives
us under the square root. Lets call this D.
2

2

2

2

2

2 2

D = −2mbcx + 2mcx cy − b + 2bcy − cy + r + r m − m cx

D



< 0 No intersection point (ray




doesnt touch the lens).






this is at the end a circle).

= 0 The line touches the lens tangentially.
> 0 Two intersection points(as

We are only interested in the positive value as the lenses
point torward the positive Y axis so on this case, the x component of the intersection point Qi where the ray touches the
lens ends up being:
x=

=

−mb + cx + mcy +
1 + m2
q
−2mbcx + 2mcx cy − b2 + 2bcy − c2y + r2 + r2 m2 − m2 c2x
1 + m2

Step 3: Lens Refraction.

(3)

After finding the closest lenticule center Cj from a given ray
intersection Pi we can continue with the ray direction ~r and
finally find the intersection point Qi with the lenticule Lj
where we can calculate the ray refraction (fig. 8). ~rf.
Finding lens intersection point: To find Qi (fig 8) we can
treat each lenticule as a circle and the rays that come from
each pixel as lines and then the lens-ray intersection point
can be treated as a line-circle intersection as follows (AmBrSoft 2018) (Projects 2011):
Given a circle with center (cx , cy ) with radius r representing the lenticule with center Cj and a line representing the

Finally, just by replacing this value (eq. 3) on the line equation from the ray one can get the y component of Qi .
Generating refracted ray from the lens: After finding
the point of intersection where the ray (coming from the
pixel) touches the lens (Qi ). One can finally calculate the
~ )(fig. 8) using Snell’s law (De Greve
refracted pixel ray (rf
2006).
Snell’s Law states that the products of the index of refraction and sines of the angles must be equal (eq. 4).
n1 sin(θ1 ) = n2 sin(θ2 )

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 11

(4)

Snell’s equation (eq. 4) can be re-written as:
n1
sin(θ2 ) =
sin(θ1 )
(5)
n2
One can immediately see a problem here, and is that if
sin(θ1 ) > nn21 then sin(θ2 ) has to be bigger than 1 which
is impossible. So when this happens, we have a TIR (Total Internal Reflection), TIR only happens if you go from a
denser material (lens) to a less dense material (air). When
TIR happens, we just ignore that ray and do nothing about
it. So eq. 5 can be written like this:
n1
n2
sin(θ2 ) =
sin(θ1 ) ←→ sin(θ1 ) ≤
(6)
n2
n1
~ , lets begin by splitting it up in a tangent and a
To find rf
normal part:
~ = rf
~ k + rf~⊥
rf
(7)
As all the vectors are normalized and any vector ~v can be decomposed in its tangent and parallel parts, and its parts are
perpendicular to each other (v~k ⊥ v~⊥ ), with basic trigonometry, the following rules apply:
sin(θ) =

|v~k |
= |v~k |
|~v |

cos(θ) =

|v~⊥ |
= |v~⊥ |
|~v |

(8)

Since Snell’s law talks about sines (eq. 6), we can use eq. 8
and rewrite eq. 6 as:
~ k | = n1 |r~k |
|rf
(9)
n2
~ k and r~k are parallel and point in the same direction,
Since rf
eq. 9 becomes:
~ k = n1 r~k = n1 (1 − cos(θr )~n)
rf
(10)
n2
n2
To find rf~⊥ one can simply use pythagoras (|~v |2 = |v~k |2 +
|v~⊥ |2 ) and end up with:
q
~ k |2~n
rf~⊥ = − 1 − |rf

(11)

Replacing eq. 9 and eq. 11 in eq. 7 we get:
q
n

n1
1
~ k |2 ~n Replacing 8
~
~r −
cos(θr ) + 1 − |rf
rf =
n2
n2
q


~ = n1 ~r − n1 cos(θr ) + 1 − sin2 (θrf ) ~n
rf
n2
n2
Finally, we need to find sin2 (θrf ) in this last equation, but
this can be easily deduced it using Snell’s law in equation 9.
 n 2
 n1 2
1
sin2 (θrf ) =
sin2 (θr ) =
(1 − cos2 (θr ))
n2
n2
With these two equations one can finally obtain the refracted
~.
vector rf
Finally, another factor that needs to be considered in the
analysis is the pixel spread. Pixel spread varies from screen
to screen and in the simulator can be adjusted from 0 (pixel
rays fully straight) to 89 degrees (fan like setting), it is worth
noting that the more pixel spread, the more cross talk there
is prone to happen in the end image for each user.

V. E XPERIMENT
To test the performance of the lenticular lenses for multiplexing user views, two experiments where made. The first
one with commercially available lenses with 10, 15, 20, 30
and 3D-40LPI mated with three types of screen: a 3D TV
LG55E6 (∼81ppi), an Asus PG278Q monitor(∼109ppi) and
the iPhone X (∼458ppi).
The second experiment was performed with a theoretical
lens specifically designed to fit the physical pixel sizes of the
Asus PG278Q monitor and a theoretical screen of 55” with
iPhone X pixel sizes.
Each of the tests were performed with three users evenly
separated by 50 centimeters between users and 1 meter away
from the screen.
Pixel spread was measured by drawing a vertical line on
the screens and with a protractor attached to a camera we
noticed that at ∼30 degrees from the center the screens contrast and brightness started to change. Two types of pixel
spread where assessed with the screens, pixels with 0 deg
spread and 30 deg spread.
If the reader noticed, the iPhone X doesn’t support stereo
nor is big enough for multiple users. This is on purpose and
is to compare increasing pixel density across the screens and
as of now in 2018 is one of the densest screens.

VI. DATA ANALYSIS
A. Commercially available lenses
Different lenses have different parameters. Depending on
the lens/screen combination used, a given number of pixels can be fit under each lenticule as seen in table 1. Each
of these values come from the lens manufacturer and affect
how the refracted pixels get displayed in the monitor. The
more pixels per lenticule the more pixels one can see from
different angles but at the same time the less pixels available
throughout the screen.
LPI

Material Refrac
Type
Idx

Substrate
thick (in)

Pitch
(in)

Radius px/lent
(in)
LG55E6

px/lent
PG278Q

px/lent
iPhoneX

10
15
20
30
3D40

Acrylic
Acrylic
PETG
PETG
PETG

0.1083
0.0894
0.0575
0.0401
0.0669

0.100
0.066
0.050
0.033
0.025

0.0601
0.042
0.0300
0.020
0.0309

10.896
7.264
5.448
3.632
2.724

45.876
30.583
22.938
15.291
11.469

1.47
1.47
1.56
1.56
1.56

8.062
5.374
4.031
2.687
2.0163

Table 1: Pixels availabe to fit per lenticule
All these values (screen and lens parameters) vary a lot depending on the combination used. In table 2, the LG55E6
TV has the largest and less dense screen (55”, ∼81ppi). A
relation between pixel density and ghosting can be seen on
both spread and straight pixels; this screen being the less
dense has the most ghosting (∼43% for the center user with
a 10LPI lens and spread rays). This means that the center
user for any given image it will perceive ∼50% of said image from other users.
The LG55E6 screen starts to show ghosting on the center user with straight pixels with just 20LPI, something that
doesn’t happen with the other monitors; with regards the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 12

Asus PG278Q monitor, one can see in table 3 that ghosting is further reduced for the center user and even more in
table 4.
Tables show that ghosting rapidly increases when the LPI
increases for spread pixels, this is due to factors like uncomplete number of pixels that can be fit under each lenticule as
the LPI raises among others, the 3D 40LPI lens suffers an
abrupt change on shared pixels per user and ghosting values
due to the fact that in this lens, the angle of view is narrower
than a normal lens. It is also worth mentioning that the more
“straight” the pixel rays are the better.
LPI

Ray
type

10

Straight

15

Straight

20

Straight

30

Straight

3D
40

Straight

10

Spread

15

Spread

20

Spread

30

Spread

3D
40

Spread

pixels
User 1

Shared
User 1

pixels
User 2

Shared
User 2

pixels
User 3

Shared
User 3

613px
(15.96%)
715px
(18.62%)
821px
(21.38%)
962px
(25.05%)
1094px
(28.49%)

0px
(0%)
0px
(0%)
2px
(0.24%)
228px
(23.70%)
324px
(29.62%)

1015px
(26.43%)
1172px
(30.52%)
1337px
(34.82%)
1725px
(44.92%)
1873px
(48.78%)

0px
(0%)
0px
(0%)
5px
(0.37%)
459px
(26.61%)
653px
(34.86%)

630px
(16.41%)
733px
(19.09%)
836px
(21.77%)
1164px
(30.31%)
1436px
(37.40%)

0px
(0%)
0px
(0%)
3px
(0.36%)
231px
(19.85%)
329px
(22.91%)

1177px
(30.65%)
1342px
(34.95%)
1384px
(36.04%)
1504px
(39.17%)
1330px
(34.64%)

412px
(35.00%)
650px
(48.44%)
725px
(52.38%)
1120px
(74.47%)
745px
(56.02%)

1941px
(50.55%)
2192px
(57.08%)
2280px
(59.38%)
2684px
(69.90%)
2340px
(60.94%)

828px
(42.66%)
1292px
(58.94%)
1451px
(63.64%)
2214px
(82.49%)
1496px
(63.93%)

1198px
(31.20%)
1363px
(35.49%)
1420px
(36.98%)
1807px
(47.06%)
1666px
(43.39%)

416px
(34.72%)
642px
(47.10%)
726px
(51.13%)
1096px
(60.65%)
751px
(45.08%)

Table 2: User separation LG55E6 (3840x2160, ppi: ∼81).
LPI

Ray
type

10

Straight

15

Straight

20

Straight

30

Straight

3D
40

Straight

10

Spread

15

Spread

20

Spread

30

Spread

3D
40

Spread

Pixels
User 1

Shared
User 1

Pixels
User 2

Shared
User 2

Pixels
User 3

Shared
User 3

323px
(12.62%)
401px
(15.67%)
397px
(15.51%)
510px
(19.92%)
342px
(13.36%)

0px
(0%)
0px
(0%)
0px
(0%)
31px
(6.08%)
199px
(58.19%)

812px
(31.72%)
961px
(37.54%)
1060px
(41.41%)
1291px
(50.43%)
1866px
(72.89%)

0px
(0%)
0px
(0%)
0px
(0%)
66px
(5.11%)
419px
(22.45%)

342px
(13.36%)
417px
(16.29%)
422px
(16.48%)
538px
(21.02%)
382px
(14.92%)

0px
(0%)
0px
(0%)
0px
(0%)
35px
(6.51%)
220px
(57.59%)

685px
(26.76%)
817px
(31.91%)
748px
(29.22%)
883px
(34.49%)
668px
(26.09%)

254px
(37.08%)
454px
(55.57%)
417px
(55.75%)
650px
(73.61%)
667px
(99.85%)

1544px
(60.31%)
1793px
(70.04%)
1779px
(69.49%)
2036px
(79.53%)
2559px
(99.96%)

519px
(33.61%)
930px
(51.87%)
838px
(47.11%)
1312px
(64.44%)
1398px
(54.63%)

728px
(28.44%)
869px
(33.95%)
784px
(30.62%)
920px
(35.94%)
731px
(28.55%)

265px
(36.40%)
476px
(54.78%)
421px
(53.70%)
662px
(71.96%)
731px
(100%)

Table 3: User separation PG278Q (2560x1440, ppi:∼109).
With ∼34% ghosting from the Asus PG278Q commercially
available lenticular lenses are not feasible for multiplexing
images.

LPI

Ray
type

10

Straight

15

Straight

20

Straight

30

Straight

3D
40

Straight

10

Spread

15

Spread

20

Spread

30

Spread

3D
40

Spread

Pixels
User 1

Shared
User 1

Pixels
User 2

Shared
User 2

Pixels
User 3

Shared
User 3

123px
(5.05%)
143px
(5.87%)
157px
(6.44%)
189px
(7.76%)
0px
(0%)

0px
(0%)
0px
(0%)
0px
(0%)
0px
(0%)
0px
(0%)

478px
(19.62%)
553px
(22.70%)
585px
(24.01%)
689px
(28.28%)
1191px
(48.89%)

0px
(0%)
0px
(0%)
0px
(0%)
0px
(0%)
0px
(0%)

59px
(2.42%)
69px
(2.83%)
76px
(3.13%)
98px
(4.02%)
0px
(0%)

0px
(0%)
0px
(0%)
0px
(0%)
0px
(0%)
0px
(0%)

500px
(20.53%)
580px
(23.81%)
582px
(23.89%)
611px
(25.08%)
3px
(0.12%)

178px
(35.60%)
282px
(48.62%)
276px
(47.42%)
335px
(54.83%)
2px
(66.67%)

1328px
(54.52%)
1494px
(61.33%)
1411px
(57.92%)
1487px
(61.04%)
2435px
(99.96%)

356px
(26.81%)
548px
(36.68%)
544px
(38.55%)
648px
(43.58%)
2px
(0.082%)

386px
(15.85%)
446px
(18.31%)
463px
(19.01%)
480px
(19.70%)
0px
(0%)

178px
(46.11%)
266px
(59.64%)
268px
(57.88%)
313px
(65.21%)
0px
(0%)

Table 4: User separation iPhone X 5.85”, ppi: ∼458.

B. Designing a theoretical lens
Commercially available lenses are still difficult to accomodate to screens without ghosting regardless the pixel density.
Which values should be tweaked then to generate a lens that
maximizes the the number of pixels per user for multiplexing images while minimizing ghosting the most?
Some values are easier to modify than others in the physical world we live in. The index of refraction is a variable
that is innate from the material the lens is made of; if one
decided to modify the index of refraction one would need to
change the material the lens is made of and find another material that has a solid structure, can be molded to the other
lens variable requirements and has the index of refraction
one is looking for. Due to this, is easier to modify other values from the lens like the substrate thickness, lens pitch and
lenticule radius and leave the index of refraction unchanged.
An ideal lens for any specific screen lies on the mix of
the optimum values for the correct substrate thickness, lens
pitch and lens radius. All these values should minimize the
ghosting among the interacting users while maximizing the
number of unique pixels seen by any given user regardless
of their position.
To find the correct lens values a geometric approximation
to this problem was thought. A three axis space of combinations between pixels per lenticule (x), substrate thickness(y)
and maximum ghosting perceived from all the interacting
users(z) or the minimum number of pixels either of the users
perceive(z’) is generated. All of these values separated by
an increase of the minimum radius allowed for each of the
lenses with increases of 10% up until double the minimum
radius permitted by each lens.
On the pixels per lenticule axis, values go from 1 to 50
pixels with 1 pixel increment. With regards to the substrate
thickness axis, values vary from 0 to 3mm with increments
of 0.25mm. Finally, the third axis lets one assess the maximum percentage of ghosting between the interacting users,

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 13

the minimum unique pixels a user perceives or the minimum
percentage either of the user sees from the screen.

400 pixels for a user is just not enough to assemble an image
with enough detail for today’s standards.

B.1 Theoretical lens for the Asus PG278Q monitor.
With a resolution of 2560 pixels with a pixel size of
0.2331mm, pixel spread of ∼30 degrees and 55 rays per pixels calculated from the screen, the reader can look on figure
9 how the different lens values for a substrate thickness of
0.5mm looks like.

B.2 Theoretical lens 55” screen with iPhone X px size.
How would the multi user experience be with a 55” screen
with iPhoneX physical pixel sizes?. Said screen will consist
of 21580 pixels and as shown in figure 10 it will provide at
a minimum, 5K images for either of the users.

Figure 10: Lens combinations theoretical 55” screen.
21580px, 30deg spread, 55 rays per pixel.
Figure 9: Lens combinations Asus PG278Q monitor. 30deg
spread, 55 rays per pixel
If the reader notices, the ideal lens radius for this monitor
lays between 40% and 60% increase from the base model.
As one can see in figure 9, with a radius increase of 50% and
18-21 pixels per lenticule there is minimum ghosting for the
three users with a maximum number of unique pixels for
either the three with around a minimum of 18% screen share
from the users.
To keep the 3D graphs readable, only lens radius increments from 30% to 70% where included in the graphs. If
the reader would like to see a specific lens value and how it
affects each of the three users, one can refer to the data in
(Simulator a)
In figure 9 the reader can see that the lens that maximizes
the minimum number of unique pixels perceived by either of
the three users and at the same time minimizes ghosting the
most is a lenticular lens that has 21 pixels per lenticule with a
radius increase of 50% from the minimum supported lens radius for a screen that has physical pixel sizes of 0.2331mm.
Unfortunately, if the reader notices, a minimum of around

If the reader looks closer at figure 10, one will notice that
the value that maximizes the minimum amount of unique
pixels and minimizes ghosting is a lenticular lens with 18
to 19 pixels per lenticule with a radius increase of 40-50%
from the minimum lenticule radius.
In figures 11, and 12 the reader can also see how the substrate thickness affects the values. These values where generated with 20 to 80% radius increase in order to make it
easier to understand the graph.
All of these values where calculated with the simulator;
If the reader is interested to see how the other lens radiuses
affect the graph, one can look in (Simulator b) to check the
data.

VII. C ONCLUSIONS
A simulator for lenticular lenses was presented and the
mathematics of the lenticular lens where introduced and explained. It was shown that pixel density is a big factor that
contributes to ghosting when multiplexing images.
Ghosting is one of the factors that deter the most the experience when multiplexing different images through one

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 14

ule, a radius increase of 40-50% from the minimum lenticule
radius and a substrate thickness of 0.5mm.
In the case it’s not possible to physically make a lenticular lens of said requirements due to the substrate thickness;
it’s feasible to increase the thickness to some extent without
inducing ghosting.

VIII F UTURE WORK

Figure 11: Lens combinations theoretical screen minimum
unique pixels per user.

Figure 12: Lens combinations theoretical screen for maximum ghosting users get.

screen. Pixel spread is a contributing factor when using
lenticular lenses, as shown in this paper, the more straight
the rays from pixels are the less ghosting the system will
produce.
Commercially available lens values where tested and
assessed with commercially available screens and it was
shown that they are not good enough for multiplexing users;
still. With straight pixel rays these lenses could be usable.
A theoretical lens was designed for the Asus PG278Q
monitor and it was shown that the lens that maximizes the
pixels perceived per user and minimizes ghosting the most is
a lens with 50% radius increase from the minimum possible
radius with 21 pixels per lenticule and a substrate thickness
of 0.5mm.
A theoretical screen of 55inch with pixel sizes of the
iPhone was also presented and analyzed. The minimum
number of pixels a user perceives is a 5k image with zero
ghosting with a lens that has between 18-19 pixels per lentic-

Pixel spread is a contributing phenomenon to ghosting. Privacy screens reduce the spread to a degree but they don’t
completely make pixel rays straight. It would be worth
checking for materials from privacy screens to see if its possible to completely make the rays from pixels straight.
The lenticular lens simulator helped assessing and finding
the best lens values for any given screen setup. Unfortunately it doesn’t show the amount of color banding from
subpixel augmentation the lenses produce. A ray tracer
could show how much banding is generated with the optimal lens values covered in this paper and perhaps find a
method to know to what extent banding is produced for any
given pixel size and lens without needing to have the physical lens/screen.
Subpixel layout is a critical factor that either can make
colors look different from pixel to pixel on some screens
or can produce subpixel augmentation when using low LPI
lenses; it would be worth doing a further analysis to asses
which subpixel layout would be best for reducing subpixel
augmentation from low LPI lenses and at the same time not
lose pixel brightness similarity across the screen.
User movement should be considered for the data in order
to see if the best lens values are consistent regardless user’s
positions within limits.
R EFERENCES
[1] AmBrSoft. 2018. Intersection of a circle and a
line. http://www.ambrsoft.com/TrigoCalc/
Circles2/circlrLine_.htm. Accessed: 2018Nov-14.
[2] Bolas, M.; McDowall, I.; and Corr, D. 2004. New
research and explorations into multiuser immersive display systems. IEEE computer graphics and applications
24(1):18–21.
[3] Brar, R. S.; Surman, P.; Sexton, I.; Bates, R.; Lee,
W. K.; Hopf, K.; Neumann, F.; Day, S. E.; and Willman,
E. 2010a. Laser-based head-tracked 3d display research.
Journal of Display Technology 6(10):531–543.
[4] Brar, R. S.; Surman, P.; Sexton, I.; and Hopf, K. 2010b.
Multi-user glasses free 3d display using an optical array.
In 3DTV-Conference: The True Vision-Capture, Transmission and Display of 3D Video (3DTV-CON), 2010, 1–
4. IEEE.
[5] De Greve, B. 2006. Reflections and refractions in ray
tracing. Retrived Oct 16:2014.
[6] Kikuta, K., and Takaki, Y. 2007. Development of
svga resolution 128-directional display. In Stereoscopic
Displays and Virtual Reality Systems XIV, volume 6490,
64900U. International Society for Optics and Photonics.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 15

[7] Kooima, R.; Prudhomme, A.; Schulze, J.; Sandin, D.;
and DeFanti, T. 2010. A multi-viewer tiled autostereoscopic virtual reality display. In proceedings of the 17th
ACM Symposium on Virtual Reality Software and Technology, 171–174. ACM.
[8] LenstarLenticular. 2018. Possible lenticular effects.
https://www.lenstarlenticular.com/lenticular-effects/ Accessed: 2018-Nov-12.
[9] Lipton, L., and Feldman, M. H. 2002. New autostereoscopic display technology: the synthagram. In Stereoscopic Displays and Virtual Reality Systems IX, volume
4660, 229–236. International Society for Optics and Photonics.
[10] Little, G. R.; Gustafson, S. C.; and Nikolaou, V. E.
1994. Multiperspective autostereoscopic display. In
Cockpit Displays, volume 2219, 388–395. International
Society for Optics and Photonics.
[11] Matsumoto, K., and Honda, T. 1997. Research of 3d
display using anamorphic optics. In Stereoscopic Displays and Virtual Reality Systems IV, volume 3012, 199–
208. International Society for Optics and Photonics.
[12] Matusik, W., and Pfister, H. 2004. 3d tv: a scalable
system for real-time acquisition, transmission, and autostereoscopic display of dynamic scenes. In ACM Transactions on Graphics (TOG), volume 23, 814–824. ACM.
[13] Microlens.
2018.
Choosing the correct lenticular lens sheet. http://www.microlens.com/
pages/choosing_right_lens.htm. Accessed:
2018-Nov-12.
[14] Nakanuma, H.; Kamei, H.; and Takaki, Y. 2005.
Natural 3d display with 128 directional images used for
human-engineering evaluation. In Stereoscopic Displays
and Virtual Reality Systems XII, volume 5664, 28–36. International Society for Optics and Photonics.
[15] Nguyen, D., and Canny, J. 2005. Multiview: spatially
faithful group video conferencing. In Proceedings of the
SIGCHI conference on human factors in computing systems, 799–808. ACM.
[16] Nguyen, D. T., and Canny, J. 2007. Multiview: improving trust in group video conferencing through spatial
faithfulness. In Proceedings of the SIGCHI conference on
Human factors in computing systems, 1465–1474. ACM.
[17] Omura, K.; Shiwa, S.; and Miyasato, T. 1998. Lenticular autostereoscopic display system: multiple images for
multiple viewers. Journal of the Society for Information
Display 6(4):313–324.
[18] Pollock, B.; Burton, M.; Kelly, J. W.; Gilbert, S.; and
Winer, E. 2012. The right view from the wrong location:
Depth perception in stereoscopic multi-user virtual environments. IEEE Transactions on Visualization & Computer Graphics (4):581–588.
[19] Projects, A. 2011. Circle and line intersection.
http://apetrilla.blogspot.com/2011/
12/circle-and-line-intersection.html.
Accessed: 2018-Nov-14.

[20] Simulator. Lens combinations pg278q. https://
goo.gl/vD5Bbd. Accessed: 2018-Nov-20.
[21] Simulator.
Lens combinations theoretical 55in.
https://goo.gl/VaKf2h. Accessed: 2018-Nov20.
[22] Surman, P.; Sexton, I.; Hopf, K.; Lee, W. K.; Bates,
R.; IJsselsteijn, W.; and Buckley, E. 2006. Head tracked
single and multi-user autostereoscopic displays.
[23] Szalavári, Z.; Schmalstieg, D.; Fuhrmann, A.; and Gervautz, M. 1998. “studierstube”: An environment for collaboration in augmented reality. Virtual Reality 3(1):37–
48.
[24] Takaki, Y., and Nago, N. 2010. Multi-projection of
lenticular displays to construct a 256-view super multiview display. Optics express 18(9):8824–8835.
[25] Takaki, Y.; Yokoyama, O.; and Hamagishi, G. 2009.
Flat panel display with slanted pixel arrangement for 16view display. In Stereoscopic Displays and Applications
XX, volume 7237, 723708. International Society for Optics and Photonics.
[26] Takaki, Y. 2005. Thin-type natural three-dimensional
display with 72 directional images. In Stereoscopic Displays and Virtual Reality Systems XII, volume 5664, 56.
International Society for Optics and Photonics.
[27] Takaki, Y. 2009. Super multi-view display with 128
viewpoints and viewpoint formation. In Stereoscopic
Displays and Applications XX, volume 7237, 72371T. International Society for Optics and Photonics.
[28] Van Berkel, C., and Clarke, J. A. 1997. Characterization and optimization of 3d-lcd module design. In Stereoscopic Displays and Virtual Reality Systems IV, volume
3012, 179–187. International Society for Optics and Photonics.
[29] Van Berkel, C. 1999. Image preparation for 3d lcd.
In Stereoscopic Displays and Virtual Reality Systems VI,
volume 3639, 84–92. International Society for Optics and
Photonics.
[30] Wikipedia. 2018. Lenticular lens. https://goo.
gl/t2yQ4L. Accessed: 2018-Nov-12.
[31] Zang, S.-F.; Wang, Q.-H.; Zhao, W.-X.; Zhang, J.; and
Liang, J.-L. 2014. A frontal multi-projection autostereoscopic 3d display based on a 3d-image-guided screen.
Journal of display technology 10(10):882–886.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 16

Using VR technology to improve BIM: maintenance and
construction
Alcínia Zita Sampaio
Dep. Civil Engineering
University of Lisbon
Av. Rovisco Pais, 1049-001 Lisbon,
Portugal

zita@civil.ist.utl.pt
ABSTRACT
Research on the application of virtual reality (VR) in the
construction activity, adopted normally low-immersive desktop
VR or highly immersive VR, which involves using a mobile
headset. Immersive VR spaces can promote a better
communication between clients and specialist, around a building,
in a maintenance or inspection activity, or in the real construction
place, following the planned construction steps. The two primary
features of VR are immersion and presence. And when linked
with the building Information modeling (BIM) concept,
consulting and handling information, are complementary features
that can improve both technologies. Most VR environments
constitute a visual experience displayed on a computer screen, but
using the most advances VR technologies over the 3D BIM model
a more immerse space can be handle. Simulating the real world
can be improve with VR+BIM, as the interactive capacities
allowed by VR software, and the access to information archive in
a BIM model, supports in a more intuitive way maintenance and
construction control tasks.

Keywords
BIM, VR, VR software, VR+BIM applications, construction.

equipment, most applications have mainly used desktop VR.
Nevertheless, practical application of this technology requires that
problems concerning hardware and space be solved.
VR will not replace the traditional maintenance process on screen,
but it provides a useful addition to engineering companies helping
specialist to communicate better with other members of the team
including the client [2]. Boundary-pushing concepts can be tested
and refined at a human scale in the virtual world, long before
committing to real-world construction, and professional can use
VR to build better buildings and improve the client experience.
VR technology still needs to evolve, but as the pace of
innovations accelerates, systems allow for more novel modes of
visualization and interaction to support engineering design
reviews. Currently, the classic design review process is often
performed on a PC with the support of CAD software packages.
However, CAD on a screen cannot always meet all the
requirements and a BIM model containing, in a centralized way,
all the information, can easily support the access to all type of
information, concerning construction or maintenance [3]. The text
analyses the degree of achievement allowed by the actual software
to perform each aspect combining BIM and VR.

2. VR INTERACTION

1. INTRODUCTION
Building Information Modelling (BIM) is a methodology
supported in a virtual three–dimensional (3D) model-based
process, allowing modelling and the management of all activities
inherent to Construction. It provides architects and engineers the
visualization and the tools to plan, design, build and manage more
efficiently engineering works. BIM, as a concept, has been
coming essential; supporting the development of multitasks within
the Construction industry, including concept/design, construction
work planning/scheduling or maintenance/management of
buildings. BIM implementation is widely disseminated in the
international community and across several sectors within the
Construction field [1]. The technological progress achieved over
the past years, leads to the development of new and better
products.
VR technology has undergone continual development, produced
many applications, and is used by the construction industry to
facilitate communication and the understanding of building
components. Furthermore, VR reduces the risk of exclusion of
certain professional groups from the maintenance process or
construction work. In addition, the intuitive interaction with the
VR system allowed for a much faster entry into the inspection
activity. Considering the cost and ease of use of related

Virtual Reality and BIM can improve building construction and
asset management. The sense of presence in a virtual space,
building in inspection or local place of a construction in progress,
is much truly as advanced technological VR devices is used. The
sensation of actually being inside a building makes VR an
important powerful tool for communicating design intent. Clients,
in particular, often don’t have the ability to understand spatial
relationships and scale simply by looking at a 2D plan or 3D
model. And VR can evoke an almost real response that physical
architecture can.
Most CAD and BIM models feature extremely detailed geometry,
which is not needed for VR. Fully interactive VR software also
has extremely high performance demands, so some form of model
optimization is required when bringing BIM data into a VR
environment. This is one area where specialist VR consultancies
earn their keep with finely tuned processes for tasks like
simplifying geometry, adding lighting, fixing gaps in the model
and culling objects that will not be visible in the scene [4].
BIM can reduce risk levels and improve design management
around key activities in construction and Virtual Reality can help
to identify potential problems in the design before construction
commences and reduce coordination errors. Contractors are able

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 17

to manage and reduce risk through using BIM for 4D simulation,
on site spatial coordination, and clash detection.
In an engine VR experiences the onus tends to be on presenting a
polished vision of a proposed building, rather than delivering
practical tools for solving real world design and construction
problems. Currently there are various applications of virtual
reality tour applied in the field of construction (Figure 1):
•

Software like Navisworks or Tekla BIMsight, are
equivalent tools inside a VR environment, as they are
not VR software but BIM models can be seen and
handling inside. Navisworks project review software
lets architecture, engineering, and construction
professionals holistically review integrated models and
data with stakeholders during preconstruction to control
project outcomes. Tekla BIMsight is a professional tool
for construction project collaboration, allowing the
entire construction workflow combining models, check
for conflicts and share information using in a BIM
environment;

•

While most game engines strip out metadata, which is
important for true BIM processes, when moving from
Revit to Autodesk Stingray, data is not only retained,
but users can click on objects and view the underlying
attribute information;

•

Revizto, is a dedicated tool for turning BIM models into
navigable 3D environments, has a similar capability. In
VR mode, users can click on an object to view
information, including metadata. Revizto is a visual
collaboration software for the AEC (Architecture,
Engineering and Construction) industry. Connects BIM
and VDC specialists with stakeholders and streamlines
BIM coordination workflow;

•

IrisVR Prospect retains BIM data when importing
models from Revit, although the developers have not
yet delivered tools to work with this data. Prospect can
streamlined VR delivery process, both internally for
office design reviews;

•

WorldViz is investing in collaborative VR and is
already used for co-presence experiences where users
can interact with each other in the virtual world.
Participants of a construction planning review session
can highlight different aspects of the construction
design simply by moving their virtual hands or pointing
lasers at objects and VR scene can also stop real-life
collisions.

•

Looking to the future and taking this idea a step further,
Microsoft’s ‘mixed reality’ HoloLens could even be
used to visualise a holographic 3D BIM model in
context on site. Using augmented reality, users could
solve issues by literally seeing the design model
overlaid on a partially constructed building;

•

Virtalis is specialize in virtual reality CAVEs and walls
that use powerful projectors and 3D glasses to deliver a
1:1 scale experience. Deploying a CAVE on a
construction site, could help problem-solve issues
between the real and virtual worlds, combining as- built
data with BIM models.

Figure 1: VR use in Navisworks, Tekla BIMsight, Revizto,
IrisVR Prospect, WorldViz, HoloLens and Virtalis.
Combining BIM and VR clients can easily walking through the
actual life sized model of an on-going construction place or a
building in inspection. The specialist can be sitting virtually
inside the model and evaluate every nook and cranny of it. This is
an fruitful improvement than sitting at a table and zooming into a
15’ screen. The acceptance and expansion of VR has been
growing exponentially. Ever since embracing the idea of VR, the
AEC sector has benefited a great deal. As per the reports,
companies began seeing faster project approvals, increased
positive client interactions and higher client satisfaction. The
checklist can be prepared smoothly so as to save valuable time
and money for the company as well as the client. With all the
rapid progress happening, it is not surprising that architects,
engineers and construction techies are already exploring the
length and breadth of this technology.

3. CONSULTING BIM DATA
An accessible high end visualization and virtual reality of a BIM
model is obtain using the VR plugin of Revit, the Enscape [5].
Enscape is a virtual reality (VR) and real-time rendering plugin
for Revit. Inside Revit is possible to access the plugin Enscape
and start to walk through the fully rendered project, without
uploading to cloud or exporting to other programs. A virtual tour

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 18

together with Enscape allows the facility manager to look around
the facility and check the conditions of equipment and obtain
relevant information from the BIM model. For instances,
visualizing the BIM model of a MEP system, in a virtual tour and
using on a tablet PC, helps the facility manager to understand
what is installed behind the ceiling tile. The user can observe both
models in Revit and in Enscape (Figure 2). So, all changes in
Revit are immediately available to evaluate in Enscape.

environment and interact with the contents. Due to its immersive
experience and intuitive manipulation capability, it quickly gained
popularity in both research and industry community. But also VR
head-mounted displays (HMDs) such as the Oculus Rift have the
capacity to improve the way architects design and communicate
buildings before they are built. The wearer is instantly immersed
in a true three dimensional environment that gives an incredible
sense of scale, depth and spatial awareness that simply cannot be
matched by traditional renders, animations or physical-scale
models. The sense of presence was overwhelming.
This sensation of actually being inside a building also makes VR
an incredibly powerful tool for communicating design intent.
Clients, in particular, often don’t have the ability to understand
spatial relationships and scale simply by looking at a 2D plan or
3D model [9]. VR can evoke a visceral response in exactly the
same way that physical architecture can play an important role at
all stages of the design-to-construction process, from evaluating
design options and showcasing proposals, to designing out errors
and ironing out construction and serviceability issues before
breaking ground on site.
Fully interactive VR software also has extremely high
performance demands, so some form of model optimization is
required when bringing BIM data into a VR environment. This is
one area where specialist VR consultancies earn their keep with
finely tuned processes for tasks like simplifying geometry, adding
lighting, fixing gaps in the model and culling objects that will not
be visible in the scene. Once the model is inside the VR
environment, things like materials, lighting, furniture and other
small details that make the VR experience feel real are added.

Figure 2: Enscape plugin of Revit
With the plugin, the user is able to quickly explore different
design options and present projects to clients. If the client wants
to see alternative solutions or changes in the design, Enscape will
immediately show the changes the designer makes in the project.
With the option to create standalone files, is possible to send an
Enscape file to the clients or colleagues which allow the engineer
to quickly demonstrate the project. Enscape has become a
standard application in projects worldwide. As the Revit allows
the user can work over the model applying all the capacities of
modelling, consulting the information linked to the parametric
objects used in the model process, and obtain cuts over
perspectives allowing the analyses of the composition of all
elements of the BIM model. So the aspect of linking the
consulting capacity and the VR ability of walking around is a very
important improvement in the use of BIM methodology. To
improve VR experience Enscape can also be used together with
Oculus Rift.
In construction activity the use of Virtual Reality brings great
potentials. Presenting BIM models of projects in VR environment
redefines communication and collaboration in the field and in the
office. BIM technology and VR have the ability to innovate the
building industry. Collaboration in VR can be the future of VR
BIM. At a first glance, many feel the benefits of using a BIM
model with VR are purely for marketing leveraging the 3D model
for visual aids, but taking a deeper look at a BIM model it will
reveal many practical reasons to adopt BIM with VR.
CAVE-like platforms have been developed for immersive VR
experience as they track user’s head and control wand usually
with 6 degrees of freedom, to navigate inside the virtual

Compared to dedicated AEC design review software like
Navisworks or Tekla BIMsight, equivalent tools inside a VR
environment are still very much in their beginning. This is
particularly true of game engine VR experiences, where the onus
tends to be on presenting a polished vision of a proposed
building, rather than delivering practical tools for solving real
world design and construction problems. When moving from
Revit to Autodesk Stingray, data is not only retained, but users
can click on objects and view the underlying attribute
information. In VR mode, users can click on an object to view
information, including metadata.
In view of growth of Building Information Modelling in various
EU Member States, ACE has established in 2015 a work group to
look at the legal, technical and financial issues surrounding the
advent of BIM, develop its policy and engage with work to
develop an European (CEN) standard. The Conference ''BIM IN
EUROPE'' will present the results of two years of work of the
ACE BIM Work Group, together with interventions by the EU
BIM Task Group, CEN Committee and BIM users. The event is
organized with the support of the Creative Europe Program of the
European Union. The next summit will be held in December in
Brussels, Belgium.

4. CONCLUSIONS
For architects and designers, VR+BIM enable them to better
communicate design. A challenge for architects is that of
communicating concepts and visions for buildings. The advantage
to using VR is in the communication of ideas, concepts and the
vision for their building. This enables all the parties to more
quickly reach a full appreciation of the building plan. When
everyone shares a common understanding of the design, the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 19

project is executed more efficiently from the outset. This current
BIM with VR topic require dissemination; application in real
cases and pointed out, in reports, achievements and limitation;
following the technologic advances that supports the BIM use and
the visualization of data, in real time while the interacting with the
model made possible by VR technology. BIM + VR provide an
opportunity to analyze and explore BIM models within virtual
environments.

5. REFERENCES
[1] Eastman, C., Teicholz, P., Sacks, R. and Liston, 2011, K,
BIM Handbook: A Guide to Building Information Modeling
For Owners, Managers, Architects, Engineers and
Contractors. Publisher John Wiley & Sons Inc, Publication
date 18 May 2015, Edition Statement 2nd Edition, ISBN10
1118942760, ISBN13 9781118942765.

[2] J. Wolfartsberger, J. Zenisek, C. Sievi, M. Silmbroth, 2017,
A Virtual Reality supported 3d environment for engineering
design review, in: 2017 23rd International Conference on
Virtual System Multimedia (VSMM).
[3] Kieferle, J., and Woessner, U. 2015. BIM Interactive - About
Combining BIM and Virtual Reality, A Bidirectional
Interaction Method for BIM Models in Different
Environments, Proceedings of the eCAADe 33, 14 – 15
September, 2015, Vienna, Austria, 69-75.
[4] Breen, J., 2017, Design and Build Better with VR and BIM,
Proceedings of the Auckland Build 2017
[5] Enscape plugin of Revit https://enscape3d.com/

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 20

New collaborative game experiences,
the example of “Game Jockey”
Rémy SOHIER

INReV EA 4010 – AIAC
University Paris 8
2 Rue de la Liberté
93526 Saint-Denis France
+33 1 49 40 71 12

remy.sohier@gmail.com

ABSTRACT

We present a research on the possible interactions between the
disk jockey and video game creation. After a description of the
practice of disk jockey, we propose a theoretical model applied to
the video game we call: game jockey. Based on this model, we
present and analyze three creations. The last one offers an
interactive interface allowing to mix elements characteristic of
other games with activated clips.

Keywords

Video game, remix, jockey, game master, research-creation,
technologies, visual interface, digital, interaction

1. INTRODUCTION

Our approach is part of an artistic desire to hybridize the practice
of disk jockey (DJ) with video game creation. We observe that the
disk jockey practice proposes a creative method based on the mix,
using technologies, allowing an exchange with the public that
could renew our way of creating video games. We call this new
form of DJ-inspired video game creation: the Game Jockey.

together." [1]. The practice is of interest to researchers who do not
hesitate to develop new technologies and uses[2]. We join this
enthusiasm for the practice of disk jockey: to have more or less
complex instruments to produce an aesthetic experience for an
audience, while exploiting a library of sounds selected in advance.
To define the practice of the game jockey, we need to define the
basis of the video game mix. For this we rely on two conceptions
of video game from a fragmentary point of view[3] : the game
design pattern[4], allow us to consider models common to games
that can be exchanged, and the game feel[5], allow us to consider
the specific interactions of video game through variables that can
evolve over time.
From these references we propose the following game jockey
model: it is a practice where a jockey with technological tools
mixes patterns and the game feel in live performance for an
audience of players. This performance evolves with the
interactions between the three components: the product mix, the
jockey and the players (see Fig. 1).

It is in this context that we ask ourselves how to proceed to carry
out this hybridization: what characteristics could be important in
the creation of video games for mixing? How to distinguish
players from a jockey in a video game device? What kind of video
game could be produced with the game jockey?
We will answer these questions on the basis of a research-creation
with 3 experiments trying to reproduce the DJ model for
videogame creation. First, we propose a description of the practice
of disk jockey and the examination of our video game creation
needs, so we will be able to formulate a model for the Game
Jockey, on which our three creative experiences will be based. In
the end, we will be able to offer a software solution to produce a
DJ-like experience applied to video games creation.

2. MIX THE DISK JOCKEY WITH THE
CREATION OF VIDEO GAME

The disk jockey is a musical performance activity on stage where
a person (the jockey) produces a mix (selection of musical parts)
using more or less complex technological instruments, in
interaction with an audience. To use Frank Broughton's
expression: "DJs track down greatness in music and squeeze it

Figure 1. Diagram of Game Jockey model.

3. CREATION EXPERIMENTS BASED ON
THE THEORETICAL MODEL OF THE
GAME JOCKEY
3.1 First experiment of variations of the game
in live performance

To experiment with our theoretical jockey game model, we started
with two creations: Avoid and Starship.
Avoid is a video game with 1 to 6 players in competition where
the goal is to avoid obstacles and survive the longest (Fig. 2).
Players have two ground buttons that allow them to move the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 21

character to the left or right. We have integrated a limited practice
of the jockey through an operator next to the screen with a mixer
allowing to modify several parameters of the game: visual
elements, speed of movement of the obstacles, frequency of their
appearance, size of the obstacles.
Starship is the second video game with 1 to 6 players in
cooperation where the goal is to get the most points by destroying
opponents who can also destroy us (Fig. 3). To do this, players
have 4 buttons to generate a different attack and two buttons to
move the players' starship to the left or right. In this way, players
contribute together to the control of the starship. It should be
noted that the button can have a specific color programmed. The
color of the button change with gameplay. So, each time the
starship is touched by computer-controlled opponents, the
function and the color of the buttons is reversed, requiring players
to communicate with each other to define a new role (verbally or
with gestures). If the sound is too lound during the performance,
players use gestures with arms or jump to precise the new role of
buttons. Like Avoid, we have integrated a Jockey figure that can
intervene on visual and gameplay elements by modifying the size
and speed or the characters' appearance. We have also enhanced
this function by saving the game jockey mix, so that we can reuse
them in live performance or modify them accordingly.

producing the mix) and the players (performers playing the mix
produced by the jockey).
From the last jockey game system, we were able to demonstrate
that it was possible to practice a mix of game elements that could
come from different origins. The choice to develop a solution
separate from the original set allowed us to avoid being locked in
the modification of a pre-existing set.

Figure 2. Avoid ingame (left) and its diagram (right)

3.2 Mix variables in live performance with
activatable clips

Although the first two experiments allowed us to experience a
jockey practice through live performance, we noticed that it was
mainly a modification of the values of a pre-existing game. It is
through our third and final creation that we will move closer to
this objective thanks to the complexity of its mechanism. It is
always a cooperative game called Rez-jockey for 1 to 4 players
where a series of opponents must be destroyed. Nevertheless, the
environment, opponents or possibilities of action will be able to
be changed in a more profound way by the jockey.
The jockey device consists of a mixer, a Launchpad and a
computer with a gameplay mixing application. The application is
inspired by FL Studio's Live Performance: we organize clips
(finished sound recordings) that can be activated and deactivated
at any time. We have reproduced this principle and adapted it to
the real-time of video games in Unity 3D (Fig. 4). We found very
interesting the idea that clip playback could be done in an
intelligent way (playing clips in sequence, random playback, onetime playback, etc.). Thanks to this process we were able to record
events that signal to the game our desire to make this or that
element of the set appear, to make this or that amount of opponent
appear, to change the function of the players, etc.
To make our mix, we considered that each clip was like a
characteristic element of other games: the way to move around in
one game, the appearance of sets in another game, the rule system
specific to yet another game, etc. This live video clip choice
brings us closer to the attitude of the disk jockey, applied to video
game creation.

4. CONCLUSION

The game jockey is a live performance device involving three
components: the mix (result produces performance, perceived as
new game), the jockey (performer using technological tools

Figure 3. Starship ingame (left) and jockey interface (right).

Figure 4. Game jockey with activated clips.

5. REFERENCES

[1] Broughton F. and Brewster. 2003. How to DJ right. The art
and science of playing records. Grove Press, New York. P.
11
[2] Beamish T., Maclean K. and Fels S. 2004. Manipulating
Music : Multimodal Interaction for DJs. Proceeding CHI ’04
of the SIGCHI Conference on Human Factors in Computing
Systems. ACM.
https://doi.org/10.1145/985692.985734
[3] Lenhart, I. 2013. Defragmentation and Mashup : Ludic
Mashup as a Design Approach. Proceeding of the 2013
DiGRA International Conference: DeFragging Game Studies.
[4] Björk S. and Holopainen J. 2004. Patterns In Game Design.
Charles River Media, Boston.
[5] Swink S. 2008. Game Feel: A Game Designer’s Guide to
Virtual Sensation. Morgan Kaufmann, Burlington.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 22

Effects of a Physical Prop on Torso Movement in a Virtual
Reality Shooter
Anders Hansen

Aalborg University
Aalborg, Denmark
ahans15@student.aau.dk

Helene H. Nielsen

Martin Viktor

Aalborg University
Aalborg, Denmark
mvikto13@student.aau.dk

ABSTRACT
Lower back pain is becoming an increasing problem due to low
physical activity and prolonged sitting. For the purpose of rehabilitation, this project measures torso movement of different shooting
methods in a Virtual Reality game and tests whether a physical
rifle prop increases movement and immersion. Four different shooting methods were implemented and compared to each other. Tests
were conducted on healthy participants with motion trackers placed
between their shoulders and on their lower back to measure the
difference in rotation between the two trackers. Furthermore, the
participants evaluated the shooting methods through a questionnaire where they ranked them in terms of fun and comfort as well
as picking the most immersive and overall preferred way of interaction. Test results show that the rifle prop was the most immersive,
fun and overall preferred. Analysis of the rotation data showed that
the prop increased rotation around the horizontal axis compared to
using the rifle without a prop. The single handed pistol increased
rotation around the vertical axis and increased a sidewise tilting
motion compared to the three other shooting methods. In conclusion the prop did increase immersion, but only increased rotation
around one axis. Further evaluation under different conditions are
needed in order to single out the best method for rehabilitation.

CCS CONCEPTS
• Human-centered computing → Interactive systems and tools;
Virtual reality; • Software and its engineering → Interactive
games; Software creation and management;

KEYWORDS
Props, Virtual Reality, Games, Exergaming, Low Back Pain, Rehabilitation

1

Kirstine B. Larsen

Aalborg University
Aalborg, Denmark
hhni14@student.aau.dk

INTRODUCTION

Lower back pain (LBP) is one of the world’s most common and
costly medical problems today [4, 16], and is mostly due to low
physical activity and prolonged sitting [4]. Advice for treating LBP
is to avoid bed rest, stay active and continue with daily activities.
Furthermore, medication like paracetamol can help relieve LBP
symptoms [12]. To help with treatment of LBP, physiotherapists
may prescribe an exercise regime for the patient. It is, however,

Aalborg University
Aalborg, Denmark
kbla15@student.aau.dk

Martin Kraus

Dep. of Architecture, Design & Media
Technology, Aalborg University
Aalborg, Denmark
martin@create.aau.dk
important to individualise the needs of the patient, as change of
lifestyle can be challenging and the dropout rates lie around 50%
[28]. If patients experience pain during therapy, they may develop
a fear of movement due to pain known as kinesiophobia. This may
lead patients to refrain from movement and hinder their rehabilitation progress [32].
Games have become a prevalent form of entertainment in recent
years and are an interactive entertainment form popular with people of all ages [10]. This is also why the concept of "gamification" 1
has become popular, as it can be used to provide motivation to do
monotonous and repetitive tasks [2]. By overcoming the challenges
that games present to the player, who is immersed in the virtual
world, their body can release chemicals that make them feel physical and mental pleasure. This was seen in the game Dance Dance
Revolution which was discovered to have an impact on weight loss
and attitudes towards health and exercise [17]. Utilising movement
as interaction appears to be a good way to combine games and
rehabilitation. One emerging tool that uses the human body for
input is Virtual Reality (VR).
One popular genre of VR games are first person shooters (FPS). In
real life, there are different stances and different ways to hold a gun.
Especially two-handed guns are challenging to simulate in VR. One
study [19] tried to make a controller that can swap between one
and two-handed guns. The video "Guns in VR"[3] claimed that twohanded guns with scopes, e.g. a sniper rifle, are the most difficult
guns to simulate realistically in VR. The reason being that people
aiming with a scope rely on their shoulder to stabilise the gun and
help them to aim. To overcome this challenge, we built a prop that
can be used alongside VR, both to encourage torso movement and
to increase immersion to combat kinesiophobia.
Using this as inspiration, this paper explores the idea of implementing a VR FPS game and evaluating different shooting methods
and immersion, for the purpose of rehabilitation.
Our hypothesis is: A two handed physical prop will encourage torso movement and increase immersion in Virtual Reality
The following section discusses VR as a rehabilitation tool and
summarizes previous works with shooters and props.

1 Using

video game elements in non-game related contexts [8, 9]

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 23

2 BACKGROUND RESEARCH
2.1 Virtual Reality as a Rehabilitation Tool
VR has been utilised as a means to distract people from kinesiophobia during physical rehabilitation. A VR game called ’Cryoslide’
was tested against patients’ own techniques of distracting them
from their chronic pain. The test was conducted on 20 participants,
10 using their own techniques such as reading or playing mobile
games, while the other 10 played ’Cryoslide’. To gather data, a
questionnaire of pain intensity was used. The results showed that
participants reported a 36.7% lower pain intensity and thought
about their pain 56% less [18] while playing ’Cryoslide’. Pain intensity in VR was also tested in another study on a patient that
suffered severe trauma in an accident and had to go through physical therapy. The patient went through regular therapy sessions
with a within-subject design study, where the patient tried therapy
with and without VR. Once each session had been completed, the
patient answered three pain rating scales, ranging from 0 to 10.
Here the patient reported feeling less pain and unpleasantness in
VR, while thinking about pain less and having more fun [14].
A recent study on VR’s impact on pain perception and exercise
benefits showed that people report feeling less pain and having to
put in less effort while using VR, resulting in reaching exhaustion
slower than the non-VR group it was tested against [23]. This study
used verbal pain and perceived exertion ratings, while also using
a self-report questionnaire to evaluate immersion. VR has created
new possibilities for combining technology and exercise, and these
are evaluated by the Virtual Reality Institute of Health and Exercise
which has two professors in Kinesiology on their team [27].
This background research shows that the benefits of VR on physical fitness are still being evaluated, but showing a lot of potential
for countering kinesiophobia while improving patients’ overall
experience of physical therapy.

2.2

Virtual Reality with Physical Props

Some examples of commercially available VR FPS are "Pavlov VR"
[7], "Hover Junkers" [33] and the upcoming "Space Junkies" [30].
Two studies [20, 21] took a FPS game and compared it across VR
and PC gaming. Another study on FPS games showed favourable results towards props in VR and tested a self-transforming controller
that was meant to simulate a pistol and rifle in-game [19]. The
prop had a formed grip, similar to that of a pistol, and the length
of the barrel was adjustable, transforming between a rifle and a
pistol. Furthermore, weight had been added to the prop, making
it 3.5 times heavier than the HTC Vive controller. The prop was
evaluated by 29 players, split into two groups. One group played a
game using the standard HTC Vive controllers first, while the other
group started with the self-transforming controller. To understand
what the participants thought of the solution, they were asked to
answer several questionnaires – the Game Experience Questionnaire, Player Experience of Need Satisfaction Questionnaire, Igroup
Presence Questionnaire, Consumer Products Questionnaire and
last Device Assessment Questionnaire [19]. Performance and ingame behaviour was also logged. The results showed an overall
preference for the self-transformable controller. The study also reported on fatigue levels being very low. However, the participants
only went through 4 minutes of play time before answering five

questionnaires, and then proceeded to play 4 more minutes. This
amount of play time may not be enough to evaluate fatigue levels
as players rarely play games for 4 minutes at a time. The idea of
using props in a VR environment has been explored numerous
times. One study tested physical objects’ impact on how realistic a
virtual environment felt. The study was conducted on two groups
of people. One group used physical objects, while the other did not.
The results showed that the participants using physical objects considered the virtual environment to be more realistic and simulating
laws of physics [13].
These studies show that props make the virtual environment
seem more realistic. When it comes to kinesiophobia, distraction,
from the real world is essential to lower attention on pain. Therefore, props could potentially immerse users such that they pay less
attention to their fear of pain.
The next section describes our inspiration, design and implementation for a VR FPS game.

3 THE VIRTUAL REALITY SHOOTER
3.1 Stances, Hand Positions & Props
Based on preliminary research, we decided to implement a shooter
game in VR to encourage movement of the torso. As previously
mentioned, there are different stances and hand positions for different weapons.
For shooting pistols, there are several stances. Two of the most
commonly mentioned are the Isosceles and Weaver stance. The
Isosceles stance received its name because the arms and chest form
an isosceles triangle, where two sides are the same. The shooter’s
body should face the target, holding the pistol in front of the middle
of their chest and their feet at a shoulder-length distance from
each other, with the toes facing the target [15, 29]. In the Weaver
stance, the non-dominant foot is placed slightly ahead of the other
foot for support. The arm accompanying the hand on the trigger
is stretched out, elbow locked, while the other arm is bent at a 45
degree angle [15, 29]. Pistols can also be fired with one hand. The
recommended stance for this is the Power Point stance, where the
shooting arm is stretched out, and the gun-side foot is placed in
front of the other [5].
When shooting rifles, hunters stand with their non-dominant
foot in front of the other, their non-dominant hand holding the
forestock of the rifle [24]. In a tactical situation for soldiers, the
stance is similar but deeper, meaning their knees are bent and their
upper body is slightly leaning forward [22].
As research encourages you to turn your body in the direction
you are shooting, and not just twist the torso, we decided to test
whether this translates to players in VR. In order to see if people
would naturally do this, we conducted a small test on six participants to observe how much they moved their feet. We found that if
the enemies spawned within 180 degrees, the players barely moved
their feet and instead rotated their upper body. Furthermore, it was
observed that some of the participants did not hold the guns out in
front them to aim, which led to lack of rotation. This was due to a
laser sight assisting their aim. Upon this observation, we removed
the sight to encourage proper aim and torso movement.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 24

Figure 3: The 3D printed prop

3.2

Figure 1: Hand positions of gun with one (top) and two hands
(bottom)

Creation of the Virtual Environment

When creating the VR game, we considered in particular ethical
concerns surrounding games and the intended target group of the
solution. While games can cater to many people, it is far from
everyone that spends their free time engaging in games. Therefore,
the game should not cater to hardcore players but should be made
for casual players, and it should not include any gore despite being a
shooter game. We decided to refrain from creating a realistic shooter
game, and use a cartoonish design instead, which was based on a
tutorial for a shooter with plush toys as targets [31]. Furthermore
lighting was added using publicly available assets [11]. Figure 4
shows parts of the created virtual environment.

Figure 2: Hand positions of the rifle with no prop

The HTC Vive controllers, are ideal for gun shooter games as
it has a grip and a trigger. The use of the HTC Vive controllers as
guns can be seen in Figure 1.
When using a rifle, there is basically only one way to hold it,
which requires two hands. We implemented the rifle such that one
controller determined the position of the rifle in the virtual space
while the other controller determined rotation. That way people
had to use two hands to use the rifle in-game, as seen in Figure 2.
For the physical prop for HTC Vive controllers, we used a publicly available model for 3D printing [25]. Figure 3 shows the prop
with the Vive controllers attached.
The 3D printed frame was very light, which suited the solution
well as a physiotherapist recommended that weights should not be
used for physical therapy without supervision. In other cases, to
increase immersion, additional components may have been added
to increase the weight to that of a real rifle.

Figure 4: The virtual environment created with the Unity Assets
A physiotherapist informed us that without weights all types
of torso movement are encouraged. The priority is that patients
move. Therefore, we place the player virtually on a platform above
ground level. Enemies first spawn on the floor within a 180 degree
angle in front of the player to make them rotate from side to side.
The enemies then move towards the platform where the player
is standing. Next, new enemy types spawn; these fly and spawn
in a vertical line in front of the player to make them aim up and
down. Finally, the enemies spawn both vertically and horizontally
to make the player rotate diagonally. The three different enemy
types can be seen in Figure 5. While this is essentially supposed to
be a game, there were some game aspects that we did not include.
For example, the game does not have a lose condition. We decided

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 25

that this should not be implemented as it would disrupt our data
collection when comparing torso rotation between the different
weapons. To create a goal for the players, we allowed the players
to see their accuracy and score in the game.

Figure 6: Evaluation Setup
Figure 5: The different enemy types

4

TESTING MOVEMENT IN VIRTUAL
REALITY
4.1 Pre-Test Meeting With Physiotherapist
A preliminary test was conducted with a physiotherapist. She gave
us feedback regarding the system as a tool for rehabilitation. She
also talked with us about the different movements that she observed
being encouraged and gave input on how to motivate other important movements. In her opinion, the amount of torso rotation was
sufficient, but she was missing more tilting from side to side. She
did see potential for more than LBP patients.

4.2

Procedure

As the purpose of this study was not to evaluate a VR rehabilitation
system, but to evaluate how much people move in VR with different
types of weapons and hand positions, the most important data is
from the trackers. For this, no interviews were needed and the
data is purely quantitative. However, while the movement is the
main aspect of this, the trackers cannot measure which weapon the
participants preferred to use. If the participants do not enjoy using
the weapon, there is a chance that regular use will be discontinued.
Therefore, a questionnaire about weapons was included in the
evaluation. Additionally, we were interested in why the participants
ranked the weapons the way they did, so this was asked after the
participants had filled in the questionnaire and noted down.
The test was conducted using a repeated measures design [26] –
each participant went through all the weapons. This can sometimes
lead to confounding variables, undesirable results due to fatigue,
practice effect etc. Therefore, counterbalancing was used and the
participants used the weapons in different orders.
The participants were a mix of students, some were from different
semesters of Medialogy and others were random participants picked
out at the University from other studies. Their experience with VR
also varied, as some had worked with VR as part of their study and
others had never tried it.

The evaluation was conducted in a lab room at the university.
The setup of the room can be seen in Figure 6. Between the two test
days, the room was re-calibrated, thus the facing direction changed.
The system used for testing had a 6GB 980Ti GPU, with a i7-4770
CPU running at 3.4GHz. The computer had 32GB of RAM running
at 1333 MHz. An HTC VIVE was used as the head mounted display
(HMD), running at 90Hz on two 1080x1200 pixels screens.
The participants were informed that they would go through two
different weapons, and two different ways of using each weapon
and they would not be evaluated on their performance, but if they
wanted to, they could look at their accuracy in the game.
The observers helped the participants strap on the two trackers.
As seen in Figure 7, one tracker was strapped between the shoulder
blades, and another on the lower back.

Figure 7: Tracker placement
Once inside the game, the participants were asked to look straight
ahead for calibration of the system and given instructions on how to
aim with the rifle and with the pistol. Before starting the actual test,

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 26

the participants had a chance to practice on targets. This starting
scene can be seen in Figure 8. When ready, the participants would
initiate the test themselves by shooting a "Play" button and the
tracking of their torso movement would be logged. Between each
wave of enemies, the observers would assist the participants in
changing weapons.

last because they felt fatigued when using them. Two participants
commented on the size of the rifle prop, being either too small or
too big. For one participant, who was significantly taller, the prop
lagged a lot due to the positioning of the HTC Vive Lighthouse
stations in the room. This participant also said that this influenced
ranking of the prop.
4.3.2 Tracker data. In the tests between the x-means, which is the
average of the subscales for the horizontal rotation axis, for the
overall rotation over time, it was evident that there was a significant
difference between using the rifle with a prop and using the rifle
with no prop with a respective p-value of 0.030. When compared to
the boxplot seen in Figure 9, it is clear that the prop has an advantage over these two conditions. There was, however, no significant
difference between using the prop and using the pistol with one
hand.

Figure 8: Tutorial before the test
The participants were asked to fill out a questionnaire after the
test about their experience with the different weapons and hand
positions. They were asked to rank the weapons from most fun to
least fun, most comfortable to least comfortable, and to pick which
one felt most immersive, and which one was overall preferred to
play with. Once a participant had filled out a questionnaire, they
were asked why they ranked the weapons the way they did.

4.3

Results

The open-source tool RStudio [1] was used for a statistical analysis
of the tracker data. We used boxplots to get an idea of how the
data looks and the Wilcoxon Signed Rank tests for comparing the
results [6].
4.3.1 Questionnaire data. The rankings by the participants varied
depending on whether they were asked about comfort or fun. There
was no significant difference between the rankings of fun. However,
the rankings of comfort showed that there was a significant difference between the two handed pistol and the rifle without a prop.
The pistol being most comfortable. When asked which of the four
types they felt was most immersive and which they preferred overall, 14 (58%) participants found the rifle prop the most immersive
which was significantly higher than the others. 12 (50%) participants preferred to use the prop, which also showed a significant
difference.
During the testing and when asking the participants why they
ranked the weapons the way they did, there was a wide variety
of answers. One participant said that the no prop rifle felt like
holding a bow and arrow. Here the participant was referring to
how shooting with bow and arrow is often implemented in VR:
one hand to hold the bow and aim, and one to pull the arrow back.
This participant also preferred the pistol with one hand, and said
they felt like Tomb Raider’s Lara Croft when shooting. Another
participant wanted to try it out with two pistols rather than just one.
During game play, it was noted that a participant said the solution
felt accurate and responsive. Two participants ranked a weapon

Figure 9: Boxplot over the overall rotation over time on the
horizontal axis for all the conditions.

In the analysis for the y-means, which is the average of the
subscales for the vertical rotation axis, there was only a significant
difference between using the pistol with one hand or with two
hands with a respective p-value of 0.021. When considering the
boxplot in Figure 10, it is clear that compared to the two other
conditions, rifle with no prop and rifle with prop, using the pistol
with one hand slightly increases rotation.
In the analysis for the z-means, which is the average of the subscales for the side-to-side tilting, there was a significant difference
between using the pistol with one hand and using the pistol with
two hands with a respective p-value of 0.0037. There was also a
significant difference between using the pistol with one hand and
using the rifle with no prop with a p-value of 0.014. It can be seen
in the boxplot in Figure 11 that the pistol is in advantage over these
two other conditions. In comparison with using the prop, they seem
to be equal in the boxplot with respect to the median, but the upper
quartile is higher for the pistol.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 27

5.2

Figure 10: Boxplot over the overall rotation over time on the
vertical axis for all the conditions.

5.3

Figure 11: Boxplot over the overall rotation over time of sideto-side tilting for all the conditions.

5 DISCUSSION
5.1 User Questionnaire
On the 2nd day of the test, the tracking system had been re-calibrated
such that participants were facing a different direction. Unfortunately, this caused a lot of issues with the prop, as the tracking was
considerably worse. Despite that, the prop was still considered the
most immersive and it was the overall preferred interaction. The
prop was ranked third in comfort. This could be due to the frame
not being adjustable. One tall participant said the prop felt small
and unnatural to hold. The opposite was the case for a smaller participant, who felt that the prop was too large. This could potentially
influence how comfortable the participants felt the prop was to use.
When we asked the participants why they ranked the weapons the
way they did, two participants said they felt fatigued using this type
of interaction. It was noted that it was two different conditions but
both were the last condition they went through. This shows why it
was important to use counterbalancing of confounding variables,
as this influenced how some participants ranked the weapons.

Tracker Data

The results showed a significant difference in up and down rotation
between using the rifle prop and the rifle with no prop, as well as
the two handed pistol, however there was no significant difference
between the rifle prop and the single handed pistol. In regards to
sidewise tilting and left and right rotation, the single handed pistol
made people rotate significantly more than using the pistol with
two hands. The results of the tracker data suggests that different
weapons may encourage different types of movement, which could
be useful for focused exercises.
As this was a game, we wanted the players to be immersed,
and keep them on their toes. Therefore, we made the enemies
move towards the player as a means of motivation. However, this
created unreliable variables, as rotation, especially up and down
rotation, depended on the distance between the player and the
enemy. We are uncertain whether this influenced the results as
people tended to attempt to shoot the enemies as fast as possible.
However, sometimes enemies would make it all the way to the
platform that the players were standing on before being taken out.
This may have skewed our results. A more static environment for
testing rotation may have been better as we would be in full control
of the variables.
Overall, the subjective preference of weapons ranked by the
users, does not match well with what the trackers say encourage
the most rotation. The prop which won the overall preference and
immersion only provided significant torso movement on one axis
in comparison to not using the prop and the pistol with two hands.

Future Work

In order to fully determine which shooting method would be most
suitable for rehabilitation, further testing is needed with static targets and also in collaboration with experts. Based on the interview
we had with a physiotherapist, there is much room for improvement
for the game mechanics to encourage more types of movement in
the game, specifically sidewise tilting of the torso, as the therapist
felt that it was missing from the current iteration of the game. This
could be included by implementing enemy types that throw things
at players’ heads, forcing them to dodge the incoming attacks.

6

CONCLUSION

The purpose of this study was to test torso movement in VR using different gun shooting methods and evaluate whether a prop
increases immersion for the users. Our hypothesis was that a two
handed physical prop encourages torso movement and increases
immersion in Virtual Reality. The results showed that the prop
encourages increased rotation around the horizontal axis in comparison to the pistol with two hands and the rifle without a prop,
while a pistol held with one hand encouraged significantly more side
to side rotation and sidewise tilting compared to the two handed
pistol. However, regarding immersion, the users felt significantly
more immersed with the prop, and overall preferred to use this
shooting method. In order to determine whether immersion and
preference comes before movement efficiency, further tests under
different conditions are required.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 28

ACKNOWLEDGEMENTS
The authors wish to thank Bo Geng from the Aalborg University
Health Department for her help and supervision throughout the
project. We also wish to thank Mette Duus Hansen for collaboration
on this project.

REFERENCES
[1] Joseph J. Allaire. 14/12/2018. RStudio: Open Source and Enterprise-Ready Professional Software for R. https://www.rstudio.com/. Accessed:14.12.2018.
[2] D. Basten. 2017. Gamification. IEEE Software 34, 5 (September 2017), 76–81.
https://doi.org/10.1109/MS.2017.3571581
[3] BrandonJLa. 2017. Guns in VR. Found at https://www.youtube.com/watch?v=
iYrkXK3V2ik, last accessed 13/12/2018.
[4] Eric R. Castillo and Daniel E. Lieberman. 2015. Lower Back Pain. Evolution,
Medicine, and Public Health 1 (2015), 2–3. https://doi.org/10.1093/emph/eou034
[5] Chris Christian. 14/12/18. Found at https://www.shootingillustrated.com/articles/
2016/4/5/top-five-shooting-stances/.
[6] Michael J. Crawley. 2015. Statistics : An Introduction Using R (2nd ed.). Wiley,
New York, USA.
[7] davevillz. 2017. Pavlov VR. Found at https://store.steampowered.com/app/
555160/Pavlov_VR/.
[8] Sebastian Deterding, Dan Dixon, Rilla Khaled, and Lennart Nacke. 2011. From
Game Design Elements to Gamefulness: Defining "Gamification". In Proceedings
of the 15th International Academic MindTrek Conference: Envisioning Future Media
Environments (MindTrek ’11). ACM, New York, NY, USA, 9–15. https://doi.org/
10.1145/2181037.2181040
[9] Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton O’Hara, and Dan
Dixon. 2011. Gamification. Using Game-design Elements in Non-gaming Contexts.
In CHI ’11 Extended Abstracts on Human Factors in Computing Systems (CHI EA ’11).
ACM, New York, NY, USA, 2425–2428. https://doi.org/10.1145/1979742.1979575
[10] Michele D. Dickey. 2005. Engaging By Design: How Engagement Strategies
in Popular Computer and Video Games Can Inform Instructional Design. Educational Technology Research and Development 53 (2005), 67âĂŞ83. Issue 2.
https://doi.org/10.1007/BF02504866
[11] Flatriver. 2016. Retrieved from: https://assetstore.unity.com/packages/3d/props/
simple-wall-lamp-69411.
[12] Nadine E. Foster, Johannes R. Anema, Dan Cherkin, Roger Chou, Steven P. Cohen,
Douglas P. Gross, Paulo H. Ferreira, Julie M. Fritz, Bart W. Koes, Wilco Peul,
Judith A. Turner, and Chris G. Maher. 2018. Prevention and treatment of low
back pain: evidence, challenges, and promising directions. The Lancet 391 (2018),
2368–2383. Issue 10137. https://doi.org/10.1016/S0140-6736(18)30489-6
[13] Hunter G Hoffman. 1998. Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments. In Virtual Reality Annual
International Symposium, 1998. Proceedings., IEEE 1998. IEEE, Atlanta, GA, USA,
59–63.
[14] Hunter G. Hoffman, David R. Patterson, Maryam Soltani, Aubriana Teeley,
William Miller, and Sam R. Sherar. 2009. Virtual Reality Pain Control during
Physical Therapy Range of Motion Exercises for a Patient with Multiple Blunt
Force Trauma Injuries. Cyber Psychology & Behavior 12 (2009), 47–49.
[15] Sam Hoober. 14/12/18.
Found at http://gunbelts.com/blog/
3-basic-shooting-stances/.
[16] D. Hoy, P. Brooks, F. Blythc, and R. Buchbinder. 2010. The Epidemiology of low
back pain. Best Practice & Research Clinical Rheumatology 24 (2010), 769–781.
https://doi.org/10.1016/j.berh.2010.10.002
[17] Katherine Isbister. 2016. How games move us: Emotion by design. MIT Press,
Cambridge, Massachusetts, London, England.
[18] Weina Jin, Amber Choo, Diane Gromala, Chris Shaw, and Pamela Squire. 2016. A
Virtual Reality Game for Chronic Pain Management: A Randomized, Controlled
Clinical Study. Medicine Meets Virtual Reality 22 (2016), 154–160.
[19] Andrey Krekhov, Katharina Emmerich, Philipp Bergmann, Sebastian Cmentowski,
and Jens Krüger. 2017. Self-Transforming Controllers for Virtual Reality First
Person Shooters. In Proceedings of the Annual Symposium on Computer-Human
Interaction in Play. ACM, Amsterdam, The Netherlands, 517–529.
[20] Jean-Luc Lugrin, Marc Cavazza, Fred Charles, Marc Le Renard, Jonathan Freeman,
and Jane Lessiter. 2013. Immersive FPS games: user experience and performance.
In Proceedings of the 2013 ACM international workshop on Immersive media experiences. ACM, Barcelona, Spain, 7–12.
[21] Jeann-Luc Lugrinn, Fred Charles, Marc Cavazza, Marc Le Renard, Jonathan
Freeman, and Jane Lessiter. 2012. Are immerrsive FPS games enjoyable?. In
Proceedings of the 18th ACM symposium on Virtual reality software and technology.
ACM, Toronto, Ontario, Canada, 199–200.
[22] S.W.A.T. Magazine. 14/12/18.
Found at http:
//www.swatvault.com/weapons-training-and-tactics/
the-fighting-stance-maximize-your-shooting-ability/.

[23] Maria Matsangidou, Chee Siang Ang, Alexis R Mauger, Jittrapol Intarasirisawat,
Boris Otkhmezuri, and Marios N Avraamides. 2018. Is your virtual self as sensational as your real? Virtual Reality: The effect of body consciousness on the
experience of exercise sensations. Psychology of Sport and Exercise 7 (2018).
[24] Texas Parks and Wildlife. 14/12/18. Found at https://tpwd.texas.gov/education/
hunter-education/online-course/shooting-skills/rifle-positions.
[25] SGU7. 2016. G36. Found at https://www.thingiverse.com/thing:1828084.
[26] Martyn Shuttleworth. 2009. Repeated measures design. Experiment Resources.
Found at https://explorable.com/repeated-measures-design.
[27] Aaron Stanton and Sidian Jones. 2017. Found at https://vrhealth.institute/.
[28] Marek Szpalski, Robert Gunzburg, BjÃűrn L. Rydevik, Jean-Charles Le Huec, and
Michael Mayer. 2010. Surgery for Low Back Pain. Springer, Berlin.
Found at https://www.pewpewtactical.com/
[29] Pew Pew Tactical. 14/12/18.
shooting-stance-grip/.
[30] Ubisoft. 2019. Space Junkies. Found at https://store.steampowered.com/app/
647590/Space_Junkies/.
[31] UnityTechnologies. 2018. Retrieved from: https://assetstore.unity.com/packages/
essentials/tutorial-projects/survival-shooter-tutorial-40756.
[32] Johan W.S. Vlaeyen, Jeroen de Jong, Mario Geilen, Peter H.T.G. Heuts, and Gerard
van Breukelen. 2011. Graded exposure in vivo in the treatment of pain-related
fear: a replicated single-case experimental design in four patients with chronic
low back pain. Behaviour Research and Therapy 39 (2011), 151–166. Issue 2.
https://doi.org/10.1016/S0005-7967(99)00174-6
[33] Stress Level Zero. 2016. Hover Junkers. Found at https://store.steampowered.
com/app/380220/Hover_Junkers/.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 29

MITMI Man-In-The-Middle Interaction
The human back in the loop
Cédric Plessiet†

Jean-François Jégo

cedric.plessiet@univ-paris8.fr

jf.jego@gmail.com

INREV AIAC(EA4010)
Université Paris VIII
Saint-Denis France

INREV AIAC(EA4010)
Université Paris VIII
Saint-Denis France

ABSTRACT
"The medium is the message" from McLuhan [13] showed
and still shows how the medium influences how the message is
perceived. This article proposes a new model for computer-human
interaction named MITMI for (hu)man-in-the-middle Interaction in
the context of digital art. This approach questions the concept of
mediated interaction by placing the human as the driver and as the
"translator" of the interaction process from a theoretical point of
view. Indeed, mediated interaction is based on the fact that two
humans are able to interact through a technological device [5]. We
develop the idea of putting human back as an intermediary between
the user/spectator and the technology. Our model thus introduces a
way of conceptualizing interactive digital artworks, where the artist
considered as an operator and creator can also take a central
position and is no longer the only producer of the work taking thus
entirely part of the artwork. We first explore the need to put the
human back in the loop, then we will describe how the principles
of mediated interaction help in defining the MITMI model. We will
stress this model through several installations and interactive
artworks based on the idea of shifting the artist position from the
creator, to the mediator between the real world and the virtual
world. Beyond the artistic approach, this model proposes to
redefine and to explore new modes of co-creation or mediation,
where digital media supports human-to-human interaction.

Keywords

computer-human-interaction,
interactive
digital
performance, computer-mediated interaction, mixed reality

1

art,

INTRODUCTION

McLuhan [13] showed and still shows how the medium
influences the message’s perception in his quote "The medium is
the message.” Crowley et al. [5] also consider media as a form of
extensions of man. In the creative process, despite new
technologies offers more degrees of freedom regarding immersion
and interaction, we identify the need to question back this
consideration about technology. Following these reflections, we
propose to develop the idea of putting human back as an
intermediary between the user/spectator and the technology

theorizing a new model for computer-human-interaction in the
context of digital art.

2

HUMAN IN THE LOOP IN DIGITAL
CREATION

2.1

Computer-mediated interaction

The approach of placing human in the loop in digital creation
questions the concept of mediated interaction by placing the
spectator or user as the driver and, so, as the "translator" of the
interaction process from a theoretical point of view. Computermediated interaction is based on the fact that two humans are able
to interact together through a technological device [9] (Figure 1).
In the context of digital theater and performances, several studies
explore different models to assess the question of intersubjectivity
between real and virtual actors. The open source project The
machine to be another [3] is a typical use of computer-mediated
interaction where through head-mounted displays, a user is able to
see a first-person video with the eyes’ perspective of a second
person (the performer) who follows the user’s movements. This
experiment investigates the relation of identity or the empathy
using embodiment and virtual body extension.
De Loor et al. [7] proposed a model which uses artificial
models based on enactive considerations for the digital theater play
Il était Xn fois. They design ontogenetic mechanisms for complex
dynamic systems which were guided by people. The authors report
their model addresses the fact that man can "learn from the
machines which can learn in return" providing a clear parallel with
the concepts of co-evolution and co-constitution of
phenomenology.
Batras et al. designed a Virtual Reality model for
improvisation in digital theater [2] that connects a real actor activity
to a semi-autonomous agent switch from imitation to improvisation
behaviors with the real actor. In this specific case, the actor is in a
situation of computer-mediated interaction with himself as a
deforming mirror for improvisation.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 31

.
interacting. In the next section, we present several digital art
artworks described using the MITMI model. The different types of
interaction involved presented here allows to reconsider the link
between the artist/performer, the public and the technology.

Figure 1: computer-mediated interaction between two humans

2.2

The MITMI model

We could see through these interactions an opposite model of
the "mediated interaction" paradigm [9] where the computer tool
serves as a communication channel between humans (Figure 1). In
the cases described above, the exchanges are very different. Indeed,
if in the first case the computer tool serves as an intermediary,
whereas in our case a person is acting as an intermediary between
the user and the machine. We call this interaction model MITMI
(Figure 2), for Man-In-The-Middle Interaction, in reference as the
Man-In-The-Middle attack (MITM) in computer security where the
attacker secretly deforms the communication between two parties
who believe they are directly communicating with each other [9].
This type of interaction leads, of course, to the notion of
interpretation of the intermediate link also so-called "telephone
game” [4] where misunderstandings accumulate in the retellings
creating usually an interesting and significant difference between
the statement announced by the first player and the one from the
last player of the chain.

Figure 2: computer-mediated interaction between two humans
The Wizard of Oz technique [6] is part of this model and
could be seen as a special case of MITMI. It consists of proposing
an experience in which users interact with a computer system they
believe it is autonomous, but which is in fact controlled by a human.
This technique was used in the design experiments of an
improvisation Virtual Reality application [3] dedicated to digital
theater to understand how actors start an improvisation and feed it
through computer-mediated interaction, the actors were invited to
interact with a puppet (i.e. an entity manipulated from the outside)
without being aware that the virtual entity is not autonomous. The
user was therefore confronted with an entity controlled by a third
party.
There is a strong link between mediated interaction and the
MITMI model in the way that in both cases the device is at the
center of the interaction, the great difference with this model is that
the MITMI model takes into account the human interacting as a
whole and in "his presence in the real world" and not through an
interface that would restrict the perception of the user. Since the
computer is no more filtering interaction, this leads to a finer
perception and understanding of the "state of mind" of the human

3

MITMI APPLIED TO INTERACTION
IN DIGITAL ART

3.1

Theater Play

In the theater play Cassandre-Materiaux [12] the actress and
director C. Chabalier is performing the role of Cassandra from
Greek mythology. At the beginning of the play, she is immersed in
the world of the predictions of the god Apollo presented as a giant
virtual character (Figure 3).

Figure 3: Photography of the Stage in Cassandre-Materiaux
The actress is immersed and is interacting in the virtual world
on stage using a virtual reality headset and a Kinect camera to
detect gestures. The virtual god is also projected on an invisible
tulle in order to be made visible to the spectators but not from the
other actors on stage. She is therefore isolated in her own world
without any vision of the real stage, the other actors and the
spectators. In this case, the user-actor is observing a virtual entity
which look completely isolated in its world but is still interacting
with the actor and is the mediating interaction with the other real
actors on stage (Figure 4).

3.2 Installation and Performance
In the installation-performance Between The Lines [17]
presents the digital double of the performer M. Passedouet playing
a fortune-teller. This character was projected on a tulle in a closed
space which scenography represents the caravan of a fortune-teller.
The actress dressed in the same way as her digital double walks
silently through the gallery, leads the spectator to the caravan and
play with the crystal ball. Her digital double comes to life and
makes predictions. Here, the installation requires a mixed reality
technology and a performance of the artist (Figure 4).

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 32

these cases a woman) serves as a connector between the "real world
of humans" and the "virtual world” (Figure 4). In more
anthropological approach, we propose to apply this model to the
artistic installation Ganesh Yourself [11] from E. Grimaud, Z. Paré
and A. Dubos.

Figure 4: The MITMI model applied to the artworks
Cassandre-Materiaux and Between the lines

3.3 Artistic Video Game
Our approach also creates perspectives in video games. The
Asile Escape project (Figure 5) created by T. Fevrier and al. [8]
proposes an asymmetrical gameplay using virtual reality
technologies to create an escape game, i.e. a puzzle game where the
player must escape, played by two people. The plot takes place in
real old toilets of a university. One of the players is wearing a
straitjacket and equipped with a virtual reality helmet and is only
able to observe around him on the replica of the real room, the
second player is free to move and can use the VR controllers.

Figure 6: Robot of Ganesh Yourself [11]
This installation allows to control a robotic puppet where the
face of the user or the spectator interacting is rear-projected on a
Ganesh robot (Figure 6). Anyone can thus interpret the role of the
Hindu god Ganesh which is a hybrid entity between a man and an
elephant. It is interesting to note how different participants, both
those who dialogue with the robot, and the robot operators or
manipulators get caught up in playing the game of interaction as if
the robot was really Ganesh.

Figure 5: Asile Escape [8], scenography and VR point of view
The two players’ mission is to get out of the cell, despite the
fact that they share same the real and the virtual space, they have to
find different clues around them. The tied-up character immersed
see a distorted and nightmarish vision of reality through his reality
helmet and was able to see a certain number of clues, in return he
could not interact with his environment and therefore had to give
instructions to the unsubmerged player.
Each player was locked in his own plausibility bubble,
holding information inaccessible to the other, and they have to
dialogue to describe their respective bubble. Several immersed
players report they really felt like they were locked in their bubble
despite the presence of the second player with whom they were able
to talk. This extreme isolation can refer to a modern Pythia whose
visions are subjected to interpretation when confronted with reality.

3.4 Interactive Sculpture
Thus, we can see in the two previous examples of CassandreMateriaux and Between The Lines how this man-in-the-middle (in

Figure 7: MITMI Model applied to Ganesh Yourself
This installation follows the previous works of the artist Z.
Paré conducting performance with the robot replica of the
researcher Hiroshi Ishiguro in his company Intelligent Robotics
Laboratory. There, the robot was controlled by an external
manipulator, which makes it "basically as a big puppet with
sensors" [10]. According to the Ishiguro regarding a female robot
he has created, "more importantly, we found that people forget that
she is an Android while interacting with her. Consciously, it is easy
to see that she is an android, but unconsciously, we react to the
android as if she were a woman" [18]. In this case, the figure of the
Man-In-The-Middle disappears in favor of putting the emphasis on
the manipulated robot. We refer here to the term "manipulactor"
(Figure 7) as the puppeteer who controls the device.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 33

.

4 DISCUSSION & PERSPECTIVES
4.1 Toward a co-creative process
From these examples, we now explore how the MITMI model
proposed for digital art application but it could be applied in CHI
prototyping. It could be for instance used as a new cultural
mediation system, by creating for example an exchange process
where each interlocutor equipped with more or less immersive and
interactive mixed reality technologies from AR to VR [14] can
confront their point of view in a co-creative approach. In the field
of movie production, new processes called previz-on-set allows to
previsualize the filename result of a movie (post production) from
the beginning of the production pipeline with the actors on set [15].
Usually, communication during the pre-production project does not
take place in the virtual environment. The OutilNum project [16]
proposes a collaborative approach (Multi-users and distributed
VR/AR/MR) where each of the interlocutors becomes the man-inthe-middle of the others, thus offering a co-creation space based on
interaction both on a real and virtual level. The MITMI model thus
allows us to acquire from each participant point of view new
elements of analysis.

4.2 Perspective for sharing
Beyond the artistic and collaborative axis, we could develop
the MITMI model as a mediation tool for sharing the use of new
technologies for specific audiences who could for instance not able
to use computer tools for sensorimotor or cognitive, cultural or
aging reasons. To do so, we could imagine enriching the model
with qualitative study or interviews refining the transformations
that take place between the user, the man-in-the-middle and the
computer tool. Our model thus enables both the ability to
understand the other as an empathy process sharing the point of
view and the ability to disappear behind the technology. This could
allow a progressive access to technology and understand its
potential but also the limits of its communication channels. It can
increase interaction as well as alter it, which in any case helps in
feeding improvisation and sharing creativity.

REFERENCES
[1]

Asokan, N., Niemi, V., and Nyberg, K. 2003. Man-in-themiddle in tunneled authentication protocols. In International
Workshop on Security Protocols, pp. 28–41. Springer, Berlin,
Heidelberg
[2] Batras, D., Guez, J., Jégo, J.-F. and Tramus, M.-H.. 2016. A
virtual reality agent-based platform for improvisation between
real and virtual actors using gestures. In Proceedings of the
2016 Virtual Reality International Conference VRIC, p. 34,
Laval, France, ACM
[3] Bertrand, P., Gonzalez-Franco, D., Cherene, C., and Pointeau,
A.,2014. The Machine to Be Another: embodiment
performance to promote empathy among individuals.
Celebrating 50 years of the AISB.
[4] Blackmore, Susan J., 2000. The Meme Machine. Oxford
University Press

[5] Crowley, D. J., and Mitchell, D. 1994. Communication theory
today. Stanford University Press
[6] Dahlbäck, N., Jönsson, A., and Ahrenberg, L., 1993. Wizard of
Oz studies—why and how. in Knowledge-based systems, 6(4),
pp. 258–266
[7] De Loor, P., Windelschmidt, C., Martinaud, K., and Cabioch,
V., 2010. Connecting Theater and Virtual Reality with
Cognitive Sciences: Positioning from computer science and
artist meeting. in Proceedings of Virtual Reality International
Conference (VRIC 2010), 7–9 April 2010, Laval, France
[8] Fevrier, T., Carré, D., and Cocat, G., 2016. Asile Escape.
Master création numérique parcours Arts et Technologies de
l’Image, Université Paris VIII Vincennes Saint-Denis
[9] Grebennikova Krasautsava, I., 2008. L’interaction médiatisée
à travers le chat comme dispositif sociotechnique. In Les
Enjeux de l’information et de La Communication, 2008(1), pp.
20–30
[10]Grimaud, E., 2014. Pour quelques secondes de confusion
ontologique Echecs et réussites du contact dans une expérience
de Télé-robotique. In La marionnette : Objet d’histoire, œuvre
d’art, objet de civilisation, pp. 143–168
[11]Grimaud, E., 2016. Ganesh Yourself. Retrieved from
http://www.filmdocumentaire.fr/4DACTION/w_fiche_film/47610_1
on
december, 20, 2018
[12]Guez, J. and Jégo, J.-F., 2017. Creating digital art installations
with VR headsets in Virtual Reality Headsets—A Theoretical
and Pragmatic Approach, Dir. Fuchs P. CRC Press
[13]McLuhan, M., and Lapham, L. H., 1994. Understanding media:
The extensions of man. MIT press
[14]Milgram, P., and Kishino, F., 1994. A taxonomy of mixed
reality visual displays. In IEICE Transactions on Information
and Systems, 77(12), pp. 1321–1329
[15]Okun, J. A., & Zwerman, S., 2010. The VES handbook of
visual effects: industry standard VFX practices and procedures.
Taylor & Francis
[16]Plessiet, C., Chaabane, S., & Khemiri, G. (2015). Autonomous
and interactive virtual actor, cooperative virtual environment
for immersive Previsualisation tool oriented to movies. In
Proceedings of the 2015 Virtual Reality International
Conference (VRIC 2015) pp. 1–4
[17]Plessiet, C., and Passedouet, M., 2012. Between the lines.
Galerie Crypt St Pancras – Londres - Royaume Uni ; City of
Women Festival — Lubjiana - Slovénie ; Galerie 51 rue de
Rivoli – Paris, France ; Zoom, espace de culture numérique Bourg en Bresse- France ; Ars electronica Festival -Linz Austria
[18]Whitehouse, D., 2005. Japanese develop “female” android.
Retrieved
from
http://news.bbc.co.uk/2/hi/science/nature/4714135.stm
on
december, 20, 2018

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 34

Emerging Affect Detection Methodologies in VR and
future directions
Ifigeneia Mavridou†

Maya Perry

Ellen Seiss

Centre of Digital Entertainment
Bournemouth University, UK
imavridou@bournemouth.ac.uk

Department of Neuroscience
University of Sussex, UK
maya.perry@outlook.com

SC2AN, Dept. Psychology
Bournemouth University, UK
eseiss@bournemouth.ac.uk

Theodoros Kostoulas

Emili Balaguer-Ballester

Department of Computing
Bournemouth University, UK
tkostoulas@bournemouth.ac.uk

Department of Computing
Bournemouth University, UK
eb-ballester@bournemouth.ac.uk

ABSTRACT
The uses of Virtual reality are constantly evolving, from
healthcare treatments to evaluating commercial products, all of
which would benefit from a better understanding of the emotional
state of the individual. There is ongoing research into developing
specially adapted methods for the recognition of the user’s affect
while immersed within Virtual Reality. This paper outlines the
approaches attempted and the available methodologies that embed
sensors into wearable devices for real-time affect detection. These
emerging technologies are introducing innovative ways of
studying and interpreting emotion related data produced within
immersive experiences.

Keywords
Virtual reality, Affect, Emotion, Emerging Technologies

1. INTRODUCTION
The applications for Virtual Reality (VR) are expanding rapidly,
from research and training facilities to entertainment and
healthcare. VR is no longer exclusive to laboratory settings as
recent technological advancements have brought low-cost
personal and portable VR headsets to the consumer market. This
allowed for the sale of 13.5 million headsets units in 2017 [1].
From a research perspective, VR provides the platform for
controlled experimental conditions while granting ecological
validity and content resources [2]. Consequently, it is expected
that more and more researchers will adapt VR for experimental
design and execution. This is evidenced by the fact that there were
over a million articles involving VR published in the last decade
alone [3].
Regardless of the technological advances providing real time
content interaction via input controllers, movement
synchronisation and body capture, the emotional state recognition
tools are only just emerging, due to the growing demand. The
potential applications of affect detection in VR are abundant.
With the visions of Affective computing in mind, emotionally
intelligent algorithms for VR applications could unlock new paths
for interactive realistic experiences, leading to a potentially better
understanding of the process of immersion and presence in VR.
Simultaneously, quantifying the state of the user can contribute to

medical and psychology related applications, either to identify
possible pathologies or to assist in the development of well-being
tools and health-care related solutions [4]. Most of these
applications require real-time data acquisition and vigorous
analysis from multichannel sources, which in turn requires further
technological and analytical advancement.

2. RELATED WORK
The nature of VR poses limitations to traditional affect detection
techniques. The Head-Mounted Displays (HMD) cover almost
two thirds of the user’s face, which prevents expression detection
via conventional camera tracking methods as well as the use of
additional, external modalities around the area of the face and the
head of the user. Additionally, due to the freedom of movement in
room-scale VR experiences, the use of limb-embedded
physiological sensors for affect recognition can be often erroneous
while also insufficient for affect detection. Explicitly defined and
adapted methodologies for affect recognition in VR are required,
considering the parameters of the wearability, usability and
comfort of the user while also the quality and value of the data
handled.

2.1 Affect Detection Methods
Please Understanding the emotional state of the user in VR could
assist in a range of use cases. It would aid real-time continuous
affect recognition and the awareness of the user’s state changes,
affective design and adaptive control of the surrounding
environment. Adaptive control is when specific signals can be
utilised to alter the environmental parameters, which in turn can
possibly alter the user’s affect, as a feedback loop. In research on
affective computing for real-time applications, most researchers
prefer the use of traditional emotion models, such as the
circumflex dimensional model [5]. This model is preferred over
others as the various affective states are illustrated within a 2dimensional space consisting of two primary axes; valence
(positive or negative polarity of affect) and arousal (the
excitement or intensity of the affective state).
Typically, the most common methods for systematic emotion
analysis include biometric signal acquisition (e.g. speech, facial
expressions, gestures, physiological signals) and analysis, in
conjunction with subjective ratings from users (i.e. self-reports)
and behavior-related observations [6]. However, an issue with the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 35

use of self-report techniques such as surveys, interviews and selfrating questionnaires is that they are reported to provide highly
subjective responses and therefore those responses can be variant
between participant results [7], meaning the data obtained often
does not correspond to the actual emotional experience and
concurrent physiological readings. This effect can be due to the
subjective nature of interpersonal and cultural differences when
rating emotion [8, 9]. Additionally, the incorporation of selfratings in VR settings, either verbal or visual, could impede the
user’s overall experience of presence and immersion while also
interrupting the narrative, or indeed any given task [10]. Ideally,
researchers and experience designers would benefit from the
combination of methods and the utilization of unobtrusive and
continuous, objective measures throughout a VR experience.
Apart from conventional unimodal methods such as camera
tracking or heart-rate sensors, recent software and hardware
prototypes have emerged that combine multimodal approaches
and affective read-outs specifically adapted for real-time
applications. Commercial technologies including, Emotiv Epoc,
LooxidLabs, Enobio, Neurable and EmteqVR [11, 12, 13, 14, 15]
have emerged in recent years to provide real-time emotional
feedback and affect recognition readings in VR. Although only a
small amount of studies using these technologies in VR are
published, we were able to gather some of the more relevant
findings as well as the practical implications of each technology.
Arousal detection in VR, and especially the detection of stress,
has been synonymised with analysing heartrate and electrodermal
activity (EDA) changes [16]. The Q sensor by Affectiva [17] a
wireless wearable biosensor has been used on a wide variety of
studies, including one which investigated the levels of stuttering
whilst in anxiety provoking VR environments. [18]. Although the
Q sensor is no longer available on the market, Affectiva has
designed and developed software solutions for affect detection,
offering a software development kit (SDK) for developers using
the Unity3D game engine [19].
For valence detection in VR, researchers and developers can
utilise technologies that incorporate electroencephalography
(EEG) sensors and/or electromyography (EMG) sensors. A recent
study aiming to assess emotional responses induced in virtual
reality found statistically significant correlations between the
reported valence and arousal picture ratings and the EEG bands
outputted from the Emotiv EPOC+ 14 channel EEG headset [20].
The system is light and easy to use, involving a short preparation
of hydration of the sensors before usage. A limitation when using
this headset alongside the HTC Vive VR system is the difficulty
of ensuring precise localization of the electrodes which can
increase variability of readings between participants but also
between sessions of the same participant. Therefore, the Emotiv
EPOC+ should be used in the correct context to ensure accurate
affect detection.
Additionally, a new wave of portable EEG devices designed for
gaming and VR purposes has emerged. The Neurable headset
combines EEG sensors with the HTC Vive [21] HMD to ensure
consistent localization, allowing user intent to be detected and
used as interaction input in virtual environments [22, 23]. Further
to this, Neurable have also developed an SDK for Unity 3D for
developers [24]. Combining the SDK with the ability to measure
gamma waves means there is potential for real-time affect
detection in VR, as it has been found that gamma waves correlate
with emotionality [25]. In 2016, a study examining the effect of

body ownership in virtual reality using a different EEG sensor
technology, Enobio (32 sensor set-up) noted that both augmented
and virtual reality produce higher brain activity in beta and
gamma waves than when present in the real world, which is
something to consider when using EEG sensors in Virtual Reality
research [26]. Another technology that came out in 2018 is the
LooxidLabs headset, which combines 9 dry EEG electrodes and
built-in eye tracking cameras into their own VR HMD.
Unfortunately, we have little evidence of the system’s accuracy of
detecting affective states as it has not yet been used in an emotion
related VR research study.
Currently, emotional valence is difficult to measure in room-scale
VR (non-seated experience) and the current EEG approaches may
add additional movement constrains to the user. The method of
measuring electromyographic signals (EMG) from the face of the
user in VR could give us a reliable indication of their affective
state [27]. In this context, another recent example of multimodal
affect detection technology is EmteqVR interface, whereby EMG
and Photoplethysmograph (PPG) sensors are embedded on a foam
VR insert, allowing it’s use on commercial head-mounted
displays (HMDs). Studies investigating the detection of valence
and arousal using this device have shown promising results [28,
29]. EmteqVR and the aforementioned technologies could be
improved further by the addition of eye motion tracking, to
monitor the individual’s gaze while in the virtual environment,
thus, allowing a fully rounded analysis of the individual’s
affective state when experiencing an emotional stimulus.
The technologies presented in this paper, showcase the growing
need for multimodal signal analysis to understand the user’s
emotional state in VR. As sensors become smaller and easier to
integrate, we expect a rapid growth of affect-detecting
technologies in the next years. The importance of heir unobtrusive
wear-ability and usability in VR is paramount for VR research, as
low levels of immersion and presence are correlated with
hardware related distracting factors and reduced freedom of
movement [30]. Ideally, researchers and developers in VR would
benefit from the combination of metrics for simultaneous arousal
and valence recognition, in user-centred hardware approaches that
promote free movement and easy integration with HMDs.

3. CONCLUDING REMARKS
The continuously evolving affect recognition technologies for
Virtual Reality are forming a strong new emerging technologies
category. This could go on to enable new avenues for
personalized experiences, user-centered interactions and wellbeing applications. The affect detection approaches, research
findings and limitations per interface are briefly discussed to
provide future directions towards further development of VRembedded biometric sensors for activity and affect recognition for
immersive technologies.

4. ACKNOWLEDGMENTS
This work is supported by the UK EPSRC via the Centre of
Digital Entertainment (“Emotion in VR”, Grant No.
EP/L016540/1).

5. REFERENCES
[1] Statistica. Global VR Sales (2017). Retrieved 2018 from
https://www.statista.com/statistics/752110/global-vr-headsetsales-by-brand/

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 36

[2] Thomas D. Parsons. 2015. Virtual Reality for enhanced
ecological validity and experimental control in the clinical,
affective and social neurosciences. Frontiers in human
neuroscience. 9, 660.
DOI= http://dx.doi.org/10.3389/fnhum.2015.00660
[3] Google Scholar. Google Scholar Virtual Reality Page (2018)
Retrieved from:
https://scholar.google.co.uk/scholar?hl=en&as_sdt=0%2C5&
q=virtual+reality&btnG=&oq=virt
[4] Sarah Consentino, Sessa Salvatore, and Takanishi Atsuo.
2016. Quantitative laughter detection, measurement, and
classification – A critical survey. IEEE Reviews in
biomedical engineering, 9, 148-162.
DOI= http://dx.doi.org/10.1109/RBME.2016.2527638
[5] James A. Russell. 1980. A circumplex model of affect.
Journal of personality and social psychology, 30, 6, 1161.
DOI= http://dx.doi.org/10.1037/h0077714
[6] Rafael A. Calvo and Sidney D’Mello. 2010. Affect detection:
An interdisciplinary review of models, methods, and their
applications. IEEE Transactions on affective computing 1, 1,
18-37. DOI= http://dx.doi.org/10.1109/T-AFFC.2010.1
[7] Tiffani Razavi. 2001. Self-report measures: An overview of
concerns and limitations of questionnaire use in occupational
stress research. Discussion Papers in Accounting and
Management Science, 1-175
[8] William F. Wright and Gordon Bower. 1992. Mood effects
on subjective probability assessment. Organisational
behaviour and human decision processes, 52, 2, 276-291.
DOI= http://dx.doi.org/10.1016/0749-5978(92)90039-A
[9] Daniel Kahneman. 2000. Evaluation by moments: Past and
future. Choices, values and frames, 693-708. Cambridge
University Press & Russell Sage Foundation, Cambridge.
[10] Bob G. Witmer and Michael J. Singer. 1998. Measuring
presence in virtual environments: A presence questionnaire.
Presence. 7, 3, 225-240.
[11] Emotiv EPOC, 2018. Emotiv HomePage. Retrieved in 2018
from https://www.emotiv.com/
[12] LooxidLabs, 2018. LooxidLabs HomePage. Retrieved in
2018 from https://looxidlabs.com/
[13] Enobio 32, 2018. Neuroelectrics Products Page. Retrieved in
2018 from
https://www.neuroelectrics.com/products/enobio/enobio-32/
[14] Neurable 2018. Neurable HomePage. Retrieved in 2018 from
http://neurable.com
[15] Emteq VR, 2018. Emteq HomePage. Retrieved in 2018 from
https://emteq.net/
[16] Mel Slater, Pertaub, and Anthony Steed. 1999. Public
speaking in virtual reality: Facing an audience of avatars.
IEEE Computer Graphics and Applications, 19, 2, 6-9.
DOI= http://dx.doi.org/10.1109/38.749116/
[17] Affectiva 2018. Affectiva HomePage. Retrieved in 2018
from https://www.affectiva.com/
[18] Gareth V. Walkom. 2016. Virtual Reality Exposure Therapy:
To benefit those who stutter and treat social anxiety. In

Interactive Technologies and Games (iTAG) International
Conference. 36-41.
DOI= http://dx.doi.org/10.1109/iTAG.2016.13
[19] Affectiva, 2018. Affectiva Product Page Emotion SDK.
Retrieved in 2018 from
https://www.affectiva.com/product/emotion-sdk/
[20] Marko Horvat, Marko Dobrinić, Matej Novosel, and Petar
Jerčić. 2018. Assessing emotional responses induced in
virtual reality using a consumer EEG headset: A preliminary
report. In 41st International Convention on Information and
Communication Technology, Electronics and
Microelectronics MIPRO 2018.
DOI= http://dx.doi.org/10.23919/MIPRO.2018.8400184
[21] Vive. Vive Products (2016). Retrieved in 2018 from
https://www.vive.com/uk/product/
[22] Arnaldo Pereira, Dereck Padden, Jay Jantz, Kate Lin, and
Ramses Alcaide-Aguirre. 2018. Cross-Subject EEG EventRelated Potential Classification for Brain-Computer
Interfaces Using Residual Networks. HAL.
DOI= http://dx.doi.org/10.13140/RG.2.2.16257.10086
[23] Jay Jantz, Adam Molnar, and Ramses Alcaide. 2017. A
brain-computer interface for extended reality interfaces. In
ACM SIGGRAPH 2017 VR Village.
DOI= http://dx.doi.org/10.1145/3089269.3089290
[24] Neurable, 2018. Neurable Developers Page. Retrieved in
2018 from http://www.neurable.com/developers
[25] Matthias M. Muller, Andreas Keil, Thomas Gruber, and
Thomas Elbert. 1999. Processing of affective pictures
modulates right-hemispheric gamma band EEG activity.
Clinical Neurophysiology, 110, 11, 1913-1920.
DOI= http://dx.doi.org/ 10.1016/S1388-2457(99)00151-0
[26] Filip Škola, Fotis Liarokapis. 2016. Examining the effect of
body ownership in immersive virtual and augmented reality
environments. The Visual Computer, 32, 6-8, 761-770.
DOI= http://dx.doi.org/10.1007/s00371-016-1246-8
[27] John T. Cacioppo, Richard E. Petty, Mary E. Losch, and Hai
Sook. Kim. 1986. Electromyographic activity over facial
muscle regions can differentiate the valence and intensity of
affective reactions. Journal of personality and social
psychology, 50, 2, 260.
DOI= http://dx.doi.org/10.1037//0022-3514.50.2.260
[28] Ifigeneia Mavridou, Ellen Seiss, Mahyar Hamedi, Emili
Balaguer-Balester, Charles Nduka. 2018. Towards valence
detection from EMG for Virtual Reality applications, In
Proc. 12th intern. Conf. ICDVRAT.
[29] Lorcan Reidi. 2018. A Gamified Approach to Cognitive
Training using Virtual Reality and Affective Adaptation.
Master’s thesis. University of Cambridge, London, United
Kingdom.
[30] Barfield Woodrow, Claudia Hendrix, and Karl-Erik Bystrom.
1999. Effects of stereopsis and head tracking on performance
using desktop virtual environment displays. Presence:
Teleoperators & Virtual Environments 8, 2 (April 1999)
237-240.
DOI= http://dx.doi.org/10.1162/105474699566198

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 37

Windtherm: A Wearable VR Device That Provides
Temperature-Controlled Wind
Masatoshi Suzuki

Akihiro Matsuura

School of Science & Engineering
Tokyo Denki University
Hiki, Saitama, Japan

School of Science & Engineering
Tokyo Denki University
Hiki, Saitama, Japan

15rd098@ms.dendai.ac.jp

matsu@rd.dendai.ac.jp

ABSTRACT
We present a wearable VR device, called Windtherm, used with a
Head Mounted Display (HMD) that provides tactile stimuli of
temperature-controlled wind to a player’s face in accordance with
events in the virtual environment. Windtherm consists of a Wind
Module that produces and blows wind whose temperature, force,
and direction are controlled using elements such as Peltier elements,
a fan, thermal sensors, and a servo motor; and a Control Module
that controls physical conditions of wind incorporating with PC.
We have made a prototype of Windtherm and confirmed that
Windtherm is capable of producing wind of temperature in front of
a user’s face at maximum around 9 ℃ higher than the room
temperature through PWM control and is also capable of blowing
the wind synchronously to the visual and auditory information in
the virtual environment.
Figure 1: Windtherm attached to HMD VIVE Pro.

CCS CONCEPTS
•Human-centered computing→ Mixed / augmented reality;
Virtual reality

Keywords
Head mounted display, Temperature, Wind, Virtual reality,
Multimodal interaction

1 INTRODUCTION
Research and development for realizing immersive virtual reality
(V R) environments have recently made great progress
incorporating with advance of related devices such as HMDs and
various types of sensors and interfaces. For enhancing sense of
immersion and presence in a virtual environment, multisensory
stimuli including tactile (wind, thermal, etc.), olfactory, taste, and
muscle stimuli as well as visual and auditory stimuli should be
taken into account since such stimuli are what we have been
experiencing in the real world. In [1], it is noted that lack of
additional environmental feedback to the basic stimuli such as
visual and auditory ones restricts potential immersive features and
can have negative effects on user’s sense of presence and the ability
to interact with virtual environments. There are a number of study
and systems that explore impact of multisensory stimuli to user’s
perception and to the sense of immersion and presence.

When we restrict the tactile sensations to wind and thermal ones,
researches in psychology have suggested that wind and thermal
stimuli have strong impact on how people perceive environmental
surroundings [2, 3, 4]. There are also researches that utilize wind
and/or thermal feedbacks in the context of virtual reality. Ogi and
Hirose [5] utilizes airflow generated by small fans and blown on
user's hands to represent vector information in a scientific
visualization system. Head Mounted Wind [6] utilizes fan actuators
regularly distributed around a user’s head to provide wind of
appropriate forces and directions. Kojima et al. [7] presents a
wearable device with speakers and air tubes placed besides user’s
ears to provide local wind. AIRREAL [8] delivers tactile sensation
in free air by generating air vortex and by providing it locally to a
user’s body. As for thermal sensation for VR, Hülsmann et al. [9]
utilizes exterior fans and infrared lamps in a three-sided CAVE for
enhancing immersive factors in VR experiences. Thermovr [10]
utilizes Peltier elements placed at the contact part of the HMD
around user’s eyes and provides tactile sensation of heating and
cooling. Ambiotherm [11] and Season Traveller [12] utilize
wearable devices with Peltier elements, attached to a user’s neck,
and fans attached to the HMD to simulate the ambient temperature
and wind conditions in the virtual environments. Haptic Around
[13] utilizes a haptic device for providing hot air, heat, wind, rain

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 39

drops and mist, set above a user, to recreate multiple tactile
sensations in virtual reality for enhancing the immersive
environment.
In this paper, we present Windtherm shown in Figure 1, a VR
accessory for a HMD that generates and blows temperaturecontrolled wind to a user’s face to enhance the sense of immersion
and presence in the virtual world. The advantage and uniqueness of
our device lie in providing local wind whose temperature is
interactively controlled and in the portability of the device with a
HMD while moving and walking. In our study, we first developed
an exterior fan device that is fixed on a table and blows
temperature-controlled wind to a user; and conducted preliminary
experiments to examine user’s sense of warmth for the wind blown
with visual and auditory information in the virtual environment.
Then, we developed a wearable device Windtherm attached to a
HMD that consists of a Wind Module that generates and provides
temperature-controlled wind using Peltier elements, heat sinks,
thermal sensors, a fan, and a servo motor; and a Control Module
that controls the temperature, force, and direction of wind. We have
so far confirmed that the prototype system of Windtherm has basic
capabilities of generating temperature-controlled wind and of
interactively blowing it to a user’s face synchronously to visual and
auditory information of a VR application.

HMD

Peltier elements
propellers
wind

Arduino Uno
transistor thermal sensor

Figure 2: Overview of the system with HMD and exterior fan.

2 PRELIMINARY EXPERIMENTS USING
EXTERIOR FAN DEVICE
Before developing the wearable device Windtherm for a HMD, we
developed a VR system for examining how visual and auditory
information in the virtual environment presented with the
temperature-controlled wind affects user’s subjective sense of
temperature. The system consists of a HMD, an exterior fan capable
of producing temperature-controlled wind, and a VR application.
The overview of the system is illustrated in Figure 2.

2.1

Exterior Fan Device

The fan device consists of propellers with a motor, Peltier elements
(TES1-12705, 15.4V, 5A, capable of -55℃ to 80℃), heat sinks,
and a thermal sensor. It is connected to Arduino Uno that controls
temperature inside the fan device through Peltier elements with the
transistor (2SD2390, 160V, 10A). It also controls the force and
timing of the blown wind, where the force is changeable in 256
levels through PWM control using the transistor (BC547B, 50V,
0.2A) and the timing of wind is decided by the commands sent from
the VR application through port communication. The fan device
with Arduino Uno is shown in Figure 3.

2.2

Figure 3: The implemented fan device and Arduino Uno.
The designed VR experience is to be in a dark cave and have
multisensory stimuli of the images and sound of burning firewood
and actual temperature-controlled wind. To examine how warm
users feel the wind in the virtual environment, the following three
types of statuses of firewood are prepared: (a) Extinguished; (b)
Burning weakly; and (c) Burning strongly. Images of the statuses
and the visual effect of the virtual wind are shown in Figure 4.

(a) Extinguished

(b) Burning weakly

VR Application

We used the game engine Unreal Engine 4 (UE4) to construct a VR
application and to interact with the fan device. The application was
developed using Windows 10, the Blueprints Visual Scripting
system of UE4, and HTC VIVE as a HMD.

(c) Burning strongly

(d) Effect of Wind

Figure 4. Images of firewood in the VR application.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 40

Experiments and Evaluation

2.3.1 Experimental Conditions
To examine how stimuli of temperature-controlled wind
incorporating with visual and auditory stimuli by the VR
application affects the sense of warmth, we conducted preliminary
experiments using the aforementioned exterior fan device and the
VR application
Experiments: we conducted experiments using two kinds of actual
wind: (i) wind with room temperature; and (ii) wind with
temperature two centigrade higher at the thermal sensor than the
room temperature, which we denote “room temperature+2℃.”
In the experiments, we prepare all of the combinations of
conditions for choosing two conditions (Conditions A and B) from
the three statuses of firewood (a) “Extinguished”; (b) “Burning
weakly”; and (c) “Burning strongly.” There are 6 such
combinations. Then we randomly choose a pair of conditions
among them and make participants experience Conditions A and B
in this order. In the experiments, participants are first relaxed for
two minutes to get used to the environment and then wear the HMD.
After having the VR experience of firewood under Conditions A
and B with the actual wind with room temperature or that with room
temperature+2℃, they answer how warm they feel for both of the
conditions. The 5 candidates of the answer for the questionnaire are
the following:

2.3.2 Results
Figures 6 and 7 show the results for the Questionnaire in Section
2.3.1, where Numbers 1 to 5 represent the answers of the degree of
warmth/coolness participants feel. The letters “E”, “W”, and “S”
represent the statuses of the firewood (a) Extinguished; (b) Burning
weakly; and (c) Burning strongly, respectively. We mean by “E to
W” for example the pair of experiments in which first the
extinguished firewood is experienced and then the firewood
burning weakly. The row axis represents the 6 pairs of change of
statuses, from Condition A to B (A, B ∈{E, W, S}, A≠B). The
column axis represents the number of participants for each of
evaluation of the Questionnaire 1 to 5. Figure 6 is for the case of
room temperature and Figure 7 is for the case of room
temperature+2℃.

Number of participants

2.3

1. cool; 2. a little cool; 3. same with room; 4. a little hot; 5. hot

Environments and Participants: The room temperature for the
experiments was set to 27℃ (±1℃). The humidity was 40~70%.
The light in the room was put off during the experiments. As shown
in Figure 5, the fan was fixed on a table with a distance about 50
cm from the face of the participants. Wind was aimed at being
blown to the area between a nose and a neck of a participant. There
were 15 participants of ages ranging from 19 to 25.

E to W

W to E

E to S

S to E

W to S

S to W

6 pairs of Conditions A and B (changed from A to B)

Figure 6. Results of the Questionnaire (room temperature).

Number of participants

In one round of the experiment, VR image and sound of firewood
are presented for 20 seconds, then actual wind is additionally blown
for 3 seconds, and finally only VR images and sound are presented
for 10 seconds. So, the total time in one round is 33 seconds. A
participant experiences the above 6 pairs of conditions in random
order with two kinds of wind temperature. A participant makes
totally 12 rounds of VR experiences.

E to W

W to E

E to S

S to E

W to S

S to W

6 pairs of Conditions A and B (changed from A to B)

Figure 7. Results of the Questionnaire (room temp.+2℃).

Figure 5. The fan on the table and a participant.

Next, Figure 8 shows mean and SD (standard deviation) values of
the Questionnaire for the wind with room temperature and room
temperature+2℃. In the case of room temperature, mean and SD
for the extinguished firewood were 1.95 and 0.67, those for the
weakly burning firewood were 2.25 and 0.79, and those for the
strongly burning firewood were 2.42 and 0.94, respectively. In the
case of room temperature+2℃, mean and SD for the extinguished
firewood were 2.18 and 0.85, those for the weakly burning
firewood were 2.83 and 0.92, and those for the strongly burning
firewood were 2.82 and 0.99, respectively.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 41

room temp.+2℃

Mean for Questionnaire

room temp.

front of a user’s face, a servo motor (SG92R, 4.8V, Torque
2.5kgf ∙ cm) set on the top of the Wind Module to change the
direction of the wind, and a ping-pong ball that calms the wind not
to be blown too directly to a user’s face. The Control Module
consists of Arduino Nano and control circuits for controlling the
movement of the DC fan and the servo motor and for controlling
the temperature of wind through PWM control. Windtherm works
synchronously to the image and sound of the VR application
presented by the HMD VIVE Pro and blows the temperaturecontrolled wind to a user’s face. The overall flow of the processes
is described in Figure 11.

Figure 8. Mean and SD for the Questionnaire on warmth.
2.3.3 Discussion
In Figures 6 and 7, we observe that in most of the cases, score 2 (a
little cool) was chosen by the largest proportion of the participants
even in the cases of temperature+2℃. This could be because wind
usually has the effect of making one feel cooler than the actual
temperature. More specifically, in the extinguished case, the
number of participants who chose score 2 was always the largest
among all of the 5 scores, while in the cases of burning firewood
(weakly/strongly), scores 3 (room temp.) and 4 (a little hot) were
chosen more frequently. This implies that statuses of the firewood
in the virtual space should affect the participants’ sense of warmth
in both cases of room temperature and room temperature+2℃.
From Figure 8, we can also deduce that participants tend to feel
warmer when the firewood is burning weakly/strongly for both
room temperature and room temperature+2℃. However, when the
two cases of burning weakly and strongly are compared, especially
in the temperature+2℃-case, the mean value in the case of burning
weakly was even a little larger (2.83) than the case of burning
strongly (2.82). Several reasons are considered for this. First, the
distance of the exterior fan from the participants and the air of
temperature+2℃ are so subtle and might not have clear impact on
participants’ sense of warmth. Second, such subtleness with the
images of strongly burning firewood might cause inconsistency in
terms of reality and the actual wind with temperature+2℃ cannot
felt warm as expected from the images.

wind
Control Module

Wind Module

Figure 9. Windtherm with two modules in action.

wind

Figure 10. Design and elements of the Wind Module.

3 WEARABLE DEVICE “WINDTHERM”
3.1 System Description
Now, we introduce a wearable VR accessory for a HMD,
Windtherm, that interactively provides temperature-controlled
wind to a user’s face. Windtherm consists of a Wind Module that
generates and blows temperature-controlled wind and a Control
Module that controls the temperature, force, and direction of the
wind. The overview of the system of Windtherm and the Wind
Module are illustrated in Figures 9 and 10, respectively. The Wind
Module consists of Peltier elements used in Section 2, heat sinks
for radiation, a DC fan (F5010ET-05PCV, 5V, 0.3A,
40mmx50mmx10mm, max air flow 0.27m3/min), two thermal
sensors for detecting temperatures inside the Wind Module and in

Figure 11. System flow of Windtherm.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 42

The fabricated Wind Module and the Control Module of
Windtherm are shown in Figures 12 and 13. Weight of the Wind
Module plus the parts for attaching it to VIVE Pro is about 276.8g.

Temperature [℃]

3.2 Prototype of Windtherm

Time [s]

Figure 12. The fabricated Wind Module.

Figure 13. The Control Module with Arduino Nano and control
circuits.
We have confirmed that Wind Module works synchronously with
VR applications and achieves the inside temperature in the range of
30 ℃ to 40 ℃ through the PWM control. Namely, the goal
temperatures of 30, 35, and 40℃ were achieved and stabilized,
starting from the room temperature of around 26.0℃ within 30 to
60 seconds as shown in Figure 14.
We also conducted experiments on the relation between the
temperature inside the Wind Module and that in front of a user’s
face. We first raised the inner temperature to room
temperature+x ℃ for x = 5, 10, 15, 20, waited for five seconds to
stabilize the temperature, and then blew the wind for five seconds
and measured the temperatures in front of a user’s face throughout
using the outer thermal sensor. We executed this process 5 times
for all the cases x = 5, 10, 15, 20 and computed the mean of the
increase value of temperature in front of a user’s face from the
initial room temperature after wind blow after five-seconds. The
results are shown in Figure 15. The means of the increase of
temperatures for x = 5, 10, 15, 20 were 3.0, 5.2, 7.0, 8.9,
respectively. So, the increases of temperature in front of a user’s
face were almost proportional to the increases of temperature inside
the Wind Module. Consequently, we confirm that Windtherm can
send temperature-controlled wind in front of a user’s face in this
range of temperatures.

Increase of temperature

Figure 14. Convergence of temperature through PWM
control.

Figure 15. Relation of increase of temp. inside the Wind
Module and increase of temp. in front of a user’s face.

3.3

VR Application and Demonstration

We again used Unreal Engine 4 (UE4) and constructed a VR
application for which wearable and mobile capabilities of
Windtherm are effectively used. For this purpose, we constructed a
virtual space of temperate desert where users experience some
occurrences of wind, such as mass of dead branches, called
tumbleweed, comes rolling with auxiliary breeze (Figure 16, Left);
and sandstorm comes and pass by (Figure 16, Right). For these
events, sound effects of blowing wind are also added.

Figure 16. Scenes in desert with tumbleweed and sandstorm.
Finally, we show in Figure 17 an image of a user experiencing the
virtual desert and receiving the actual temperature-controlled wind
using Windtherm.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 43

Figure 17. User experiencing virtual desert using Windtherm.

4 CONCLUDING REMARKS
We presented a wearable device for a HMD that provides
temperature-controlled wind for enhancing the sense of presence in
the virtual environments and confirmed its basic capabilities of
generating and blowing wind with aimed temperature, force, and
direction. Future work includes evaluating how Windtherm with
the temperature-controlled wind affect the sense of
warmth/coolness users feel and how it affects the user’s sense of
presence in the virtual environments. To develop novel digital
contents using Windtherm is also our future work.

5 ACKNOWLEDGMENTS
This work was supported by JSPS Grant-in-Aid for Scientific
Research(C) 16K00507

6 REFERENCES

[2] Ahmed, K. S. 2003. Comfort in urban spaces: defining the boundaries of outdoor
thermal comfort for the tropical urban environments. Energy and Buildings, 35
(1), 103–110.
[3] Andrade, H., Alcoforado, M.-J., and Oliveira, S. 2011. Perception of temperature
and wind by users of public outdoor spaces: relationships with weather
parameters and personal characteristics. International Journal of Biometeorology,
55(5), 665–680.
[4] Knez, I. and Thorsson, S. 2006. Influences of culture and environmental attitude
on thermal, emotional and perceptual evaluations of a public square.
International Journal of Biometeorology, 50(5), 258–268.
[5] Ogi, T. and Hirose, M. 1996. Multi-sensory data sensualizaton based on human
perception. Proceedings of the IEEE 1996 Virtual Reality Annual International
Symposium, 66–70.
[6] Cardin, S., Thalmann, D., and Vexo, F. 2007. Head Mounted Wind. CASA, No.
VRLAB-CONF-2007-136, 101-108.
[7] Kojima, Y., Hashimoto, Y., and Kajimoto, H. 2009. A novel wearable device to
present localized sensation of wind. Proceedings of the International Conference
on Advances in Computer Entertainment Technology, 61-65.
[8] Sodhi, R., Poupyrev, I., Glisson, M., and Israr, A. 2013. AIREAL: Interactive
tactile experiences in free air. ACM Transactions on Graphics (TOG), 32(4),
Article No. 134.
[9] Hülsmann, F., Fröhlich, J., Mattar, N., and Wachsmuth, I. 2014. Wind and
warmth in virtual reality: implementation and evaluation. Proceedings of the
2014 Virtual Reality International Conference, ACM, 24, 8 pages.
[10] Peiris, R. L., Peng, W., Chen, Z., Chan, L., and Minamizawa, K. 2017. Thermovr:
Exploring integrated thermal haptic feedback with head mounted displays.
Proceedings of the 2017 SIGCHI Conference on Human Factors in Computing
Systems, ACM, 5452-5456.
[11] Ranasinghe, N., Jain, P., Karwita, S., Tolley, D., and Do, E. Y.-L. 2017.
Ambiotherm: Enhancing sense of presence in virtual reality by simulating realworld environmental conditions. Proceedings of the 2017 SIGCHI Conference
on Human Factors in Computing Systems, ACM, 1731–1742.
[12] Ranasinghe, N., Jain, P., Tram, N. T. N., Koh, K. C. R., Tolley, D., Karwita, S.,
Lien-Ya, L., Liangkun, Y., Shamaiah, K., Tung, C. E. W., Yen, C. C., and Do, E.
Y.-L. 2018. Season Traveller: Multisensory narration for enhancing the virtual
reality experiences. Proceedings of the 2018 SIGCHI Conference on Human
Factors in Computing Systems, ACM, 577, 13 pages.
[13] Han, P.-H., Chen, Y.-S., Lee, K.-C., Wang, H.-C., Hsieh, C.-E., Hsiao, J.-C., Chou,
C.-H., and Hung, Y.-P. 2018. Haptic Around: Multiple tactile sensations for
immersive environment and interaction in virtual reality. Proceedings of the 24th
ACM Symposium on Virtual Reality Software and Technology (VRST’18), Article
No. 35.

[1] Witmer, B. J. and Singer, M. J. 1998. Measuring presence in virtual environments:
A presence questionnaire. Presence: Teleoperators and Virtual Environments,
7(3), 225–240.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 44

VR Conferencing: communicating and collaborating in
photo-realistic social immersive environments
Simon N.B. Gunkel

Hans M. Stokking

Rick Hindriks

Tom de Koninck

TNO
Den Haag, Netherlands

TNO
Den Haag, Netherlands

TNO
Den Haag, Netherlands

TNO
Den Haag, Netherlands

simon.gunkel@tno.nl

hans.stokking@tno.nl

rick.hindriks@tno.nl

tom.dekoninck@tno.nl

ABSTRACT

While Virtual Reality applications become more multi-user
experiences, mostly artificial avatars are used for the
representation of users in the virtual environment. This is fine for
many multi-user scenarios but is less than ideal in more social
settings like talking to your family, friends or business partners. In
this paper, we like to outline the concept of Social VR based on
real-time photorealistic capture and representation of users.
Particularly, we introduce our web-based framework that allows
for both the creation and consumption of photo-realistic social
VR in 3D and 360-degree environments. Further, we evaluated
our system both in a 360-degree remote collaborative meeting and
in a full 3D experience. Our initial results indicate that our photorealistic approach indeed offers a natural communication to the
users with a high presence and immersion. With our system we do
not only allow to explore, and experience VR together but also
allow new forms of remote communication and collaboration.

Keywords

Virtual Reality, VR, Social VR, WebRTC, WebVR, interactive
content, immersive virtual environments.

1. SOCIAL VR

Nowadays, social VR is mostly associated with graphical avatars
in a graphical environment. Main examples are Facebook Spaces 1,
AltSpaceVR 2, Mozilla Hubs 3 and many more. All these consist of
a shared virtual environment, using graphical avatars to represent
the users, and offering various possibilities, such as shared
gaming and content consumption and multi-user communication
and collaboration. The transfer of user motion to avatar motion is
achieved by employing HMD and controller tracking. These
environments offer a compelling experience of togetherness in
which immersive video is combined with spatial and 3D audio
[5], still, they are limited in the way you perceive other users.
“The social cues that you would normally have about someone
being creepy or safe weren’t there.” [3]
The alternative to a graphical avatar is a photo-realistic
representation of users. When it comes to photo-realistic social
VR we can distinguish between 3 types: i) capturing the user and
environment at the same time (e.g. with omni-directional
cameras), ii) capturing the user alone (e.g. with depth or stereo
1

https://www.facebook.com/spaces

2

https://altvr.com/

3

https://hubs.mozilla.com/

Figure 1: multi user 360-degree experience
cameras) [1][2][4][6] and iii) capturing a full (volumetric) 3D
representation of the user [7]. However. due to the complexity of
processing and transmission of 360-degree and volumetric data
the first and third approaches still needs more research to get
closer to market ready. Our approach focuses on an individual
user capture via RGB and depth sensors. This offers the benefit of
being able to reuse traditional media processing and transmission
and thus having a reliable end-to-end media chain.

2. TOGETHER VR

Our social VR framework called Together VR extends current
video conferencing with new VR functionalities, based on web
standards and technologies. The framework is modular and allows
for an easy creation of VR experiences that are social and the
consumption of these experiences, using off-the-shelf hardware.
With the framework, we aim to allow users to interact and
collaborate while being immersed in interactive VR content.
To capture a photorealistic representation of a user, we first need
to capture a color and depth image with the help of a depth
camera (e.g. Microsoft Kinect v2 or Intel Realsense). In a second
step the image is analyzed, and the user is separated from the
background (the background is replaced with a chroma-key
color). The resulting image is transmitted via a web client as video
over WebRTC to another user’s web client. Finally, the complete
virtual environment is rendered in the browser and the users are
placed into the environment (making the chroma-key background
transparent) so that he/she naturally blends into the surroundings.
Each end-point consists of a VR capable laptop, a single depth
camera, audio headset and a VR HMD.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 45

Our next steps in development will focus on the following:
•

Eye-gaze: to look each other in the eye. This year we
will have a first version that replaces the HMD in a user
representation with a photorealistic model of the face.

•

Scalability: allowing large groups of people to join one
session. This year we will scale our system to allow up
to 10 users in one communication session, using a VR
conferencing bridge.

•

AR and mobility. As both augmented reality and mobile
devices are becoming more important we are currently
investigating how to extend and map our current
framework to support AR and mobile display devices.

•

Long term trials. We are currently setting up long term
trials allowing people to use our system in their normal
work environment for remote collaboration and
meetings.

Figure 2: Social VR 3D-degree experience (incl. self-view)

3. MULTI-USER 360-DEGREE
EXPERIENCE

In our multi-user 360-degree experience [2][4], 2-4 people sit
around a table in VR. The view of each user is the same, each user
sees the other users on the opposite side of the table and
optionally a video or presentation on the top of the table (see
Figure 1). To evaluate this experience, we held a 1-day
experiment session in an informal and uncontrolled setting at our
lab facilities, asking groups of 3 people to communicate while
watching a video. We collected feedback through a short
questionnaire from 54 participants (avg. age of 33 and 43%
Female). People expressed a high level of interaction and
immersion within the conversations. As part of the questionnaire
the users rated the system with a “good” overall quality of 4.35
(SD 0.51) and video quality of 3.65 (SD 0.77) on a 5-point scale.

4. MULTI-USER FULL 3D EXPERIENCE

In our full 3D experience (see Figure 2), we include a full 3D
environment and use both the color and depth information of the
user capture to display users in 3D (as point cloud or mesh). This
means, additionally to the replacement of the background with a
chroma, a new image format combines the color image and the
depth image. This is, we map the depth image into a grayscale and
transmit the image as 2D video. To display this image as a point
cloud or mesh we developed an optimized WebGL shader that
maps each pixel into the 3D coordinates of the space [1]. This
approach is also used to display a self-representation to each user.
To evaluate this experience, we held a 2-day experiment session
in an informal and uncontrolled setting at a conference venue. We
collected feedback through a short questionnaire from 25
participants (avg. age of 35,26 and 12% Female) where 2 people
were asked to communicate in our system while switching the
environment between a 3D and a 360-degree virtual space. As
part of the questionnaire, people expressed a preference for the 3D
experience (60% 3D, 28% 2D, 12 % no clear preference) with a
“good” overall quality of 6.92 (SD: 1.55) on a 9-point scale.

5. CONCLUSION AND FUTURE WORK

Overall, our current social VR web framework offers the
possibility to easily create photo-realistic social VR experiences
with a good user experience and natural interactions. It also
allows to evaluate different types of VR technology and to quickly
extend the software components with new functionalities.

6. ACKNOWLEDGMENTS

This paper was partly funded by the European Commission as part
of the H2020 program, under the grant agreement
762111(VRTogether, http://vrtogether.eu/).

7. REFERENCES

[1] Gunkel, S.N., Stokking, H.M., Prins, M.J., van der Stap, N.,
Haar, F.B.T. and Niamut, O.A., 2018, June. Virtual Reality
Conferencing: Multi-user immersive VR experiences on the
web. In Proceedings of the 9th ACM Multimedia Systems
Conference (pp. 498-501). ACM.
[2] Gunkel, S., Stokking, H., Prins, M., Niamut, O., Siahaan, E.
and Cesar, P., 2018, June. Experiencing Virtual Reality
Together: Social VR Use Case Study. In Proceedings of the
2018 ACM International Conference on Interactive
Experiences for TV and Online Video (pp. 233-238). ACM.
[3] Leibrick, Suzanne, “Why women don't like social vr”,
https://extendedmind.io/social-vr
[4] Prins, Martin J., et al. "TogetherVR: A framework for
photorealistic shared media experiences in 360-degree VR."
SMPTE Motion Imaging Journal 127.7 (2018): 39-44.
[5] Waltemate, T., Gall, D., Roth, D., Botsch, M. and Latoschik,
M.E., 2018. The Impact of Avatar Personalization and
Immersion on Virtual Body Ownership, Presence, and
Emotional Response. IEEE transactions on visualization and
computer graphics, 24(4), pp.1643-1652.
[6] Woodworth, J.W., Ekong, S. and Borst, C.W., 2017, March.
Virtual field trips with networked depth-camera-based
teacher, heterogeneous displays, and example energy center
application. In Virtual Reality (VR), 2017 IEEE (pp. 471472).
[7] Zioulis, N., Alexiadis, D., Doumanoglou, A., Louizis, G.,
Apostolakis, K., Zarpalas, D. and Daras, P., 2016,
September. 3D tele-immersion platform for interactive
immersive experiences between remote users. In Image
Processing (ICIP), 2016 IEEE International Conference (pp.
365-369).

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 46

Virtuality of Virtual Reality: Indiscernibility or Ontological
Model?
Suzanne Beer
INREV Laboratory
Université de Paris 8
suzbeer@gmail.com
oppose to the actual but to the real. At the opposite, Pierre Levy and
Couchot understand the virtual as really being opposed to the actual
and part of the real. Which difference is there between these two
ways of conceiving virtuality of VR?

ABSTRACT
“Virtual” is now widely used in computer sciences. The classical
conception states that virtual is typified by technological qualities,
in which virtual and real are partially indiscernible. This paper
finds a coherence between historical use of “virtual” in optical
physics, which applies philosophical meanings of virtual, and
technological ones in VR. It builds an ontological model of VR
virtuality as being divided between a “virtual reality”, an equivalent
of “real object”, "real world", a computer-generated actualization
by “real images” on displays and a second virtualization by “virtual
3D images” in neural activity which forms a virtual reality in a
“perceived object”, "perceived world" sense. Virtual Reality does
not exclusively pertain to the objective technological side but
centers on the subjective psychological self.

Keywords
Virtuality; Virtual/Actual; Virtual Reality; Reality; Presence;
Virtual Body; Virtual Image;

1

INTRODUCTION

The term “virtual“ is commonly used in computer graphics
technologies. Virtual reality, virtual embodiment, virtual
incarnation, virtual cognition, virtual humans and, virtual
collaborative world, etc. We aim to get a precise definition of what
is virtual in Virtual Reality. The conception of defining the virtual
by its technological characteristics is a starting point. The virtual is
reduced to an empirical concept as being partially indiscernible
from reality. Could not ontology, used as a scientific tool, help
understand this highly connoted concept? Or is the carnapian
gesture really useful to VR sciences?

2. VIRTUAL REALITY: A

COMPUTATIONAL REALITY WITH A
PARTIAL REAL/ VIRTUAL
INDISCERNIBILITY

2.1

Opposition of conceptions of virtual

Two conceptions of virtuality in a scientific and technological area
of VR seem to get into an opposition. One way is conceiving along
with Berthier, Nannipieri and J. Guez that the virtual does not

2.2 Implicit definition: the virtual connotes
computers’ sides
The main technical definition of virtual reality in the international
scientific community is given in Virtual Reality: Concepts and
Technologies: “Virtual reality is a scientific and technical domain
that uses computer science (1) and behavioral interfaces (2) to
simulate in a virtual world (3) the behavior of 3D entities, which
interact in real time (4) with each other and with one or more users
in pseudo-natural immersion (5) via sensorimotor channels."[1].
What does ‘virtual’ actually denotes? Virtual is the qualifier of the
world in which interactions in real-time, interfaces and 3D entities
find their reality: computer-generated objects.
Therefore, how is the world which is virtual? A virtual world stands
for a kind of world which differs from a “real world”. Hence, “The
definition proves to be different from a simplistic vision of virtual
reality being a “mere copy” of the “real” world.” [2] A “virtual
world” is more than in a relation of representing the real world. It
makes specific worlds “a digitally created artificial world, […] can
be imaginary, symbolic or a simulation of certain aspects of the
real world” [3] Simulations can have a realistic side, but it still
keeps a difference to bring an interest to VR compared to its real
counterpart. Symbolic and imaginary worlds open a gate to
incomparable, creative worlds.
Virtual qualifies a kind of world which can be in a semi-realistic
relation to a real world but also to worlds which are independent
from reality. The specificity of virtuality would be its technological
properties.

2.3 The main definition of “virtual” is being
partly indiscernible to the real
Even though VR opens gaps with the real world, one main part of
it is its capacity to act as a substitute for it. Berthier explains that
“virtual” is congruent with “virtue”, force, strength of a being. “Is
virtual what is, without being real, what is operatively equivalent
to the real » [4] As an example, a virtual world can have properties
which are going to be real as the reflection in a mirror will bring
effects on reality. The virtual image of a light is really adding light.
A plane simulation will really train a future pilot. “There is no
double dimension, virtuality and reality are indiscernible along
“general principles”” [5] The virtual belongs to a same array of
phenomenon which can only “easily be differentiated from the real
by any normal person resorting to another sensori-motor channel.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 47

Therefore, virtual reality is a member of virtuality which is a kind
of phenomenon happening in reality, similar to it but different in
some empirically defined other ways.

3. ONTOLOGICAL MODEL OF VR
3.1 The virtual and the actual in VR
This technical model of virtuality is opposed to a so called
“ontological” one: “these authors […] seek to highlight an
ontology by granting concepts a predominant role” [6] states
Nannipieri.
In this conception, developed by a majority of philosophers,
Bergson, Deleuze and Pierre Levy who applies it to understand
development of cultures, typically of transformation of reality by
technical means, virtuality is understood as a pole opposed to
actuality. Concepts denotes a metaphysical realm devoid of
phenomenal expertise. « It is mainly the effects of the virtual as a
phenomenon which interests me and its conception related to
technology: the effects which I can watch as well as their implied
transformations”[7] explains Judith Guez aiming towards creation
of illusions between virtual and reality along an approach of digital
magic.
However, these concepts originate in the need to explain
observations. The opposition between potency (versus virtuality)
and actuality in Aristotelian philosophy stands as a model of reality
complex generation. It has been worked through by scolastics,
arrives in modern days through philosophy and sciences with
Leibniz. Virtual and actual are ways of being of the real. Reality
exists in a virtual way when it is a latent potency. Reality exists in
the present when its virtuality is actualized, manifested.
The fact of virtuality/ actuality as being a computer quality is
attested in a digital artistic reflexion, in Couchot’s writings.
“To put it simply, we will say that the virtual in the computer exists
in potency- a potency capable of updating in sensitive forms
(images, sounds, texts or perceptive stimulations) during the
dialogue man-machine. The virtual thus appears as the mode of
being of simulation. It is not opposed to the real, but to the
present.” [8]
The virtual is an attribute of simulation, which is computer
generated. Virtual is taken in a clearly philosophical, Aristotelian
definition. A computer generates a kind of reality which is not
limited to one temporal dimension, the present. Its force is to be
able to generate versatile present effects through real-time
interaction with a human. This would be impossible if there was no
latent power which held alternative simulation models. Hence, one
is able to describe the array of its simulations.
Therefore, virtual is the mode of being of simulation. Computers
algorithms are potential generations made actual through
interfaces. Actualizing is a result of movements being impulsed by
user-machine interaction through interfaces, by its processing. No
processing could happen if there would not be calculating means in
potency, latent algorithms waiting in the dark for their activation.
It is quite obvious that ontological concepts are operational to
understand and plan virtual phenomena.

3.2

Dialogy: reality outside the computer,
inside the computer, virtualization,
actualization

The interest of keeping a virtual/actual differentiation of the real is
that it manages to design a dynamical production of reality. In a
very general way, reality is perceived as being divided in two
processes, one which stands inside of a being as a potential rule, the
other which defines the being at one present state. The first state is
the virtual one, the second the actual one. The actual state is
produced by the virtual rule. As a reaction, the virtual rule will react
to its actualization, etc.
Reflecting about VR, Couchot has named this movement of reality
a “dialogy”. In this sphere, it is the relation between the user and
the machine, between external reality, the plane of the user and
internal “virtual” reality, the plane of the computer. “The necessary
relationship that it establishes with the real, on both sides of the
interfaces, is a dialogical relation. The interfaces are the places
where reality is virtualized and where the virtual is actualized.” [9]
It is a dialogue between virtuality and actuality. “Real life” reality
has been “virtualized”, translated into algorithms. Users’
interactions with computer’s virtual, potential simulations make
them “actualize”, manifest in their specific declinations on different
sensori-motor and cognitive displays. VR introduces a kind of
dialogue within reality between differing layers. They are all real
but with a different coefficient which interrelates with one another.

4

VIRTUAL REALITY AND VIRTUAL
IMAGE

“Virtual” and its actual counterpart have shown their validity in
describing computer generations. But the virtuality of virtual reality
has yet one more dimension to investigate.

4.1

Virtual image and VR: optical meaning

The point here is to compare virtual images and virtual reality and
to wonder what would be a coherent way to understand it. A
“virtual image” is an optical term formed in XVIIth century
physical sciences. It characterizes an image which is impossible to
project on a material plane by contradistinction to a real image. A
virtual image is a physical phenomenon, but an intangible one, as
an illusion.
Optically, a virtual image is defined as the point of convergence of
the continuation of the line of reflection from the real object in the
imaginery space of the mirror and the line going to the point of
view. The eye is integrated in the model of a virtual image. As the
point of reception of the virtual image, the eye actualizes the image.
But, a virtual image in a mirror would never exist if there was no
intelligence to distinguish its ontological source. Animals watch
real objects not images. According to psychoanalysis and animal
ethology, understanding an image requires having passed through
the “mirror stage”, which very few living being besides infants do.
This optical potentially illusionary property is an important vector
for fantasy as shown in Lewis Carroll imaginative use of the
“looking glass”[10].
In a similar way, Grau shows that Virtual Reality only exists as an
effect in someone’s brain.
“In virtual reality, 3-D images are projected in HMD monitors as
two 2-D images. The spatial effect results from stereoscopic vision
and is formed in the observer’s cortex. Thus, the images leave their
media in a twofold sense: a 3-D image, which has no physical

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 48

existence except, perhaps, in the excited neurons of the brain, forms
a constitutive unit together with the observer and is non separable
from him or her.” [11]
VR really produces an illusory 3D reality. The screen can only
show 2D images. 3D image exists only for the viewer, in his cortex.
The brain is the technological device able to give its reality status
to virtual environments.

4.2 Virtual Reality, models, displays and
virtual 3D images
We shall understand this idea in analogy with the model of optical
sciences. When Grau assumes that VR really exists in the brain, it
means that a dissociation must be made between screens and brains.
It is in the brain that the 3D image of the virtual model becomes
real. It is real as virtual reality, as the only way an actualization of
a virtual model can reach reality, which is in a consciousness.
However, a virtual model does exist in a computer as the effect of
a certain number of algorithms. The brain only sees one part of the
virtual model, the one which is perceived and could be called
“projected world”, following G. Desmarais’ world distinctions:
"Between the real physical world (PW) and the projected world of
sensitive manifestation (PW), there is an impassable hiatus” [12]
The reality of virtual reality belongs to a kind of perceived reality,
a highly scientific, cultural one, not a raw physical reality. Yet, in
a dualistic realistic conception, a perception is a kind of image of a
reality. Therefore, we would deduct that 3D sensori-motor effect in
the brain is the place where the optical virtual image lies. It is
virtual in the physical science sense of not being projected on a
screen. It is located in an unprojectable space, a neural interrelated
unpositionable place which Stéphane Dehaene has called “neuronal
global working space”. [13]
Therefore, VR model could be explained as follows: “VR” in the
computer should be named Virtual Model of Reality. It is not a real
object as it has no tangible material existence but it stands as one
of a kind. It is a 3D model standing in the place of the real object
of the optical virtual image model. And it is virtual in the
ontological meaning of being a latent processor which is going to
be actualized through real-time interactions.
Secondly in the actualizing moment, this virtual model is projected
on screens and other displays, realizing immersive effects like
CAVE, HMD, Force Feedback Systems, with a movement of 2D
images. As the definition of what is projected on a screen, it is a
“real image”, a materialized one. These are images actualizing the
model but not the virtual image itself of the virtual (real) model
(object) which needs a 3D perception/action. How could one call
“virtual” these projections on screens? It would not be conceptually
coherent with the scientific use of an optical virtual image. 2D
screen images have a kind of transactional status, being the sensorimotor means communicating machine states to the mind.
Thirdly, virtual reality is really watchable by the synthesis of the
cortical visual center and other parts of the brain, keeping the first
sense of “virtual image”. In the synthesis of the brain, there is an
actualization of the virtual model in interaction with the user’s
input. This actualization has a depth; it is a kind of 3D moving
image of the model. This is the equivalent of the virtual image of
the optical model.

The “virtual 3D image” in the brain plays with a sense of reality
which, going back to Desmarais’ analysis stands for the “Perceived
World” [12]. The “Perceived World” stands as an actualization of
the “Real World”, the modeled world, the simulated model. It exists
in the virtual part, the computer. Therefore, the modelled world can
be called “Virtual Reality” as it stands at the optical model “real
object” place and is virtual as a computer generated object. “

4.3

Virtual 3D Image and PVR

Virtual Reality has been elucidated as the equivalent of “RealWorld”, but one has to add that it is a kind of image, a simulation
of the world. Still it stands as the objective side of the subjective
side of the image which lives in the mind. It actualizes itself in real
images which produce virtual 3D images in the brain. Presence
feeling is comparable to virtual 3D image. Writing about
spectators’ experience in a theatre, Merleau-Ponty describes the
mental movements and perceptions.
“This virtual body moves the real body to the point that the subject
no longer feels in the world where he really is, and that instead of
his true legs and arms, he feels the legs and arms that one would
have for walking and acting in the reflected room, he lives in the
performance” [14].
Extrapolating from Merleau-Ponty’s conception of a virtual body,
a virtual 3D image is like a mental body giving a feeling of presence
in a fictional reality. Virtual perceptions and movements take the
place of “real”, actual ones since they happen in a purely mental
world as well as they give the illusion of “being there”. Theatre is
a paradigm of fictitious spaces which carry imagination and puts
the spectator inside of a kind of virtual world, a world existing only
in minds, where virtuality and actuality become simultaneous. In a
virtual 3D image, virtuality of the image combines itself with its
inner simultaneous actualization.
Associating presence feeling, virtual/actual inner body and virtual
3D images show that virtual reality does not exclusively belong to
the objective side of computer simulation model. Going back to
Desmarais’ distinction[12], a real world (RW) is always associated
to a perceived world (PW). In this sense, we find a virtual 3D image
is a mental virtual reality. The importance of this part of reality as
being the only side which is conscious and to which all VR is
focused, thinking in terms of subjective VR, “PVR” (perceived
virtual reality) is a step forward.is

5

CONCLUSION

The position of partial indiscernibility between virtuality and
reality, the parti of putting aside concepts as not being effective to
understand what happens in the world of experience is simpler,
looks more empirical, but lacks precisions. Taking into account the
optical model of “virtual images”, the virtuality of VR denotes a
computer generated simulating model which stands as a reality
model. Virtuality points to an ontological concept designating a
potential part of reality as opposed to its actualization, referring to
the materially existing reality, the part of reality which has been
realized. VR devices are seen as “real images” of VR models,
transactional realities which lack the true 3D reality of the model.
Virtuality of VR truly exists as a virtual 3D image, as a mental state
in the user’s brain. Close to theatre, Virtual Reality relates to the
complex of simultaneous virtualizing/actualizing in the inner

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 49

world. Therefore, VR really is a PVR (perceived virtual reality).
VR researches area could profitably englobe several other sciences
as e.i. neural sciences, cyberpsychology and sociology in its
domain.

[7]

GUEZ, J. (2019). Illusions entre le réel et le virtuel
(IRV) comme nouvelles formes artistiques : présence et
émerveillement.
https://www.theses.fr/195162412Gue, Judith in her PhD
thesis p. 79.

[8]

COUCHOT, E. (2007). Des images, du temps et des
machines, dans les arts et la communication. N.mes: J.
Chambon, p. 201

[9]

COUCHOT, Ibid. p.11.

[10]

CARROLL, L., ((1865)-2009). Alice's Adventures in
Wonderland and, Through the looking-glass.New York:
Penguin Classics.

[11]

GRAU, O. (2003). Virtual art : from illusion to
immersion. Cambridge, Mass.: MIT Press, p. 251.

[12]

DESMARAIS, G. Dynamique du sens. Septentrion,
1998, p. 16.

[13]

DEHAENE, S. (2019). Code de la conscience - Éditions
Odile Jacob.

REFERENCES
[1] FUCHS, P., MOREAU, G., & GUITTON, P. (2011) Virtual
Reality: Concepts and Technologies p. 5
[2] FUCHS, P., Virtual Reality: Concepts and Technologies, p. 6
[3] FUCHS, P., Ibid.
BERTHIER, D. (2004). Méditations sur le réel et le virtuel:
[4] Harmattan. p. 85.
[5] BERTHIER, Ibid., p. 75.
[6] NANNIPIERI, O. Les paradoxes de la présence dans les
environnements immersifs : de la réalité à la réalité virtuelle,
Thèse, Université de Toulon, dir. Philippe Dumas, 2013.p.
132.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 50

TAMED CLOUD: Sensible interaction with a swarm of data
Francois GARNIER

Fabienne TSAI

Dionysios ZAMPLARAS

EnsadLab, Ensad, PSL University
Paris, France

EnsadLab, Ensad, PSL University
Paris, France

EnsadLab, Ensad, PSL University
Inrev Laboratory, Paris 8 University
Paris, France

francois.garnier@ensad.fr

fabienne.tsai@ensad.fr

Florent LEVILLAIN

dionysios.zamplaras@ensad.fr
David BIHANIC

EnsadLab, Ensad, PSL University
Paris, France

EnsadLab, Ensad, PSL University
Paris, France

Forent.levillain@ensad.fr

d.bihanic@ensad.fr

ABSTRACT
Faced with an overload of dematerialized information, would it be
possible to propose an aesthetic, sensitive, even affective
relationship with a cloud of information, thought of as a living and
malleable entity? Would it be possible to interact in immersion
with my data, to tame it, to sculpt it, to reclaim it for myself? Our
purpose is to create such an aesthetical, sensible and affective
relationship with a set of data, based in particular on gesture and
speech interactions, inside an immersive virtual reality
environment. The Tamed Cloud project aims at proposing new
modalities of accessing, processing and memorizing these vast
amounts of data. We present in this paper our research, and our
first results, a functional virtual reality experiment, currently
under evaluation.

collection, according to gestural and postural modalities, as well
as voice conversation, thanks to the IBM Watson Cognitive
Platform.

2. HYPOTHESIS
The intention of our research team is to explore a new approach to
digging, exploring and manipulating unstructured data, based on
an intuitive gestural and verbal interaction language. Our
hypothesis is that it is possible to propose an aesthetic, sensitive,
even emotional relationship - based in particular on gesture and
speech recognition.

Keywords
Virtual Reality, Immersive experience, Artificial intelligence,
Data design.

1. INTRODUCTION
The project TAMED CLOUD, sensible interaction with a swarm
of data, was born from the observation that we are currently in an
era of an overloading of dematerialized information. These huge
amounts of data overwhelm us, making us lose touch with them,
in both our professional and personal lives. The volume of data to
consider has reached a critical threshold. Traditional visualization
techniques fail to simultaneously, allow access to specific pieces
of information, provide a comprehension of grand ensembles, and
favor the discovery of unexpected associations. For this reason,
new visual representation models need to be conceived. Such
models should include possibilities for interactive manipulation
and processing of these large sets of data, to tailor experiences
better adjusted to the body dimensions and to the spaces derived
from them.
In this article, we detail our research intentions and explain the
methodology we followed to produce a prototype, which should
soon lead to a Proof of Concept (POC). The prototype we have
realized consists of an immersive experience in virtual reality,
offering the possibility to interact with an ensemble of pictures
from the MoMA's (Museum of Modern Art, New York City)

Figure 1. user in the TAMED CLOUD environment
( Composite picture).
The project is to create an immersive installation for visualizing
data presented in the form of a 3D cloud with autonomous and
reactive behavior that will be taking shape according to the user’s
movements. The user - in VR immersion and acting as a
researcher, a collector or a curator - will be invited to manipulate,
sculpt this "vaporous" mass of data and tame it plastically with her
gestures. She will be able to make gesture selections of data
groups in the cloud and organize them spatially, according to nonpredictive rules and conventions. Through speech, it will be
possible to ask the cloud to organize itself according to different
criteria, or even to extract groups of data and present them
according to some of their specific characteristics. The cloud
behaviors would then promote certain data matching at the user’s
request, but they would also generate unexpected and intuitive
associations that the user would not necessarily have thought of.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 51

This approach could meet our objective of providing better access
to large amounts of data and open the door to new ways of
accessing and processing them.
In this research, we will examine two main issues:
- How can proprioceptive sensitivity contribute to data mining
work? We want to study the advantages and/or constraints that
may arise when a body in immersion (visual and proprioceptive
perception) is engaged with data, in tasks such as sorting,
selecting and categorizing them.
- How could a behavioral data organization stimulate the user in
her data mining work?
Our hypothesis is that the user will adopt de facto singular
postures towards the visual organization of the data. The analysis
of these postures will give us the possibility to modulate the data
behaviors and re-shape the interaction relationships between user
and data.
To carry out this project we have gathered a multidisciplinary
team of researchers from the fields of information science, data
design, cognitive science, behavioral objects, and art and
interaction design.
The developed functionalities would be assisted by artificial
intelligence
technologies,
(Alchemy
Language,
Visual
Recognition, Natural Language, Machine Learning etc.)

2.1 Data design
Data behaviors will be of two main types, semantic data behaviors
built on the basis of the meaningful relationships that bind them
together and behaviors that depend on the quality of the
interactive exchanges between the user and the data. One of the
main research axis opened here refers to the dynamic visualgraphic arrangement of the dataset, according, firstly, to their
intrinsic meaning relationships and, secondly, to the functional,
interactional possibilities that it proposes. In short, it becomes
interesting to study what could be the good or relevant data
patterns for each data arrangement and behaviors. We could then
find new affordance principles at the crossroad of meaning
expression and interaction possibilities. Such a study supposes
reactivating open questions, and concepts developed earlier,
between data design and cognitive science (behavioral
psychology). Key initiators of such an approach include Stuart K.
Card, Jock D. Mackinlay, Ben Shneiderman (Card et al., 1999),
George G. Robertson (Robertson et al., 1989), John Stasko
(Stasko, 1993, 1996), James D. Hollan (Hollan et al., 1986) and
Edwin Hutchins (Hutchins, 1995, Hutchins et al., 1985) or George
W. Furnas (Furnas and Bederson, 1995), also various laboratories
(HCI) attached to the Xerox PARC Research Center (Palo Alto),
the University of California (San Diego), the University of
Maryland, the Georgia Tech Institute (Georgia Tech), the
University (Virginia Tech), IBM, AT T and Bell laboratories, etc.
Initially we will focus on the consultation of visual data paintings and photos - for which our approach seems particularly
suitable. However, in the long term, we hope to be able to
dynamically use any form of data potentially available in the
cloud, whether numerical, textual, audio or visual (photos and
films).
The online MoMA database counts more than 200 000 references,
in various artistic specialties. In our first prototype, we used 4000
paintings and pictures issued from it, as well as some
accompanying metadata, such as title of the piece of work, name

of the artist and date of creation. In order to enrich this database,
we plan to include later, additional information on the works and
artists, in particular texts and comments written within the
exhibition catalogues published by the museum, such as
biographies, reviews of works, views of artists or collectors and so
on.

2.2 Behavioral objects
2.2.1 Autonomous behavior of the cloud
We imagine a cloud of data animated by spontaneous movements.
Much as a swarm of birds or a fish bank, spontaneously
organizing into emergent states (Sumpter, 2006; Couzin, 2009),
the cloud could display a self organizing behavior with expressive
variations. Designing behaviors for an autonomous artifact is
about channeling human intuitions about movement (Levillain
Zibetti ; Hoffman Ju, 2014). It is about creating movement
patterns that an observer may construe as goal-oriented, possibly
intentional actions (Heider Simmel, 1944; Scholl Tremoulet,
2000). In addition, the representation of a collective behavior
solicits other intuitions, such as the possibility to compute
properties of an ensemble (Ariely, 2001), or Gestalt qualities
attached to a set of moving elements (Britten, 1992; Williams,
1984). Besides the immediate empathic connexion promoted by a
self-generated behavior, giving spontaneous movements to a
cloud of data is of interest from the point of view of data
visualization. In relation to the data structure, the cloud's behavior
may serve the purpose of highlighting different data sets and
connexions between those sets. The cloud could for instance
organize as a collection of distinct swarms, each corresponding to
a data set, or it could use properties of alignment between the
elements of the cloud to represent gradients. The cloud could also
shape into recognizable figures, and display the data according to
known formalisms, based on the relationships inside the data
structure (hierarchical, discoidal, concentric or eccentric
organizations, tabular, matrix, etc.).

2.2.2 Engagement behavior of the cloud with the
user
The engagement behavior of the cloud corresponds to its
transformations in response to the user’s proximity and actions.
Maintaining an engagement with a user is one of the foremost
issues in social robotics. By giving some social abilities to the
artificial agent, such as facial and gesture recognition capacities,
or the ability to predict and adapt to the user's response (Anzalone
et al 2015), a form of connexion with the user may be initiated
and maintained, involving the control of the user's attention and
motivation. With a cloud of data, such an engagement behavior
may consist in reconfiguring the cloud to facilitate the interaction,
for instance by enveloping the user or displaying the data on a
single plane. It may also consist in prompting the user to interact
with the data, possibly by highlighting some data immediately
accessible, or display meaningful configurations, for example by
aggregating similar data. Finally, elements of personalization
could be added to the cloud's behavior, such that the cloud would
respond differentially depending on the user's personality, as
manifested for instance by the way she explores the environment,
the hastiness or delicacy of her gestures.

2.3 Interaction design
For more than a century, research has reaffirmed the central role
of the body in the processes of perception and understanding of

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 52

the world, in philosophy (Poincare, 1902; Merleau-Ponty, 1945),
in cognitive science (Varela al, 1993) or in art and
communication, particularly with the emergence of digital media
(McLuhan, 1964; Grau, 2003; Jones, 2006; Couchot, 2012).

demanding it. Such interaction styles try to take advantage of the
users’ preexisting knowledge and experience of everyday life, as
well as some communication models that are already familiar to
the user. (Georgakopoulou et al, 2018)

It has been shown that the gesture and particularly the gripping
gesture can be considered as a meaningful contribution to the
exploration and memorization of our environment (Berthoz, 1997,
2013), these perceptual capacities being of different natures,
depending on the degree of proximity of objects in our
environment (Hall 1971; Bennequin, 2017)

We already implemented some of these principles in our
prototype of Tamed Cloud, through simple gestural and speech
interactions that we describe in detail here under (Paragraph 3.
The Prototype realized).

These principles have been applied in virtual reality (Fuchs, 2018)
and have demonstrated the ability of digital paradigms to produce
for the user the senses of presence, agentivity and embodiment
(Heeter, 1992; Lee, 2004; Slater, 1999, 2009), that stimulate body
engagement and vice versa.
When discussing about virtual reality experiences in design, many
researchers and artists highlight the importance of the storytelling,
the behavioral feedback and experience learning (Bilda Al, 2008;
Wright McCarthy, 2008; Tramus Al, 2003) and the natural user
interfaces (NUI), as all these components stimulate the user's
engagement in creative tasks. (Vertegaal Al, 2008; Wigdor Al
2011)

2.3.1 User interaction behavior
In Tamed Cloud, interactions with the data cloud are guided by
certain gestures, partly based on gestures made in the physical
space, with a view to preserve as much as possible the user's
intuitiveness. The instrumentation - user's handheld controllers –
is calibrated according to the body and proprioceptive field and
will serve as an extension of this body and proprioceptive field.
Our aim is to propose new visual representation models, ones that
offer intuitive interactions that immerse the user in the virtual
world. We seek to provide the user a tailored environment in
which she is invited to create a personal relationship with the data,
through various interactions, in a fluent and intuitive way. We
nevertheless are faced with the technological equipment required
to achieve an immersive VR experience. Although the public is
getting more and more familiarized with this kind of technology,
its integration often poses inevitable questions around the
complexity of its usage, as well as the target group of the final
application. Wearing an HMD and holding the controllers, all the
while having to deploy multiple buttons and gestures in order to
achieve certain interactions, may have a certain advert effect on
the immersion process. To avoid the complexity of the usage of
the VR controller, we have implemented the majority of
interactions through the use of just a single button and also, by
realizing certain interactions through gestures inspired from the
user's everyday life. We chose to avoid the use of a complex
graphical user interface (GUI) that would provide the necessary
information and explanations around the controller's usage. The
above solution is inspired of a more general problem related to the
interface technologies, as nowadays they tend to separate the
functions of our body and as a result, to put us out of touch with
ourselves and the environment around us, whether it is real or
virtual (McLuhan, 1964). The reception of the majority of the
information that comes from the digital world is made through
devices that we carry on with us. These devices tend to remove us
from our surroundings and demand our attention. Inspired by the
principles behind natural users interfaces (NUI), many researchers
are concerned by this fact and try to create more calm
technologies, where the interactions are more fluid and intuitive
and take place in our periphery, drawing our attention rather than

We also consider implementing other gestural interactions in the
experience, such as:
- Individual data selection, zoom and display of properties: The
user can designate a data, validate its selection and bring it to the
foreground and then access the metadata describing the selected
image.
- Aggregation in the form of a data trail: The user can select a data
from the cloud, pull it in a certain direction and extract it from the
cloud. The selected image becomes a leader and attracts data with
similar characteristics behind it, forming a data trail that the user
can stretch as she wishes. Data enrichment by Artificial
Intelligence takes place in this selection phase. The aggregate data
within the trail being identified by AI as having aspects of
similarity, according to the analysis of certain of their
characteristics.
- Data trail cut: After stretching the data trail, the user could break
it (e. g. by mimicking a cutting action) and thus isolate the data
from the rest of the cloud, making it available for new
interactions. In addition to the three selection and aggregation
functions described above, we also consider two other "highlevel" functions.
- Data shifting: The user has the ability to filter the data still in the
cloud, or the data previously extracted from the cloud, by
adopting appropriate gestures (e. g. shaking the device), thus
causing the aggregation of data with similar characteristics.
- Depositing and memorizing: Another way to reclaim our data,
after the processes of selection, organization, and sorting, is to
perform a gesture of deposit/attachment of a group of the
processed data in a dedicated storage space - either a pre-designed
scenography or a customized space, for example by a 3D scan of
an intimate space. This operation of memorization via a place
(Topos) will be based on the specific cognitive capacities related
to spatial memory. The aim is to implement and invoke in the
digital spaces techniques of memorization (Ars Memoriae) and
appropriation of information (Curiosity cabinets).

2.4 Artificial intelligence and deep learning
During further development, we will also integrate artificial
intelligence and machine learning functions, simulating cognitive
functionalities. These tools aim to facilitate user interactions with
the data by promoting intuitive interactions. In our case, speech
and gesture interactions, AI will assist the user in her sorting
actions, her research in the data, as well as in modulating the
behavior of the cloud, according to the user’s actions and
biometric criteria.
Verbal interaction: integration of Voice Recognition and Natural
Language functionalities will allow the user to establish a
dialogue with the data, without going through menu-type
interactions, or keywords. Gestural interaction: Integration and
development of tools for qualitative analysis of the gesture
associated with learning tools, will allow a personalized

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 53

recognition of the user’s gesture and its semantic (the meaning of
the gesture) and psychological qualities (the user’s stress state).

environment. The user then can focus on an image by magnifying
it and simultaneously reveal and consult its metadata.

Data and metadata structuring assistance: Integration of Visual
Recognition tools will provide assistance to the user when
searching and organizing metadata, as well as when selecting a
corpus among online data.
Experience management: Integration of machine learning tools
will control and modulate the cloud's behaviors, according to the
user’s experience level, behavior, and biometric data.

3. PROTOTYPE

Figure 3: consultation of metadatas with the magnifying glass
function.

The first exploratory phase of the four major aspects of the project
described here above, led to the realization of a prototype in the
form of a functional immersive virtual reality experience.

She can also chase away the images selected in the cloud, with a
single movement. The user can therefore manipulate and sort this
mass of data, by taming it plastically, physically and emotionally.

3.1 Scenario and Protocol of the experience

3.1.3 Verbal interaction functionalities.

3.1.1 Behavioral functionalities of the cloud

When she enters the virtual environment, the user is faced with
the cloud of data, out of his reach and behaving like an
autonomous swarm of birds. She is right away invited to use vocal
commands to engage in a relation with it.

When she enters the virtual environment, the user is presented
with a cloud of about 4000 pictorial and photographic works that
shows an active and reactive autonomous behavior towards her.
When the user approaches and comes into contact with the cloud,
the latter organizes itself around her and reacts to her presence
and gestures, according to the nature, intensity and rhythm of the
user's behavior. The Tamed Cloud installation provides several
types of interactions through gesture and speech.

3.1.2 Gestural interaction functionalities
One case scenario for the gestural interactions that we
implemented in Tamed Cloud is the controller-as-an-imaginaryobject function. The user is invited to imagine holding the
adequate object to perform each kind of interaction. For example,
a magnifying glass is a tool which is used to closely examine the
details of an image or a text.
The gestural operations are partly modeled on the gestures
performed in a real physical space, thus promoting intuitiveness
and a sense of presence. The interaction tools - the controllers,
that act as an extension of the body and proprioceptive field, as
well as the real-time responses to the user’s gestures and
commands, guarantee an optimal immersion.

Figure 4: selection of an image group by criteria, by voice
recognition.
Using voice control and the functionalities provided by the IBM’s
Watson cognitive platform, the user can ask the cloud to change
its position and formation. According to each given command, the
cloud will react in a specific way. More concretely, the user can
ask the cloud:
- to come, the cloud will accordingly approach and stand idle in
front of the user,
- to go away, the cloud will then move away from her and enter in
its original state of autonomous behavior;
- to reorganize itself either to the dominant color of the depicted
artwork or by chronological order of the dates of the artworks.
As a result, for each of these commands, the cloud will
accordingly reorganize itself either in a circle of colors or a spiral
of time. For both these cases, the cloud will form around the user
wherever she may be, inside the virtual environment.

Figure 2: selection and deployment of a group of images.
By gesture - using the controller - the user can select a group of
images, move it, isolate it and deploy it. By isolating the packages
of artworks she has selected, and by placing them away from the
cloud, she can sketch out a first approach to the functions of
organization and memorization.
During any time of the experience, the user can activate the
functionality of the magnifying glass, which allows her to focus
and examine something more closely. All she has to do, is to perform the gesture that she would perform when holding this object.
Then the controller-as-magnifying-glass is activated. According to
her gesture, a pointer appears in order to help her visualize and
demonstrate the point of focus of the controller inside the virtual

Figure 5: selection of the image organization mode, by voice
recognition.

As an added extra layer of voice interactions, the user can ask the
cloud to isolate a subassembly according to a color criterion, i.e.
red, green or blue color. The use of natural language through

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 54

conversation and vocal commands towards the cloud constitutes a
bridge between the real and the virtual that enhances the feeling of
immersion in the user's environment. To this end, we
implemented in Tamed Cloud a set of artificial intelligence and
machine learning tools provided by the IBM's Watson platform.
Again, in this usage scenario, we had to, first of all, find a way to
integrate these tools in the virtual reality interactive environment
and then, invent a way to seamlessly and fluently activate these
functionalities, when needed from the user. Here once more, we
referred to our everyday activities and real life experience in order
to create a natural interface for the user through her gestures. The
user, in this case, is invited to imagine holding a microphone and
performing the adequate gesture that would allow her to talk in an
actual microphone. Accordingly, the representation of the
controller inside the virtual environment changes to a sphere
which helps her understand and visualize the recording of her
voice. The gesture allows for the activation of a process which
will send data over to the Watson service and wait for the
response, which may trigger interactions with the cloud of data.

3.2 Technical description and development
In terms of software, Tamed Cloud was developed using the
environment provided by Unity 3D. Additional tools, plugins and
frameworks were implemented, in order to create the immersive
virtual world and the interactions inside it, such as the IBM Watson SDK for Unity, which is used to develop and implement the
cognitive interactions that will be described below. The prototype
is presented with an HTC Vive virtual reality headset, deploying
both controllers for the necessary gestural and speech interactions.
Nevertheless, along the development period we aim in producing
a platform independent environment, able to be adapted in various
cases and create seamless interactions across multiple virtual
reality platforms. For the prototype, we chose to visualize a part
of about 4000 images from the MoMA's collection. The works of
art included in this collection are fully described with metadata
inside JSON files, which are analyzed and accessed in real time
throughout the immersive experience. The number of pictures
showcased should by no means be considered as a limitation, as
the handling of large amounts of data resides in the fundamental
principles of Tamed Cloud. In order to enable our virtual reality
platform to handle the visualization and interactions with such
amounts of data, we implemented general-purpose computing on
graphics processing units (GPGPU) algorithms, through the use of
compute shaders in Unity. These types of algorithms serve to
execute massively parallel tasks that run on the graphics card,
outside of the normal rendering pipeline, thus accelerating parts of
the virtual environment rendering and allowing us to significantly
increase the amount of data to be processed and visualized.
Technical Information:
•

HTC Vive headset - 90 Hz refresh rate - 110° viewing
angle - 1,200 x 1,080 px - equipped with sensors,
gyroscope, accelerometer and laser position sensors,

•

Two wireless controllers with gyroscope, accelerometer
and laser position sensors.

• Interaction simulator: Multi-platform game engine developed by
Unity Technologies. One of the most widespread in the video
game industry, as well as in the experimentation of art and science
paradigms, due to its efficiency in prototyping.
• Written in JavaScript, C and Boo

• Artificial intelligence functions: Watson integrates a Docker
container for Unity, and a SDK Unity.1

4. DISCUSSION
4.1 Feedback
The prototype was presented on October 5, 2018 in Paris at La
Cité des Sciences et de l’Industrie, as part of the RT Day
organized by the Institut Cognition. About fifty subjects were able
to experience Tamed Cloud. We did not encounter any
immersion-related disorders or discomforts in the various
subjects. After a rather small phase of about 1 minute, during
which the user was getting acquainted with the virtual
environment and the handling of the controllers, the subjects
moved and interacted quite naturally with the cloud. They seemed
particularly attracted and thrilled by the mass of images
surrounding them when they first came into contact with the cloud
through their presence or gestures. The use of voice recognition
has sometimes been made difficult due to the hustle and bustle of
the physical environment in the demonstration room.
It is very encouraging to see that the experience we were offering
aroused the interest of companies from a wide range of sectors. In
particular, some of them identified a strong potential for
improving business performance and supporting decision-making.
The general reception from the public was positive, however it is
difficult to draw definitive conclusions from this first
presentation.

4.2 Evaluation
The first phase of our research did not allow us to carry out an
evaluation of the system. It is therefore planned as an important
step of the second phase of the development which, at the time of
this publication, is about to start. This second phase will lead us to
a finalized and fully functional Proof of Concept version of
Tamed Cloud, and it is that version that will be used in order to
conduct the scientific evaluation with the proper user audience.
This second phase of the Tamed Cloud project, will aim to
evaluate whether our system promotes optimized access and
processing of the visualized data. We are currently working on
defining in advance our evaluation protocol that will be used for
the study of about ten participants. The data collection and
analysis during this evaluation will constitute an essential link in
our project, as it will allow us to define the next phase, which will
consist, among other, of integrating new artificial intelligence
functionalities, such as image recognition, e-learning and
qualitative movement analysis. Moreover, we plan on measuring
certain physiological data of the subject during the immersive
experience, in order to use them to generate forms of behavioral
interactions of the cloud and data in real time as a response to the
emotional state of the user. This evaluation will allow us, in the
long term, to better develop the concept of Tamed Cloud for
applications in various and diverse fields.
As it stands, the Tamed Cloud allows the visualization of
undifferentiated masses of data, without hierarchical
representation, as well as the selection of data on the y. This type
of data visualization has particular advantages (as well as
disadvantages) that we need to define more precisely, in relation

1

https://github.com/watson- developer- cloud/unityinstalling- the- sdk- source- into- your- unity- project

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 55

sdk#

to the exploration and manipulation constraints specific to
professional tasks.
In particular, we would like to answer the following questions:
• For which type of task could non-hierarchical visualization of
dynamic data masses be the most adequate?
• Does incorporation and living in the same space as the data have
a particular impact on how data are represented and accessed?
• Which profession could benefit particularly from this type of
visualization?
Through the comparison of the Tamed Cloud device with
traditional 2D visualization methods, we will seek to evaluate the
contributions of the particular visualization method introduced by
Tamed Cloud.

behavior. The Prototype was designed and realized under the
direction of François Garnier (EnsadLab), in collaboration with
Fabienne Tsaï (EnsadLab), David Bihanic (EnsadLab), and
Florent Levillain (EnsadLab), as part of a research partnership
with IBM and in collaboration with IBM Europe Human Centric
Innovation Center’s team. Directors of the IBM / EnsadLab
partnership: Marc Yvon, head of IBM Human Centric Innovation
Center, Emmanuel Mahe, head of research EnsAD, and Célestin
Sedogbo, head of Institut Cognition.
IT development
(EnsadLab): Dionysis Zamplaras and Leon Denise. Scientific
advisor: Jean Lorenceau (Institut de la Vision). Special thanks to
the members of the EnsadLab Spatial Media team who
participated in the production of the prototype : Pierre Hénon,
Christophe Pornay, Loup Vuarnesson, Sofia Kourkoulakou, Léa
Mercier, Maxime Tibay.

We will thus propose to the participants a series of tasks to be
carried out according to the two visualization methods; for
example, locating a specific data, or ranking the data on the basis
of criteria of different natures, for example well-defined vs.
poorly defined, concrete vs. abstract, surface lines vs. structural
vs. functional. Visualization methods will be, as a result,
compared on the basis of measurements including the time
required to complete a given task, the prerequisites required to
complete it, cues of frustration in completing the task, etc.

6. REFERENCES

4.3 Future development

[3] Berthoz A., La Vicariance, le cerveau créateur de mondes,
Odile Jacob, 2013

Based on the above described evaluation methods and protocols,
we will be able to better demonstrate to potential future partners
the benefits that the use of our system could bring in their
professional field, in order to recruit and involve them in its
further development.
Our goal in the long term is to initiate large-scale partnerships,
within the framework of a chair dedicated to Artificial and
Cognitive Intelligence. We want to federate around this research a
multidisciplinary team gathering over 3 years, researchers in art
and design, cognitive sciences and computer science. The
objectives of this research group will be:
• Research, develop and evaluate these new modalities of access
to information and research within large data sets;
• Explore and develop the concept of Tamed Cloud to other
potential application areas:
•

In communication: consultation of visual or audiovisual
data;

•

In research: manipulation/analysis of scientific data;

•

In medicine: treatment of memory and behavioral
disorders;

•

In the field of security: assistance in decision-making
when dealing with digital data;

•

Etc.

5. ACKNOWLEDGMENTS
The Tamed Cloud prototype is the product of a collaboration
between EnsadLab, the research laboratory of EnsAD-PSL and
IBM. It is part of the actions taken by the Institut Cognition on the
topic Artificial and Cognitive Intelligence, a domain of research
concerned with the articulation of quantitative data with
biological, biomechanical and psychologic models of human

[1] Ariely, Dan 2001. Seeing sets: Representation by statistical
properties. Psychological Science, 12 2 157–162. PMID:
11340926.
[2] Bennequin D, Berthoz A, (2017) Several Geometries for
Movements Generations, J.-P. Laumond et al. (eds.),
Geometric and Numerical Foundations of Movements,
Springer Tracts in Advanced Robotics 117, DOI
10.1007/978-3-319-51547-2_2

[4] Berthoz A., Le sens du mouvement. Odile Jacob, 1997
[5] Bilda Z., Candy L., & Edmonds E., Designing for creative
engagement, Design Studies, 2008
[6] Card S. K., Mackinlay J. D., Shneiderman B. (1999).
Readings in Information Visualization. Using Vision to
Think, San Francisco, Morgan Kaufmann.
[7] Couchot, E., La nature de l'art : ce que les sciences
cognitives nous révèlent sur le plaisir esthétique, Hermann,
Paris, 2012
[8] Couzin, I. D. (2009). Collective cognition in animal groups.
Trends
in
Cognitive
Sciences,
13(1),
36–43.
https://doi.org/10.1016/j.tics.2008.10.002
[9] Fuchs Philippe. Théorie de la réalité virtuelle, les véritables
usages. Presses de l'Ecole des Mines, 2018
[10] Furnas G. W., Bederson B. B. (1995). Space-Scale
Diagrams: Understanding Multiscale Interfaces, In
Proceedings of CHI’95, pp. 234-241.
[11] Grau, O., Virtual Art: From Illusion to Immersion, MIT
press, 2003
[12] H. Britten, Kenneth, N. Shadlen, Michael, T. Newsome,
William, and Movshon, J. Anthony 1992. The Analysis of
Visual Motion: A Comparison Psychophysical Performance
of Neuronal. Journal of Neuroscience, 12 12 4745–4765.
[13] Hall Edward T. The Hidden dimension, Doubleday & Co,
1966 / La dimension cachée. Seuil, 1971
[14] Heeter C. (1992). Being There: The Subjective Experience of
Presence. Presence: Teleoperators and Virtual Environments,
1(2):262–271. xxvii, 25, 30, 31, 32, 34

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 56

[15] Heider F., & Simmel M. (1944). An experimental study of
apparent behavior. The American Journal of Psychology,
57(2), 243. http://doi.org/10.2307/1416950
[16] Hoffman G., & Ju W. (2014). Designing Robots with
Movement in Mind. Journal of Human-Robot Interaction,
3(1), 89. https://doi.org/10.5898/JHRI.3.1.Hoffman

[28] Salvatore M. Anzalone, S Boucenna, Serena Ivaldi,
Mohamed Chetouani. Evaluating the Engagement with
Social Robots. International Journal of Social Robotics,
Springer, 2015, pp.1-14. <10.1007/s12369-015-0298-7>.
[29] Scholl B., & Tremoulet P. (2000). Perceptual causality and
animacy. Trends in Cognitive Sciences, 4(8), 299–309.

[17] Hollan J. D., Hutchins E., Mccandless T. P., Rosenstein M.,
Weitzman L. (1986). Graphical interfaces for simulation. La
Jolla, Institute for Cognitive Science of the University of
California.

[30] Slater, M. (2009). Place illusion and plausibility can lead to
realistic behaviour in immersive virtual environments.
Philosophical Transactions of the Royal Society of London
B: Biological Sciences, 364(1535):3549–3557. 34, 55

[18] Hutchins E. (1995). Cognition in the Wild. Cambridge, MIT
Press.

[31] Stasko J. T. (1993). Animation in User Interfaces: Principles
and Techniques. Trends in Software, Special issue on User
Interface Software, 1, Chap. 5, pp. 81-101.

[19] Hutchins E., Hollan J., Norman D. (1985). Direct
Manipulation Interfaces. Human-Computer Interaction, 1(4),
pp. 311-338.
[20] Jones C.A. (ed), Sensorium: embodied experience,
technology, and contemporary art. MIT press, 2006
[21] Lee K. M. (2004). Presence, explicated. Communication
Theory, 14(1):27–50. xxvii, 30, 31, 32, 33, 34, 35, 36, 68, 75,
76, 78, 81
[22] Levillain F., & Zibetti E. (2017). Behavioral objects: The
rise of the evocative machines. Journal of Human Robot
Interaction, 6(1), 4-24.
[23] M.-H. Tramus, M. Bret, E. Couchot, La seconde
interactivité. In Arte e vida no século XXI, Organizadora
Diana Domongues, UNESP, Brasil (2003).
[24] Merleau-Ponty M., Phénoménologie de la perception.
Gallimard. Paris, 1945
[25] McLuhan M., 1964. Understanding Media - The extensions
of man. McGraw-Hill Education, Canada.
[26] Poincare H (1902) La Science et l’hypothèse. Paris:
Flammarion.
[27] Robertson G. G., Card S. K., Mackinlay J. D. (1989). The
cognitive coprocessor architecture for interactive user
interfaces. In Proceedings of SIGGRAPH’89, pp. 10-18.

[32] Stasko J. T. (1996). Future research directions in humancomputer interaction. ACM Computing Surveys, 28(4),
Article 145.
[33] Sumpter, D. J. T. (2006). The principles of collective animal
behaviour. Philosophical Transactions of the Royal Society
B:
Biological
Sciences,
361(1465),
5–22.
https://doi.org/10.1098/rstb.2005.1733
[34] Varela, F., Thompson, E., Rosch, E., 1993. L’inscription
corporelle de l’esprit. Editions du Seuil. Paris.
[35] Vertegaal R. and Poupyrev I., 2008. Organic User
Interfaces: Introduction to Special Issue. Communications of
the ACM, 51(6), pages 26-30.
[36] W. Williams, Douglas, and Sekuler, Robert 1984. Coherent
global motion percepts from stochastic local motions. Vision
Research, 24 55–62.
[37] Wigdor D., Wixon D., 2011. Brave NUI world: designing
natural user interfaces for touch and gesture. eBook
ISBN: 9780123822321
[38] Wright P. & McCarthy J. (2008). Empathy and experience in
HCI. In Proceedings of the Conference on Human Factors in
Computing Systems (SIGCHI),

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 57

Exploratory research on the gamification of exercise for
Fibromyalgia using virtual reality
Katherine Hoolahan, MSc
Virtually Healthy LTD
London, UK

katie@virtuallyhealthy.co.uk
ABSTRACT

Fibromyalgia (FM) causes debilitating pain and muscular
stiffness, amongst other symptoms, which can be eased over time
through regular exercise. As FM patients have an abnormal
perception of pain, adherence to exercise programmes can be
difficult due to the pain experienced whilst exercising. Virtual
reality (VR) has been shown to distract users from physical and
mental pain and therefore may be utilized to increase adherence to
exercise. This project involved the creation of a VR game using
the HTC Vive, designed to induce exercise at a moderate intensity
and distract Fibromyalgia patients from pain experienced whilst
exercising. This game was tested using 8 participants who
completed a questionnaire regarding the enjoyability of the game
as well as the intensity of exercise whilst playing the game. All
except two participants felt as if they had exercised at a moderate
intensity or higher whilst playing the game, with the remaining
two participants just under the threshold for moderate exercise.
All participants found the game to be fun, exciting, enjoyable and
entertaining, and all except one would play it again and spend
money on this or similar games. The participant who would not
play this or similar games again fell into the 65+ age range,
supporting research which states that over 65s have little interest
in gamified VR. Half of the participants answered that their
frequency of exercise (for at least 20 minutes) would increase if
they owned this or similar games, with the remaining participants
already exercising regularly.

Keywords

Fibromyalgia, Chronic Pain, Exercise, Mental Health, Virtual
Reality, Motivation, Adherence to Exercise

1. INTRODUCTION

Fibromyalgia (FM) is the second most common condition that is
seen by rheumatologists [1]. FM is a chronic condition which
causes widespread, debilitating pain and muscular stiffness, along
with various other symptoms and common comorbidities such as
fatigue, insomnia and psychological issues [2]. Patients suffering
with FM have an abnormal perception of pain, reacting strongly to
stimuli that would not normally be thought to be painful. It is
thought that the pain leads to anxiety disorders and fatigue leads
to depression [3]. These symptoms associated with FM lead to a
poor quality of life (QoL), with an average ‘perceived present
QoL’ scored at 4.8 on a scale of 1-10 (1 being low and 10 being
high) [4]. Although there is no standardized treatment for FM,
treatment which consists of a combination of approaches such as
psychological interventions, medication and exercise programmes
are the most effective [5]. Psychological interventions generally
include Cognitive Behavioural Therapy and medication may
include painkillers, antidepressants and neuromodulating
antiepileptics [2]. Drugs have recently been developed to
specifically manage pain in FM patients, however these are
thought to come with significant side effects and the efficacy of
using them is questionable [5]. Exercise programmes for FM
focus on increasing cardiovascular fitness, muscular endurance
and flexibility [6][7]. Increased physical activity has been shown

to lead to a decrease in reported pain as well as an increase in the
response of pain regulatory functions within the brain. However,
exercise programmes created for patients with FM generally have
low adherence rates due to the increased pain experienced whilst
exercising, leading to poor physical health and a poor QoL [8].
Gamification refers to the production of game-like experiences
which promote flow, mastery, achievement and intrinsic
motivation with an aim to alter the user’s behaviour and
perceptions [9].
This study explores gamified virtual reality (VR) as a means of
increasing motivation to exercise, with game design focused for
FM patients. The game was designed to provide a distraction to the
physical pain experienced by FM patients whilst exercising, and
increase QoL through increasing cardiovascular fitness and
flexibility (as well as strengthening when used with body weights).
Although the game has been designed for FM patients, it could also
be used by able-bodied players to increase motivation to exercise,
or simply for entertainment.

2. LITERATURE REVIEW

VR has been shown to distract users from anxiety, depression and
physical pain, possibly even changing “the way that the brain
physically registers pain” rather than simply the way it perceives
painful stimuli [10]. Due to these benefits, the use of VR in the
gamification of exercise would be beneficial to affect the
motivation of FM patients to exercise and improve QoL. Recent
research has proven the effectiveness of VR for improving the
self-management of diseases, physical activity, distracting from
pain and discomfort, rehabilitating neuro-cognitive and motor
performance and assisting in the treatment of psychological issues
such as depression and anxiety [11]. It has been found that the
more immersive the environment, the more distracted the user is
from physical and mental pain [12]. The near complete absorption
into the environment whilst using a Head-Mounted Display
(HMD), such as the HTC Vive or Oculus Rift, is best suited to
help with more intense pain [11].
The most widely accepted model in explaining the mechanisms
behind the pain relief induced by using VR is the ‘gate theory’ of
attention, which states how VR absorbs and diverts attention away
from pain, therefore reduces the perception of pain. Research into
the cognitive-affective-motivational role of attention in pain
processing supports the importance of gaming and play elements
in VR interventions aimed at the reduction of pain [11].
Exercise interventions in FM patients have been shown to
improve well-being through decreased fatigue, depression and
pain as well as increased pressure that must be applied to a tender
point before a pain response is triggered [8][13]. It is
recommended that exercise should be performed at a moderate
intensity (approximately 60% of maximum heart rate) 2-3 times
per week [13]. Setting realistic goals and expectations is important
to help deal with the condition throughout daily life. A positive
mindset of the patient is important for actively endeavouring to

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 59

reduce symptoms through methods such as exercise [6]. The
assistance with depression and anxiety that gamified VR presents
(especially when combined with exercise – a treatment for
depression) should be able to help with this [11].
Conflict was found between patients and healthcare professionals
due to the limited resources available (such as time for
consultations), and psychological support was not provided in a
meaningful capacity. A gamified experience of exercising, allows
patients to take a more pro-active approach which may alleviate
conflict between FM patients and healthcare professionals caused
by time restraints. The use of gamified VR exercise may also
reduce the need for psychological support, which is being sought
from healthcare professionals who feel they cannot provide the
required support [5].

3. METHODS
3.1 Game Development

Unity (5.6.0) [19] with SteamVR [20] was used to develop and
build the game and a HTC Vive was used in the creation and
playing of the game. Microsoft Visual Studio [21], Blender 2.78c
[22] and Audacity 2.1.3 [23] were also used in the creation the
game. The game was designed to induce moderate exercise which
works the whole body – legs through bending to pick up a
snowball, arms and chest through throwing (throwing with both
arms is the most effective method) and the core through dodging
snowballs.

Gamification of an exercise programme encourages participation
and positively influences attitude and behaviour towards exercise
whilst allowing for easy goal setting and progression [14]. Goh &
Razikin [15] found that gamification of exercise increases
motivation to participate by shaping behaviour as well as
improving the enjoyment of and general attitude towards exercise.
Gamification is thought to consist of three main parts –
implemented motivational affordances, resulting psychological
outcomes and further behavioural outcomes. Motivational
affordances include aspects such as points, leader boards,
achievements, levels, clear goals, stories, themes, feedback,
progress and challenge. The psychological outcomes of gamified
exercise include increased self-efficacy, improved attitude
towards exercise and decreased perception of pain; all of which
results in the behavioural outcome of increased participation in
exercise [14][16]. These benefits are particularly important in FM
patients due to the psychological and physical barriers to
participation. Changing behaviour and perception of exercise may
be the difference between inactivity, causing deterioration in QoL
and an active lifestyle which vastly improves symptoms over time
and increases QoL.
Figure 2: Screenshots of the game being played

3.2 Testing and Questionnaire

Figure 1: A breakdown of gamification in exercise
Although gamification mostly produces positive results, in some
settings no effect is seen. This is thought to be down to a variety
of factors such as the nature of the gamified experience, the
motivation and preferences of the individual and design aspects
[17]. Gamification may appeal to a large population of people,
however there are populations and individuals that may not find
these concepts as appealing. More consistently positive results
have been found for individuals aged between 20-49 years,
suggesting that this is the most effective age range for gamified
VR. It is thought that people over the age of 65 have little interest
in using VR [18]. These factors may affect the use of VR in
therapy for some populations.

The inclusion criteria for participating in the testing of the game
included; able-bodied or diagnosed with FM, physically fit
enough to participate in moderate exercise and over the age of 18.
Exclusion criteria included a diagnosis of any physical disability
other than FM or chronic pain or any physical injury which may
be affected by participation. Ethical approval was given for this
study to be carried out. To ensure the safety of all participants, a
risk assessment was also carried out and steps were taken to
minimise the risks of participation. 8 participants were recruited
who completed an informed consent form before participating.
The objectives of the game, the VR system and precautions to
minimise the risks were explained to the participants before
participation. Two 5-minute games were played – the first at an
easy difficulty and the second at a slightly harder difficulty. After
playing the game, participants completed a questionnaire
containing 29 questions regarding the enjoyment, playability and
effort put into the game. These were answered using a Likert scale
with a section at the end for any comments. A table of showing
the Rate of Perceived Exertion (RPE) scale [24] was also used to
determine how hard the participant felt they exerted themselves
whilst playing the game. The questionnaire was based off the
‘need satisfaction’ questionnaire [25] which evaluates exergames.
Basic demographic data was collected to assess whether these
factors may affect the answers given. In order to prevent bias, all
answers were kept anonymous.

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 60

3.3 Data analysis

Frequency tables, bar charts and pie charts were constructed to
analyse the data as well as to create a visual representation of it.
This data was analysed both qualitatively and quantitatively.

4. RESULTS & DISCUSSION

The game was found to be fun, exciting, enjoyable and
entertaining by all participants, as seen in figure 1.
Figure 4. Bar charts showing questions regarding how often
this or similar games would be played if owned. Likert scale:
1= never, 2 = occasionally, 3 = two or three times a week, 4 =
once a week, 5 = twice a week, 6 = >twice a week, 7 = every
day

Figure 3. Bar chart showing questions regarding the
enjoyability of the game. Likert scale: 1 = describes the game
poorly, 7 = describes the game very well
Subjects 3 and 8 found the game to be boring; however, these
participants also found the game to be fun, exciting, enjoyable and
entertaining – which is contradictory. Subjects 3 and 8 also did not
feel as if they had exercised at a moderate intensity, giving a
rating of 11 on the RPE Scale (light exercise). This may imply
that the game was too easy, which may be one reason as to why it
was found to be boring and could be fixed by increasing the
difficulty or adding in different game modes. The inadequate
difficulty is supported by Subject 2’s comment “if I was an actual
gamer (regularly played console games) it would probably be too
easy. Would be cool to have levels” as well as subject 4’s
comment “it would be more effective if there were more difficult
levels”. Subjects 3 and 8 also reported that the game was slightly
uncomfortable to play. Subject 3 commented “maybe wireless
goggles would be better”, suggesting that the trailing wire was the
cause of the discomfort – the wire will likely become obsolete
over time as the technology develops. However, this did not seem
ruin the immersion as subject 3 still answered that they felt
immersed in the game. Although subject 3 found the game to be
boring and slightly uncomfortable to play, this participant would
still play this game again and spend money on this or similar
games. Subject 8, whilst finding the game to be fun, exciting,
enjoyable and entertaining, also found it to be boring and slightly
uncomfortable to play - this participant would not spend money on
or play this or similar games again. As subject 8 is in the 65+ age
range, the answers given support research that has shown that
people aged 65 and over generally have little interested in VR and
may not benefit as much from VR therapy [16]. Although subject
8 had not been diagnosed with FM, she believed herself to have
FM, which may have contributed to the discomfort of playing this
game. However, since no formal diagnosis of FM had been made
for subject 8 and no comments were written, no conclusion can be
made regarding the reason for discomfort.
All participants, with the exception of subject 8, indicated that
they would play this or similar games once a week or more. Half
of the participants answered that their frequency of exercise would
increase upon owning this or similar games, this is shown in
figure 2.

Subject 8 answered that frequency of exercise would significantly
decrease upon owning this game. This is thought to be an anomaly
- possibly due to not reading or understanding the question
correctly. The remaining participants answered that they already
exercised frequently – two of whom were already exercising at the
maximum frequency given in the question, therefore frequency
could not be increased. The final participant, subject 3, already
exercised more than twice a week and indicated that there would
be no difference in frequency; which could be due to the large
difference between exercising twice a week and exercising every
day (the options given for 6 and 7 on the Likert scale). Therefore,
if subject 3 would have exercised between one to three extra days
extra per week, this may not have shown in the results of this
question. This participant did however indicate they would play
this or similar games more than twice a week, suggesting that their
exercise frequency may increase. Subject 6, who already exercises
daily, commented that the game was “surprisingly exhausting,
however definitely a good workout and fun whilst being a
challenge”. Subject 1 also noted in the comments section “good
way to distract someone, whilst getting exercise without thinking
about it”. This supports the gate theory of attention that the VR
game acts as a distraction, taking away elements of attention that
would otherwise be focused on pain or exhaustion from exercise.
All participants also felt that they were immersed in the game.
These points indicate that this game would likely distract FM
patients from pain experienced whilst exercising – however
further testing would be required to validate this assumption, and
determine to what extent pain is eased.

Figure 5. Bar charts showing the intensity of exercise whilst
playing the game, using the RPE scale by Borg [24] & Likert
scale: 1 = not true at all, 4 = somewhat true, 7 = very true
All except 2 participants (subjects 3 and 8) felt that they had
exercised at a moderate intensity or higher. Subject 7, who
indicated that they exercised intensely (17 RPE) commented “I
would like to start with 3 minutes work time up slowly”. This
indicates that a wider range of difficulties and different lengths of
time for each game is required to meet the needs of different
people, which could be easily implemented.
All participants indicated that they were motivated to get a high
score and put a lot of effort into the game, as shown in figure 3,
each getting a higher score on the second, herder, level than on the

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 61

first. This shows that the motivational affordance of the scoring
system had a positive effect on participants motivation and effort
put into the game.

Figure 6. Bar charts showing questions regarding motivation
and immersion within the game. Likert scale: 1 = not true at
all, 4 = somewhat true, 7 = very true
With the exception of subject 8 who did not answer the question,
all participants answered that they would play this or similar
games for 20 minutes or longer (up to 60 minutes) at a time. This
meets and exceeds the 20 minutes of gameplay per session that
this study was aiming for.

participants did not feel they had worked at a moderate intensity,
their RPE was on the upper limit of light exercise, which may
have been increased by raising the difficulty of the game, as
testing was completed at a limited difficulty. Subject 8 (in the 65+
age range) found the game to be fun, exciting, enjoyable and
entertaining, however would not choose to play this or similar
games again. This supports previous research stating that people
over 65 are uninterested in gamified VR - however as the younger
population grows older alongside computer technology, this may
change over time. The remaining participants would play again
and spend money on this and similar games, with the majority
answering that owning this and similar games would increase their
frequency of exercise. With the exception of subject 8 who did not
answer, all participants stated that they would play the game for
20 minutes or longer at a time – the option of playing other games
as well would lower the risk of the one game becoming stale, i.e. a
decrease in motivation to play that game over time. The scoring
system provided motivation and incentive, with all participants
stating that they put a lot of effort into the game and were
motivated to obtain a high score. Online scoreboards would allow
for competition between players as well as the option for allowing
easy access for health professionals to monitor patients progress.
From the results of this study, gamified VR is a viable means of
exercise which is fun, exciting, enjoyable and entertaining and
would likely increase motivation and frequency of exercise. This
study provides preliminary research into the effectiveness of using
VR for exercise, with a focus on FM. However further in-depth
research is necessary for the benefits - and any disadvantages - of
gamified VR to be fully understood.
[1]

Figure 7. Pie chart showing the maximum amount of time in
minutes that participants would play this or similar games
per session
Overall, all participants rated the game as satisfactory to highly
satisfactory. This, combined with the other results and comments,
shows that the game was mostly successful at inducing moderate
exercise in a fun and distracting manner, which was satisfying to
play. Although these results are not generalizable to a wider
population due to the small sample size, this gives a good basis on
which to conduct further research in this area. As this study only
focused on preliminary research into the effectiveness of VR as a
means of exercise, it was more feasible to test the game using
able-bodied participants than participants with FM. Future
research should focus on patients with FM and include a larger
number of participants, including participants of various age
ranges, to obtain more generalizable and reliable results. This data
could also then be used to determine differences in the
effectiveness of using VR to exercise between various
populations. To obtain more accurate results, future research in
this area should also test a wider variety of difficulties and length
of games.
There are a variety of extras that could have been added into the
game, however due to time constraints and ease of testing, the
game was kept simple. Extras should be added in at a later date to
increase variability, difficulty and induce different types of
exercise. The implementation of an online scoreboard and
platform to track progress would allow both participants and
health professionals to monitor progress and set goals.

5. CONCLUSION

The game has been shown to be fun, exciting, entertaining and
enjoyable whilst immersing participants within the game and
providing a moderate intensity of exercise. Although two

[2]

[3]

[4]

[5]

[6]
[7]

[8]
[9]

[10]
[11]

[12]

6. REFERENCES

Clauw, D.J., Arnold, L.M. & McCarberg, B.H., 2011. The
science of fibromyalgia. Mayo Clinic Proceedings, 86(9),
907–911.
Bellato, E. et al., 2012. Fibromyalgia syndrome: Etiology,
pathogenesis, diagnosis, and treatment. Pain Research and
Treatment, 2012, 1–17.
Kurtze, N. & Svebak, S., 2001. Fatigue and patterns of pain
in fibromyalgia: Correlations with anxiety, depression and
co-morbidity in a female county sample. Brit. J. Med.
Psychol., 74(4), 523–537.
Centers for Disease Control and Prevention, 2015.
Fibromyalgia.
Available
at:
www.staff.science.uu.nl/~ooste108/ExpC/website5/v1/arthrit
isbasicsfibromyalgia.html. Accessed [25/07/17].
Briones-Vozmediano, E., Vives-Cases, C., Ronda-Pérez, E.
& Gil-Gonzales, D., 2013. Patients’ and professionals’ views
on managing Fibromyalgia. Pain Research and Management,
18(1), 19–24.
Hawkins, R., 2013. Fibromyalgia: A clinical update. J. Am.
Osteopath. Assoc., 113(9), 680–689.
Redondo, J.R. et al., 2004. Long-term efficacy of therapy in
patients with fibromyalgia: A physical exercise-based
program and a cognitive-behavioral approach. Arthritis Care
& Research, 51(2), 184–192.
Busch, A.J. et al., 2011. Exercise therapy for fibromyalgia.
Current Pain and Headache Reports, 15(5), 358–367.
Huotari, K. & Hamari, J., 2012. Defining gamification - A
service marketing perspective. In MindTrek. Tampere: ACM,
17–22.
Wiederhold, B.K., Riva, G. & Wiederhold M.D., eds. ARCTT
2015: Virtual Reality in Healthcare. 158–162.
Trost, Z. et al., 2015. The promise and challenge of virtual
gaming technologies for chronic pain: the case of graded
exposure for low back pain. Pain Management, 5(3), 197–
206.
Tong, X., Gromala, D., Amin, A. & Choo, A., 2015. The
design of an immersive mobile virtual reality serious game in
cardboard head-mounted display for pain management. In

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 62

[13]

[14]

[15]
[16]

[17]

[18]

Proc 5th Int. Symp. Pervas. Comput. Paradig. Mental Hlth.
Milan: Springer International Publishing, 284–293.
Busch, A.J., Barber, K.A., Overend, T.J., Peloso, P.M. &
Schachter, C.L., 2007. Exercise for treating fibromyalgia
syndrome. In Cochrane Database of Systematic Reviews.
Chichester, UK: John Wiley & Sons, Ltd.
Hamari, J. & Koivisto, J., 2013. Social motivations to use
gamification: an empirical study of gamifying exercise. In
Proc. 21st ECIS, Utrecht, Netherlands, June 5–8.
Goh, D. & Razikin, K., 2015. Is gamification effective in
motivating exercise? Human-Computer Interaction, 608-617.
Brauner, P., Valdez, A., Schroeder, U. & Ziefie, M., 2013.
Increase physical fitness and create health awareness through
exergames and gamification the role of individual factors,
motivation and acceptance. In Human Factors in Computing
and Informatics. Berlin: Springer-Verlag, 349–362.
Hamari, J., Koivisto, J. & Sarsa, H., 2014. Does gamification
work? — A literature review of empirical studies on
gamification. In 47th HICSS. Hawaii: IEEE, 3025–3034.
Johnson, C., 2016. The future of connected home health,

Great Chesterford: Plextek. 1-16.
(5.5.6), 2017. Unity 3D. Available at:
https://unity3d.com. Accessed [20/03/17].
Unity 3D, 2017. SteamVR Plugin. Available at:
www.assetstore.unity3d.com/en/#!/content/32647. Accessed
[20/03/17].
Microsoft Visual Studio, 2017. Visual Studio. Available at:
www.visualstudio.com. Accessed [07/06/17].
Blender
2.78c,
2017.
Blender.
Available
at:
www.blender.org. Accessed [20/03/17].
Audacity 2.1.3., 2017. Audacity. Available at:
www.audacityteam.org/. Accessed [07/06/17].
Borg, G.A.V., 1982. Psychophysical bases of perceived
exertion. Medicine and Science in Sports and Exercise,
14(5), 377-382.
Peng, W., Lin, J.H. Pfeiffer, K.A. & Winn, B., 2012. Need
satisfaction supportive game features as motivational
determinants: An experimental study of a self-determination
theory guided exergame. Media Psychol., 15, 175–196.

[19] Unity
[20]

[21]
[22]
[23]
[24]

[25]

Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 63

ConVRgence

3 Days Scientific Conference

2019

VRIC 2019 PROCEEDINGS
VIRTUAL REALITY INTERNATIONAL CONFERENCE
Editor: Simon RICHIR - Publisher: Laval Virtual
Laval Virtual Center Rue Marie Curie 53810 Changé France
2019, April 20-22
www.laval-virtual.com

ISBN
978-2-9566251-2-4
Laval Virtual VRIC 2019 Proceedings - ISBN 978-2-9566251-2-4 - Page 64

EAN
9782956625124

