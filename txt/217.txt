A Real-time Control Approach for Unmanned Aerial Vehicles using
Brain-computer Interface*

arXiv:1809.00346v1 [cs.RO] 2 Sep 2018

Ravi M. Vishwanath1 , Saumya Kumaar1 and S N Omkar1
Abstract— Brain-computer interfacing (BCI) is a technology
that is almost four decades old and it was developed solely
for the purpose of developing and enhancing the impact
of neuroprosthetics. However, in the recent times, with the
commercialization of non-invasive electroencephalogram (EEG)
headsets, the technology has seen a wide variety of applications
like home automation, wheelchair control, vehicle steering etc.
One of the latest developed applications is the mind-controlled
quadrotor unmanned aerial vehicle. These applications, however, do not require a very high-speed response and give
satisfactory results when standard classification methods like
Support Vector Machine (SVM) and Multi-Layer Perceptron
(MLPC). Issues are faced when there is a requirement for
high-speed control in the case of fixed-wing unmanned aerial
vehicles where such methods are rendered unreliable due to
the low speed of classification. Such an application requires the
system to classify data at high speeds in order to retain the controllability of the vehicle. This paper proposes a novel method
of classification which uses a combination of Common Spatial
Paradigm and Linear Discriminant Analysis that provides an
improved classification accuracy in real time. A non-linear SVM
based classification technique has also been discussed. Further,
this paper discusses the implementation of the proposed method
on a fixed-wing and VTOL unmanned aerial vehicles.

I. INTRODUCTION
Research in neurological studies hit a roadblock when the
need arose to understand brain waves. The complexity of
such waves could only be studied using advanced computational tools. Brain-Computer Interfaces (BCIs) or MindMachine Interfaces (MMI) were born out of the need to
capture and analyze the signals on computers. BCI is an artificial system that incorporates the communication between
the brain and an external device or a computer. [1] [2] In
this system the activity of the brain during a certain action
is tapped into using electroencephalogram (EEG) devices
which are sent to central system to be processed and extract
them into control signals, initially devised for rehabilitation
to help people regain motor skills that are lost or absent
now promises a new field of research for both medical and
Engineering applications. The first research on BCIs took
place in 1970 at University of California at Los Angeles
which was aimed at evaluating the ability of BCI to bolster
neuroprosthetics [3][4].
BCI gained further traction when it was used to treat
locked-in syndromes and neuromuscular disconnections. Auditory, visual and facial muscles often lose the reliability
and are often exhausted when in frequent or prolonged
1

All authors are with the Indian Institute of Science, Bangalore
* This project was funded by the JATP, Defense Development Reseach
Organization (DRDO), Govt. of India
Copyrights - 978-1-5386-5323-4/18/$31.00 ©2018 IEEE

use [1]. Every thought or action gives rise to a particular
electrical activity. A state of daydreaming or deep meditation
emits delta waves (0-3 Hz). Sleeping emits theta waves (3-7
Hz). A state of consciousness emits alpha waves (8-12 Hz).
Engagement in a particular activity or problem-solving emits
beta waves (12-38 Hz) [5][12]. An EEG uses this frequency
domain feature to display the users intent in terms of brain
waves onto an external device to monitor and control an
external device. The very first of BCI systems were the P300
spellers which could read the brain signals and read letters
for communication between the patients who were physically
challenged [4]. California medical center came up with braincontrolled prosthetic legs with the idea of helping the patients
with spinal cord injuries walk. The system could be used
by the patient to move the prosthetic in real time [5]. A
similar system was developed in Washington to restore the
hands for the people with neural disconnection [6]. A BCIcontrolled wheelchair was developed for the patients who
could not use the joystick or the same. Different models of
the wheelchair were developed depending on the amount of
control left to the users discretion. [7] The scope of research
of BCI was initially confined to only medical applications
for detection of different brain states such as alertness,
emotion, attention; however, it has now extended to include
engineering and industrial applications as well. In a paper
proposed by Bastian Venthur et. al, the concentration level
of an operator in a factory was examined over a period of
time. They prepared a model that would access the response
time of an operator to the different levels of alert messages.
This would serve helpful in avoiding accidents that could
otherwise occur due to low alertness or fatigue [8]. The
introduction of BCIs in the field of home automation has
been explored by Wei Tuck Lee, et al, wherein they have
prepared a virtual home environment in which an individual
can control appliances directly with the mind [9]. BrainComputer Interface is also found promoting itself into the
field of Aerospace Engineering, there have been successful
attempts made to control a multi-rotor unmanned aerial
vehicle (UAV) using BCI. A successful prototype has been
developed wherein a quadcopter is made to maneuverer
upward, downward, right and left directions depending on the
command received by the pilots brain signals [10]. In another
successful attempt by Vijayendra et. al. [18], the authors
have demonstrated 95% accuracy in brain-computer interface
based control of UAVs, with the trade-off between accuracy
and real-time implementation. The proposed approach is very
efficient but has limited rate of 25 Hz. Moreover, there has
been no attempt so far to control a fixed-wing UAV due

to the complexity of streaming of data at a very high rate
and their subsequent classification of the EEG signals that
is requisite for high-speed UAV control. This paper aims to
solve the complexities involved in the implementation of a
BCI controlled fixed wing aircraft. The main contribution
of this paper is designing the control methodology for
maneuvering a delta-winged UAV using Common Spatial
Paradigm (CSP) and Linear Discriminant Analysis (LDA)
of the EEG signals and its implementation. These methods,
with data processing rate of around 97 Hertz, provide a rapid
classification platform for establishing a stable control link
to the UAV and also achieve a high level of classification
accuracy of 85%.
II. METHODOLOGY
A. Experimental Subjects
There were a total of 14 subjects involved in this experimental study, out of which 5 were female and 9 male
subjects, all aged between 18-24 years. The Selected Subjects
did not have any prior experience in any BCI/HMI related
tasks and a written consent of their participation in the study
was submitted to the ethical committee of Indian Institute of
Science.

Fig. 2: MI tasks for Acquisition

C. Subject Training
Focused thought is critical to design a BCI system of
requisite accuracy as diffused thought process induces noise
in the acquired data. The training of the subjects in this
experiment were performed with the assistance of a cognitive suite called the Xavier Control Panel ,which comes
bundled with the Emotiv SDK. The subjects train to focus
on the movements of a virtual box under minimal sensory
distractions to enable efficient MI task execution.
D. EEG and Brain Data

B. Acquisition Protocol
A protocol was defined and followed for acquisition of
EEG data from the subjects. The protocol dictates four motor
imagery task be performed with rest breaks incorporated in
between each tasks. This ensures that the motor imagery
tasks are reinforced and the retention of information in the
signal is sufficient. Fig.1 visualizes the protocol followed for
data acquisition.

The brain activity is recorded with the assistance of
a commercially available EEG headset, called EPOC+ by
Emotiv Inc.(Fig. 3) which provides 14-channel EEG data.

Fig. 1: Workflow of MI tasks
The Motor Imagery (MI) Acquisition procedure is split
into four tasks. Task 1 involves visualizing left-hand motion
without any physical movement. Task 2 is the same as Task
1 but is performed with the right-hand. Task 3 requires
the subject to visualize left-hand movement along with the
movement of fingers and elbow. Task 4 replicates Task 3 but
is performed with the right-hand. The EEG data acquired
during these 4 tasks is depicted in Fig. 2. The variations in
the EEG data are visually discernable and the accuracy with
which the features can be extracted is solely depended on
the algorithm used.
Finger and elbow movements are used to increase the number of activations in the sensory-motor cortex. Time intervals
in which the tasks are performed were kept arrhythmic so that
the amplitude of activations are preserved which otherwise
tend to wane out when performing a repetitive action.

Fig. 3: EPOC+ Headset (14-channel)
The EEG bands in the human brain activity are usually
classified as :
• Delta - Delta band a frequency of 3 Hz or below. It
tends to be the highest in amplitude and the slowest
waves.
• Theta - This band has a frequency of 3.5 to 7.5 Hz and
is classified as ”slow” activity.
• Alpha - This region is localized between 7.5 and 13 Hz
and is usually best seen in the posterior regions of the
head on each side.
• Beta - Beta activity is ”fast” activity and has frequency
of 14 Hz and greater.

•

Gamma - A gamma signal is a pattern of neural
oscillation in humans with a frequency between 25 and
100 Hz, though 40 Hz is typical.

E. Algorithms
As discussed in the previous sections, in this study we
have used a combination of common spatial paradigm (CSP)
for feature extraction and linear discriminant analysis (LDA)
for model training. The algorithm definitions and appropriate
graphical analysis are discussed in this section.
1) Commom Spatial Paradigm (CSP): CSP algorithm
disintegrates the signal into additive components so that they
have maximal variance difference in two windows.
EEG signals are usually formalized as :
{En }N ∈ Rch × time

Σ1 + Σ2 = UDUT

(9)

where, U is a collection of eigenvectors, and D is a diagnol
matrix
of eigenvalues. Next we try to find the value of P =
√
D−1 UT , and :
Σ̂1 = PΣ1 PT

(10)

Σ̂2 = PΣ2 PT

(11)

Now, any orthonomral matrices Vsatisfy the condition
V T (Σ̂1 + Σ̂2 )V = I Finally, we disintegrate as :

(1)
Σ̂1 = VΛV T

(12)

The range of time trials varied from 100 to 300 seconds.
Now, to apply EEG signals for classification, we must
transform them first. The transformation to a feature vector
takes place as follows :

where, V is a collection of eigenvectors, and Λ is a diagnol
matrix of eigenvalues. The CSP filter set is obtained as :

En ∈ Rch × time 7−→ xn ∈ Rd

W = PT V

(2)

Major points of concern here are :
• Noise reduction has to be done
• Frequency band selection for optimal performance
• Channel selection
Feature matrix then could be obtained as :
X ∈ Rd× N

The descriptions would be :

λ1

T
W Σ1 W = Λ = 

Rd× ch


..

(14)




.
λch

(3)

The purpose of using a spatial filter like CSP, in this
study, is that the signals provided by the algorithm are
easier to classify even with simple methods. The objective of
this section is to design spatial filters that result in optimal
variances for the classification of two motor imagery signals
related to left and right arm movements. The CSP filter (by
Muller-Gerking et al.) is mathematically written as :
S = WTE

(13)

(4)


1 − λ1

T
W Σ2 W = I − Λ = 


..




.

(15)

1 − λch
where λ1 ≥ λ2 ≥ · · · ≥ λch . Hence, the first CSP filter
ω1 provides maximal variance for 1st class, and the last filter
ω2 for 2nd class. The first and last m filters are selected in
the following manner :

Rd× time

where W ∈
is a spatial filter matrix and S ∈
is the filtered signal matrix. The fundamental of CSP is to
maximize Eq. 5 :
trW T Σ1 W
(5)

ωch ) ∈ R2m×ch
(16)
and the filtered signal mathematically is :

W csp = (ω1

···

ωm

ωch−m+1

···

with subject to Eq. 6
W T (Σ1 + Σ2 )W = I

s(t) = W Tcsp e(t) = (s1 (t)

(6)

En ETn
trEn ETn

Σ2 = Exp

En ETn
trEn ETn

sd (t))T ,

(17)

i.e., d = 2m.
Feature vector x = (x1 , x1 , . . . , xd )T , is then calculated as

where,
Σ1 = Exp

···

En ∈ {class1}

(7)

:


En ∈ {class2}

(8)

Above equations are solved with the help of generalized
eigen value problem. Initially, we decompose as :

var[si (t)]
xi = log
d
∑i=1 var[si (t)]


(18)

Now that we have our feature space constructed, we wo
ahead by beginning the model training using LDA.

F. Linear Discriminant Analysis
Fisher’s linear discriminat analysis (LDA), a very popular
binary classifier, is based on mean vectors and covariance
matrices of patterns of each individual class. Here, we
attempt to convert a d-dimensional vector x to a scalar z
as :
z = wT x

(19)

Basically, the LDA provides us with an optimal projection
w so that z becomes easy to discriminate. The fundamental
of LDA (criterion) is maximizing :
J(w) =

(m1 − m2 )2
,
s1 + s2

(20)

where,
• m1 and m2 are averages for zn ∈ class 1 and zn ∈ class
2 respectively
• s1 and s2 are variances for zn ∈ class 1 and zn ∈ class
2 respectively
The variables are so defined as :
(m1 − m2 )2 = (wT µ1 − wT µ2 )(wT µ1 − wT µ2 )T
(s1 + s2 ) = wT Σ1 w + wT Σ2 w

Fig. 4: Binary Classification using LDA

(21)
(22)

where,
• µ1 and µ2 are averages for xn ∈ class 1 and xn ∈ class
2 respectively
• Σ1 and Σ2 are variances for xn ∈ class 1 and xn ∈ class
2 respectively
The cost function J(w), can then be written as :
J(w) =

wT SB w
wT SW w

(23)

where,
SB = (µ1 − µ2 )(µ1 − µ2 )T

(24)

SW = Σ1 + Σ2

(25)

and then the final solution is given by the following :
−1
ŵ ∝ SW
(µ1 − µ2 )

(26)

Finally, an appropriate z0 threshold is chose for accurate
categorization of any x by :
ŵT x ≥ z0 → x ∈ {class1}

(27)

ŵT x < z0 → x ∈ {class2}

(28)

A classification example from our testing is shown below in
Fig. 4 :
The overall workflow of the process is demonstrated by
the flow diagram in Fig. 5 :

Fig. 5: Workflow for EEG Classification

G. Support Vector Machine with Non-Linear Kernel
With the LDA-based approach we were abale to achieve
a classification rate of 89% for a 2-class input system. However, for a multi-class categorization, we cannot use LDA,
so we used non-linear SVM classification technique and
implemented it for 4-class inputs. SVM was used primarily
because it has avery small memory footprint and is almost
real-time when put to test.
Suppose we have a dataset X (i) , Y (i) , i = 1,2,. . . ,m and
X ∈ Rd , Y ∈ R1 with the seperating hyperplane defined as
W T X + b = 0 such that,
WT X + b > 0

if

Y (i) = +1

(29)

WT X + b < 0

if

Y (i) = −1

(30)

Now we suppose that the training data (linearly seperable)
satisfy the following conditions :
WT X + b ≥ +1
T

W X + b ≤ −1

f or Y (i) = +1

(31)

(i)

(32)

f or Y

= −1

Combining the above to a single condition, we get :
Y (i) (WT X + b) ≥ 1

f or

∀i

(33)

Our task now is to find a hyperplane(W,b) with maximal
distance between itself and the closest data points, while
obeying the mentioned constraints. Mathematically :
max(

2
)w.r.t.(W, b)
||W||

primal = max(minL(W, b, α))

where the primal problem is defined as :
 T 
W W
min
2

K(x, z) = hφ (i) , φ ( j) i

(34)

The trick to designing a SVM is to solve the DUAL of
the above inequality. It can be proved that :
min

maxi:Y (i) =−1W ∗T X (i) + mini:Y (i) =1W ∗T X (i)
(44)
2
Now if the input feature space is non-linearly seperable, we
map the input feature space into another space where it can
be classified linearly. We assume that Φ : X → F be a nonlinear map form input X to a higher dimensional feature
space, F. So, the inner product hX (i) , X ( j) i in the higher
dimensions is hφ (i) , φ ( j) i.
The easiest way to compute the inner product in the feature
space (higher dimensional space) is by using the Kernel
Function. It is defined as :
b=−

(45)

Graphically, the transformationm would look something as
shown in Fig. 6.

(35)

(36)

To begin solving our above problem, we construct the
Langrangian :
m

1
L(W, b, α) = ||W||2 − ∑ αi [Y (i) (WT X (i) + b) − 1]
2
i=1

(37)

where, α is a Langragian multiplier with the condition αi ≥
0. We have to minimize it w.r.t. W and b;we set the respective
derivatives equal to zero. Derivative w.r.t. W and b if set to
zero:
m
W = ∑ αiY (i) X (i)

(38)

Fig. 6: Non-linearly seperable data (on the left) projected
onto a space where it is linearly seperable (on the right)
using a non-linear Kernel Function.
So, the optimization and decision functions are rendered
respectively as :
m

max

D(α) = ∑ αi −
i=1

i=1

m

m

∑ αiY (i) = 0

(39)

i=1

Putting the results back into the Lagrangian leaves us with :
m

1 m (i) ( j)
Y Y αi α j (X (i) )T (X ( j) )
2 i,∑
i=1
j=1
(40)
Now the DUAL problem reduces to the eq.:

F(X) = Sign( ∑ αiY (i) K(X (i) , X) + b)

m

D(α) = ∑ αi −
i=1

1 m (i) ( j)
Y Y αi α j (X (i) X ( j) )
2 i,∑
j=1

(41)

Solving the above optimization would give us αi . Moreover,
the Karush-Kuhn-Tucker condition is satisfied on this solution :
αi [Y (i) (W T X (i) ) − 1] = 0

The above equations are solved with appropriate choice
of the kernel function, which in our case, we chose a
Polynomial Kernel Function of degree d, defined as:
K(X,Y ) = (X T Y + 1)d

(48)

Solving the above equations, we get the classification/prediction label for each time stamped EEG data.
III. HARDWARE INTERFACE
In order to demonstrate the real-time computational capabilities of our BCI, we integrated it with a fixed wing
UAV (2-command control) and to a multi-rotor (4-command
control).
A. Delta Wing

f or

i = 1, 2, . . . , m

(42)

Now, W and b could be found using respectively :
m

W = ∑ αiY (i) X (i)
i=1

(47)

i=1

L(W, b, α) = D(α) = ∑ αi −

max

1 m (i) ( j)
Y Y αi α j KhX (i) X ( j) i (46)
2 i,∑
j=1

(43)

The testing was done on a prototype first, before deploying
it in actual flight. The output from the prediction framework
(LDA based 2 class classification) is sent to a microcontroller
unit. An Arduino (ATmega328P) board is used to control a
prototype elevon of a delta configuration made of chloroplast.

The model is pre-programmed to do a particular action
with the help of servo motors depending upon the type of
input commands received. The fig below (Fig. 7) shows the
working model.

UAV platform (Fig. 9) so that the inherent stability is not
compromised.

Fig. 7: Prototype Demonstration
Fig. 9: Real-time testing
B. Multi-Rotor
In order to establish a proof of concept , an off-the-shelf
quadrotor UAV platform (Fig. 8) compatible with the Python
programming language was used. The algorithm, also written
in Python, is hosted on a ground station that is in constant
duplex communication with the UAV via WiFi. The UAV
host an array of flight instruments such as
• front camera
• bottom camera (low field-of-view)
• 3-axis accelerometer
• 3-axis gyroscope
• a magnetometer
• barometer
• ultrasonic sensor
• motherboard

IV. RESULTS
The interface works pretty well in real time [18], at a
rate of 90 Hz. Individual results of each subject have been
tabulated below :
TABLE I: Performance Evaluation
Subject ALDA ANLSV M T f ocus Tmax
1
81%
79%
150 s 287 s
2
83%
83%
143 s 295 s
3
82%
79%
156 s 304 s
4
89%
83%
151 s 309 s
5
85%
80%
144 s 322 s
6
77%
78%
139 s 254 s
7
82%
81%
149 s 322 s
8
79%
77%
155 s 330 s
9
87%
82%
129 s 310 s
10*
98%
96%
133 s 505 s
11
91%
88%
143 s 303 s
12
92%
90%
166 s 367 s
13
90%
85%
162 s 332 s
where, ALDA represents the testing accuracy for LDA (2class), ANLSV M represents the testing accuracy for NonLinear SVM (4-class) classifier, T f ocus represents the average
time taken to focus on a certain motor imagery task and Tmax
is the maximum focus time.
*The increased performance in case of subject 10 could
be attributed to the fact that the subject had been doing Yoga
for some years. Yoga has been known to enhance mental
and physical performance in terms of memory, focus duration
(Akhtar et al. [16]) and physical stabililty (Omkar et al. [17]).

Fig. 8: AR Parrot 2.0 used for testing

V. CONCLUSIONS

The inner control loop program is embedded onto the
motherboard while the outer navigation loop is dictated
by an open-source Python library called python-ardrone
which is hosted on the ground station. Sensor data fusion is
achieved by using Extended Kalman Filtering (EKF) method
extensively. The implemented algorithm sends only highlevel commands (NLSVM based 4-class classification) to the

A system was developed which takes EEG (electroencephalography) signals as input, modifies the signal for
feature extraction and interfaced with elevon for controlling
it wireless connection. The raw EEG data was extracted from
the brain of the subject using the EMOTIV EPOC+ headset.
The raw EEG data is a result of only intuitive thinking
without any actual physical movements. The data was then
transformed in a suitable data form to be processed. Further

removal of artefacts, unwanted frequencies and irregular data
was done. The processed data was then used for preparing
the model for machine learning using LDA analysis method.
Suitable markers to mark the Event Related Potential (ERP)
to train the machine for evaluate were added. A twin dataset
is applied to the created model, to calculate the misclassification and the error percentage in the same dataset and
the twin dataset respectively. The error percentage is 11%.
An offline analysis was done by encoding the signals into
commands for the fixed wing Elevon. The final interfacing
was done with the Elevon and the repeated creation, testing,
evaluation, and deployment of the models were done to reach
this accuracy. The BCI is also tested on an off-the-shelf
multi-rotor, with classification accuracy as high as 90%, for
4-class based control. This work can further be extended
to control other kinds of UAV and the complexity can be
increased and customized based on the requirement. The
modularity, remote access and control of interfaced Elevon
based on pure brain signals in a BCI system is demonstrated.
ACKNOWLEDGEMENTS
We give warm thanks to Mehvesh Ibrahim, Satya Shree
and Kapil Bharadwaj for providing assistance with the data
collection for the Delta-Wing. We also thank Priya Rao,
Ankita Verma, Akshay Khokkar and Likith Reddy from NIT
Sringar, for helping collect data and testing the classification
on the multi-rotor. We also extend our regards to Navaneethkrishnan B for the helpful discussions.
R EFERENCES
[1] Cheng, Ming, Xiaorong Gao, Shangkai Gao, and Dingfeng Xu.
”Design and implementation of a brain-computer interface with high
transfer rates.” IEEE transactions on biomedical engineering 49, no.
10 (2002): 1181-1186.
[2] Graimann, Bernhard, Brendan Allison, and Gert Pfurtscheller. ”Braincomputer interfaces: A gentle introduction.” In Brain-Computer Interfaces, pp. 1-27. Springer Berlin Heidelberg, 2009.
[3] Vidal, JJ (1973). ”Toward direct brain-computer communication”.
Annual Review of Biophysics and Bioengineering. 2 (1): 15780. PMID
4583653. doi:10.1146/annurev.bb.02.060173.001105.
[4] J. Vidal (1977). ”Real-Time Detection of Brain Events in EEG” (PDF).
IEEE Proceedings. 65 (5): 633641. doi:10.1109/PROC.1977.10542.
[5] Wolpaw, Jonathan R., Niels Birbaumer, William J. Heetderks, Dennis
J. McFarland, P. Hunter Peckham, Gerwin Schalk, Emanuel Donchin,
Louis A. Quatrano, Charles J. Robinson, and Theresa M. Vaughan.
”Brain-computer interface technology: a review of the first international meeting.” IEEE transactions on rehabilitation engineering 8, no.
2 (2000): 164-173.
[6] Fazel-Rezai, Reza, Brendan Z. Allison, Christoph Guger, Eric W. Sellers, Sonja C. Kleih, and Andrea Kbler. ”P300 brain computer interface:
current challenges and emerging trends.” Frontiers in neuroengineering
5 (2012).
[7] Sivaranjani, B., and P. Uvarekha. ”NEURAL NETWORKS BRAIN
CONTROLLED ARTIFICIAL LEGS.” NEURAL NETWORKS 3, no.
2 (2014).
[8] Chen, Chih-Wei, Chou-Ching K. Lin, and Ming-Shaung Ju. ”Hand
orthosis controlled using brain-computer interface.” Journal of Medical
and Biological Engineering 29, no. 5 (2009): 234-241.
[9] Carlson, Tom, and Jose del R. Millan. ”Brain-controlled wheelchairs:
a robotic architecture.” IEEE Robotics & Automation Magazine 20,
no. 1 (2013): 65-73.
[10] Venthur, Bastian, Benjamin Blankertz, Manfred F. Gugler, and Gabriel
Curio. ”Novel applications of BCI technology: psychophysiological
optimization of working conditions in industry.” In Systems Man and
Cybernetics (SMC), 2010 IEEE International Conference on, pp. 417421. IEEE, 2010.

[11] Lee, Wei Tuck, Humaira Nisar, Aamir S. Malik, and Kim Ho Yeap.
”A brain computer interface for smart home control.” In Consumer
Electronics (ISCE), 2013 IEEE 17th International Symposium on, pp.
35-36. IEEE, 2013.
[12] LaFleur, Karl, Kaitlin Cassady, Alexander Doud, Kaleb Shades, Eitan
Rogin, and Bin He. ”Quadcopter control in three-dimensional space
using a noninvasive motor imagery-based braincomputer interface.”
Journal of neural engineering 10, no. 4 (2013): 046003.
[13] Hong, Keum-Shik, and Hendrik Santosa. ”Current BCI technologies in brain engineering.” In Robotics, Biomimetics, and Intelligent
Computational Systems (ROBIONETICS), 2013 IEEE International
Conference on, pp. 1-4. IEEE, 2013.
[14] Hundia, Rohan. ”Brain Computer Interface-Controlling Devices Utilizing The Alpha Brain Waves.” International Journal of Scientific &
Technology Research 4, no. 01 (2015).
[15] Belluomo, Paola, Maide Bucolo, Luigi Fortuna, and Mattia Frasca.
”Robot Control Through Brain Computer Interface For Patterns Generation.” In AIP Conference Proceedings, vol. 1389, no. 1, pp. 10311034. AIP, 2011.
[16] Akhtar, Pooja, Sujata Yardi, and Murtaza Akhtar. ”Effects of yoga on
functional capacity and well being.” International journal of yoga 6,
no. 1 (2013): 76.
[17] Omkar, S. N., and S. Vishwas. ”Yoga techniques as a means of core
stability training.” Journal of bodywork and movement therapies 13,
no. 1 (2009): 98-103.
[18] Vijayendra, Abijith, Saumya Kumaar Saksena, Ravi M. Vishwanath,
and S. N. Omkar. ”A Performance Study of 14-Channel and 5-Channel
EEG Systems for Real-Time Control of Unmanned Aerial Vehicles
(UAVs).” In 2018 Second IEEE International Conference on Robotic
Computing (IRC), pp. 183-188. IEEE, 2018.
[19] A video of the project could be watched at the following link :
https://www.youtube.com/watch?v=zt1AdiktwXs

