Real-time EEG-based Emotion Recognition and
its Applications
Yisi Liu, Olga Sourina, and Minh Khoa Nguyen
Nanyang Technological University
Singapore
{LIUY0053,EOSourina,RaymondKhoa}@ntu.edu.sg

Abstract. Since emotions play an important role in the daily life of
human beings, the need and importance of automatic emotion recognition has grown with increasing role of human computer interface applications. Emotion recognition could be done from the text, speech,
facial expression or gesture. In this paper, we concentrate on recognition of “inner” emotions from electroencephalogram (EEG) signals. We
propose real-time fractal dimension based algorithm of quantification
of basic emotions using Arousal-Valence emotion model. Two emotion
induction experiments with music stimuli and sound stimuli from International Affective Digitized Sounds (IADS) database were proposed
and implemented. Finally, the real-time algorithm was proposed, implemented and tested to recognize six emotions such as fear, frustrated,
sad, happy, pleasant and satisfied. Real-time applications were proposed
and implemented in 3D virtual environments. The user emotions are recognized and visualized in real time on his/her avatar adding one more
so-called “emotion dimension” to human computer interfaces. An EEGenabled music therapy site was proposed and implemented. The music
played to the patients helps them deal with problems such as pain and
depression. An EEG-based web-enable music player which can display
the music according to the user’s current emotion states was designed
and implemented.
Keywords: emotion recognition, EEG, emotion visualization, fractal
dimension, HCI, BCI

1

Introduction

Nowadays, new forms of human-centric and human-driven interaction with digital media have the potential of revolutionising entertainment, learning, and
many other areas of life. Since emotions play an important role in the daily life
of human beings, the need and importance of automatic emotion recognition
has grown with an increasing role of human computer interface applications.
Emotion recognition could be done from the text, speech, facial expression or
gesture. Recently, more researches were done on emotion recognition from EEG
[6, 18, 27, 28, 33, 43]. Traditionally, EEG-based technology has been used in medical applications. Currently, new wireless headsets that meet consumer criteria

2

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

for wearability, price, portability and ease-of-use are coming to the market. It
makes possible to spread the technology to the areas such as entertainment, elearning, virtual worlds, cyberworlds, etc. Automatic emotion recognition from
EEG signals is receiving more attention with the development of new forms of
human-centric and human-driven interaction with digital media. In this paper,
we concentrate on recognition of the “inner” emotions from EEG signals as humans could control their facial expressions or vocal intonation.
There are different emotion classifications proposed by researchers. We follow
two-dimensional Arousal-Valence model [38]. This model allows mapping of the
discrete emotion labels to the Arousal-Valence coordinate system. One of emotion definitions is as follows: “The bodily changes follow directly the perception
of the exciting fact, and that our feeling of the changes as they occur is the emotion” [20]. Our hypothesis is that the feeling of changes can be noticed from EEG
as fractal dimension changes. We focused on study of fractal dimension model
and algorithms, and proposed a fractal based approach to emotion recognition.
To evoke emotions, different stimuli could be used: visual, auditory, and combined. They activate different areas of the brain. Our hypothesis is that emotions
have spatio-temporal location. There is no easily available benchmark databases
of EEG labeled with emotions. But there are labeled databases of audio stimuli for emotion induction - International Affective Digitized Sounds (IADS) [8]
and visual stimuli - International Affective Picture System (IAPS) [26]. Thus,
we proposed and carried out one experiment on emotion induction using IADS
database of labeled audio stimuli. We also proposed and implemented an experiment with music stimuli to induce emotions by playing music pieces. Both
experiments were carried out with prepared questionnaires for the participants
to label the recorded EEG with the corresponding emotions.
There are a number of algorithms for recognizing emotions. The main problem of such algorithms is a lack of accuracy. Research is needed to be carried
out to evaluate different algorithms and propose algorithms with the improved
accuracy. As emotion recognition is a new area, a benchmark database of EEG
signals for different emotions is needed to be set up, which could be used for
further research on EEG-based emotion recognition. Until now, only limited
types of emotions could be recognized. Research could be done on more types of
emotions recognition. Additionally, most of the emotion recognition algorithms
were developed for off-line data processing. In our paper, we target on real-time
emotion recognition and its applications. The user emotions are recognized and
visualized in real time on his/her avatar. We add one more so-called “emotion
dimension” to human computer interfaces. Also an EEG-based music therapy
and a music player are implemented with our real-time emotion recognition algorithm. Although in this paper, we describe standalone implementations of
emotion recognition and its applications, it could be easily extended for further
use in collaborative environments/cyberworlds.
In Section 2.1, review on emotion classification is given. In Section 2.2, emotion induction experiments are introduced. In Section 2.3, emotion recognition
algorithms from EEG are reviewed. In Section 2.4, a fractal dimension algorithm

Real-time EEG-based Emotion Recognition and its Applications

3

proposed by Higuchi is described. Our approach, emotion induction experiments,
a novel fractal-based emotion recognition algorithm, data analysis and results
are given in Section 3. Real-time emotion recognition and visualization of human
emotions on 3D avatar using Haptek system [2], the EEG-based music therapy
and the EEG-based music player are described in Section 4. In Section 5, conclusion and future work are given.

2
2.1

Related Works
Emotion Classification

There are different emotion classification systems. The taxonomy can be seen
from two perspectives: dimensional and discrete one [32]. Plutchik defines eight
basic emotion states: anger, fear, sadness, disgust, surprise, anticipation, acceptance and joy. All other emotions can be formed by these basic ones, for
example, disappointment is composed of surprise and sadness [36]. Another approach towards emotion classification is advocated by Paul Ekman. He revealed
the relationship between facial expressions and emotions. In his theory, there
are six emotions associated with facial expressions: anger, disgust, fear, happiness, sadness, and surprise. Later he expanded the basic emotion by adding:
amusement, contempt, contentment, embarrassment, excitement, guilt, pride in
achievement, relief, satisfaction, sensory pleasure, and shame [13].
From the dimensional perspective, the most widely used one is the bipolar model where arousal and valence dimensions are considered. This emotion
classification approach is advocated by Russell [38]. Here, the arousal dimension ranges from not aroused to excited, and the valence dimension ranges from
negative to positive. Another fundamental dimension is an approach-withdrew
dimension which is based on the motivating aspects of the emotion [32]. For
example, in this theory, anger is an approach motivated emotion in some cases,
as it could encourage the person to make effort to change the situation.
The dimensional model is preferable in emotion recognition experiments due
to the following advantage: dimensional model can locate discrete emotions in
its space, even when no particular label can be used to define a certain feeling
[10, 32].
2.2

Emotion Induction Experiments

In order to obtain affective EEG data, experiments are carried out with different
kinds of stimuli such as audio, visual, and combined ones to induce emotions.
Among the EEG-based emotion recognition works which implemented experiments using audio stimuli to collect EEG data, there are some works where
subjects’ emotions were elicited by pre-labeled music with emotions. For example, in [28], it was reported that emotions were induced in 26 subjects by
pre-assessed music pieces to collect EEG data. A 90% classification accuracy
rate was received to distinguish four kinds of emotions: joy, anger, sadness and

4

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

pleasure. 32 channels were used, and a Multiclass Support Vector Machine was
applied for the classification.
Another types of audio stimuli used in works on emotion recognition from
EEG are retrieved from the labeled databases of audio stimuli, IADS database.
For example, in [6], four kinds of emotion states including positive/aroused, positive/calm, negative/calm and negative/aroused were induced by sounds clips
from IADS. The Binary Linear Fisher’s Discriminant Analysis was employed
to do the classification. They achieved 97.4% maximum rate for arousal levels
recognition and 94.9% maximum rate for valence levels with Fpz and F3/F4
channels.
For experiments using visual stimuli, IAPS is a preferred choice. [6] also
selected pictures from IAPS, however, it was reported that the EEG data collected with visual stimuli experiments are more difficult to classify. eNTERFACE
project described in [42] established an EEG database using pictures selected
from IAPS as stimuli, and 3 emotional states were elicited as follows: exciting
positive, exciting negative, and calm state. Though this project did not target
emotion recognition from EEG signals, they published EEG data labeled with
emotions that were cited by other works on EEG-based emotion recognition as
a benchmark. For example, [22] combined correlation dimension with statistical
features which improved the results from 66.6% to 76.66%. [43] also used IAPS
and it was reported that valence level was recognized with 62.07% accuracy.
Another form of visual stimuli that was used in [35] employed a Mirror Neuron
System. Pictures of affective facial expressions were used.
Combined stimuli were used in [51]. Films were selected to be the stimuli in
that work.

2.3

Emotion Recognition Algorithms

There are an increasing number of researches done on EEG-based emotion recognition algorithms. In [28], short-time Fourier Transform was used to calculte
the power difference between 12 symmetric electrodes pairs with 6 different
EEG waves for feature extraction and Support Vector Machine (SVM) approach
was employed to classify the data into different emotion modes. The result was
90.72% accuracy to distinguish the feelings of joy, sadness, anger and pleasure. A
performance rate of 92.3% was obtained in [6] using Binary Linear Fisher’s Discriminant Analysis and emotion states among positive/arousal, positive/calm,
negative/calm and negative/arousal were differentiated. SVM was applied in [18]
for emotion classification with the accuracy for valence and arousal identification as 32% and 37% respectively. By applying lifting based wavelet transforms
to extract features and Fuzzy C-Means clustering to do classification, sadness,
happiness, disgust, and fear were recognized in [33]. In [43], optimization such
as different window sizes, band-pass filters, normalization approaches and dimensionality reduction methods were investigated, and it achieved an increase
in accuracy from 36.3% to 62.07% by SVM after applying these optimizations.
Three emotion states: pleasant, neutral, and unpleasant were distinguished. By

Real-time EEG-based Emotion Recognition and its Applications

5

using Relevant Vector Machine, differentiation between happy and relaxed, relaxed and sad, happy and sad with a performance rate around 90% was obtained
in [27].
Although the number of researches done on EEG-based emotion recognition
algorithms in recent years has been increasing, EEG-based emotion recognition is still a new area of research. The effectiveness and the efficiency of these
algorithms, however, are somewhat limited. Some unsolved issues in current algorithms and approaches are listed as follows:
1. Time constrains. The performance time consists from the time of feature
extraction and time of classification. The number of electrodes used in the
emotion recognition puts another time constrain on the algorithms. As a
result, to our best knowledge, most of the algorithms were proposed for
off-line emotion recognition.
2. Accuracy. The accuracy of the EEG-based emotion recognition still needs to
be improved. The accuracy decreases when more emotions are needed to be
recognized.
3. Number of electrodes. From the perspectives of the time to set up the EEG
device, the comfort level of the users who wear the device and the amount
of features to process, the number of electrodes should be reduced. However,
most of the current works still require relatively big number of electrodes.
For example, 16 channels were used in [43], and more than 32 channels were
used in [11, 12, 28].
4. Number of the recognized emotions. Although there are varieties of emotional states to describe the human’s feelings, until now only limited types
of emotions can be recognized using EEG. The best performance obtained
was reported in [35] where 3 channels were used and 83.33% maximum accuracy was achieved for differentiating 6 emotions.
5. Benchmark EEG affective databases. So far, a very few benchmark EEG
databases with labeled emotions are available. EEG affective databases with
different stimuli such as visual and audio are needed to be set up for future
researches.
Additionally, as the brain is a complicated system, the EEG signal is nonlinear and chaotic [23, 24]. However, little has been done to investigate chaos of
brain for emotion recognition. [6, 18, 27, 28, 33, 43] were based on linear analysis, however, linear analysis such as Fourier Transform only preserves the power
spectrum in the signal, but destroys the spike-wave structure [49].
A fractal dimension analysis is suitable for analyzing nonlinear systems and
could be used in real-time EEG signal processing [4, 29]. Early work such as
[37] showed that fractal dimension could reflect the change of EEG signal; [30]
showed that fractal dimension varied for different mental tasks; a more recent
work like [24] revealed that when brain processed tasks which were of emotional
difference only, fractal dimension can be used to differentiate these tasks. [44, 46]
used music as stimuli to elicit emotions, and applied fractal dimension for the
analysis of the EEG signal. [47, 52] applied fractal dimension to detect the concentration level of the subjects and developed EEG-based “serious” games. All

6

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

these works show that fractal dimension is a potentially promising approach to
investigate EEG-based emotion recognition. In our research, fractal dimension
model is explored to provide better accuracy and performance in EEG-based
emotion recognition.
2.4

Fractal Dimension Model

For calculation of fractal dimension value, we implemented and analyzed two
well-known algorithms: box-counting [5] and Higuchi [17]. Both of them were
evaluated using Brownian and Weierstrass functions where “true value” is known
[31]. Higuchi algorithm was chosen to process the data since it gave a better
accuracy as it was closer to the theoretical FD values [53] and it outperformed
in the processing of EEG data [45].
Let us describe the Higuchi algorithm as we apply it for FD calculation in
our proposed fractal-based emotion recognition algorithm shown in Section 3.
Let X(1), X(2), . . . , X(N ) be a finite set of time series samples, the new time
series is constructed as follows:
Xkm : X(m), X(m + k), X(m + 2k), . . . , X(m + [

N −m
] · k) .
k

(1)

where m = 1, 2, . . . , k, m is the initial time and k is the interval time.
Then, k sets of Lm (k) are calculated as follows:

Lm (k) =

n P N −m
o
]
[
−1
( i=1k |X(m + ik) − X(m + (i − 1) · k)|) [ NN−m
]·k
k

k

.

(2)

hL(k)i denotes the average value over k sets of Lm (k) and the relationship
exists as follows:
hL(k)i ∝ k −D .

(3)

Finally, the fractal dimension can be obtained by logarithmic plotting between different k and its associated hL(k)i [17].

3

Fractal Dimension Based Approach to Emotion
Recognition

In this paper, we proposed a fractal dimension based approach to EEG-based
emotion recognition. First, we designed and implemented emotion induction experiments using two-dimensional model to describe emotions. Then, we analyzed
EEG recordings with Higuchi algorithm and our proposed algorithm for online
emotion recognition.

Real-time EEG-based Emotion Recognition and its Applications

3.1

7

Emotion Induction Experiments

As we mentioned in Introduction, there is no easily available benchmark database
of EEG recordings with labeled emotions. We designed two experiments to elicit
emotions with audio stimuli. Five truncated songs which lasted for 1 minute each
were used in Experiment 1. The following music was chosen for the targeted emotions: “Wish to Wish” (S.E.N.S) for negative/low aroused (sad), “Cloud Smile”
(Nobue Uematsu) for positive/low aroused (pleasant), “A Ghost in the Machine”
(Angelo Badalamenti) for negative/high aroused (fear), “William Tell Overture:
Finale” (Gioachino Rossini) for positive/high aroused (happy) and “Disposable
Hero” (Metallica) for negative/high aroused (angry). 10 participants, 2 female
and 8 male students whose ages ranged from 23 to 35, participated in the music
experiment.
Sound clips selected from the International Affective Digitized Sounds (IADS)
were used in Experiment 2. All the sounds in the IADS database are labeled with
their arousal and valence values. IADS provides a set of standardized sound stimuli to evoke emotions that could be described by Arousal-Valence model. For
example, positive valence and high arousal values define happy emotion. 27 clips
were chosen to induce five emotional states: 3 clips for neutral with mean arousal
ratings ranging between 4.79 and 5.15, mean valence rating ranging from 4.83 to
5.09; 6 clips for each of positive/low aroused, positive/high aroused, negative/low
aroused and negative/high aroused emotions with mean arousal rating ranging
between 3.36 and 4.47; 6.62 and 7.15; 3.51 and 4.75; 7.94 and 8.35 respectively,
and mean valence rating ranging between 6.62 and 7.51; 7.17 and 7.90; 4.01 and
4.72; 1.36 and 1.76 respectively. 12 subjects, 3 female and 9 male students whose
ages ranged from 22 to 35, participated in the sound experiment. None of the
subjects participated in both experiments had history of mental illness.
The procedures for both experiments are described as follows. After a participant was invited to the project room, he/she was briefly introduced to the
experiment protocol and the usage of self-assessment questionnaire. Then, the
participant was seated in front of the computer which played the audio stimuli.
He/she was required to keep still and eyes closed during experiment sessions to
avoid muscle movement and eye blinking artifacts.
As shown in Fig. 1, Experiment 1 was consisted from five sessions. Each
session targeted one type of emotions. There was a 60 seconds silent period at
the beginning of each session for the subject to calm down and get ready for
the session. After that, one piece of music truncated to 1 minute duration was
played to the subject.
For Experiment 2 using stimuli from IADS, 5 sessions, namely: session 1
- neutral, session 2 - positive/low aroused, session 3 - positive/high aroused,
session 4 - negative/low aroused, session 5 - negative/high aroused were prepared
as shown in Fig. 2. In each session, there was a 6 seconds’ silent break, then 6 clips
of IADS stimuli aiming at one particular emotion were played to the subjects.
For neutral state, since only three sounds clips were available, each clip was
played twice in order to keep the same interval of each session.

8

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Fig. 1. The procedure of music experiment.

Fig. 2. The procedure of IADS experiment.

For both experiments, the subjects needed to complete the questionnaire after
listening to music/sounds. In the questionnaire, the Self-Assessment Manikin
(SAM) technique [7] with two dimensions: arousal and valence and five levels
indicating the intensity of both dimensions, was employed for emotion state
measurement. Additionally, the subjects were asked to write down their feelings
in a few words such as happy, sad, etc.
Emotiv wireless headset [1] was used for carrying out experiments. Emotiv
device has 14 electrodes locating at AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8,
FC6, F4, F8, AF4 (CMS/DRL as references) following the American Electroencephalographic Society Standard [3]. The sampling rate of the Emotiv headset
is 128Hz. The bandwidth of the device is 0.2-45Hz, and digital notch filters are
at 50Hz and 60Hz. The A/D converter is with 16 bits resolution.
3.2

Data Analysis and Results

The data collected in our two experiments using the Emotiv headset were analyzed to find spatio-temporal emotion patterns of high and low arousal level
with positive and negative valence level.
In the following analysis, we only focus on the analysis of data with negative/high aroused, positive/high aroused, negative/low aroused, and positive/low
aroused labels. Since we have the questionnaires which give us the true reaction
of the subjects to the stimuli, we ignore the cases in our processing when the

Real-time EEG-based Emotion Recognition and its Applications

9

subjects’ feelings were not compatible with the targeted emotions.
A 2 to 42 Hz band-pass filter was applied to the raw data as the major EEG
waves (alpha, theta, beta, delta, and gamma) lie in this band [40, 41]. Then,
Higuchi fractal dimension algorithm described in section 2.4. was applied for FD
values calculations. We implemented the algorithm with a sliding window where
the window size was 1024 samples and 99% overlapping was applied to calculate
FD values of the filtered data.
In the first experiment using music stimuli, the data from 13th to 24th seconds of recording were processed. In the second experiment using IADS clips,
the data from 2nd to the 13th seconds of recording were processed.
The arousal level could be identified from different electrode locations. FC6
was selected for the arousal level recognition as the FD values computed from it
gave better arousal difference compared to other channels. The mean of FD values computed from FC6 aiming at recognizing the arousal level for all subjects in
music and IADS experiments are shown in Table 1 and Table 2. Two FD values
for high arousal level with negative and positive valence, and two FD values for
low arousal level with negative and positive valence are presented in the tables as
follows: negative high aroused (N/HA), positive high aroused (P/HA), negative
low aroused (N/LA), and positive low aroused (P/LA). In Table 1, it is shown
that 10 subjects participated in the Music experiment. In Table 2, it is shown
that 12 subjects participated in IADS experiment. Based on the self-assessment
questionnaires, 46 pairs of comparisons from different subjects between high
aroused and low aroused states regardless of the valence level were used. 39/46
(84.9%) showed that the higher arousal was associated with the larger FD values. This phenomenon is illustrated in Table 1 and 2 as the mean of FD values
for the high aroused states (N/HA and P/HA) is larger than the low aroused
states (N/LA and P/LA). For example, for the subject #1 N/HA value 1.9028 is
larger than N/LA value 1.7647 and P/LA value 1.8592, and P/HA value 1.9015
is larger than N/LA value 1.7647 and P/LA value 1.8592. In Table 1 and 2, we
denoted the FD value as X if the subject’s feeling was different from the targeted
emotion by self-assessment questionnaire report. Thus, we eliminated such cases
from our analysis.

The asymmetrical frontal EEG activity may reflect the valence level of emotion experienced. Generally, right hemisphere is more active during the experience of negative emotions while left hemisphere is more active during positive
emotions. It was found that when one is watching a pleasant movie scene, a
greater EEG activity is appeared in the left frontal lobe, and with unpleasant
scene, right frontal lobe shows relatively higher EEG activity [21]. Another set of
evidence supporting this hypothesis is described in work done by Canli et al. [9].
They used fMRI to investigate the human brain’s response to visual stimuli, and
got the results that greater left hemisphere activity is shown during the positive picture exposure but greater right hemisphere activity for negative pictures.
However, there are also studies such as [25, 34] that oppose this hypothesis.
In our study, the difference between FD values from electrode pair AF3 (left

10

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Table 1. Mean FD values for arousal level analysis of music experiment
Music
Subject #1
Subject #2
Subject #3
Subject #4
Subject #5
Subject #6
Subject #7
Subject #8
Subject #9
Subject #10

Emotion State FD Value
N/HA P/HA N/LA P/LA
1.9028
X
1.9104
1.9842
1.7909
1.9111
1.9352
X
1.9253
1.8507

1.9015
1.9274
1.9274
1.956
1.8351
X
1.9231
X
1.939
1.8842

1.7647
X
1.7579
1.8755
1.8242
X
1.9106
X
X
X

1.8592
1.9268
1.8426
1.9361
X
1.9127
1.9204
X
1.9203
1.8798

Table 2. Mean FD values for arousal level analysis of IADS experiment
IADS
Subject #1
Subject #2
Subject #3
Subject #4
Subject #5
Subject #6
Subject #7
Subject #8
Subject #9
Subject #10
Subject #11
Subject #12

Emotion State FD Value
N/HA P/HA N/LA P/LA
1.8878
1.9599
1.9418
1.9215
1.7215
1.8898
X
X
1.9261
1.9223
1.8796
X

1.8478
1.922
1.9507
1.917
1.9271
1.8902
X
X
1.9241
1.9333
1.8543
X

X
1.938
1.9201
1.9265
X
1.888
X
X
1.7437
X
1.7183
X

1.8042
1.8237
X
1.9118
1.8659
X
X
X
X
1.9026
X
X

Real-time EEG-based Emotion Recognition and its Applications

11

hemisphere) and F4 (right hemisphere) were used to identify valence level and
test the lateralization theory. It was found that more stable pattern can be
achieved by differentiating valence level within the same arousal level, either
high aroused or low aroused. In Fig. 3, a hierarchical scheme we used to distinguish emotions is shown.

Fig. 3. The heriachical scheme of emotion recognition.

The results of the valence level recognition also revealed partial support
for the asymmetric frontal hypothesis. Although not all the subjects’ dominant
hemisphere for positive or negative emotions was the same as expected in the
asymmetric hypothesis, the pattern of lateralization for a particular subject was
consistent among different experiments with similar arousal level. 10 subjects’
data were available for comparison of positive and negative emotion states with
similar arousal level. 9/10 (90%) has shown the consistent pattern as follows. For
example, one subject’s EEG data showed the larger difference between AF3 and
F4 FD values for negative emotions than for positive emotions in all experiment
trails with different valence levels but similar arousal levels. Five subjects had
the larger difference of FD values between left hemisphere and right hemisphere
(AF3-F4) during the experiencing of negative emotion, while 4 subjects had the
larger difference of FD values when they experienced positive emotions. This
phenomenon may indicate that the frontal lateralization exists with individual
differences, and it may not be applicable for everyone that the left hemisphere
is more active for positive and right hemisphere is more active for negative emotions. It could be opposite for some individuals, and this outcome complies with
the conclusion made in [16] that individual difference may affect the processing
of emotion by brain.
Based on the result of our analysis, we developed the following real-time
emotion recognition algorithm described in the next section.

12

3.3

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Real-time Emotion Recognition Algorithm

As it was mentioned in Introduction, we follow two-dimensional Arousal-Valence
model described in section 2.1. This model allows the mapping of the discrete
emotion labels in the Arousal-Valence coordinate system as shown in Fig. 4.
The advantage of using this model is that we can define arousal and valence
levels of emotions with the calculated FD values. For example, the increase in
arousal level corresponds to the increase of FD values. Then, by using ranges
of arousal and valence level, we could obtain discrete emotions from the model.
Finally, any emotion that can be represented in the Arousal-Valence model can
be recognized by our emotion recognition algorithm.

Fig. 4. Emotion labels in arousal-valence dimension(Adapted from Russell’s circumplex
model [39]).

The emotion recognition algorithm for real time is illustrated in Fig. 5. The
raw EEG data gathered from AF3, F4 and FC6 are the input to the 2 to 42 Hz
band-pass filter. Then, Higuchi fractal dimension algorithm with a sliding window of window size 1024 and 99% overlapping is applied to the filtered data. The
benefit of the usage of the sliding window is that it enables real-time processing.
The FD value calculated from FC6 is used to distinguish the arousal level
independently by comparing with a default threshold extracted from our experiments’ results described in section 3.2. As shown in Fig. 4, the change of
FD could be mapped along the arousal axis since our experiments revealed that
higher arousal level was associated with larger FD values. Based on this observation, continuous recognition of changing emotion from low arousal to high
arousal is enabled. For example, satisfied, pleasant, and happy are all positive
emotions but with different arousal levels - ranging from low arousal to high

Real-time EEG-based Emotion Recognition and its Applications

13

arousal level, and their corresponding FD values also ranges from small one to
large one.
The difference of FD values between left hemisphere and right hemisphere
(AF3-F4) is computed simultaneously. After the arousal level has been identified, the valence level of emotions is recognized within the similar arousal level
by comparing the difference of FD with another threshold which is set for valence level recognition.
Finally based on the arousal level and valence level, the emotions are mapped
into 2D model.

Fig. 5. The emotion recognition algorithm overview.

In the algorithm, we set default thresholds for real-time emotion recognition
based on the experiments’ results. However, because of the existence of individual difference which means the pattern of emotion for one particular subject is
consistent but FD values may vary among different subjects, a training session
is needed to be introduced in order to improve the accuracy.
The training session scheme is illustrated in Fig. 6. The procedure is similar
to the real-time scheme, except the input is EEG data of the labeled emotions
of the particular subject. Then, thresholds are calculated and the lateralization
pattern is found based on the data collected from the training session for each
subject. When the subject wants to use this system after training, the procedure is illustrated as the lower part below the dash line in Fig. 6. The pattern of
newly collected EEG data is recognized according to the comparisons with the
calculated thresholds obtained from the training session.

4

Real-time Applications

The real-time EEG-based emotion recognition can be applied to many fields such
as entertainment, education, medicine, etc. In our work, we implemented three
applications: an emotional avatar, EEG-based music therapy, and EEG-based
music player.

14

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Fig. 6. An emotion recognition scheme with training session.

4.1

Emotional Avatar

In order to visualize the recognized emotions, we implemented our algorithm
with Haptek activex control system [2]. Microsoft Visual C++ 2008 was used in
this project. Haptek software is a 3D model with predefined parameters for controlling facial muscles visualization, thereby enables users to create customized
emotions and expressions. Haptek supports stand-alone and web-based application.
Data Acquisition EEG data are acquired using Emotiv headset at 128 Hz. We
used Emotiv Software Development Kit for acquiring raw data from the device.
Three out of fourteen Emotiv’s channels at locations AF3, F4 and FC6 are fed
into the algorithm for the emotion recognition process.
Data Processing A data stream from the Emotiv device is stored in a buffer.
Every time a read command is triggered, all the samples in the buffer are taken
out and the buffer is cleared. Therefore, the number of data obtainable at a time
depends on how long the samples have accumulated in the buffer.
The fractal algorithm requires data to be fed in a bunch of 1024 samples at a
time for one channel. Therefore, we use a queue to buffer the data from Emotiv’s
buffer to the algorithm. The queue is refreshed by the current number of samples
in Emotiv’s buffer every time the read command is triggered as shown in Fig. 7.
In the algorithm, those obsolete values in the queue are replaced by latest values
in the Emotiv buffer at the time.
Emotion Mapping to Haptex Haptek Activex control provides functions
and commands to change facial expressions of 3D avatars. We used an avatar
available with version of Haptek development package [2]. The face of the avatar

Real-time EEG-based Emotion Recognition and its Applications

15

Fig. 7. Implementation process of the emotional avatar application.

can be changed according to the photo image of the user’s face. We defined six
emotions by changing the parameters controlling the facial muscles of the Haptek emotional avatar. Those emotions are: fear, frustrated, sad, happy, pleasant
and satisfied. The above emotions can be recognized by the proposed emotion
recognition algorithm described in the section 3.
For the mapping, arousal and valence levels are transformed into discrete
values using thresholds. After this step, arousal level can only take one of the
following values 0, 1 or 2 and valence 0 or 1 as shown in Fig. 8. Combination of
discrete values of arousal and valence level gives six types of emotions.

Fig. 8. Illustration of discrete arousal and valence levels.

16

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Mapping of discrete values of (Arousal level, Valence level) into 6 emotions
is shown in Table 3.
Table 3. Mapping of combinations of (Valence, Arousal) and corresponding emotions
(Valence, Arousal)
(0,0)
(0,1)
(0,2)
(1,0)
(1,1)
(1,2)

Emotion
Sad
Frustrated
Fear
Satisfied
Pleasant
Happy

Picture of the user with the Emotiv headset and emotional avatar and pictures of six emotions created using Haptek are shown in Fig. 9 and Fig. 10
respectively.

Fig. 9. User with Emotiv headset and emotional avatar.

4.2

EEG-based Music Therapy

Music therapy is considered as a nonpharmacological intervention to help the
patients deal with the stress, anxiety and depression problems. In [50], the patients reported that their anxiety was released by listening to music during their
surgery. [15] also gave a positive support to the effectiveness of the music therapy
in the treatment of the patients who suffered from Alzeheimer’s disease. Their
anxiety level dropped sharply after the music therapy session.

Real-time EEG-based Emotion Recognition and its Applications

17

Fig. 10. Six visualized emotions with Haptek (a) Fear (b) Frustrated (c) Sad (d) Happy
(e) Pleasant (f) Satisfied.

18

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Since music therapy is proved to be a helpful approach in the medical area,
we combined it with our real-time EEG-based human emotion recognition algorithm. By this, we can identify the patient’s current emotional state and adjust
the music therapy based on the patient brain feedback in real time.
The general music therapy can be described as follows. First, the patient
needs to choose the type of music therapy such as pain management, depression,
etc. Then the corresponding music is selected and played. The emotion state
of the patient is continuously checked by his/her EEG in real time, and if the
currently playing music does not effectively evoke the targeted emotion of the
therapy, the music is changed and another song is played. If the present emotion
is in the accordance to the targeted emotion state, the music is played to maintain the target emotion. When a piece of music is over, another piece of music
is played.
The length of the music therapy usually lasts from 25 to 40 minutes [14].
In our application, we denote the duration of maintaining the patient in the
targeted emotion for one particular music therapy as t1. Then, t1 is compared
with t which is accumulated by the efficient time slices tei of each music piece
played and evoked the targeted emotion in a whole therapy treatment as follows:

t=

n
X

tei .

(4)

i=1

where n is the number of the music pieces played in one music therapy, and
tei defines the efficient time slices of all music pieces played during the music
therapy. It can be a whole piece of music duration, or only part of a song. For
example, music 1 keeps the patient feel positive for 2 minutes, and then, it fails
to induce the positive emotion, so it is replaced by music 2. Suppose music 2 is
displayed for 4 minutes as it can induce the positive state through that duration.
Then, these 2 minutes and 4 minutes time-intervals compose two components
in tei as te1 = 2, te2 = 4, and they are reckoned in t. When the constantly
accumulating summation of t is larger than t1, the music will be stopped, and
the end of one music therapy session is reached.
Fig. 11 shows the music therapy website we implemented. For implementation, emotion recognition algorithm is packaged as an ActiveX Component so it
can be used in the Internet Explorer environment. Visual C++ was used to integrate the ActiveX Component. The user’s “inner” emotion is recognized from
the EEG signals in real time. For music therapy on pain management, happy
(positive/high aroused) songs are played to the user to distract his/her attention
from the pain he/she is suffering. This strategy is compatible with [48] which
implemented EEG-based games to switch patient attention from the pain feeling. The user’s emotion state is checked in real time by his/her EEG data. If
the happy emotion is not evoked by the current song, the player automatically
switches to another one. For music therapy dealing with depression, pleasant
(positive/low aroused) songs are played to the user to make him/her feel re-

Real-time EEG-based Emotion Recognition and its Applications

19

laxed. The song is changed according to the EEG feedback.

Fig. 11. Subject is accessing the music therapy website.

4.3

EEG-based Music Player

Another application of real-time EEG-based emotion recognition is an EEGbased music player website. In this application, the user’s current emotion state
is recognized, and then, the corresponding music is played according to the identified emotion. The user’s emotion is detected by the algorithm running behind
the scene. Songs are categorized into six emotion types: fear, sad, frustrated,
happy, satisfied and pleasant.
The design of the player is shown in Fig. 12. Information about the current
emotional state of the user and the music being played is given on the display of
the player. For example, as shown in Fig. 12, the emotion state is recognized as
pleasant, and the music which is categorized as pleasant music is played to the
user.

5

Conclusion and Future Work

In this paper, emotion classifications, emotion evoking experiments and emotion recognition algorithms were reviewed. We proposed and implemented a
novel fractal dimension based algorithm for recognition of emotions from EEG
in real time. We implemented our algorithm with Haptek system. The system
allows visualization of emotions as facial expressions of personalized avatars in

20

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

Fig. 12. Subject with EEG-based music player.

3D collaborative environments in real time. We also developed a prototype for
an EEG-based music therapy and one EEG-based music player. Compared with
other works, our algorithm uses fewer electrodes. We recognized emotions with
AF3, F4 and FC6 electrodes, however, for example, in [33] and [43], 63 and
16 electrodes were used respectively. Until now, to our best knowledge there
is no real-time EEG-based emotion recognition algorithms reported. We implemented a novel real-time emotion recognition algorithm based on fractal dimension calculation. In this paper, we implemented recognition of six emotions: fear,
frustrated, sad, happy, pleasant and satisfied. However, our approach based on
FD calculation allows recognize even more emotions that can be defined in 2dimensional Arousal-Valence model.
Currently, the real-time emotion recognition and its applications are standalone implementations. The next step of the project is an integration of our
tools in Co-Spaces on the Web targeting entertainment industry.
Short videos about the emotion recognition algorithm implemented in real
time with the Haptex system and the music player, and more information about
our project EmoDEx are presented in [19].
Acknowledgments. This project is supported by grant NRF2008IDM-IDM004020 “Emotion-based personalized digital media experience in Co-Spaces” of National Research Fund of Singapore.

References
1. Emotiv, http://www.emotiv.com
2. Haptek, http://www.haptek.com

Real-time EEG-based Emotion Recognition and its Applications

21

3. American electroencephalographic society guidelines for standard electrode position nomenclature. Journal of Clinical Neurophysiology 8(2), 200–202 (1991)
4. Accardo, A., Affinito, M., Carrozzi, M., Bouquet, F.: Use of the fractal dimension for the analysis of electroencephalographic time series. Biological Cybernetics
77(5), 339–350 (1997)
5. Block, A., Von Bloh, W., Schellnhuber, H.J.: Efficient box-counting determination
of generalized fractal dimensions. Physical Review A 42(4), 1869–1874 (1990)
6. Bos., D.O.: EEG-based emotion recognition [online] (2006), http://hmi.ewi.
utwente.nl/verslagen/capita-selecta/CS-Oude_Bos-Danny.pdf
7. Bradley, M.M.: Measuring emotion: The self-assessment manikin and the semantic
differential. Journal of Behavior Therapy and Experimental Psychiatry 25(1), 49–
59 (1994)
8. Bradley, M.M., Lang, P.J.: The international affective digitized sounds (2nd edition; IADS-2): Affective ratings of sounds and instruction manual. Tech. rep., University of Florida, Gainesville (2007)
9. Canli, T., Desmond, J.E., Zhao, Z., Glover, G., Gabrieli, J.D.E.: Hemispheric asymmetry for emotional stimuli detected with fMRI. NeuroReport 9(14), 3233–3239
(1998)
10. Chanel, G.: Emotion assessment for affective-computing based on brain and peripheral signals. Ph.D. thesis, University of Geneva, Geneva (2009)
11. Chanel, G., Kierkels, J.J.M., Soleymani, M., Pun, T.: Short-term emotion assessment in a recall paradigm. International Journal of Human Computer Studies
67(8), 607–627 (2009)
12. Chanel, G., Kronegg, J., Grandjean, D., Pun, T.: Emotion assessment: Arousal
evaluation using EEG’s and peripheral physiological signals (2006)
13. Ekman, P.: Basic emotions. In: Dalgleish, T., Power, M. (eds.) Handbook of Cognition and Emotion. Wiley, New York (1999)
14. Grocke, D.E., Wigram, T.: Receptive Methods in Music Therapy: Techniques and
Clinical Applications for Music Therapy Clinicians, Educators and Students. Jessica Kingsley Publishers, 1st edn. (2007)
15. Guetin, S., Portet, F., Picot, M.C., Defez, C., Pose, C., Blayac, J.P., Touchon, J.:
Impact of music therapy on anxiety and depression for patients with alzheimer’s
disease and on the burden felt by the main caregiver (feasibility study). Interets
de la musicotherapie sur l’anxiete, la depression des patients atteints de la maladie
d’Alzheimer et sur la charge ressentie par l’accompagnant principal 35(1), 57–65
(2009)
16. Hamann, S., Canli, T.: Individual differences in emotion processing. Current Opinion in Neurobiology 14(2), 233–238 (2004)
17. Higuchi, T.: Approach to an irregular time series on the basis of the fractal theory.
Physica D: Nonlinear Phenomena 31(2), 277–283 (1988)
18. Horlings, R.: Emotion recognition using brain activity. Ph.D. thesis, Delft University of Technology (2008)
19. IDM-Project: Emotion-based personalized digital media experience in co-spaces
(2008), http://www3.ntu.edu.sg/home/eosourina/CHCILab/projects.html
20. James, W.: What is an emotion. Mind 9(34), 188–205 (1984)
21. Jones, N.A., Fox, N.A.: Electroencephalogram asymmetry during emotionally
evocative films and its relation to positive and negative affectivity. Brain and Cognition 20(2), 280–299 (1992)
22. Khalili, Z., Moradi, M.H.: Emotion recognition system using brain and peripheral
signals: Using correlation dimension to improve the results of eeg. In: Proceedings
of the International Joint Conference on Neural Networks. pp. 1571–1575 (2009)

22

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen

23. Kulish, V., Sourin, A., Sourina, O.: Analysis and visualization of human electroencephalograms seen as fractal time series. Journal of Mechanics in Medicine and
Biology, World Scientific 26(2), 175–188 (2006)
24. Kulish, V., Sourin, A., Sourina, O.: Human electroencephalograms seen as fractal
time series: Mathematical analysis and visualization. Computers in Biology and
Medicine 36(3), 291–302 (2006)
25. Lane, R.D., Reiman, E.M., Bradley, M.M., Lang, P.J., Ahern, G.L., Davidson, R.J.,
Schwartz, G.E.: Neuroanatomical correlates of pleasant and unpleasant emotion.
Neuropsychologia 35(11), 1437–1444 (1997)
26. Lang, P., Bradley, M., Cuthbert, B.: International affective picture system (IAPS):
Affective ratings of pictures and instruction manual. Tech. rep., University of
Florida, Gainesville, FL. (2008)
27. Li, M., Chai, Q., Kaixiang, T., Wahab, A., Abut, H.: EEG emotion recognition
system. In: In-Vehicle Corpus and Signal Processing for Driver Behavior, pp. 125–
135. Springer US (2009)
28. Lin, Y.P., Wang, C.H., Wu, T.L., Jeng, S.K., Chen, J.H.: EEG-based emotion
recognition in music listening: A comparison of schemes for multiclass support
vector machine. In: ICASSP, IEEE International Conference on Acoustics, Speech
and Signal Processing - Proceedings. pp. 489–492. Taipei (2009)
29. Liu, Y., Sourina, O., Nguyen, M.K.: Real-time EEG-based human emotion recognition and visualization. In: Proc. 2010 Int. Conf. on Cyberworlds. pp. 262–269.
Singapore (2010)
30. Lutzenberger, W., Elbert, T., Birbaumer, N., Ray, W.J., Schupp, H.: The scalp
distribution of the fractal dimension of the EEG and its variation with mental
tasks. Brain Topography 5(1), 27–34 (1992)
31. Maragos, P., Sun, F.K.: Measuring the fractal dimension of signals: morphological
covers and iterative optimization. IEEE Transactions on Signal Processing 41(1),
108–121 (1993)
32. Mauss, I.B., Robinson, M.D.: Measures of emotion: A review. Cognition and Emotion 23(2), 209–237 (2009)
33. Murugappan, M., Rizon, M., Nagarajan, R., Yaacob, S., Zunaidi, I., Hazry, D.:
Lifting scheme for human emotion recognition using EEG. In: Information Technology, 2008. ITSim 2008. International Symposium on. vol. 2 (2008)
34. Pardo, J.V., Pardo, P.J., Raichle, M.E.: Neural correlates of self-induced dysphoria.
American Journal of Psychiatry 150(5), 713–719 (1993)
35. Petrantonakis, P.C., Hadjileontiadis, L.J.: Emotion recognition from EEG using higher order crossings. IEEE Transactions on Information Technology in
Biomedicine 14(2), 186–197 (2010)
36. Plutchik, R.: Emotions and life : perspectives from psychology, biology, and evolution. American Psychological Association, Washington, DC, 1st edn. (2003)
37. Pradhan, N., Narayana Dutt, D.: Use of running fractal dimension for the analysis
of changing patterns in electroencephalograms. Computers in Biology and Medicine
23(5), 381–388 (1993)
38. Russell, J.A.: Affective space is bipolar. Journal of Personality and Social Psychology 37(3), 345–356 (1979)
39. Russell, J.A.: A circumplex model of affect. Journal of Personality and Social
Psychology 39(6), 1161–1178 (1980)
40. Sammler, D., Grigutsch, M., Fritz, T., Koelsch, S.: Music and emotion: Electrophysiological correlates of the processing of pleasant and unpleasant music. Psychophysiology 44(2), 293–304 (2007)

Real-time EEG-based Emotion Recognition and its Applications

23

41. Sanei, S., Chambers, J.: EEG signal processing. John Wiley & Sons, Chichester,
England ; Hoboken, NJ (2007)
42. Savran, A., Ciftci, K., Chanel, G., Mota, J., Viet, L., Sankur, B., Akarun, L.,
Caplier, A., Rombaut, M.: Emotion detection in the loop from brain signals and
facial images (2006), http://www.enterface.net/results/
43. Schaaff, K.: EEG-based emotion recognition. Ph.D. thesis, Universitat Karlsruhe
(TH) (2008)
44. Sourina, O., Kulish, V.V., Sourin, A.: Novel tools for quantification of brain responses to music stimuli. In: Proc of 13th International Conference on Biomedical
Engineering ICBME 2008. pp. 411–414 (2008)
45. Sourina, O., Liu, Y.: A fractal-based algorithm of emotion recognition from EEG
using arousal-valence model. In: Biosignals 2011. Rome, Italy, accepted (2011)
46. Sourina, O., Sourin, A., Kulish, V.: EEG data driven animation and its application.
In: 4th International Conference on Computer Vision/Computer Graphics Collaboration Techniques, MIRAGE 2009. vol. 5496 LNCS, pp. 380–388. Rocquencourt
(2009)
47. Sourina, O., Wang, Q., Liu, Y., Nguyen, M.K.: A real-time fracal-based brain
state recognition from EEG and its application. In: Biosignals 2011. Rome, Italy,
accepted (2011)
48. Sourina, O., Wang, Q., Nguyen, M.K.: EEG-based ”serious” games and monitoring tools for pain management. In: Proc. MMVR18. Newport Beach, California,
accepted (2011)
49. Stam, C.J.: Nonlinear dynamical analysis of EEG and MEG: Review of an emerging
field. Clinical Neurophysiology 116(10), 2266–2301 (2005)
50. Stevens, K.: Patients’ perceptions of music during surgery. Journal of advanced
nursing 15(9), 1045–1051 (1990)
51. Takahashi, K.: Remarks on emotion recognition from multi-modal bio-potential
signals. In: Industrial Technology, 2004. IEEE ICIT ’04. 2004 IEEE International
Conference on. vol. 3, pp. 1138–1143 (2004)
52. Wang, Q., Sourina, O., Nguyen, M.K.: EEG-based ”serious” games design for medical applications. In: Proc. 2010 Int. Conf. on Cyberworlds. pp. 270–276. Singapore
(2010)
53. Wang, Q., Sourina, O., Nguyen, M.K.: Fractal dimension based algorithm for neurofeedback games. In: Proc. CGI 2010. p. SP25. Singapore (2010)

