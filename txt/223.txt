Engagement Estimation in Advertisement Videos with EEG
Sangeetha Balasubramanian

Shruti Shriya Gullapuram

Abhinav Shukla

International Institute of Information
Technology
Hyderabad, India
sangeetha.balasubramanian@
students.iiit.ac.in

University of Massachusetts Amherst
Amherst, United States
sgullapuram@umass.edu

International Institute of Information
Technology
Hyderabad, India
abhinav.shukla@research.iiit.ac.in

arXiv:1812.03364v1 [cs.HC] 8 Dec 2018

ABSTRACT
Engagement is a vital metric in the advertising industry and its
automatic estimation has huge commercial implications. This work
presents a basic and simple framework for engagement estimation using EEG (electroencephalography) data specifically recorded
while watching advertisement videos, and is meant to be a first
step in a promising line of research. The system combines recent
advances in low cost commercial Brain-Computer Interfaces with
modeling user engagement in response to advertisement videos.
We achieve an F1 score of nearly 0.7 for a binary classification of
high and low values of self-reported engagement from multiple
users. This study illustrates the possibility of seamless engagement
measurement in the wild when interacting with media using a non
invasive and readily available commercial EEG device. Performing engagement measurement via implicit tagging in this manner
with a direct feedback from physiological signals, thus requiring no
additional human effort, demonstrates a novel and potentially commercially relevant application in the area of advertisement video
analysis.

CCS CONCEPTS
• Human-centered computing → HCI theory, concepts and
models; User centered design;

KEYWORDS
Engagement; Advertisements; EEG; Multimodal; Computational
Advertising

1

INTRODUCTION

The popularity and versatility of social networks allows organizations to reach their chosen target audience and offer products
and services that suit their constantly changing needs best. In this
digital era, advertisements are the best way to do so. Emotion and
engagement play a powerful tool in determining a person’s intent
to buy a product. Positive and negative emotions are used by advertisers to promote their product and drive users into buying them.
Brands associated with positivity has been shown to increase engagement. However, negative emotion is also used to effectively
garner consumer attention, where certain life choices are portrayed
as beneficial and improving one’s quality of life, while others are
portrayed as harmful and potentially fatal. Advertisement valence
(pleasantness), arousal (emotional intensity) and engagement (emotional involvement) are key properties that play a major role in consumer attitudes associated with the advertised product [9, 10, 16].
Analyzing advertisements in terms of emotional content can also
help optimize user experience [25].

Figure 1: An EEG recording session with a subject

This work expressly investigates the utility of physiological EEG
(electroencephalography) signals in the task of estimating engagement in affective advertisement videos (evenly distributed over the
emotional dimensions). 14 channel EEG data is acquired via the
commercial Emotiv headset while the test subjects are watching
the advertisements in question, and then subsequently analyzed for
engagement signatures. An implicit tagging based approach is used
to estimate the engagement level, without requiring any additional
effort from the end user.
In summary, we make the following contributions: 1) Our work
is one of the first to correlate user engagement in advertisement
videos with the recorded EEG signal 2) We demonstrate a simple
yet effective way that a low cost commercial EEG sensor can be
employed for engagement recognition in the wild
Our paper is organized as follows. Section 2 discusses related
work and espouses the novelty of our work with respect to existing
literature. Section 3 describes the ad dataset used in our experiments.
Section 4 shows the EEG data acquisition apparatus and protocol
via the Emotiv headset. 5 discusses experiments and key respective
findings. Section 6 concludes the paper and highlights important
potential future implications and considerations.

2

RELATED WORK

To highlight the novelty of our work, we briefly review prior works
examining (i) Conversational Systems, (ii) Engagement in online
content, and (iii) Online advertising.

2.1

Conversational Systems

Various multimodal features and machine learning algorithms are
used to predict engagement or involvement in human-human and
human-robot conversations [7, 27]. Some work use more fine grained

Figure 2: Representative thumbnail images from the videos
in our dataset

features, such as Oertel et al. [13] used gaze features obtained by
an eye tracker to model engagement in multiparty conversations.
In more recent work [26], human behaviors, such as smiles and
speech volume was used to coordinate with users’ engagement on
the fly via techniques such as adaptive conversational strategies
and incremental speech production.

2.2

Engagement in online content

Many researchers have analyzed engagement behavior towards
web content.The line of work that measures video engagement is
often around measuring engagement from the video popularity
dynamics (number of views, likes, etc.) [15]. More recent work [24]
focuses on measuring video engagement from public information,
such as video context, topics, number of times a video is watched
and channel, without observing any user reaction. While a lot of
work is focused on engagement and attention in online web content
and videos, there is very little work determining the engagement
in advertisement videos.

2.3

Online advertising

Advertising is most effective when it elicits an emotional response
and engages the viewer on a personal level. There has been extensive study in the context of display advertising [2, 17] and sponsored search [5, 18, 22]. A lot of work focused on predicting performance of an ad based on its click-through rate. Later efforts
has been on matching the web queries or pages to the context of
the ad [4, 5, 11, 12]. More recent work [3] focuses on analyzing
the level of user engagement with ads as the length of time a user
spends after landing on an ad page. There have also been recent
attempts to use EEG to mine affective attributes from data [20] and
use it for more emotionally relevant advertising. A very recent work
also uses contextual and visual attention data from eye tracking to
suggest emotionally driven design principles for advertisements
[21].
However, we specifically study the potential contribution of the
physiological EEG modality to engagement estimation. We use
an emotional advertisement dataset curated by [19], the details of
which follow in the next section. These ads are then presented as

Figure 3: Engagement rating distribution with Gaussian pdf
overlay
stimuli to test subjects whose EEG signal data is recorded. The subjects also self-report engagement on a 5-point scale. Engagement
estimation is achieved via EEG signals acquired via the wireless and
wearable Emotiv headset, which facilitates naturalistic user behavior and can be employed for large-scale engagement recognition
from multimedia content.

3

ADVERTISEMENT DATASET DESCRIPTION

This section presents details regarding the ad dataset used in this
study..
We use an affective advertisement dataset curated by [19] for our
study. Defining valence as the degree of pleasantness/unpleasantness
and arousal as the intensity of emotional feeling, five experts carefully compiled a dataset of 100, roughly 1-minute long commercial
advertisements (ads). These ads are publicly available1 . The authors
of [19] chose the ads based on consensus among five experts on
valence and arousal labels (either high (H)/low (L)). High valence
ads typically involved product promotions, while low valence ads
were social messages depicting ill effects of smoking, alcohol and
drug abuse.
Table 1: Mean correlations between self-rated attributes. Significant correlations (p < 0.05) are denoted in bold.

Arousal Valence Engagement
Arousal
Valence

1

-0.19

0.36

1

0.11

Engagement

1

To evaluate the effectiveness of these ads as affective control
stimuli, [19] examined how consistently they could evoke target
emotions across viewers. To this end, the ads were independently
rated by 14 annotators for valence (valence) and arousal in [19].
All ads were rated on a 5-point scale, which ranged from -2 (very
unpleasant) to 2 (very pleasant) for valence and 0 (calm) to 4 (highly
aroused) for arousal.
1 On

video hosting websites such as YouTube

Figure 4: Rating prompt shown to the subject after each
stimulus advertisement

In this work, we chose a similar setting for the rating of engagement. Defining engagement as the emotional involvement or
commitment while viewing an audio-visual stimulus on a 5-point
scale from 0 (least engaging) to 4 (most engaging) as shown in Fig. 4,
we employed 23 annotators for the engagement rating task, with
each person viewing a variable number of ads. For each advertisement, we calculated the mean of ratings of all annotators and then
thresholded this average by the grand average of all ratings of all
ads to get a binary label that we treated as ground truth in all our
engagement estimation experiments.
The distribution of engagement ratings (Fig. 3) is roughly uniform resulting in a Gaussian fit with large variance, with the mean
observed at the median scale value of 2.
Pearson correlation was computed between the arousal, valence
and engagement (see table 1) dimensions by limiting the false discovery rate to within 5% [6]. This procedure revealed a weak and
insignificant negative correlation of 0.17, implying that ad arousal
and valence scores were largely uncorrelated. However, we found
that arousal and engagement had a statistically significant positive
correlation of 0.36, which goes to show that advertisements with
high affective activation tend to be more engaging.

4

EEG ACQUISITION PROTOCOL

As the annotators rated the ads for engagement upon watching
them, we acquired their Electroencephalogram (EEG) brain activations via the Emotiv wireless headset as shown in Fig. 1. To
maximize attention and minimize fatigue during the rating task,
these raters took a break after every 20 ads, and viewed the entire set of 100 ads over five sessions (some users viewed slightly
lesser ads). Upon viewing each ad, the raters had a maximum of
10 seconds to input their engagement scores via mouse clicks using a prompt shown in Fig 4. The Emotiv device comprises of 14
electrodes (locations shown in Fig 5), and has a sampling rate of
128 Hz. Upon experiment completion, the EEG recordings were
segmented into epochs, with each epoch denoting the viewing of a
particular ad. Upon removal of noisy epochs. We recorded a total
of 1738 epochs. Each ad was preceded by a 1s fixation cross to
orient user attention, and to measure resting state EEG power used
for baseline power subtraction. The EEG signal was band-limited
between 0.1–45 Hz, and independent component analysis (ICA)
was performed to remove artifacts relating to eye movements, eye
blinks and muscle movements. The following section describes the

Figure 5: The electrode locations of the 14 channel Emotiv
EEG headset

techniques employed for content-centered AR and user-centered
AR.

5

EXPERIMENTS AND RESULTS

We first provide a brief description of the classifiers used and settings employed for engagement estimation, where the objective
is to assign a binary (high/low) label for engagement evoked by
each ad, using the extracted EEG features. The ground truth here
is provided by the mean of annotator ratings for each ad followed
by thresholding by the grand mean of all ad ratings. Experimental
results will be discussed hereafter.
Classifiers: We employed the Linear Discriminant Analysis (LDA),
linear SVM (LSVM) and Radial Basis SVM (RSVM) classifiers in our
AR experiments. LDA and LSVM separate high/low labeled training data with a hyperplane, while RSVM is a non-linear classifier
which separates high and low classes, linearly inseparable in the
input space, via transformation onto a high-dimensional feature
space. We notice that RSVM classifier produces the best F1-scores
for engagement.
Metrics and Experimental Settings: We used the F1-score (F1),
defined as the harmonic average of the precision and recall as our
performance metric, due to the unbalanced distribution of positive
and negative samples.
The 1738 clean epochs obtained from the EEG was used for
user-centered analysis. To maintain dimensional consistency for
subsequent principal component analysis (PCA), we performed
user-centric AR experiments with (a) the first 3667 samples (30s of
EEG data), (b) the last 3667 samples (30s of EEG data) and (c) the last
1280 samples (10s of EEG data) from each epoch. Each epoch sample comprises data from 14 EEG channels, and the epoch samples

Table 2: Advertisement Engagement Prediction from EEG analysis. F1 scores are presented in the form µ ± σ .

Method

Engagement
F1 for first 30 s (F30) F1 for last 30 s (L30) F1 for last 10 s (L10)

LDA

0.6954 ± 0.0207

0.6781 ± 0.0256

0.6644 ± 0.0222

LSVM

0.7018 ± 0.0201

0.6888 ± 0.0217

0.6647 ± 0.0211

RSVM

0.7091 ± 0.0224

0.6803 ± 0.0204

0.6732 ± 0.0263

were input to the classifier upon vectorization and dimensionality
reduction of the time-domain EEG data by PCA.
In this work, we use only time-domain EEG information. As we
evaluate engagement performance on a small dataset, AR results obtained over 10 repetitions of 5-fold cross validation (CV) (total of 50
runs) are presented. CV is typically used to overcome the overfitting
problem on small datasets, and the optimal SVM parameters are
determined from the range [10−3 , 103 ] via an inner five-fold CV on
the training set. Finally, in order to examine the temporal variance
in AR performance, we present F1-scores obtained over (a) first 30s
(F30), (b) last 30s (L30) and (c) last 10s (L10). These settings were
chosen to study the efficiency with which engagement prediction
can be realized at different temporal segments of the video to study
whether there is a potential accuracy drop-off over time.

5.1

Discussion

Table 2 summarizes the results that we get for the various experiments for engagement estimation. The best performance that we
achieve among the tested methods is using the RSVM classifier for
the first 30 seconds of the ads (F1 = 0.709). RSVM is also uniformly
the best classifier as compared to both LDA and LSVM, which points
to the possibility that the decision boundary for engagement related
statistics in EEG data is non-linear.
From the observations we infer that engagement recognition is
lower in the L30 and L10 conditions, with a clear drop in accuracy
later in the video. This could be because of a drop in engagement
levels towards the end of the advertisement where strictly product
related information is more prevalent as opposed to an engaging
storyline in the beginning.
The observations also highlight the limitation of using a single
engagement label for the whole video (as opposed to dynamic
labeling).

6

CONCLUSION AND FUTURE WORK

This work presents a first step towards using EEG data to directly
predict the engagement level of a user while watching a video
advertisement. There certainly exist many limitations that can be
worked on in the future. Some of these are:• The level of engagement is treated here as a binary high
or low label only. A much better and more rigorous way
of doing this would be to annotate a continuous level of
emotion at every time point using an annotation tool like
Feeltrace [8] and a regression to estimate the continuous

value instead of a categorical classification. However, such
an annotation is very time consuming and has issues to deal
with like synchronization.
• This study only deals with the EEG modality. Combining
it with other modalities that could be useful in a similar
environment such as facial expressions, eye tracking, GSR,
or even audiovisual content analysis would be an interesting
avenue to explore.
• This study only considers conventional classifiers (LDA and
SVM). It would be interesting to see how the recent advances
in deep learning (especially using transfer learning for small
dataset sizes) can work on such a problem.
• This study only considers the raw time domain EEG data
that is cleaned using ICA and subsequently vectorized, dimensionality reduced, and then passed to a classifier. A lot
of EEG related literature suggests that spectral features from
the alpha, beta, gamma and theta bands are useful for EEG
classification. However, these conventional features did not
work as well for us and the time domain summary statistics
using PCA gave us the best performance.
However, despite these limitations, the findings of this study
offer an important insight into how a portable and commercially
viable EEG device may be used for engagement measurement in the
wild. This can have important industrial applications in the field
of ad impact analysis. There exist modern tools by companies like
Realeyes [1] such as Creative Testing that provide second-by-second
insights into advertisement engagement levels. They are based on
principles of implicit tagging [14, 23] that are based on the user not
having to consciously annotate any content in the video but their
implicit responses (via EEG in our case) being able to automatically
deduce the required value. The low cost and portability of the EEG
headset used in the study further illustrates that the prototypical
engagement estimation system can be subsequently developed into
a viable commercial product.

REFERENCES
[1] 2018. RealEyes Emotional Intelligence. https://www.realeyesit.com/. (2018).
[2] Javad Azimi, Ruofei Zhang, Yang Zhou, Vidhya Navalpakkam, Jianchang Mao, and
Xiaoli Fern. 2012. Visual appearance of display ads and its effect on click through
rate. In Proceedings of the 21st ACM international conference on Information and
knowledge management. ACM, 495–504.
[3] Nicola Barbieri, Fabrizio Silvestri, and Mounia Lalmas. 2016. Improving post-click
user engagement on native ads via survival analysis. In Proceedings of the 25th
International Conference on World Wide Web. International World Wide Web
Conferences Steering Committee, 761–770.

[4] Hila Becker, Andrei Broder, Evgeniy Gabrilovich, Vanja Josifovski, and Bo Pang.
2009. Context transfer in search advertising. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.
ACM, 656–657.
[5] Hila Becker, Andrei Broder, Evgeniy Gabrilovich, Vanja Josifovski, and Bo Pang.
2009. What happens after an ad click?: quantifying the impact of landing pages
in web advertising. In Proceedings of the 18th ACM conference on Information and
knowledge management. ACM, 57–66.
[6] Yoav Benjamini and Yosef Hochberg. 1995. Controlling the false discovery rate:
a practical and powerful approach to multiple testing. J. Royal Stat. Soc. Series B
(Methodological) 57, 1 (1995), 289–300.
[7] Oya Celiktutan, Efstratios Skordos, and Hatice Gunes. 2017. Multimodal humanhuman-robot interactions (mhhri) dataset for studying personality and engagement. IEEE Transactions on Affective Computing (2017).
[8] Roddy Cowie, Ellen Douglas-Cowie, Susie Savvidou*, Edelle McMahon, Martin
Sawey, and Marc Schröder. 2000. ’FEELTRACE’: An instrument for recording
perceived emotion in real time. In ISCA tutorial and research workshop (ITRW) on
speech and emotion.
[9] Morris B Holbrook, Rajeev Batra, and Rajeev Batra. 1987. Assessing the Role
of Emotions as Mediators of Consumer Responses to Advertising. Journal of
Consumer Research 14, 3 (1987), 404–420. DOI:https://doi.org/10.1002/job.305
[10] Morris B Holbrook and John O Shaughnessy. 1984. The role of emotlon in
advertising. Psychology & Marketing 1, 2 (1984), 45–64.
[11] Jung-Hyun Lee, Jongwoo Ha, Jin-Yong Jung, and Sangkeun Lee. 2013. Semantic
contextual advertising based on the open directory project. ACM Transactions on
the Web (TWEB) 7, 4 (2013), 24.
[12] Richard J Oentaryo, Ee-Peng Lim, Jia-Wei Low, David Lo, and Michael Finegold.
2014. Predicting response in mobile advertising with hierarchical importanceaware factorization machine. In Proceedings of the 7th ACM international conference on Web search and data mining. ACM, 123–132.
[13] Catharine Oertel and Giampiero Salvi. 2013. A gaze-based method for relating
group involvement to individual engagement in multimodal multiparty dialogue.
In Proceedings of the 15th ACM on International conference on multimodal interaction. ACM, 99–106.
[14] Maja Pantic and Alessandro Vinciarelli. 2009. Implicit human-centered tagging
[Social Sciences]. IEEE Signal Processing Magazine 26, 6 (2009).
[15] Minsu Park, Mor Naaman, and Jonah Berger. 2016. A Data-Driven Study of View
Duration on YouTube.. In ICWSM. 651–654.
[16] Michel Tuan Pham, Maggie Geuens, and Patrick De Pelsmacker. 2013. The
influence of ad-evoked feelings on brand evaluations: Empirical generalizations
from consumer responses to more than 1000 {TV} commercials. International

Journal of Research in Marketing 30, 4 (2013), 383 – 394.
[17] Rómer Rosales, Haibin Cheng, and Eren Manavoglu. 2012. Post-click conversion
modeling and analysis for non-guaranteed delivery display advertising. In Proceedings of the fifth ACM international conference on Web search and data mining.
ACM, 293–302.
[18] D Sculley, Robert G Malkin, Sugato Basu, and Roberto J Bayardo. 2009. Predicting
bounce rates in sponsored search advertisements. In Proceedings of the 15th ACM
SIGKDD international conference on Knowledge discovery and data mining. ACM,
1325–1334.
[19] Abhinav Shukla, Shruti Shriya Gullapuram, Harish Katti, Karthik Yadati, Mohan
Kankanhalli, and Ramanathan Subramanian. 2017. Affect Recognition in Ads
with Application to Computational Advertising. In Proceedings of the 2017 ACM
on Multimedia Conference (MM ’17). ACM, New York, NY, USA, 1148–1156. DOI:
https://doi.org/10.1145/3123266.3123444
[20] Abhinav Shukla, Shruti Shriya Gullapuram, Harish Katti, Karthik Yadati, Mohan
Kankanhalli, and Ramanathan Subramanian. 2017. Evaluating Content-centric vs.
User-centric Ad Affect Recognition. In Proceedings of the 19th ACM International
Conference on Multimodal Interaction (ICMI 2017). ACM, New York, NY, USA,
402–410. DOI:https://doi.org/10.1145/3136755.3136796
[21] Abhinav Shukla, Harish Katti, Mohan Kankanhalli, and Ramanathan Subramanian. 2018. Looking Beyond a Clever Narrative: Visual Context and Attention
are Primary Drivers of Affect in Video Advertisements. In Proceedings of the 20th
ACM International Conference on Multimodal Interaction. ACM.
[22] Eric Sodomka, Sébastien Lahaie, and Dustin Hillard. 2013. A predictive model
for advertiser value-per-click in sponsored search. In Proceedings of the 22nd
international conference on World Wide Web. ACM, 1179–1190.
[23] Mohammad Soleymani, Jeroen Lichtenauer, Thierry Pun, and Maja Pantic. 2012. A
multimodal database for affect recognition and implicit tagging. IEEE Transactions
on Affective Computing 3, 1 (2012), 42–55.
[24] Siqi Wu, Marian-Andrei Rizoiu, and Lexing Xie. 2018. Beyond Views: Measuring
and Predicting Engagement in Online Videos. (2018).
[25] Karthik Yadati, Harish Katti, and Mohan Kankanhalli. 2014. CAVVA: Computational affective video-in-video advertising. IEEE Trans. Multimedia 16, 1 (2014),
15–23.
[26] Zhou Yu. 2015. Attention and engagement aware multimodal conversational
systems. In Proceedings of the 2015 ACM on International Conference on Multimodal
Interaction. ACM, 593–597.
[27] Zhou Yu, Alexandros Papangelis, and Alexander Rudnicky. 2015. TickTock: A
non-goal-oriented multimodal dialog system with engagement awareness. In
Proceedings of the AAAI Spring Symposium, Vol. 100.

