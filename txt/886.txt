ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

Using brain-computer-interface for
robot arm control.
Jayshiv Limbasia.
Opole University Of Technology, Computer and Electrical engineering, 45-271,
Opole, Poland.

Abstract. In this paper, we analyze the principles of
brain computer interface that convert the electrical activity of
the human brain into commands for a computer of any
embedded device. For this reason, we present the existing
devices and applications from the area of brain- computer
interfaces with advantages and disadvantages. Further, we
propose a solution for brain control of a robotic arm. We
develop the model and simulate the entire system functioning,
both the robotic arm control and the brain signals processing.
The final purpose of our research is to achieve a braincomputer system that controls a robotic arm that can replace a
human arm.

1. Introduction

Over the time, people have fantasized about the ability to communicate and
interact with the machine by simply thinking or creating devices that read
thoughts. These ideas have captured people's imagination in the form of ancient
myths and science fiction stories.
In recent years, control devices “by the power of the mind” is a very
controversial topic, but highly researched, both in the last generation gadget, such
as smart phones, laptops and tablets and even televisions intelligent, but also in
medicine, to be used by people with disabilities for which these technologies
could be the only way of communication with the external environment. Today,
brain-machine interfacing (BMI) development [brain- computer interfacing or
(BCI)] has a rapid growth of research because it provides a unique way of
communication between a human and a machine (or device) without any neuromuscular intervention [1].
Brain-computer interfaces are often used to help paralyzed people or with
motor functions diminished by connecting to a robotic arm; it’s important that
these people should not be fixed in one place, as it happens when they are
connected to the interface brain- computer through a cable, but they can move

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

around the room - thus defending the interest for a wireless connection. The basic
idea of a BCI system is to measure electrical activity, magnetic or any other
physical manifestations of brain activity and convert them into commands for a
computer or any other device (wheelchair, neuroprosthesis). A BMI is a system
based on digital signal processing tools and machine learning how to identify and
predict the user's cognitive state from corresponding brain signal [1].
The brain signals are recorded either invasively or noninvasively [1]. Noninvasive
means are widely used by most BMI/BCI researchers for their simplicity in user
interface, although invasive means provides better performance in terms of
accuracy and precision. The most widely used noninvasive recording technique is
electroencephalography (EEG - where the signals are recorded by electrodes
placed on the scalp) because it is inexpensive, portable, easily available and has
high temporal resolution [1].

2. Applications
Neurospeller application ensure a way for communication for people that suffer
from severe communication disabilities gained from neurological diseases and
offers a new mode for human interaction to improve their life [2]. BCI
applications for communication purposes are based on a virtual keyboard [3] that
is shown on the display that offers the user possibility to select a letter from the
alphabet by means of a Brain-Computer Interface (BCI) [4].

Fig. 1. User display in P300 Speller [3].

Wheelchair Control with Mental Tasks could be represent a real help for person
that suffer from hemiplegic because they don't have a full control of their body
and they cannot use a joystick of an electric wheelchair [5]. This technology
offers for these person a chance to obtain independence and don't have any more
to request help from a companion to push their wheelchair [6].

Fig. 2. BCI - Brain-Controlled-Wheelchair [5].

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

Fig. 3. BCI-based navigation in a complex VE [9].

Brain-controlled video gaming technology can be used both for medical and
non- medical purposes [7]. As possible purposes can be: to allow the persons with
severe mobility deficiency (e.g. spinal cord injuries) that cannot use a keyboard, a
mouse, or a joystick to play video-games; to recover the lost functions result from
an disease or a trauma at the level of brain (e.g. stroke or neurodegenerative
diseases); for relaxation and entertainment to bring experience to a new level [8].
Brain-controlled Robotic Prosthetic Limb offers a solution for people who have
suffered limb amputations resulted in the development of Brain-Computer
Interface for controlling robotic limb prosthesis through EEG signals that offer a
better solution than the existing ones (e.g. control of robotic human prosthesis by
electromyography sensors placed on the muscles of healthy limb to replicate the
movements) and can be helpful for completely paralyzed peoples [10].

Fig. 4. Diagram of a neural prosthetic system [11].

3. Software
BioSig is an open source software library developed by Graz University of
Technology for biomedical signal processing. BioSig provides open source
software tools for many different areas applications: neuroinformatics, braincomputer interfaces, neurophysiology, psychology, cardiovascular systems and
sleep research [12].

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

Fig. 5. BCILAB Sample GUI.

Open Vibe is developed by INRIA and is a software platform used to
designing, testing and using brain-computer interfaces. Open Vibe is a free open
source software for real-time neurosciences (real-time processing of brain
signals) because it can be used to acquire, filter, process, classify and
visualize brain signals in real time. Some of Open Vibe application fields are:
medical (assistance to disabled people, real-time biofeedback, neurofeedback,
real-time diagnosis), multimedia (virtual reality, video games), robotics and all
other application fields related to brain-computer interfaces and real-time
neurosciences. BCILAB is a MATLAB toolbox and EEGLAB plug-in for the
design, prototyping, testing, experimentation with, and evaluation of BrainComputer Interfaces (BCIs) and other systems in the same computational
framework.

4. Brain

controlled robotic arm

The system developed in this paper, which is still in research and needs
improvements, aims to replace lost functions due to amputations for the human
arm. The system includes a robot arm with five fingers and BCI headset with five
channels.
In accordance with the real model of a human arm that provides seven degrees
of freedom, the proposed robot arm provides 7 degrees of freedom for arm and 16
degrees of freedom for hand [14].

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI:http:/doi.org/10.6084/m9.figshare.13292918

The robot allows seven types of movement along the x, y, and z axes, as
follows:3 degrees of freedom for the shoulder joint (movement right-left, up-down
and forward- back); 2 degrees of freedom, 1 for the elbow joint (movement
forward-back/flexion- extension) and 1 for forearm (pronation/supination); 2
degrees of freedom for the wrist (rotation between the shoulder–elbow and elbow–
wrist parts).
We apply the Denavit-Hartenberg formalism to determine the direct kinematic
model of the robot arm in order to model and simulate the movement in MatLabSimulink.
Using the parameters identified in the Denavit-Hartenberg table from figure 6,
we determine the movement matrix for each finger, based on the formula:

Ti–1,i =

cos8i
[sin8i
0
0

−sin8icosαi
sin8isinαi
cos8i cosαi −cos8isini
sinαi
cosαi
0
0

aicos8i
aisin8i
di
1

The mathematical model for the robot arm was next simulated in MatLabSimulink. In figure 7 there is presented the shoulder joint formed up by three
revolute simple joints linked by bodies of 0 lengths. We adopted this solution in
order to point out the structure of the shoulder joint and to be able to accurately
control the three revolution movements for this element. All the other joints are
similar to this.

Fig. 7. MatLab-Simulink model of the shoulder part of the robotic arm

The robotic hand is equipped with five high-end servomotors for advanced
robotic applications that can operate the 16 joints of the fingers, either individual
or in the same time for the gripping operations. Entire parts of the robotic arm
were projected in computer aided design software and were printed from
Polylactic acid plastic (PLA) using a 3D printer. The BCI headset that was used in
our research is Emotive Insight.

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

Fig. 8. Simulation of robotic arm (90°).

The Emotiv Xavier Control Panel interface that we use for control the human
robotic arm through mental commands is a revolutionary interface for humancomputer interaction. The interface allows 13 types of movement of a virtual
object. After that movements are training, the Emotiv Insight headset create a
pattern based on EEG principle and signals are convert into a command that can
be understood by a computer.
We proposed to use 4 mental commands for control the human robotic arm in
four complex movements. The mental commands chosen are the follows:
 lift mental command (fig. 9) for up the shoulder along the z1 axis (0°-90°) and

flexion the elbow along y3 axis (0°-90°) (fig.8,9)
 drop mental command for down the shoulder joint along the z1axis (90°-0°)
and extension the elbow along y3 axis (90°-0°)
 rotate right mental command for close the human robotic hand
 rotate left mental command for open the human robotic hand

Fig. 9. Interface Emotiv Xavier Control Panel for lift mental command.

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

Fig. 10. Emotive XavierEmoKey user interface.

In figure 8 is presented the simulation results for a lift command.
In figure 9 is presented the corresponding brain computer interface. As can be
seen the mental command for arm lift resulted from the training, based on EEG
and focus signals.
Emotive XavierEmoKey user interface (fig.10) converts the results obtain after
the measurements of mental command in predefined key sequences based on
logical rules – Mapping EmoKey defined by the user. We assigned a set of rules
for convert the mental commands in keystroke event.
5. Conclusions
Brain-computer interface is a revolutionary technology that opens a path from a
new era of human computer interaction both by the fact that it addresses a large
number of target groups but also that it can be a real help for making daily
activities much easier.
In this research we propose a robot arm that has all the joints’ movements
specific to the human arm. We developed the Denavit-Hartenberg model for this
robot arm and simulated it in MatLab-Simulink in order to validate its use.
Then, there was developed a simple brain control interface based on a
commercial BCI headset in order to prove the possibility to brain control such a
robot arm using 4 mental commands, 2 of them for the shoulder movement and 2
of them for the hand.
We propose as further research to implement within the brain-controlled
robotic arm a series of pressure sensors for each finger to control the objects grip
force. Also, we propose to develop a method to real-time adapt the braincontrolled robotic arm movements with the goal of improving response time and
performing natural movements.

ISBN Web Conference 181, 15271 (2020)
MSE 2020

DOI: http:/doi.org/10.6084/m9.figshare.13292918

References
1. T. Kaufmann, A. Herweg, A. Kübler, Journal of Neuro Engineering and
Rehabilitation, 11(7), 15

2. Sapolsky, R. M. (2017). Behave: The biology of humans at our best
and worst. Penguin
3. F. Lotte, Proceedings of the 6th International Conference on
Foundations of Digital Games. ACM Jun 2011, Bordeaux,
France, 325, (2011).
4. C. Guger, W. Harkam, C. Hertnaes, G. Pfurtscheller, Proc.
AAATE 5th european conference for the advancement of
assistive technology, 3, (1999)

