ASIAN JOURNAL OF ENGINEERING, SCIENCES & TECHNOLOGY,VOL.1, ISSUE 1

MARCH 2017

A Novel Approach to Classify Human Emotions
Based on Electroencephalogram (EEG) Signals
Syed Sajjad Hussain Rizvi

Jawwad Ahmad

Muhammad Zubair

Salman Zaidi

has three fundamental dimensions of ’arousal’, ’valence’ and
’dominance’ as illustrated in Figure 1.The point on this three
dimensional space represents a particular human emotion,
mood and/or feeling comprised of extreme values of all the
described three parameters. Table-2 represents these human
emotion, mood and/or feeling.

Abstract—An extensive growth in EEG based systems have
been observed in the recent decade. This includes treatment
and support of mentally challenged persons, gaming, marketing,
development of brain controlled devices etc. The performance of
all these applications are primarily based on precise classification
of human emotions. In this paper, an effective fractional least
mean square based emotion classifier is proposed. The rigorous
training and testing of the proposed classifier was performed on
a benchmark video stimuli based EEG signal dataset. Moreover,
the performance of the proposed classifier is also compared with
the classical and most employed approach. Results prove that
the proposed classifier for human emotions is far better than the
most employed classification approach.

TABLE II: Definition of emotional states using PAD model
Position
PLL
PLH
PHL
PHH
NLL
NLH
NHL
NHH

I. I NTRODUCTION
Human emotions have always been a very complex entity
to study, analyze, and classify. It is a psycho-physiological
process which is generated by conscious and/or unconscious
acuity of an object or situation. It is often associated with
frame of mind, temperament, traits and disposition of an
individual. Primarily, a human emotion is a function of three
parameters namely ’arousal’,’valence’ (or pleasure)’ and
’dominance’. Therefore, any human emotion state can be
mapped on a three-dimensional plane of ’arousal’, ’valence’
and ’dominance’. Table-1 shows various definitions of
emotional states on extreme levels of ’arousal’, ’valence’, and
’dominance’.

Description
Positive/ Low arousal /Low dominance
Positive/ Low arousal /High dominance
Positive/ High arousal/ Low dominance
Positive/ High arousal/ High dominance
Negative/ Low arousal /Low dominance
Negative/ Low arousal /High dominance
Negative/ High arousal /Low dominance
Negative/ High arousal /High dominance

Emotion
Protected
Satisfied
Surprised
Joyful
Sad
Unconcerned
Fear
Angry

TABLE I: Definition of emotional states on extreme values of
’arousal’, ’valance’ and ’dominance’
Parameter
Arousal

Valence
(or Pleasure)

Dominance

Level/Degree
Low Arousal
(LA)
High Arousal
(HA)
Low Valence
(LV)
High Valence
(HV)
Low
Dominance
(LD)
High
Dominance
(HD)

State
Inactive
Active

Emotion/Mood
Uninterested /
Bored
Alert / Excited

Unpleasant

Sad /Stressed

Pleasant

Happy / Elated

Helpless

Weak feeling /
Without
Control
Control of
Everything

Empowered

Fig. 1: Mehrabian and Russell’s three-dimensional PAD model
for emotion classification [1-2]
Biologically, human body comprises of thousands of nerves
of several types and structures. These nerves act as sensors to
record surrounding parameters by touch, observe and/or feel.
These nerves are covered by a fatty layer of insulation, which
is called Myelin. The Myelin converts the external signals
understandable to the brain [3].
As a result, human emotions are generated in human mind
and appeared on various parts of human body such as facial
expression, human voice, blood pressure etc. It is the brain

The comprehensive behavior of human emotions can be
analyzed from Mehrabian and Russell’s three-dimensional
pleasure arousal dominance (PAD) model [1, 2].This model

1

ASIAN JOURNAL OF ENGINEERING, SCIENCES & TECHNOLOGY,VOL.1, ISSUE 1

that actually converts the input signal into the corresponding
human emotion.
In the literature, the researchers have already devised numerous methods to identify, analyze and record human emotions.
Some of the methods are classification of human voice [4],
classification of facial expression [5], human gait classification
[6] etc.
Though researchers have proposed numerous methods to
classify human emotions. However, their performance are
found to be constrained by one or the other reasons. It has been
found that the performance can be improved to a significant
degree if the emotional signals are taken directly from the
human brain rather than the other constituent of the body like
face, voice, gait etc.[7]. In the literature, this field is technically
termed as human computer interaction (HCI).
However, because of signal impurities; many HCI systems
are unable to identify precise human emotional states [8]. To
address this issue, numerous headsets are developed such as
Emotiv, Quick-20 Dry EEG Headset, Wireless EEG headset,
etc. These headsets record the brain signals using electrodes
and translate them into electronic signals. This domain of
study is technically termed as brain computer interaction
(BCI).
The recent growth of BCI motivated the research community to develop EEG signal datasets for classifying human
emotions. These datasets are developed by providing audio,
video, or any other multimedia modalities to the participants [9-13]. The modalities includes pictures, images, facial
expressions, body gestures, emotional speeches in different
languages, etc.
In the literature, numerous datasets are available for classifying human emotions; however DEAP (dataset for emotion
analysis using physiological signals) is the only database that
uses videos as emotional stimuli and has also been declared
as benchmark by research community for EEG based emotion
classification [14].

MARCH 2017

different samples (from the DEAP dataset) were used to test
the proposed classifier.
III. E XISTING C LASSIFICATION M ODELS
Researchers have developed various classification models
such as linear discriminant analysis (LDA), support vector
machine (SVM), and principal component analysis (PCA)
for emotion classification. If we employ DEAP dataset on
these classification models, we find that all these have some
efficiency constraints. For example, SVM is adaptable in
picking a closeness capacity but it is very sensitive to noise.
LDA has elegant approach to classify various emotions but it
fails if all the scatter matrices are singular. Whereas in PCA,
a small variation in background can not be observed [15, 16].
Other than these classification models, artificial neural networks are also used as classifier for human emotions. A brief
comparative analysis of neural network radial base function
(NNRBF), PSO trained NN (PNN), and Cuckoo trained NN
(CNN) is presented [17]. This study advocates that the NN
can be used as a good classifier but it also proves that the
evolutionary neuro approach is far better than the simple neuro
approach. But at the same time, evolutionary neuro approach
has high architectural complexity.
IV. P ROPOSED C LASSIFIER FOR H UMAN E MOTIONS
The above literature review shows that neural network
would be the best candidate for human emotional classification, however it requires some essential modifications in order
to reduce its complexity and increase the performance. In this
paper we have proposed fractional neural network (FANN) as
a classifier of human emotions.

II. D ESCRIPTION OF DEAP DATASET
In DEAP dataset the EEG and peripheral physiological
signals of 32 participants were recorded. Each participant has
watched 40 different music videos of one-minute duration.
During the experiment, 8064 samples were collected for each
video from each participant. Each sample comprises of 40
channel input and four corresponding output (i.e. arousal,
valence ,dominance, and liking). This make 322560 samples of
each participant and cumulatively it counts 10321920 samples
for all the participants. This is indeed a very huge volume of
data of any classifier and demands very strong computation
and memory resources.
Because of computation and memory constraints in our
simulation setup, we have down sized the dataset for our
proposed classifier. We considered 5% of the samples of each
video for 5 videos and we took the samples from a total
of 10 participants. This makes 20160 samples in total which
were used to train our proposed classifier and same number of

Fig. 2: A typical neural network with two hidden layers
It comprised of two hidden layers as shown in Figure 2.
The input layer has 40 nodes which corresponds the 40 input
channels and the output layer has 4 neurons which corresponds
arousal, valence , dominance, and liking. The first and second
hidden layers have 155 and 75 neurons respectively.

2

ASIAN JOURNAL OF ENGINEERING, SCIENCES & TECHNOLOGY,VOL.1, ISSUE 1

MARCH 2017

As we know that the weight update equation for a simple
neural network is given by
wn+1 = wn + αya

(1)

where ,y and a represent the learning rate, the summed output
and the activation function respectively.
Unlike the above weight update for a simple neural
network, our proposed fractional neural network has the
following weight update equation
(a)
(1−v)

wn+1

wn
= wn + αya + αya
r(2 − v)

(2)

where is a fractional power variable. In our proposed classifier, we have considered the learning rete as 0.005 and the
fractional power as 0.85. By incorporating the fractional power
in weight update equation, the equation becomes a complex
function. It is therefore we used complex purelin activation
function and complex log sigmoid activation function in output
layer and input/hidden layers respectively. The training of the
proposed classifier has been done with 100 epochs.

(b)

Fig. 3: (a) Training and (b) Testing curves of ANN and FANN
for arousal output

V. S IMULATION R ESULTS AND A NALYSIS
In this section the performance of the porposed classification
model (FANN) is compared with the existing classical ANN.
We have trained and tested both the classical ANN and FANN
for the same dataset size as discussed in section 2. The
performance of the porposed FANN was then compared with
the classical ANN through their sum of square error (SSE)
curve and training/testing curves for all the outputs of arousal,
valence , dominance, and liking.
Figure 3 to Figure 6 show the training and testing patterns
of classical neural network (ANN) and the proposed fractional
least mean square based emotion classifier (FANN) for four
different output of arousal, valence ,dominance, and liking
respectively.
As discussed in section 4, we have considered the learning
rate for both the classifier as 0.005 and trained them with
100 epochs. A total of 20,000 input samples were used to
train and test both the classifier for four standard outputs of
arousal, valence ,dominance, and liking. However, a fractional
power of 0.85 is additionally used in the proposed approach
as it is a fractional based neural network. All the result
prove that the proposed classifier is much faster (efficient)
than the existing classifier during training phases for all the
four outputs. Moreover, it is much precise (effective) than the
existing classifier during testing phases for all the four outputs.
Figure 7 shows the sum of square error (SSE) curves for
both the classifier during training phase. It can be seen that
the proposed classifier has much lesser values of error as
compared to the existing classifier and it continuously reduces
as the epoch runs.

(a)

(b)

Fig. 4: (a) Training and (b) Testing curves of ANN and FANN
for valence output

3

ASIAN JOURNAL OF ENGINEERING, SCIENCES & TECHNOLOGY,VOL.1, ISSUE 1

MARCH 2017

(a)

Fig. 7: Sum of square error curves for classical classifier
(ANN) and proposed classifier (FANN)

VI. C ONCLUSION
EEG signals can be used to depict human emotions and
feelings. However, these signals require a good classification
model to precisely classify various emotional moods. A number of classification approaches are being employed since last
couple of years. This paper proposes a new classification approach based on fractional least mean square neural network.
The efficiency and effectiveness of the proposed classifier
is compared with the existing classifier using a benchmark
dataset of EEG signals. The results were highly encouraging.
The proposed classifier can effectively be employed to determine the emotions and feeling of a human on the basis of
Mehrabian PAD model or any other model.

(b)

Fig. 5: Training and (b) Testing curves of ANN and FANN
for dominance output

R EFERENCES
[1] A. Mehrabian, ”Framework for a comprehensive description and measurement of emotional states”, Genetic, social, and general psychology
monographs, vol. 121, pp. 339-361, 1995.
[2] A. Mehrabian, ”Pleasure-Arousal-Dominance: A general framework
for describing and measuring individual differences in temperament,”Current Psychology, vol. 14, pp. 261-292, 1996.
[3] Izard, Carroll E. Human emotions. Springer Science & Business Media,
2013.
[4] El Ayadi, Moataz, Mohamed S. Kamel, and FakhriKarray. ”Survey
on speech emotion recognition: Features, classification schemes, and
databases.” Pattern Recognition 44, no. 3 (2011): 572-587.
[5] Busso, C., Deng, Z., Yildirim, S., Bulut, M., Lee, C.M., Kazemzadeh,
A., Lee, S., Neumann, U. and Narayanan, S., 2004, October. Analysis
of emotion recognition using facial expressions, speech and multimodal
information. In Proceedings of the 6th international conference on
Multimodal interfaces (pp. 205-211). ACM.
[6] Atkinson, A.P., Tunstall, M.L. and Dittrich, W.H., 2007. Evidence for
distinct contributions of form and motion information to the recognition
of emotions from body gestures. Cognition, 104(1), pp.59-72.
[7] Lin, Y.P., Wang, C.H., Jung, T.P., Wu, T.L., Jeng, S.K., Duann, J.R. and
Chen, J.H., 2010. EEG-based emotion recognition in music listening.
IEEE Transactions on Biomedical Engineering, 57(7), pp.1798-1806.
[8] Bos, D.P.O., Reuderink, B., van de Laar, B., Grkk, H., Mhl, C., Poel,
M., Nijholt, A. and Heylen, D., 2010. Brain-computer interfacing and
games.InBrain-Computer Interfaces (pp. 149-178). Springer London.
[9] M. Pantic, M. Valstar, R. Rademaker, and L. Maat, Web-based database
for facial expression analysis, in Proc. Int. Conf. Multimedia and Expo,
Amsterdam, The Netherlands, 2005, pp. 317321.

(a)

(b)

Fig. 6: Training and (b) Testing curves of ANN and FANN
for liking output

4

ASIAN JOURNAL OF ENGINEERING, SCIENCES & TECHNOLOGY,VOL.1, ISSUE 1

MARCH 2017

AUTHORS

[10] E. Douglas-Cowie, R. Cowie, and M. Schr oder, A new emotion
database: Considerations, sources and scope, in Proc. ISCA Workshop
on Speech and Emotion, 2000, pp. 3944.
[11] [11] H. Gunes and M. Piccardi, A bimodal face and body gesture
database for automatic analysis of human nonverbal affective behavior,
in Proc. Int. Conf. Pattern Recognition, vol. 1, 2006, pp. 11481153.
[12] G. Fanelli, J. Gall, H. Romsdorfer, T. Weise, and L. Van Gool, A 3-D
audio-visual corpus of affective communication, IEEE Trans. Multimedia, vol. 12, no. 6, pp. 591598, Oct. 2010.
[13] M. Grimm, K. Kroschel, and S. Narayanan, The vera am mittaggerman
audio-visual emotional speech database, in Proc. Int. Conf. Multimedia
and Expo, 2008, pp. 865 868.
[14] Koelstra, S., Muhl, C., Soleymani, M., Lee, J.S., Yazdani, A., Ebrahimi,
T., Pun, T., Nijholt, A. and Patras, I., 2012. Deap: A database for emotion
analysis; using physiological signals. IEEE Transactions on Affective
Computing, 3(1), pp.18-31.
[15] Bhagat, K.S., Mahajan, D.P. and Gunjal, P., 2015. Emotion Detection Using EEG Signal Analysis. International Journal of Electronics
Communication and Computer Technology (IJECCT) Volume 5 Issue 2
(March 2015)
[16] Wang, S., Zhu, Y., Yue, L. and Ji, Q., 2015. Emotion recognition with
the help of privileged information. IEEE Transactions on Autonomous
Mental Development, 7(3), pp.189-200.
[17] Sreeshakthy, M. and Preethi, J., 2016. Classification of Human Emotion
from Deap EEG Signal Using Hybrid Improved Neural Networks with
Cuckoo Search. BRAIN. Broad Research in Artificial Intelligence and
Neuroscience,6(3-4), pp.60-73.

.

Syed Sajjad Hussain Rizvi, shrizvi@iqra.edu.pk
Department of Computer Science
Iqra University,
Karachi, Pakistan.
.

Jawwad Ahmad, jahmad@uit.edu
Electrical Engineering Department
Usman Institute of Technology NED University
Karachi, Pakistan
.

Muhammad Zubair,zubair@iqra.edu.pk Faculty of Engineering Sciences & Technology
Iqra University
Karachi, Pakistan
.

Salman Zaidi, salman.zaidi@mrt.uni-kassel.de
Department of Measurement & Control
University of Kassel
Germany
Karachi, Pakistan.

5

