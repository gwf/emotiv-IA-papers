Gestures – Emotions Interaction: e-Viographima
application for visual artistic synthesis
Ifigeneia I. Mavridou
Centre of Digital Entertainment,
Bournemouth University
Bournemouth, UK
imavridou@bournemouth.ac.uk
ABSTRACT

This study aims at developing an application, called “EViographima”, for real time artistic synthesis that utilises
low-cost human-computer interaction devices. The diverse
possibilities of artistic expression are approached through
variations of visual stimuli whose parameters are altered
according to motion captured data recorded while the users
wag their hands, together with concurrent emotional states
derived from the users’ brain activity. The hands’
movements are recorded by a motion capture system (Leap
Motion) and four different emotional states are identified
through electroencephalography signals (EEG) recorded by
a brain signal detection headset (Emotiv Epoc). The primary
scope of this project is to produce in real time a visualised
output in the form of an interactive line that results in the
creation of a synthesis out of contoured forms made by the
automated and semi-automated elements.
Author Keywords

Artistic application; Human-computer Interaction; Brain
Computer Interaction; Emotional states; Visual synthesis.
ACM Classification Keywords

J.5.2 Arts and Humanities: Arts, fine and performing
H.5.m
Information
Interfaces
and
Presentation:
Miscellaneous
INTRODUCTION

The line, as an element of drawing and painting, is one of the
structural principles of Visual Art. Actually, it consists of
consecutive points on a plane and it can be the trace of a
moving point [15]. The line itself, but also its manifestations
(e.g. curves), can constitute a number of expressive tools. In
this study, an attempt to convey the idea of line as an
expression tool into a virtual world, by giving to it a rather
"alive" and volatile character, is presented. In addition,
alternative ways of interaction between human and
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than the author(s) must be
honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from Permissions@acm.org.
MOCO'16, July 05 - 07, 2016, Thessaloniki, GA, Greece
Copyright is held by the owner/author(s). Publication rights licensed to
ACM.
ACM 978-1-4503-4307-7/16/07…$15.00
DOI: http://dx.doi.org/10.1145/2948910.2948953

computers, in order to achieve the active participation of the
humans in the visual output, constantly changing based on
their emotions and reactions, is studied.
We are wondering about the resonance of sensory
acquaintance with the image, when it is formed by lines, and
how the cognitive perception of various visual art’s elements
is formed. Based on the Model Human Processor (MHP) [3],
a human receives stimuli through sensory organs, it
processes them, compares results to information stored in
sensory memory and reacts through emotional or
sensorimotor responses. On the other hand, the computer
receives data, encodes, processes them and produces output
with visual information. The user receives them in the form
of stimuli. In interactive systems, the users can get valuable
information through continuous feedback that they can
further process in order to improve their tactics to gain
specific goals. The model of user-system interaction by
Norman, based on MHP, considers a sequentially repetitive
cycle of user’s actions until achieving the final target [22].
The cooperation of these two different systems (the computer
and the user) was until recently achieved using conventional
means of interaction, such as a mouse and a keyboard.
Today, by dint of innovative techniques and devices, such as
mobile low-cost EEG headsets and gesture recognition
devices, human-computer interaction in now feasible with
emotional states and gestures. In the case of “eViographima”, the usage of such types of inputting results
into giving “life” to a line in the virtual space.
RELATED WORK

The use of interactive technology in artistic applications and
performances has increased dramatically during the past
decade. Lately, more and more developers and computer
artists utilize the virtual space and design expressive forms
of experiences due to the apparition of accessible and
affordable devices.
A line can be used to visualize different types of data, such
as physiological signals (e.g. heart rate, brain signals etc.).
The use of line as visual element has been analysed [15, 16,
14] and incorporated in their visual synthesis by a plethora
of artists (e.g. Kadinsky, Paul Klee, Kontos D., Jeremy
Woods). An example of artistic expression using the line is
the “Action Painting”, also referred to as “Gestural
Abstraction” [10]. Osvaldo Cavandoli created a series of
narrations “La linea” [7] that were recorded in a video with

interactive capabilities and uploaded on YouTube in 2008.
Hervé Huitric and Monique Nahas, pioneers of Computer
Art, started from 1970 creating figurative 2D drawings and
developing 3D realistic techniques. In their first twodimensional digital plans (1979-1981) they experimented in
creating lines and curves, especially with B-splime curves
[13]. The form of the line has also been situated at most of
the first interactive artworks such as the “Le plume” [2],
where the user could interact by blowing on an image of
feathers created by lines.
With the evolution of technology, many modern artworks
slowly are increasingly spread in space and integrate the
public. In the case of interactive artworks, the public
intervenes in the actual work depending on the scenario of
interaction. It is common that many interaction scenarios are
designed to be carried out by multiple individuals
simultaneously. In modern art installations, the visitors are
often invited to interact, using their body, as in the interactive
installation of Scott Snibbe, "Boundary Functions" [25]. In
the case of interactive environments allowing the freeflowing collective action of users, a common atmosphere is
created, wherein a shared behavior is leading to emergent
phenomena and new unforeseen structures [4]. In the
interactive installation “Quorum Sensing” [5], a kind of
communication is established between the virtual and the
real world, where visitors can interact with multiple body
movements, from simple gestures to ritual dances, on an
interactive carpet containing sensors.
The philosopher Jean-Luc Nancy [20] argues that all
philosophical considerations agree that in art there was
always a question of gesture. For Nancy, the gesture is "a
movement or the outlining of a form" and "is a sensible
dynamism that precedes, accompanies or successes meaning
or signification". Nowadays, performers are using
technology to create multisensory experiences for the
audience using new interaction tools. Brain Computer
interfaces have entered the palette of expressive interaction
tools used by artists and developers of virtual experiences
[17] and performances; some indicative examples are the
interactive musical interface «On being invisible II» [23] by
Rosenboom, Mariko Mori's ‘Wave UFO’ [19] where
participants are manipulating audiovisual elements, Luciana
Haill’s performance [12] in which images and sounds were
created using EEG detection devices, Marina Abramovic’s
to “The Magic of the Mutual Gaze” [1] in collaboration with
the neuroscientist Suzanne Dikker [9], the “Eunoia” project
by Lisa Park [8] where EEG data are translating into sounds
using the Neurosky MindWave headset [21] and the
“Brainwarm” [11] by Leontios Hadjileontiadis, a music
performance composition system based on hand gestures and
EEG data. More and more artists are expected to engage into
using Brain - Computer Interfaces (BCIs), as besides using
them for communication and control, monitoring the user’s
affective or cognitive state can evolve into a powerful tool
for artistic expression.

E-VIOGRAPHIMA: THE INTERACTIVE LINE

The basic approach followed was driven by the visual series
"Viographima"(http://ifigeneiamavr.tumblr.com/
biographics) that were presented in relative exhibitions.
The potential of utilising the concept of "Viographima" in
the interactive e-Viographima application was explored, in
the context of developing an expressive structure
representing emotional states in real time. In the framework
of the aforementioned application, an individual can interact
through an expressive interface, thus manipulating the basic
visual elements, into creating forms, shapes and figures in
the virtual world, which accrues during a perpetual cycle of
action – reaction (Figure 1).

a

b
Figure 1. Screenshots (a, b) from the application

Apparatus

At this point, the equipment used by the interactively
participating users and the technical framework followed to
design the virtual space of the application are described.
Peripheral devices – Through various experiments, the

appropriate equipment were selected for the application, in
order to inspire the direct and creative interacting
engagement of the user. For this purpose, the input devices
Leap Motion Controller (LMC), for the tracking of hands’
movements, as well as the Emotiv Epoc (EE) headset, for the
recording of the EEG brainwaves responses, were selected
(Figure 2). For the visual output, a standard monitor of 15
inches was used.

Figure 2. Peripheral devices used: Emotiv Epoc headset (right)
and Leap motion controller (left).

The range of interaction space supported by the LMC device
(https://www.leapmotion.com/) is 0.22 m3, offering a
maximum 60cm height of its cameras’ viewing range with
an angle of approximately 150 degrees [6]. The device came
with the Leap Motion Software 2.3.1+31549 version. For the
regulation of behavior and performance diagnostics, the
Leap Motion Control Panel was used.
EE is a headset consisting of seven pairs of dry non-invasive
electrodes,
i.e.
14
electrodes
in
total
(http://emotiv.com/epoc.php). The EEG signals
from each electrode are transmitted via Bluetooth. This way,
the headset’s mobile structure provides an untethered
interaction option. The EE headset comes with the EPOC

Control Panel software for managing the device
characteristics. With the support of this software product, the
following configurations of detection suites are provided:
 Affectiv (Performance Metrics) for four basic emotional
states Excitement, Boredom / Engagement, Frustration
(Anger) and Meditation,
 Expressiv (Facial Expressions), such as blinking and
winking,
 Cognitiv (Mental Commands) that require training.

In the EmoKey mapping created for the application, a subset
of rules that correspond to the detection of the
aforementioned emotional states and facial expressions,
along with the corresponding keystroke commands, was
composed (Table 1). During runtime, the user is able to
watch the effect of her actions on the transformation of the
visual output and re-affect it within a process of continuous
feedback (Figure 3).

The EmoKey application built in the EPOC Control Panel
was utilised to match the detection results as conditions that
can be set up to trigger events. These events are then
identified by the application as predefined keystrokes.

User:
Emotional states +
Hand gestures

Designing the VE – For the design and development of the

Figure 3. Proposed solution for the affective interaction

e-Viographima application, the game engine Unity3D
running on Windows 7 operating system and the objectoriented programming language C# were selected. The
Unity3D is an independent cross-platform game engine [26]
that is widely used for the development of video games and
applications.
The key design goal was to create a seemingly twodimensional virtual line, which oscillates and behaves in a
way that approximates the natural behavior of a cord
receiving impulses at various points. For this purpose, forces
such as gravity and collisions were taken into account by
utilising the physic engines built in Unity3D, which provide
simulation of physical laws and behavior (physical
simulation).
The creation of a line responsive to emotional states and
gestures, with whom the user can interact in a virtual
seemingly two-dimensional space was intended. For this
reason, the line’s main desired characteristics were to:
• resemble realistic (ink traces) and flexible without
losing its structure and cohesion,
• remain at the same level (two-dimensional approach)
User’s state data

Keystrokes

Excitement (> 0.20)

R

+/- Frustration

Y/U

+/- Meditation

I/O

Actions

Creation of animated elements; in the
center of the curvatures of the fingers.
Increase / decrease the magnetic force
of the fingers by 10 units
Increase / decrease the magnetic
radius of the fingers of 10 units
Restore the system to its original state;
starting point

Blinking

Space

Engagement / boredom

W

Affect colors parameters (dynamic)

Frustration (> 0.55)

T

Cause displacement in different points
of the line

Table 1. The predefined rules and conditions
Functional outline – As interaction input, the emotional

states “Frustration”, “Meditation”, “Excitement” and
“Engagement/Boredom”, as well as the facial expression
“Blinking” were chosen. Thereby, training and calibration
activities were not required prior to launching the
application.

System:
e-Viographima

Monitor:
Visualisation

INTERACTION MECHANISMS

In the implementation process, the development of the
virtual environment(VE) takes place, hosting the basic visual
elements regarding the background canvas, as well as the
creation of the oscillating line. Furthermore, the design of
interaction between the user’s physical hands and their 3D
counterparts in the VE, as well as the affective interaction
layout used, will be described.
Canvas – A constant level in the VE, on which the final

outcome of the interaction activity would be shown, was
created. This digital constant level looks like a canvas on
whose surface/space the interactive line is placed. In the VE,
a virtual camera is adapted to the viewing angle of the user,
which is fixed above the canvas. That way, users can
continuously monitor their activity taking place in the VE,
thus observing the results of their actions. The background
environment in the application was created with reference to
the video entitled «Notebook», from the series
«Viographima» (https://vimeo.com/128010310).
Oscillating line – The primary objective was to create a

three-dimensional line, which could oscillate and react to
collisions. To simulate the interactive line with Unity 3D, the
Line renderer component was utilized, on which physics
were applied. That way, the line was divided into individual
interdependent parts, connected with joints. By moving one
part of the line, the forces created the movement could be
conveyed concurrently on their neighboring line parts, thus
causing the movement of the overall line. Having a smooth
flexible moving across all the parts of the virtual line, one
can use it to design various schemes from a simple straight
line to composite line forms.
User’s virtual hands – In order to stimulate movement on

the virtual line according to the user’s physical hands’
movement, their 3D counterparts were created and were
placed in the VE. The objective was to affect the virtual line
by forces of the virtual hands whose movements would
correspond to the user’s hands. During runtime, virtual hands

are generated in the VE in accordance to the placement of the
user’s hands over the LMC device (see figures 6&7).

a

b

Figure 6. Real hand's (a) and virtual hand’s (b) position
when touching the virtual line

Figure 7. Virtual hands’ interaction space
Animated designs – Ten series of two-dimensional imagery

figurative designs were created, with each series consisting
of nine different snapshots. Four of the ten series are shown
in figure 8. By giving the command to display them in the
scene, a series is randomly selected and its highlights/frames
begin to appear one after the other at a pace appropriate to
create a sense of animation (see figures 1b & 9). Their
appearance is made under certain conditions; a) when the
line embraces the virtual fingers - the line segments are
forming a curve around them, and b) when the appearance of
designs is commanded (e.g. the first predefined rule in table
1).

Figure 8. Series of figurative Figure 9. Design appearing
designs
during play-mode
Affective interaction layout – The synthesis of visual

elements constructed on the canvas is the result of the
coordination between the user’s hand gestures and her
current emotional state. However, the way we feel is usually
susceptible to external stimuli [18], with regard to e.g.
affective images, faces and sounds. In this paradigm, the user
is constantly watching the monitor while interacting with the
system. In other words, we can presume that the visual
outcome on the monitor is also the ephemeral and local
manifestation of affective feedback.
The interactive line plays the role of the taut string which
reflects the vibrations sustained by the coordination of the
aforementioned user’s actions. At the beginning of the
application the line is straight and stable (neutral state). By
waving her hands, the user can slowly affect the line’s
behavior into a flexible, oscillating line which gets easily
manipulated by her hands and also by her emotional states
(explicit interacting state). During the interaction with the
application, the constantly evolving visual output (due to e.g.
the apparition of the animated designs in conjunction with

the line movements) can fuel the user’s contingent additional
emotional stimulation, which is read and reciprocated in the
form of visual stimulus (implicit interacting state), as shown
in figure 3. The user can gradually learn how her emotional
states are manifested in the application, assisting in their
conscious monitoring and regulation.
DISCUSSION

The basic idea underlying this work is the exploration of the
usage of different emotional states’ detection, together with
the hands’ movement tracking as alternative real-time
interaction techniques. Thus, the possibilities of a creativeexpressive interaction are examined in which the user does
not simply perform specific commands or suggestions. The
user additionally explores, understands and even tries to
control her emotional states through the interface that has
been developed in the framework of the application “eViographima”. The visual output was intended to reflect the
emotional state of the user, based on the author’s personal
artistic interpretation, but also to trigger visual stimuli
indented to alternate the user’s affective state.
Since a user study was not conducted, we have concluded the
systems limitations based on our observations. In some
cases, a delay with impacts to user-system communication
can be caused when the contractility of the sensors is low or
the user moves her hand outside the tracking range. Another
suspected limitation is that the level of difficulty in
controlling the system with emotional states increases over
time, as the user may face inconveniently rapid changes in
the visual output, as a result of the explicit and implicit
interacting circle.
Overall, the use of low-cost interaction devices has proved
as a promising way to interact and encourage us to expand
our efforts to more complex interaction scenarios and
exploration of human responses to stimuli, beyond the visual.
Therefore, we plan to explore the impact of VE in terms of
visual elements in shaping the emotional state. Additionally,
physiological signals, such as heart pulse and galvanic skin
conductance could be utilised. A future attempt to enhance
this exploratory study could be achieved in collaboration
with scientists from other disciplines (e.g. psychologists,
neuroscientists). Therefore, specialized knowledge from
different domains could be contributed in providing insights
for new applications.
Nowadays, a variety of technological tools ranging from
virtual reality and gaming are used in applications beyond
entertainment; such a paradigm of «Επιλογή (epilogi) in
crisis» [24]. Moreover, similar tools are increasingly
proposed for training and simulation purposes. As a result, it
is expected that the new connection between BCI and VEs
will be able to open new application areas.
Acknowledgements

The initial version was developed within the framework of
Greek-French MA programme “Art, VR & Multiuser
systems”, Athens School of Fine Arts & Paris 8 University.

References

1. Marina Abramovic Insitute. 2011. Measuring the Magic of
the Mutual Gaze. Retrieved February 22, 2016 from
http://www.mai-hudson.org/
2. Michel Bret, Edmond Couchot and Marie-Hélène Tramus.
1986. Paris8 University archive: Les Plumes Retrieved
February 22, 2016 from http://www.archives-video.univparis8.fr/video.php?recordID=231
3. Stuartk Card, Moran Thomas and Newell Allen. 1986. The
Model Human Processor: An Engineering Model of
Human Performance. In Kenneth R. Boff, Lloyd
Kaufman, James P. Thomas. (Eds.) Handbook of
Perception and Human Performance, Vol. 2, John Wiley
& Sons Publishing Co., Inc, 1-35.
4. Chu-Yin Chen and Jean-Claude Hoyami. 2007.
Autonomous Systems for Interactive Digital Art. In
Procceeding of the 10th Generative Art Conference
(GA2007). Milano, 63-69.
5. Chu-Yin Chen. 2004. Quorum Sensing: an interactive
Installation, Leonardo 37, 2 (April 2004), 101.
http://dx.doi.org/ 10.1162/0024094041139355
6. Alex Coldgan. 2014. How Does the Leap Motion
Controller work?. Retrieved 15 April, 2015 from
http://blog.leapmotion.com/hardware-to-software-howdoes-the-leap-motion-controller-work/
7. DailyMotion. 2007. Osvaldo Cavandoli - La Linea 127.
Retrieved
February
29,
2016
from
http://www.dailymotion.com/video/x2stl1_osvaldocavandoli-la-linea-127_fun
8. Brady Dale. 2014. Slate: Reading the Artist’s Mind.
Retrieved
April
26,
2016
from
http://www.slate.com/articles/technology/future_tense/20
14/01/euonia_by_lisa_park_uses_eeg_technology_to_cre
ate_performance_art.html
9. Suzanne Dikker. 2014. Art + Science + Education
Projects: measuring the magic of mutual gaze. Retrieved
February
22,
2016
from
http://www.suzannedikker.net/art-science-education/
10. Beth Gersh-Nesic. 2015. Art History Definition: Action
Painting.
Retrieved
April
4,
2015
from
http://arthistory.about.com/od/glossary_a/a/a_action_pain
ting.htm

http://www.wired.co.uk/magazine/archive/2010/09/play/t
unes-brain-luciana-haill-eeg-art
13. Hervé Huitric and Monique Nahas. 1986. Ars Electronica
Archive: The visual artist turns to computer programming.
Retrieved
April
10,
2015
from
http://90.146.8.18/en/archives/festival_archive/festival_c
atalogs/festival_artikel.asp?iProjectID=9279
14. Tim Ingold. 2007. Lines: A brief History. Routledge, New
York, NY.
15. Wassily Kandinsky and Rebay Hilla. 1947. Point and line
to plane. Dover Publications, New York, NY.
16. Ρaul Klee. 1961. Notebooks, Vol. 1: The Thinking Eye.
Lund Humphries, London, UK.
17. Anatole Lecuyer, Fabien Lotte, Richard B.Reilly, Robert
Leeb, Michitaka Hirose and Mel Slater. 2008. Braincomputer interfaces, virtual reality, and videogames.
Computer
41,
10
(October
2008),
66-72.
http://dx.doi..org/10.1109/MC.2008.410
18. Joseph LeDoux.1998. The emotional brain: The
mysterious underpinnings of emotional life. Orion,
London.
19. Mariko Mori. 2003. Mariko Mori: Wave UFO. Kunsthaus
Bregenz. Retrieved February 28, 2015 from
www.kunsthaus-bregenz.at/ehtml/pressetext_mori.htm
20. Jean-Luc Nancy. 2010. Art Today. Journal of Visual
Culture 9, 91(April 2010), 91-99.
http://dx.doi.org/ 10.1177/1470412909354265
21. NeuroSky. 2015 Mindwave Mobile. Store. Retrieved
February
29,
2016
from
http://store.neurosky.com/pages/mindwave
22. Donald A. Norman. 1986. Cognitive Engineering. In: User
Centered Design: New Perspective on Human-Computer
Interaction, 3161. Lawrence Erlbaum Associates Inc,
Hillsdale, NJ
23. David Rosenboom. 1997. Extended Musical Interface
With The Human Nervous System. Leonardo Monograph
Series, International Society for the Arts, Sciences and
Technology (ISAST), San Francisco, CA.
24. Manthos Santorineos, et al. 2015. Eπıλoγη∗ in Crisis∗∗. In
Proceeding of the Virtual Reality (VR) IEEE 2015. Arles,
France,351-352
http://dx.doi.org/10.1109/VR.2015.7223440

11. Leontios J. Hadjileontiadis. 2014. Conceptual Blending in
Biomusic Composition Space: The “Brainswarm”
Paradigm. In Proceeding of the Joint 11th Sound and
Music Computing Conference (SMC) and 40th
International Computer Music Conference (ICMC),
Athens, Greece, 621-628.

25. Snibbe Scott. 1998. Bountary functions. Retrieved
February
25,
2016
from
http://www.snibbe.com/projects/interactive/boundaryfunc
tions/

12. Luciana Hail. 2010. Wired magazine: Tunes on the brain:
Luciana Haill's EEG art. Retrieved April 24, 2015 from

26. Unity Technologies. 2016 Company facts. Retrieved
February 28, 2016 from https://unity3d.com/publicrelations

