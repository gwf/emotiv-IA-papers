INTL JOURNAL OF ELECTRONICS AND TELECOMMUNICATIONS, 2012, VOL. 58, NO. 4, PP. 463–478
Manuscript received July 30,2012; revised November, 2012.
DOI: 10.2478/v10177-012-0061-z

Communications, Multimedia, Ontology,
Photonics and Internet Engineering 2012
Ryszard S. Romaniuk

Abstract—Wilga Symposium gathers two times a year, together around 300 young scientists active in advanced photonics,
electronics and software systems, including Internet engineering. May 2012 marked a jubilee XXXth meeting in Wilga
Resort. There were presented over 250 papers from nearly all
technical universities in this country. The organizers present
here a research survey of the WILGA Symposium work. Also
a digest of chosen Wilga references is presented [1]–[39]. This
paper is the second part of a concise review digest focused
on optoelectronics, communications, multimedia and ontology of
information technologies.

streams, machine vision, vehicles – quadrocopter and Mars
rover, analog transmission systems in noisy conditions with
digital reverse transmission channel, optoelectronic and photonic metrology, reconfigurable measurement systems, high
performance – low-jitter low-latency transmission systems
– White Rabbit, thermonuclear fusion experiments – JET
and ITER, research results update from HEP experiments –
TOTEM and CMS/LHC in CERN, integration of software and
hardware.

Keywords—Photonics, optical fibers, optoelectronics, communications, multimedia, cryptography, virtual worlds, ontology of
information systems.

III. L IQUID C RYSTAL O PTICAL F IBERS

I. I NTRODUCTION
HE XXX JUBILEE SYMPOSIUM of young scientists
WILGA 2012 on Photonics and Internet Engineering has
gathered together over 300 participants in January and May
Editions. There were presented over 250 papers – mainly
concerning the realized Ph.D. theses and participation in
research projects relevant to the topical area of the meeting.
There were also presented a few plenary papers introducing the
audience into new research areas of photonics and electronics.
The symposium is organized under the auspices of SPIE – The
International Society for Optical Engineering, IEEE Poland
Section, Photonics Society of Poland, KEiT PAN, PKOpto
SEP and WEiTI PW. The symposium is organized annually by
young researchers from the PERG/ELHEP Laboratory of ISE
PW with cooperation of SPIE and IEEE Student Branches.

T

TH

II. T OPICAL T RACKS

OF

WILGA 2012

The topical session and tracks of WILGA 2012 were as
follows: nanotechnologies and nanomaterials for optoelectronics and photonics, optical fibers for sensors and all-photonic
devices for sensors, active optical fibers, sensors and sensory networks, object oriented design of optoelectronic and
photonic hardware, photonics applications, advanced bioelectronics and bioinformatics, co-design of hybrid photonic –
mechatronics and electronic systems, computational intelligence in optoelectronics and robotics, development in the
wide-angle astronomic observations of the whole sky – pi-ofthe-sky project, processing and imaging of multimedia data
Ryszard S. Romaniuk is with Warsaw University of Technology, Institute of Electronic Systems, Nowowiejska 15/19, Warsaw, Poland (e-mail:
rrom@ise.pw.edu.pl).

Classical Liquid Crystal Optical Fibers: Liquid crystal optical fibers (where a core is a liquid crystal) are investigated for
functional photonic components compatible with standard SM
(single mode) optical fiber transmission systems. The values of
elastic constants in nematic liquid crystals play a role in certain
solutions of sensors and modulators as well as nonlinear photonics solutions. The research on liquid crystal optical fibers
and nonlinear photonics is intensely carried out at Faculty of
Physics WUT by teams of prof. T. Woliński, prof. A. Domański and prof. M. Karpierz and dr A. Rutkowska. The
liquid crystals (LC) used in optical fibers and nonlinear photonics are: smectic (SLC), nematic (NLC), and chiral nematic
(NLC*). Reorientation of the modules, in reference to initial
orientation, is influenced by an external electric field. Initial
configuration in NLC cells can be essentially homeotropic
and planar. In optical fiber the orientation against the long
fiber axis can be: planar, axial, radial and transverse, as well
as mixed. Fundamental deformations of NLC, visible over
certain threshold value of the excitation field, are: splay, twist
and bend, with the involvement of three elastic constants k11 ,
k22 , and k33 . k11 is responsible for the Frederics effect. Measurement of these constants base on electro-optical effect, but
also nonlinear self-diffraction method, z-scan and all optical
method. For self-diffraction method and homeotropic texture
(k33 ) the polarization is not important. For self-diffraction
method and planar texture (k22 ) polarization is perpendicular
to initial orientation of long axis of molecules. Self-diffraction
method of measurement of Frank elastic constants is as good
as traditional method with external electric field. LC display electrical, magnetic, optical and mechanical anisotropies.
After some threshold temperature the anisotropies disappear.
In optical domain the ordinary and extraordinary refractions
coalesce abruptly to a single isotropic refraction. Mechanism
of optical nonlinearity in LC stems from reorientation, thermal
and Kerr nonlinearities. Time responses to these phenomena
in the LCs are of the order of ms.

464

Photonic Crystal Fibers impregnated with LC: Photonic
crystal fibers (POF), or holey optical fibers, can be impregnated or infiltrated with liquid crystals (LC), leading to a new
class of Photonic Liquid Crystal Fibers (PLCF). This is an
interesting and alternative way of making highly nonlinear
optical fibers, to the fibers of classical core cladding construction, where the core is filled totally with a liquid crystal.
The POF have micro/nano capillaries not filled, as usual, by
air but by liquid crystal. Filling a POF with LC changes
dramatically it propagation characteristics (refractive, dispersion, spectral attenuation and nonlinear). Potential applications
of PLCF are: all optical switching, light beam control and
steering, amplification, nonlinear directional couplers, tunable
filters, phase shifters, frequency conversion, etc. Classical
POF can be, depending on construction – dense or rare,
highly nonlinear, highly dispersive, endlessly single-mode,
polarization maintaining, high or low numerical aperture, etc.
There are two light propagation mechanisms in a POF, also
depending on the fiber construction: mTIR – modified total
internal reflection, and PBG – photonic bandgap. mTIR POF
are broadband, while PBG POF are narrowband for optical
wavelengths. The measurements of PLCF (dr K. Rutkowska,
Physics, WUT) indicate that: power of light beam affects
the nonlinear behavior of fibers; fibers can be considered as
a matrix of waveguide channels; spatial light localization is
observed, when a discrete soliton is formed. These results may
lead to practical photonic devices.
Nematicons – spatial solitons in nematic liquid crystals:
Nematicons are spatial solitons in nematic liquid crystals. Due
to large value of nonlinear index n2 the power of soliton
for NIR in liquid crystal is on the level of mW, instead of
kW in GaAlAs, MW in SiO2 and kW in polymer fibers.
Isotropic phase of LC has random orientation of molecules
while nematic is strictly oriented in parallel. Birefringence
of typical nematics is 0.2, while in special mixtures this
increases to 0.7. Birefringence is defined as a difference
between extraordinary and ordinary refractive indices. Basic
orientations of LC molecules against the optical fiber axis
are: parallel, axial, transverse and perpendicular, or in other
words – textures: homeotropic, planar, planar+v, hybrid and
twist/chiral. Self focusing effect stems from making a gradient waveguide by the beam itself. Chiral nematics create
effectively layers (a pitch) with the thickness equal to the
period of the molecule twist. Chiral structures are equivalent to
periodic discrete diffraction elements with added nonlinearity
(nonlinear waveguide arrays). In such structures the nematicons can interact mutually, change the input beam polarization
and position, be influenced by the external field, be rerouted
(switched) between waveguide branches, etc. There are some
differences between high and low birefringence NLCs in these
characteristics. Nematicons in chiral nematics (M. Karpierz):
do not need external fields, require nW of light power, creation
time is of the order of ms, can guide probe beams, can be
redirected by changing light power – by other nematicon,
by external electric field, by changing beam polarization, the
creation mechanism can be discrete or classical diffraction in
multiple layers, can be optimized by changing the pitch and
birefringence.

R. S. ROMANIUK

IV. M EDICAL A PPLICATIONS OF ACTIVE O PTICAL F IBERS
WITH P HOTOSENSITIVE D RUGS
Optical fiber technology is developed at Białystok University of Technology, Faculty of Electrical Engineering, in
Optoelectronics Laboratory chaired by prof. J. Dorosz and
prof. D. Dorosz. One of the projects concerns upconversion
emission in double-clad tellurite optical fibres. The purpose
is to enrich medical and bioscience applications of optical
fiber lasers. Such optical sources are applied for optical
response embracing therapy, biostimulation, cosmetics and
diagnostics, as well as thermal response, including surgery and
aesthetic medicine. The applications include: photodynamic
therapy, hair and tattoo treatment, photochemical diagnostics,
endoscopy, OCT imaging, spectroscopic diagnostics, surgery –
in urology, abdominal, dental,. . ., skin resurfacing, birthmarks,
dermatology, etc. Biostimulation is a fast growing field os
applications of fiber lasers. Biostimulation mechanism is as
follows: photon, mitochondrium, increase ATP production,
activity increase T lymphocyte, fibroblast, heparin unbinding,
tissue energy increase, vitality increase. Biostimulation is
used not only in dermatology but also endoscopically in gastroenterology, pulmonology, gynaecology, and in many other
medical fields. The used light sources are He-Ne (633nm),
InGaAlP (635nm, 650nm, 660nm, 670nm) GaAlAs (780nm,
820nm, 830nm), GaAs (904nm). Photodynamic therapy bases
on application of a photosensitive drug and optical activation
of this drug. Activated drug releases locally radicals. The
process leads to very localized cell necrosis. Various photosensitive drugs have different activating wavelengths, usually
spanning from 630nm to 690nm. Some of photosensitive drugs
are: natrium porfimer, BPD-MA, m-THPC, 5-ALA, HPPH,
boron-photoporfirines, lutetium teksapiryne, ftalocyjaninum4, natrium taporfinum, and others. Fiber lasers are used in
optogenetics. Nerve fibers are stimulated electrically and then
undergo optogenetic excitation followed by optogenetic inhibition. Specialized, medical oriented optical fiber equipment
is constructed for these purposes.
Thulium and Ytterbium doped double clad telluride active
optical fibers: Double clad optical fiber radiating in visible
spectrum is subject of research for medical applications in
Optoelectronics Laboratory at Białystok University of Technology (prof. D. Dorosz). Tellurite glass of low phonon energy,
equal to 750cm−1, is used as a matrix. Thulium is used as
an acceptor, for the up-convesion quantum transition present
in the visible spectrum. Ytterbium is used as a sensitizer for
increase in the luminescence band intensity. The following
tellurite glass is subject to synthesis: TeO2 – GeO2 – PbO –
BaO – Nb2 O5 – laF3 – aYb2 O3 – bTm2 O3 , where a = 1%
mol, b = 0.025 − −0.2 % mol. The glass is melted in
900◦ C in protective inertial Ar atmosphere. Then it is annealed
in 350◦C. The obtained glass parameters are: mass density
6.21g/cm3, refractive index 2.074, spectral transmission 0.35–
5.5µm, thermal expansion coefficient 108.9 × 10−7 K−1 (100400◦ ), transformation temperature Tg = 345◦C, Dilatometric
softening point Ts = 364◦ C. Optical, mechanical and thermal
properties of the glass were measured as functions of Yb3+
and Tm3+ ions concentrations. The major absorption peak is

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

for around 990nm for a glass with 1% mol Yb3+ and 0.25%
mol Tm3+ . Secondary peaks are for around 700nm, 800nm
and 1220nm. The major luminescence peak is for 474nm and
650nm. A double clad fiber was manufactured from active
telluride glass. The parameters are: NAr = 0.32, NAp = 0.95,
active core diameter 70µm. Fiber diameter 320µm. The fiber
has emission in blue.
V. O PTICAL M EASUREMENTS
Reference tests of cameras with calibrated light source:
A measurement system is under construction with feedback
controlled light source and photometric integrating sphere for
reference tests of cameras for astronomical applications. Light
source radiation shifts towards shorter wavelengths and the
intensity increases with the increase in T . The feedback relies
on measurement the Planck curve (for a few wavelengths) of
the source and determining its temperature. The measurement
system consists of Ulbricht sphere, photodiodes, spectrometer,
shutters, halogen light source. Shutters are coarse and fine,
driven by stepper motors for intensity regulation. The driver
is based on ATmega64 processor. The light source work conditions are set basing on the measured spectrograms. Cameras
are scheduled to be tested with the calibrated light source.
Optical Diffraction Tomography: Optical tomography creates a digital volumetric model of an object by reconstructing
images made from light transmitted and scattered through the
object. Thus it is applied to at least partially light transmitting
or translucent objects, like for some kinds of medical imaging
(OCT). There a lot of variants of optical tomography (time of
flight, i.e. time resolved, coherence, spectral, etc). In optical
diffraction tomography incident optical field shines on a rotated object, is transmitted through it distorted, then is focused
by imaging lens on a CCD camera. Object rotation introduces
a runout error due to out of focus effect. The runout error can
be corrected by numerical refocusing of the image, a research
work which is done in group of prof. M. Kujawinska (Mechatronics, WUT). Runout error limits tomographic measurements
of microstructures. Minimization of runout effects with numerical correction algorithm by: reduction of diffraction and
geometrical distortions of reconstructed structures, reduction
of unwrapping errors. ODT with corrected runout error allows
for reconstruction of micrometer scale structures.
VI. P HOTONIC AND E LECTRONIC S ENSORS , C OMPONENTS
AND C IRCUITS
Organic short wavelength lasers: Short wavelength lasers
(SWL) are used for optical data storage, reprographics,
biotechnology, color displays, plastic optical fibers, remote
sensing of chemical agents. They feature high brightness
and improved color saturation. SWL are manufactured as
wide bandgap diode lasers (nitrides), by frequency mixing
(SHG – second harmonic generation and SFG sum frequency
generation) or using optical up-conversion. There are also
possible FHG – fourth harmonic generation and DFG –
difference frequency generation effects. The optical set-up
with SHG, SFG etc. is sensitive to alignment. The optical
fiber technological teams from UMCS Lublin and IMiO PW

465

are working on PMMA doped with active nano-powder. The
PMMA is used for some time as an efficient optical material.
It has high energy phonons among the polymers. Here it is
used as a matrix. It is transparent and thermoplastic, low cost,
is easy to manufacture. It possesses reasonable attenuation,
forms well developed polymer waveguides, is flexible and has
high mechanical strength. The used nanopowder is Pr3+ .It
has transitions in the blue, green and red wavelengths. It
exhibits up-conversion in the visible in combination with
YF3 . The PMMA doped with nano-powder was researched
for its structure and was optically characterized. There were
made comparisons of luminescence properties between several
different composites Pr3+ :LaAlO3 and Pr3+ :YF3 . Excitation
and emission spectra were measured. The results reveal that
the PMMA was homogenously doped and that the nanopowder crystallites shield the active ion Pr3+ from external
PMMA influences. The crystallites do not influence the optical
transitions significantly. Thus, the doped PMMA seems to be
a promising material for not too long future light sources and
amplifiers.
Photonic bio-fuel inspection: Photonic, spectroscopic
biodiesel sensors are subject of research by the research
group in IMIO WUT (dr M. Borecki). The initial design idea
stems from the following statement. There are many standards
of fuel examination, but they are made exclusively for fuel
producers. At present, one of the most important bio-fuel types
due to ecological demand, is the bio-diesel fuel. The aim
of research is to build cheap, mobile and efficient, photonic
bio-fuel quality sensor basing on optical spectroscopy. Cost
of the calibrated light source contributes most to the overall
value of the systems available on the market. The assumed
solution aims at the modularity of the source. The fuels under
analysis contain C, H, H2 O, N2 , S, Cl, O2 , SiO2 and their
compounds. The diesel fuel contains 20 major components
and about 200 minor components that may be detectable by
the photonic sensing system. Fuels are categorized against:
density, calorific value, origin, H/C ratio, etc. The following
parameters are determined for diesel fuels: RVP – Reid vapor
pressure, boiling point, flash point, density, number of carbon
atoms in particle and viscosity. The intelligent portable multifuel (gasoline and bio-fuel), multi-sensor, photonic-electronic
analyzers base mainly on computerized processing of the near
UV, optical and IR spectra, and applying mathematical models
for determination of cetane number and as many as other 40
parameters. A simple and cheap sensor is also needed measuring very fast only a few key parameters and determining
the fuel usability. The novel design bases on optoelectronic
interface working at non-typical frequency eliminating the
background radiation, fiber optic pigtails, and capillary optrode
used for photo-spectrometric analysis of the attenuation and
fluorescence. Characteristic features of the spectrum to be
analyzed for fast bio-fuel quality results are:
•

•

attenuation features: 260nm features indicating the concentration of bio-components, 480nm – presence of lubricants, 620nm – presence of unsaturated fatty acids;
fluorescence excitation for 260nm: 270–300nm features
– showing concentration of bio-components;

466

fluorescence excitation for 365nm: 400nm – presence
of washing agent, 480nm – concentration of Cl based
improvers.
The design of a cheap photonic bio-fuel analyzer bases on
building light sources and re-connectable optoelectronic units
that are able to separate efficiently all these above listed analog
signals, digitize them immediately after detection and process
for the useful results. The optoelectronic set-up of the fuel
quality sensor consists of: function generator, light source,
optical coupling, detectors, digital electronics like DaqLab, PC
with DaisyLab or similar software, Ethernet access.
Wireless/photonic sensor networks: Research is continued
(ISE WUT) on wireless telemetry systems based on mesh
networks which are compatible with the IEEE 802.15.5 standard (commonly referred to as ZigBee). A number of remote
nodes is communication wireless over 2.4GHz bandwidth or
optically to a concentrator connected in turn via the RS, USB,
Modbus etc to a PC. The nodes feature a rich collection of
the IOs digital and/or analog. Slow control of the system can
be done via the RS.
Photonic inspection of yeast: Optical fiber capillaries were
applied for measurements of spectrophotometric properties of
Saccharomyces Boulardi SB. SB is a species of tropical yeast
isolated from the peel of lychee and mangosteen fruits. SB is
a probiotic, unicellular and globular. It reproduces by budding
and grows at very high temperatures of 37◦ C. The interest
in SB is because it is the easiest available type of yeast.
However, it causes inflammation and some disorders in human
body. The results of human cytology are obtained after two
weeks. SB shows high level luminescence in the short-range
UV 375nm. The laboratory set-up consists of a PC controlled
spectrophotometer, optical fiber capillary with a sample, and
light source with a power supply. The yeast samples were
measured with and without annealing. The aim of the design is
to build an automated, cheap sensor for fast SB yeast detection.
Photonic lab-on-a-chip bio-chemical sensors: Lab-on-a-chip
principle is: multiple functions of a chemical lab are put together in single chip; operations with extremely small volumes
of reagents due to high sensitivity and use of microfluidic
circuits, a variety of types of used sensors – electronic,
MEMS, optical fluorescence, photonic, and plasmonic. Operation principle can be refractometric by measurement of
effective optical index change due to monitored bio-chemical
reaction but also other like nephelometric. The configuration
are: MZI – Mach-Zender, FP – Fabry Perot cavity, DPI – dual
polarization interferometer. A full featured photonic integrated
sensor system was presented by dr A. Kaźmierczak of TNO
Delft, performed inside the European FP6-IST Sabio project.
The system consists of: optical interrogation device, couplers,
multichannel sensor topology, simultaneous interrogation of
numerous sensor channels, on-chip signal splitting by miniature vertical grating coupler, off chip solutions, and multichannel detector. The results of measurements indicate high
tolerance and coupling repeatability of optical coupling during
the bio-chip replacement. The following tests were done:
refractometry measurements for anti-BSA protein, temperature
influence compensation, etc. A complete optical solution for
photonic bio-chemical sensing system has been proposed
•

R. S. ROMANIUK

including transducer design, optical integrated multichannel
circuit and the integration platform.
Intelligent Pipeline ADC: Pipeline ADC have interesting
characteristics indicating their applications in many applications. Adaptive estimation algorithms can be used to improve
ADC performance, including the increase in the ENOB, while
not or minimally increasing their complexity. Implementation
of the approach is possible in a new class of ADC, so called
intelligent adaptive cyclic/pipeline ADC (IPADC) in which
output codes are computed as binary words of the fixed length
(dr K. J˛edrzejewski). Optimization of IPADC relies on finding,
for every converter stage, the maximal value of gain, and
corresponding value of digital coefficient, which minimize the
mean square error of the estimate. The parameters have to
satisfy the statistical fitting condition, guaranteeing that the
probability of saturation does not exceed an assumed (and/or
given) sufficiently low level. In classical pipeline ADC, the
problem of overloading is solved by artificial decrease of the
gain in an individual stage and resignation from erroneous
bits in sub-codes. In IPADC the gain may assume any value,
not only integer power of two. Greater gain and optimal
conversion algorithm allows to achieve the final ENOB value
in IPADC greater than in standard pipeline ADC. The disadvantages of IPADC are: high requirements on resolution
and accuracy of the internal DAC. The gain is increasing in
subsequent stages of the IPADC. A modified algorithm for
IPADC was implemented which omits these disadvantages.
For each stage of conversion there are performed estimates of
input sample and maximal coefficients of the amplifier gain
are calculated. In modified IPADC the resolution of the DAC
sub-converter is low and equal to the resolution of the ADC
sub-converter. Gain of the amplifier is smaller than in standard
IPADC. As a result the following was obtained: simplification
of the IPADC architecture, lowering size, power, costs, and
increase in ENOB. ENOB is limited only by the level of
internal noise (errors). Simulation tools enable assessment of
a number of ADC stages, a number of bits in digital parts and
other parameters required to obtain an assumed ENOB value.
Inkjet printing of electronic and photonic circuits on LTCC:
Inkjet printing is increasingly frequently used as a technology
of choice for photonic and electronic components, sensors
and circuits. A team lead by prof. M. Jakubowska of WUT
and ITME is working on methods of making electric connections using inkjet printing on LTCC (low-temperature cofired ceramic) substrates. LTCC substrates are commonly used
for multilayer electronic PCBs by the electronic industry
for aerospace, automotive, medical and telecommunication
applications. Multilayer hybrid ceramic circuits consist up to
100 layers and contain for example mounting holes, embedded
active elements, printed embedded passive components like
resistors, capacitors, coils, conductive tracks, VIA holes etc.
The fabrication process includes the following steps: green
tape foil, cutting, hole drilling, VIA filling, printing, stacking,
lamination, prefiring, firing, tests, finished module, packaging.
Screen printing is widely used in LTCC technology. Using
screen printing in LTCC circuit manufacturing brings several
disadvantages: difficulties in printing on soft, porous, fragile,
unfired LTCC foil; the line width limit is 0.1mm’ not enough

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

alignment precision; VIA diameter limit is 0.25mm’ expensive
tools are needed, difficulties in mounting fragile LTCC foil
in screen printer. The aim of the research is to develop contactless method of fabrication electrical connections in hybrid
electronic circuits on LTCC substrates using inkjet printing
technique. Proposed solution is expected to aid essentially
electronic industry with hybrid electronic and photonic devices
of new generation. Circuit scale integration will increase along
with decrease of fabrication costs. This solution is expected
to enable rapid prototyping of hybrid electronic and photonic
modules. The advantages of this technology is: contactless,
printed lines width lower than 0.1mm, alignment precision
better than 5µm, effective VIA filling, 3D surface printing
enabled, additive and waste free process, low power consumption, no additional tools needed and low cost rapid prototyping.
Polygraphic inkjet printers and inks have to be modified to
be usable for electronics industry. The effective printer has
to be able to align printout with precision better than 2µm,
work with commercially available print heads, use organic
and inorganic inks in a wide range of pH (1–11) and viscosity
1–12 cPas values, generate single drops of volume in the range
of 35–150pl, pause and stop at any time. The investigated
parameters are roughness and porosity and their influence on
resistance, track/drop shape and mechanical endurance.
Reference atlas of Copper standard spectrum: The team
from Rzeszów University of Technology (dr J. Domin,
Rzeszów Univ. Technology) specializes in making precise
reference spectra for molecular research. 1300 lines/mm Eschelette type grating is used mounted at PGS-2 spectrograph.
WU1 plates by Wephota were used for expositions. The goals
are: elaboration of spectral standard in Schumann region; measuring of high precision atomic standard lines fore this spectral
region for source catalogues. Using copper atomic spectrum
for standardization of the investigated spectra it is hoped to
receive a precision of a range better than 0.003 Å. It is standard
precision which is required in molecular spectroscopy. Having
such precision one can hope for possibility of calculations of
molecular parameters.
VII. C OMMUNICATIONS
A topical track on photonic Internet consisted of papers
on photonic LANs, trunk optical lines of ultimate throughput,
new optical non-blocking codes, developments in all-optical
architectures. One of the concepts, not yet fully confirmed, is
application of organic photonics to build the framework of the
variety of access networks to the terabit photonic transport
network. Micro-structured plastic optical fibers – mPOF are
under intense studies, also in this country. They are cheap and
are user scalable, even in singlemode version.
To meet new demands, the next generation of highperformance wireless and optical networks and communication
systems (CS) must support a significant increase in data
rates, better coverage, greater spectral efficiency, and higher
reliability. The applications of high performance data transmission networks extend well beyond traditional understanding of telecom systems. Complex networked structures are
fueling the development of self configurable sensory meshes,

467

smart structure, bioengineering including body area networks,
etc. These structures using wireless and optical technologies
need extreme energy efficiency and introduce completely new
wireless networking paradigm. Overcoming these technical
challenges will require significant breakthroughs in wireless
and photonic/optical component and system design. A dominating tendency in all these domains is to go digital, independently of the fields of applications, including short distance
extremely distributed communications. The reason is obvious.
The known and traditional analogue solutions are much less
efficient and noise resistant than digital systems, have low bit
rate and non-optimally utilize the available channel bandwidth.
Theoretically, however, there is a wide class of practically
important and potentially of extensive applications analog CS.
These systems can potentially beat digital ones for similar
applications. Principal differences between analog and digital
CS are as follows. Transmitters of digital CS contain digitizing
and coding units, which cannot be described by a continuous
input-output functions. This does not allow to determine explicit form of the probability density function, which describes
the signal transmission in the CS in the presence of noises.
Transmitters of analog CS contain analogue modulators, and in
the most advanced systems only they also contain sample and
hold units. All the systems consisting of transmitter with S&H
units, channel and receiver can be described by continuous
input-output functions and allow for definition the explicit
form of the probability density function, which describes the
signal transmission in the CS in the presence of noise. This
is a profound difference between the analogue and digital
CS (ACS and DCS). It results in different measures of the
transmission quality. For DCS, the basic analytical measure
of the quality of transmission is men square error of the
output estimates of the input signal. This measure cannot
be simply formulated. For ACS, the mean square error of
output estimates can be formulated analytically, because the
probability density is determined. This difference has divided
ICT into two directions. The DCS has concentrated on development of the increasingly efficient methods of discretization,
coding, keying and reversed operations in receivers. The ACS
remained a niche. There is no general criterion of transmission
quality in DCS. Three separate non-general criterions, related
by internal parameters, are used instead for determination
of DCS quality. These are: power efficiency of transmission
measured in J/bit, channel bandwidth efficiency measured in
bit/s/Hz, and bit error rate (called commonly as the P-B
efficiency). The closeness of these two first parameters is
estimated to the Shannon’s boundary. BER is assessed additionally using known characteristics of the channel noise,
its bandwidth, gain and power of the transmitter. The aim
of optimization of DCS is maximize bandwidth efficiency
Be in bit/s/Hz, minimize energy (power) efficiency Pe in
J/bit, minimize BER expressed in probability of erroneous
transmission per a single bit. Optimal DCS should transmit
signals for the maximal possible distance with maximal rate,
quality, and reliability and should have minimal complexity,
cost and energy consumption. Shannon theorem for Gaussian
channels says that there exist encoding schemes permitting
to transmit the signals at the power – bandwidth efficiency

468

Fig. 1. A basic simplified block diagram of the AFCS. xt -input signal, xk estimates of the input signal, η-noise in the forward and feedback channels,
s-signals transmitted and received in the forward channel. Forward and
Feedback channels may be RF, open optical or cable – fiber optic.

attaining Shannon boundary with the infinitesimally small
frequency of errors. Simultaneously it says that transmission
at a greater efficiency is impossible. The characteristics of
optimal DCS are: signal transmission is with bit rate equal to
the capacity of the channel; power-bandwidth efficiency is at
the Shannon boundary; they fully use their power/bandwidth
resources for a given BER; no analytical description method
exists for DCS. In practice the best DCS solutions have their
parameters of 3–5dB far away from the Shannon’s boundary.
Adaptive feedback communication systems AFCS: Adaptive
feedback communication systems, especially in fiber optic
version, are of interest for ultimately high quality mixed analog
– digital systems (prof. A. Platonov of ISE WEiTI PW), Fig. 1.
Data transmission performance in AFCS depends on characteristics and parameters of: source, forward and feedback
channel, transmission unit (TU) and adaptive modulator (AM),
received signals processing algorithm. Source parameters are:
mean value of the input signal, variance (mean power) of the
input signal, band frequency of the input signal. TU and AM
modulator parameters are: amplitude of a signal transmitted
by TU into a channel, saturation parameter of TU, number
of samples being sent, variance of the errors in the feedback
transmission. Parameters of the channels are: bandwidth of the
forward channel, variance of the noise in the forward channel,
distance between TU and BS, channel gain. Parameters of the
signal processing algorithm in the BS are: amplitude of the
signal on the BS input, the maximum number of transmission
cycles for each sample, variables of BS algorithm. Building
a testing workbench for such defined AFCS allows to study
novel effects in optimal AFCS. It is used for analysis of
quality and efficiency of transmission depending on conditions
and fields of optimal AFCS applications. Expansion of the
analysis tool is for multimode AFCS transmission systems
(AFCS phonic networks).
Current issue of AFCS is their optimization. It is assumed
that each sample of the input signal is transmitted iteratively,
in n cycles, in the same way and independently from the
previous samples. Analog AFCS has an adaptive AM modulator with nonlinear characteristics. Analytical form of general
criterion of transmission quality (MSE) is possible using
simple mathematical models of the signal source, adaptive
modulator, forward and feedback channels and receiver in base
station (BS). Thus, an algorithm exists of transmission and
reception, where there are embedded all system parameters
and controls, which minimizes the MSE of estimates of the

R. S. ROMANIUK

signal for each cycle of the sample transmission. The algorithm
obviously bases on adaptive adjusting of the TU and signal
processing in the BS, starting the process from assumed initial
conditions. The expected result of optimal analog AFCS,
treated as a generalized communication channel, is that the
system works precisely on the Shannon’s boundary for each
cycle of adaptive and recurrent signal transmission. Such result
is unachievable in digital CS. However, expansion of the
signal spectrum greater than the threshold coefficient decreases
the P-B efficiency of transmission. Optimal analog AFCS
realize theoretically lossless information transmission. While,
digital AFCS transmission is always combined with partial
loss of information due to quantization of transmitted signal.
High quality ADCs asymptotically improve the digital AFCS
parameters to the boundary.
The optimization task for analog AFCS can be solved
analytically, while for digital AFCS only numerically. Natural
limitation which confines the application of analog AFCS
is the time delay introduced by the reverse channel. The
necessary information is sent back from the receiver (BS) to
the transmitter (TU) with some latency. For this reason, both
the analogue and digital AFCS can be used for short-range
communications like wireless or optical sensor networks,
home communications, pico-cells and femto-cells, intelligent
houses, RFID, electronic keys, Bluetooth, IRDA, and the like.
The only exception is a distribution system (usually fiber optic)
for atomic clock signals, where ultimately stable frequency and
phase is distributed. The latency is not so important in these
adaptive systems. The very favorable features of analog AFCS
are: absence of ADC coding units (increasingly complex,
expensive, and energy consuming), possibility to work with
transmitters of minimal power.
Plastic Optical Fibers for FTTH and LAN: Plastic optical
fibers are increasingly frequently used in FTTH systems.
Plastic optical fibers for these purposes are systematically
characterized by a team from IMiO (dr R. Piramidowicz).
Access networks evolve from copper wires to FTTH. FTTH
omits street cabinets and ADSL/VDSL/UTP lines to the end
user. While FTTx has a limit around 160Mb/s, the FTTH
has practically no limit, or the rate may go as high as up
to 10Gb/s. Home area network includes: home control and
security, multiple 3D HDTV and VoD, security cameras,
home working, internet browsing, IPTV and VoD, and video
conferencing. Summing this up gives a few Gbps as a required
aggregated throughput. Application of silica fibers in optical
access networks has the following advantages: it is a mature
technology, low attenuation, ease of integration with existing
core network; and the following disadvantages: there are problems with indoor wiring due to limited bending radius, problems with network reconfiguration – specialized equipment
is required, high price of active components working with
SMF. The advantages of MM POF (multi mode plastic optical
fibers) are: they have excellent mechanical properties, they
feature user reconfigurability, low system price, low cost active
components, operation in the visible; while the disadvantages
are: high propagation losses, high modal dispersion. The
areas of applications of POF in small specialized LANs are:
home areas, data communication, automotive, object LANs,

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

automation, sensing networks. There were used several fiber
for tests, among them Mitsubishi Rayon Eska Premier, Mega
and Super, step index, large diameter. The key parameters
were measured: attenuation, dispersion and bending losses.
The main loss factors are: intrinsic – absorption and Rayleigh
scattering, and extrinsic – structural imperfections, pollutants,
and perturbations in geometry. Dispersion has the following
components: modal, chromatic, waveguide and material. The
fibers were prepared for testing by: cutting with a razor
blade, mounting FC connector, polishing using decreasingly
rough polishing paper from 9µm to 0.3µm. Attenuation was
measured with cut back technique with the use of calibrated
light source and OSA (optical spectrum analyzer). The fibers
had transmission window (with attenuation below 0.2dB/m) in
the visible 400–600nm and around 650nm. Mode scrambler
was used for measurements. The fibers showed no bending
losses up till 30mm of bending radius. 1 and 3dB of bending
losses were for 20mm and around 10mm bending radius (one
full 360o turn) respectively. The measurements were done
for the spectral region spanning 385nm–940nm. Dispersion
was measured with picosecond light source and calibrated
precision delay unit. The bandwidth was in the range of
100–200MHz*100m for 650nm without mode scrambler and
40–60 MHz*100m with scrambler. Launching conditions are
critical for bandwidth measurements. The measurements can
not be done without the mode scrambler. Fiber end quality
strongly influences the measurement results. The POFs are
good enough for FTTH systems – low cost home LANs
and short distance, high speed data transmission. Further
improvements of POFs is possible by micro-structuring. Microstrutural POFs are designed by the IMiO Fiber Optic team.
The advantages of mPOFs are: endlessly single mode operation, extremely high modal areas, high numerical apertures,
lowering of macro-bending sensitivity and propagation losses.
The mPOF designed have limited number of guided modes,
are bending resistant, have relatively large modal field and are
optimized for propagation losses.
VIII. M ULTIMEDIA T ECHNOLOGIES
Multimedia streaming technology is researched for increasing efficiency and quality as well as reliability, decreasing
costs and data rates – by using novel compression techniques.
A research group of multimedia technologies under the lead of
prof. W. Skarbek (IRE, PW) is working on advanced methods
of image and audio processing and multimedia streams delivery. Huffman coding (code-blocks HCB) is used in the AAC.
A few relevant architectures are investigated for processing
efficiency. The bit rate and system requirements and then
parameters were evaluated. The system was synthesized using
a relevant FPGA circuit (G. Brzuchalski). Also other new
algorithms are efficiently implemented in Fpga. Mpeg-TS
defines container for transmission and storage of audio and
video, provides basic error correction and synchronization
features, and is applied in digital TV and blue-ray discs.
Transport stream syntax is as follows: split to packets of 188
bytes, packet begins with a synchronization byte and header,
payload is max 184 bytes, Adaptation Field may contain PCR

469

– program clock reference. PSI – program specific information
(Program definition) includes: PAT – program association
table, PMT – program map table, PID – packet ID. PES
– packetized elementary stream includes: AVD – audio and
video data, PTS/PDS – presentation and decoding time, CRC
– checking for correctness. System architecture is composed
of: µC connected via EPI (external peripheral interface) to
MPEG-TS demux, (PAT, stream, PMT, PES) parsers, memory,
H264/AVC decoder, AAC decoder, video/audio DAC. The input is Ethernet. The output are CVBS and TRS signals. Demux
and decoders are implemented in Fpga. Transport stream and
configuration data is supplied to the Fpga by a microcontroller
trough EPI. The data is transmitted to the µC via Ethernet,
using the UDP (user datagram protocol). µC is responsible for
the translation of text commands into appropriate configuration
operations. The designed, configurable, expandable system of
flexible structure (A. Abramowski) is able to receive and
process stream in real time. The research work also covers bit
rate estimation for P-frames in the rho domain (M. Wieczorek).
A subject of research are stereo images. Super-pixel
technology was applied for matching of stereo images
(M. Roszkowski). The reason to choose super-pixels is as
follows: a reduction of image resolution is often desirable
for decrease in calculation effort and shortening time of
processing; the reduction should take into account local image
characteristics; the information about the objects present in the
image should not be lost; image pyramid does not always meet
this requirement. SLIC superpixels (simple linear iterative
clustering) are relatively easy to calculate; easy to set and
change the number of super-pixels in the image; superpixels
centers form quite regular grid; superpixels frequently respect
object boundaries. Enforcing super-pixel spatial coherence requires additional calculation step. SLIC superpixel calculation
algorithms includes the steps: create new rectangular grid by
evenly sampling the original image grid; distance between the
grid points defines superpixel size; perform k-means algorithm
locally in the image; allow the pixels to be assigned only to
the superpixels, which centers are not more than superpixel
size away. Local stereo algorithm is based on calculation of
disparity cost volume function. Fast local image segmentation
can help to decrease stereo algorithm complexity. The number
of searched disparities in the local stereo algorithm is significantly reduced. The reduced disparity does not have an adverse
effect on the quality of a computed disparity map.
IX. C RYPTOGRAPHY
Distributed measurement and control networks require some
form of security. Wide distribution of a network gathering
sensitive data via unsecured communication channels makes
it susceptible to attacks. A cryptographically secure hardware
random number generator dedicated for such networks is
researched by a team lead by prof. W. Winiecki at IRE PW.
Justification on the safety research of distributed measurement and control systems is: they are characterized in large
asymmetry of computing power, narrow bandwidth of radio
communication, wide bandwidth of photonic communication,
a little power supply from battery or energy harvesting, RF

470

wireless or open optical communication channel susceptible to
eavesdropping, intentional modification of data, interference,
etc. There is a need for secure communication sub-system
in these networks. The designed system is based on random
number generator, as they are used in most cryptographic systems. True random number generator (TRNG) is based on the
unpredictable physical phenomena. Pseudo random number
generator (PRNG) is based on deterministic computational
algorithms. The system was realized on chaotic generator
using Atmega processor and Python software (P. Czernik, IRE
PW). RDieHarder random number generator testing suite with
GNU R interface was used. The realized TRNG was checked
to be satisfactory solution for the measurement systems with
asymmetric resource. Better randomness was achieved at
a lower sampling frequency. Random bits in the 10-bit sample
word are unevenly distributed. The effect of the DSP on the
resulting randomness was measured.
Public key cryptography in sensor networks of unsecured
communication channels, with large number of elements,
should be characterized by: scalability, flexibility and ease of
maintenance, low maintenance costs, less protocol overheads,
larger keys than in the case of private key cryptography,
simple key distribution. The following public key algorithm
operations are used: one-way trapdoor functions with integer
factorization problem or discrete logarithm problem; finite
field arithmetic with primary fields GF(p) (RSA, DSA), and
binary fields GF(2m ) (ECC, HECC); modular operations –
addition/subtraction, multiplication, and inverse computation.
Possible solutions to modular multiplication GF(p) are: multiplication followed by reduction including multiplication, naïve
reduction by division, fast Karatsuba-Offman multiplication
and Barrett reduction; multiplication interleaved with reduction – Montgomery and interleaved multiplications; bit-serial
versus bit-parallel architectures. The system was implemented
in vhdl (J. Olszyna, IRE PW) and estimated/simulated by
Gezel – power estimation; ModelSim – functional and timing
simulation; PowerCompiler – power consumption estimation.
The goal was to be below 100µW with power consumption
to enable power harvesting units. Implemented algorithms
can serve as building blocks for RSA cryptosystem. The
interleaved multiplier architecture seems to be well suited for
low power cryptographic applications. A possible application
of the architecture is acceleration of public key primitives in
constrained environments like sensor networks.
X. B IOMEDICAL , H IGH P ERFORMANCE AND DNA
C OMPUTING
Artificial Intelligence for Robotics: The team of Artificial
Intelligence by dr S. Jankowski of ISE WUT is working
on hardware-software co-development serving for advanced
applications of photonic and electronic systems in robotic
vehicles. AURA is a system designed for automatic obstacle
avoidance in robotics using advanced neural algorithms. The
system is placed on hardware based platform using multiprocessor structure. DSP functions are performed by FPGA.
The system uses miniature microwave radars as sensors. The
software structure is as follows. Data acquisition from peripherals is done by RDP, CRW and AHRS, which is a real time

R. S. ROMANIUK

process. Topology of neighborhood reconstruction is using the
perception map. Potential obstacle avoidance decision is using
neural algorithm. Dynamic identification is relying on reaction
to the input values via a learning procedure.
Artificial Intelligence for Unmanned Aerial Vehicles UAV:
Perception and decision systems for autonomous UAV (unmanned aerial vehicles) flight are subject to intense cooperative research at WUT and at La Sapienza Uni. Roma.
The research project consists of the feasibility study, design,
prototyping and testing of new systems for autonomous flight
of commercial UAVs for civil applications. The system includes relevant set of sensors, computer vision, data fusion
as perception, and automatic decision algorithms. All of these
components have to be embedded on a specific platform. The
UAVs are classified as: large size – weight >200kg, operating
time>24h, range>1500km, high altitude; medium and small
size – weight 5-50kg, operating time 5-24h, range<300km, altitude high/medium; micro – weight<5kg, operating time<5h,
range<1km, low altitude. Automatic navigation means that the
UAV has an electronic Autopilot System (AP) with a preloaded Flight Plan (FP). It permits: auto take-off, cruise
following waypoint, auto-landing. Autonomous flight means
that the UAV has an electronic/photonic system that permits
the “Sense and Avoid” operation which includes: detection of
obstacles (fixed or moving, in air or in the ground, cooperative
or not), computation of Time to Contact, processing and
elaboration of an escape strategy. UAV with large payload
(over 100kg) have large power source onboard, high processing capabilities of electronic system and large space in
the avionic bay. They permit an efficient use of data fusion
from multispectral cameras and extended, heavy radar systems.
UAV with medium and small payload (less than 20kg) have
poor power source available onboard, quite low processing
capabilities and little space in the avionic bay. The most
frequently used technology in such a case relies on the Optical
Flow (OF) solution for motion detection computed from the
real time vide streaming and acquired from cameras installed
onboard. The system is based on bio-inspired algorithms, like
the vision system of a fly. Data sources are GPS, IMU, Laser
Radar (Lidar), bio-inspired machine vision, data processing algorithms. All the data are subject to data fusion with the output
enabling autonomous flight. The Optical Flow technology is
defined as a pattern of apparent motion of objects, surface and
edges in a visual scene caused by the relative motion between
an observer and the scene. The OF scene analysis algorithm
relies mainly on the brightness changes of each pixel in the
frame sequence. The OF divergence activates the saccade. The
insect flies in the direction of the greater OF. The computation
cost for OF has to be decreased by dividing the frame to subframes and optimizing the calculations.
DNA Computing: The work on DNA computing is done
in WUT and WULS (Warsaw University of Life Sciences)
cooperation. One of the intermediate aims is to master the
genome browser algorithm. The Genome Sequencing Project
is the world’s largest action of producing the full dataset to
create maps of genomes. The recognized number of genomics
sequences ATCG is continuously increasing. The aim of the
GBA system is to display genomic features fast and in an

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

easy way. The research procedures go from acquisition of the
genomic data to the genome draft, which is partial assembled
genome, and include the following steps: short reads – sequencer output; contigs (physical map of the genome that is
used to guide sequencing and assembly) – sequencer output
assembly; BES – back end sequences; scaffolds – contigs and
BES assembly; markers map – known sequences and position
on chromosomes; chromosomes – with assigned all elements;
genome draft. Genome annotation, which is a process of
attaching the information to the structures, concerns two
processes – structural and functional. Structural annotation
consists of the identification of genomic elements: gene-like
structures, coding regions, location of regulatory motifs. Functional annotation consists of attaching biological information
to genomic elements: biochemical function, biological function, involved regulation and expression of interactions. The
genomic data are stored in a standardized very large tabular
format, imposed by classical biological tradition, and very
difficult for further digital processing by ICT systems. New
approach is required for “DNA computing” which employs
high throughput assays, automatic operator-less processing,
advanced statistics and smart bioinformatics. All these features
have to be combined in a single smart display of the genome
overview – a genome user-friendly browser. The information
display goes from a single nucleotide to a chromosome with
annotated data, from multiple and diverse sources, including
gene prediction and structure, proteins, expressions, regulation, variations, etc. The example is the MSU Rice Genome
Annotation Project (GBrowse). The advantages of GBrowse
are: simple and clear view of the genome structure and detailed
information of genes, easy accessible data, improvement of the
usage of knowledge stored in genome. The disadvantages of
GBrowse are: installing, configuring and updating is difficult,
input of new genome data takes very long time, there are
used scripting languages. The GenWeb – a new genome
browser. Which is a joint effort of WUT and WULS introduces
new facilities in algorithms, data structure and in software
architecture. The Intervals contain: reads, contigs, scaffolds,
structural and functional annotations data. The Intervals tree
node contains a point in sequences (index) and intervals with
common index. Software bases on client-server solution. The
client uses web browser. There are used the following programming languages and libraries: C++ (calculation library),
Python, Adobe Flex, DJango, BioPython (access to external
databases), Boost and others. The system has access to NCBI
and EBI databases. The value of genome sequences lies in their
assembly and annotation, but its practical usefulness depends
on the way of management and presentation.
There are researched different methods of decoding gene
sequences. The aim is to lower the costs, shorten the sequencing time, increase the accuracy and generally make the
method accessible for wide medical procedures. Proteins are
linear sequences of amino acids. There are at least 20 amino
acids including Alanine, Leucine, Serine (A,L.S,. . .). DNA is
a double helix of nucleotides – nucleic acids (C T A G) plus
U, instead of T mainly in RNA. Single strand RNA contains
DNA information out of the nucleus. Peptide biosynthesis goes
through DNA – mRNA, Rybosome plus tRNA plus rRNA,

471

which results in polypeptide. Codon is a triplet of DNA, eg.
GCA, ACT, CGT,. . . There are 64 codons, and each peptide
is coded by at least one codon in 1 to N relation. Codons
have specific frequencies in different proteins. The decoding
algorithms base on codon frequencies. The approach is: choose
a codon randomly, select the most frequent codon in a given
organism, randomize codon with respect to its frequency in the
organism, optimize locally in sliding window. The properties
of such methods are: require the codon frequency table, low
expression for the resulting synthetic gene, tertiary structure
(folding), biosynthesis is stopped with too low level of corresponding tRNA, do not reflect any grammar. Hidden Markov
Model (HMM) is used successfully for decoding of gene
sequence. Markov model is a stochastic model that assumes
Markov property of the researched system defined as: Q finite
set of states A, C, T, G; probability S of start states equal to 1;
transition matrix A of unitary properties. At these assumptions
the probability of getting sequence s is described analytically
as a product of elementary conditional probabilities. Hidden
Markov Model is somewhat more complicated, because the
entire state is not visible, or there is an additional hidden
level. The HMM has in this case the following arguments: V –
observations (20 amino acids); Q – hidden states (64 codons);
A – transition probability matrix, E – emission probability
matrix; S – start probability vector. Probability of getting an
assumed sequence is also a product individual probabilities
from emission and transition matrices. HMM tends to reflect
frequency table and can model grammar. The task is, thus,
to get hidden sequence knowing the output sequence. Viterbi
learning/training algorithm is applied for back reflection. Out
of the sequence Q,V,E,A,S, there are two parameters one can
change – start probability vector S and transition probability
matrix A. Assuming that in the current algorithm S is constant
and equal to S(qi ) = 1/64, and the probability matrix is
constant. The key factors in the algorithm success are: fast
Viterbi implementation, efficient optimization algorithm, more
complicated mode – higher level HMM with additional loops
in Viterbi, adding folding and other effects, application of
neural networks.
Classification of DNA strings for DNA recognition and
computing: One of the research directions on DNA is creation
of a simple and effective classifier for strings (dr S. Jankowski
ISE WUT). Feature selection was applied with Contrast
Pattern Kernel (CPK) for Phosphorylation prediction. In the
real life problems, many data is represented as strings. For
example, in bioinformatics, strings are used to represent DNA
or amino acid sequences. Some of the most effective classifiers
like kernel methods, particularly SVM, require the input data
to bee represented as real number vectors. How, in such
a case, strings can be represented? There are introduced, as
a solution, the string kernels, which are special functions
designed for maintaining string data. A kernel function is
often interpreted as a measure of similarity in a sense that the
kernel has a large value when the arguments of the function
are similar. For example, two biological sequences will be
considered similar if they have a good fit. Any sequence
consists of smaller sub-sequences (single symbols, pairs of
symbols, etc). We can represent a sequence by substrings

472

appearing in it. Two sequences will be considered similar if
they share many common substrings. Spectrum kernel is one
of the most popular string kernels. This kernel describes the
sequence x by the frequency of occurrence of all possible
substrings a of constant length l, created over alphabet S.
The kernel is defined as a dot product or RBF function
between the two sequences represented in the described way.
The task to be solved is: substrings of what length should
be used? If we have so many features, substrings for the
spectrum kernel, it is essential to select the most relevant
substrings. Thus, the basic idea is instead of creating a vector
consisting of all possible substrings, one can use contrast
patterns. A contrast pattern is a substring that is common
for one class and rare for the another one. More, one should
not consider a complete sequence but select the most relevant
position in the sequence. The most important attributes in
a sequence with respect to each class can be selected by using
the classification tree. The tests were carried out for the input
data set containing 17-symbol amino acids sequences grouped
with respect to their reactions with several selected enzymes
(kinases). They catalyze the reaction of phosphorylation for
selected sequences. Phosphorylation activates or deactivates
many protein enzymes, causing or preventing the mechanisms
of diseases such as cancer and diabetes. The least-square
support vector classifiers have been trained and tested. Thanks
to the feature selection one can obtain more effective and
simpler classifiers.
Haplotype frequency estimation: Different types of heuristics is researched for haplotype frequency estimation with
large number of analyzed loci. The research is carried in
a group lead by prof. L. Mulawka at ISE PW. Haplotype is
a combination of alleles at adjacent loci on the chromosome
that are transmitted together. Genotype is a combination of
alleles the individual carries. Polymorphism is an occurrence
of two or more alleles at the same single locus. Determining
haplotypes with laboratory methods is expensive and time consuming. In contrast, there are many cost-effective techniques
for determining genotypes. In general, it could be impossible
to infer haplotypes from genotype data. There are different
strategies for inferring haplotypes. Phasing problem is solved
by: pure parsimony, hidden Markov model and other Bayesian
approaches, and maximum likelihood estimates. The problem
is complex since the number of haplotype resolutions grows
exponentially with number of observed loci. Every algorithm
employing full space search would operate with very high
complexity. This is why it cannot be directly applied to phasing
long genotypes. Genotypes can be divided into shorter pieces
that overlap. Piece length is fixed, so is computation time.
Phasing fixed number of pieces has lower complexity. Multiple
pieces can be phased in parallel. If phasing algorithm is
convergent, the total error should not be large. The error and
execution time are function of width and overlap parameters.
Hardy-Weinberg Equilibrium states that allele and genotype
frequencies in a population remain constant.
Classification of RNA secondary structures: Prediction and
classification of RNA secondary structure is done using such
algorithms as: Naïve Bayesian Classifier (NBC), Decision Tree
Classifier (DTC), and k-nearest Neighbor Classifier (kNNA).

R. S. ROMANIUK

The aim is to optimize the algorithms against simplicity, calculation time and cost (R. Nowak, ISE WUT). The secondary
structures are: hairpin loop, internal loop, bulge and junction.
RNA secondary structures can be generated by RNAstructure
application. MBC is based on Bayes theorem which concerns
counting the value of the cause probability when the result is
known. DTC shows considerable complexity for large data
sets. kNNA relies on selection of the dominant categories
in a subset of the set’s training with the smallest value of
distance. kNNA has known problems with noisy training data.
RNA STRAND is the RNA secondary structure and analysis
database which has over 3500 trusted secondary structures of
36 different types. The categories with the highest number
of added secondary structures are: Transfer RNA, Transfer
Messenger RNA and Ribonuclease RNA. These ones represent
nearly 55% of all examples. The prospects of development of
the algorithm are as follows: connect to large secondary structures database, adding secondary structure predicting module
implemented using different algorithm, allow to create user
accounts, and storing the results of classification in a database.
Stem cell therapy: Design guidelines of quality assessment model for stem cell (SC) therapy is researched by
a collaboration of WUT, WF-UW, ICH Warsaw. There are
three basic types of stem cells, relevant to their potency:
totipotent (differentiate into anything), pluripotent (all except
placenta), multipotent (replenish skin and blood cells). SC are
from umbilical cord blood – USC, bone marrow – BMSC
and embrional – ESC. USC are beyond dispute concerning
the ethicity, as opposed to ESC. USC have greater potency
than BMSC due their younger age. USC are classified as
hematopoetic HCS, endothelial progenitor EPC, mesenchymal
MSC, unrestricted somatic USSC and very small embryonic
like VSEL. The quality model bases on linear relation between
endogenic variable (therapy quality) and the exogenic variables
(effectiveness, efficiency, cost, time, etc).The work on the
model goes along the following path: methodology evaluation,
data source choice, data cleaning, parameter selection, model
verification/validation. Conceptual and graphical resources are
taken from MEDAFAR – classification of pharmaco therapeutic referrals.
Analysis of human gait cycle for robotics and medicine:
The research on human gait cycle is done using photonic
and video sequence analyses (dr J. Dusza of WUT and
Valencia University). A simplified model of mean double
step (MDS) for gait cycle in human body movement was
introduced. Human gait is defined as bipedal, biphasic forward
propulsion of centre of gravity of human body, in which
there is alternate sinuous movements of different segments
of the body with least expenditure of energy. Gait cycle can
be precisely recorded by multi-video system with multiple
markers. The gait cycle contains: right initial contact, left preswing, eft initial contact, right pre-swing. The support phases
are: double support, right single support, double support, left
single support. Right stance phase is simultaneous partly with
left swing phase and vice versa. The following parameters are
measured using kinematic methods (by at least 4 cameras, with
reflective markers glued to the body in standardized positions):
coordinates of selected points (markers) vs. time, angles vs.

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

time (Left and Right – L and R, hip, knee, ankle, pelvic),
trajectory of selected points, cyclograms, bilateral cyclograms,
velocity, acceleration, joint rotation. At least six degrees-offreedom marker set for gait analysis is established including: L
and R, Sagittal, Coronal and Transverse. The global measured
parameters are gait length and duration of gait. These two
parameters create a circular area for many measurements,
in the global parameters plane. The used hardware is fast
multichannel frame-grabber. Reconstruction of 3D is done
with epipolar techniques. The current research problems are:
can be the movement described by simple dependencies;
parametric model of gait; what kind of process is stable
gait; understanding the asymmetry in gait; and gait model
simplifications. Analysis of the functions show that the gait is
not a periodic process, neither it is an almost periodic process.
Walking is a cyclical but not periodic process. And there is
no simple mathematical analytical definition of a cyclic signal.
The closest mathematical approach is to add periodic model,
random component and chaotic component. The periodic
model bases on MDS – Mean Double Step (J. Dusza). The
analysis includes averaging an arbitrary number of recorded
gait cycles in the spectral domain. This approach leads to
very high data compression ration and enables efficient gait
analysis, tool standardization and further research. Photonics
solutions are essential in this research.
Population study of health: Data management and quality
assurance in very large data sets and bases, especially medical,
is a very difficult and effort consuming task. It requires rigorous standardization to be useful for further research. The data
may originate from such different and nearly incomparable
sources as measurements in different conditions, including
image data, or from an interview. Image data may come from
different sources like: X-rays, tomographs, ultrasonographs,
OCT, PET, optical, IR, multispectral, etc. Different parameters
from these data contribute in a different way to the final system
model. A consortium of COI, IMW, NTNU – Trondheim, UW,
PIS, FPZ,WUT, CC-IoO inside a Polish Norwegian PONS
project (dr Z. Wawrzyniak and dr D. Paczesny, ISE PW). The
reasons of PONS study are: there is a wide gap in health and
disease in Poland compared to western European countries,
an urgent need to understand the underlying causes of these
differences, to study population with respect to important
factors related to health and wellbeing, understanding of
important causes of morbidity and mortality in Poland, to
establish a solid knowledge base for the prevention of these
major causes of premature morbidity and mortality. Data management in the system includes: abstraction, collection, transmission and storage, coding, processing, security and quality
assurance. Data abstraction includes: HSQ – health status
questionnaire, medical measurements, BioBank, e-collection
and checklist for data consistency. Data processing is done
using data mining methods. The data includes altogether
around 1000 variables. Data is good when they can be used for
healthcare decision support and provide accuracy and integrity.
Data quality assurance is a dynamic and systematic process
of profiling the data to discover inconsistencies, and other
anomalies in the data and performing data cleaning activities
(removing outliers, missing data interpretation) to improve the

473

data quality. The QA consists of the following steps: determine
the data items causing problems, profile the offending data,
determine what the data should be, determine how to finally
clean the data. QA process consists of three phases: predata collection, data collection and management, and post-data
collection. The use of modern IT, communication infrastructure, advanced database technologies and remote software and
hardware structures efficiently support the research of massive
epidemiological surveys.
Pneumography: Automatic breathing measurement systems
may base on spirometry, optical methods, electrical methods,
or combined ones, etc. Impedance pneumography is based on
relevant measurements performed on a patient’s body. Ambulatory impedance pneumography is researched in Faculty of
Mechatronics at WUT (M. Mlynczak, G. Cybulski). In clinical
stationary conditions several methods to measure respirations
may be used, which are completely unsuitable for ambulatory
conditions. On the other hand there are many clinical and
physiological reasons for requiring a reliable, long-lasting
quantitative measure of respiration parameters. Impedance
pneumography (IPG) seems to be a suitable technique satisfying the mentioned requirements. Long term respiratory
monitoring is possible, at low cost and relatively easy to
use without active cooperation of the patient. IPG measures
changes in trans-thoracic electrical impedance with electrodes
fastened to patients skin. There are two adding effects to the
increase of impedance during inspiration: gas/fluid volume
rate and conductance paths are increasing. The change is in
the range 0.02 − −0.2%, but the measurement conditions
are very noisy. The measurement system requires not only
repeated calibration but also complex noise removal. There are
many sources of noises: physiological processes, heart beating,
patient movements, etc. One of the measurement methods
may base on injecting RF signal into patients body and
measure modulation of this signal by physiological processes,
including breathing, and then filter out other side modulations
and noises. Also a multisensory method may be applied,
with sensor signal fusion. The system is predicted for regular
ambulatory practice but also for sport exercising and for
augmenting the treatment of asthma and functional respiratory
disorders.

XI. B RAIN – C OMPUTER I NTERFACE
Brain computer interface BCI is an innovative biomedical
technology that use brain activity to control computer environment. The computer, in turn, may be used to control
other systems and/or devices. The neuro-feedback systems
available are: BrainGate – invasive neural interfacing to help
disabled people; g.Tec Guger Tech. – noninvasive biopotential measurement systems; Emotiv Epoc – BCI device with
multiple electrodes for home use; NeuroSky MindWave –
simple neuroffedback; OpenEEG – the most popular neurofeedback open source platform, and other with practically
no support like BrainBay, NeuralServer, EegMir, etc. From
engineering point of view the BCI is a versatile platform
for biological signal acquisition, processing and presentation,

474

targeted at electrical brain activity. The basic features of the
BCI as researched today are: can use invasive or noninvasive
methods to acquire brain and/or other physiological signals,
including photonic, RF, mechanical, magnetic and other; can
use different methods of brain activity measurement like EEG,
fMRI; no signal input into brain except safe audio-visual
stimuli, complex analog measurements system working with
ultra-low-voltage biological signals; advanced digital structure
for signal acquisition, processing and decision making; use
neuro-feedback mechanisms for closed-loop control. A research work on BCI is carried out in the Biomedical Engineering Research Group (ISE WUT) lead by prof. A. Grzanka
(T. Cedro, K. Chojnowski, J. Fraczek,
˛
L. Czupryniak). The
assumptions for the system design are: low-cost fully featured
BCI system; modular construction for easier experimentation
and prototyping; is based on open source and free software
as development platform and user environment; overcome
existing low-cost system limitations and bring new features;
aim at a new standard for academic and home use of the BCI.
Measurement front end of the system acquires the EEG signal,
of the amplitude 5–50µV and frequency range 0–100Hz – with
delta, theta, alpha, beta and gamma waves – of the following
frequencies respectively: 1–4Hz, 4–8Hz, 8–12Hz, 12–40Hz
and over 40Hz. The measurement issues are: low signal
amplitude, high noise level and high skin impedance. Thus,
the hardware has to be quite refined. The EEG was analyzed
in real time to extract the signals responsible for simple
functions like eye close and open, hand and leg movements etc.
The BCI hardware uses new fully featured BCIOP protocol
(BCI open protocol) basing on TLV mechanism (tag length
value). BCIOP features are: exchange data between target
equipment and host PC; allow real-time applications with low
latency and high throughput; control functions and parameters
of target device; storage measurement results and off-line
analysis; implementation of user interaction and realization
of nonstandard research procedures; have library form to be
linked into small footprint firmware and full featured host
software; easily adapt to hardware specific needs such as data
format, sampling speed or time intervals, and others.
XII. O NTOLOGICAL DATABASES , L ANGUAGES , L OGICS ,
M ACHINE L EARNING AND A RTIFICIAL I NTELLIGENCE
Ontological approach to the development of some ICT technologies leads to deeper understanding of large and complex
systems, which are generally very difficult to be described
in a formal algorithmic way. These systems include, worlds,
countries, societies, large corporations and institutions, etc,
but also very large technical and civilization infrastructures,
like research, medicine, communication, transportation, environment, global and local safety, large events, etc. The
components of the more formal description trials are: different
kinds of logics and logical systems, descriptive languages,
knowledge analysis, notions definitions, automated methods of
building models and concepts in logical systems, checking the
systems for inconsistency, building inherence paths, reasoning
methods, evaluating sets of theses, checking the assumptions,
presenting use cases, and many others.

R. S. ROMANIUK

Epistemic logic and its relations to the modern ICT was
a subject of Wilga topical session prepared in cooperation
by prof. J. Mulawka (WUT) with the research team of
prof. E. Nieznański (UKSW). Ontological and epistemic approach to future photonic and electronic systems, in hardware
and in software spaces, turns out now to be an essential
development basis of the intelligent virtual worlds. Hardware
and software/firmware is a sort of a basic foundation of such
systems. Future development of photonic/electronic systems
building fully intelligent virtual worlds is expected to base
on more sophisticated ontological base than it is today. The
basic features of such worlds in the future will be very
probably full conscience of subjects and objects, and full
submergence. Today, strictly speaking, the used ontology is
simplified, if not primitive. The search for new extended bases
for artificial intelligence systems go into various directions.
Some of them are: new logic systems, epistemic logic, research
in knowledge representation, formal description of ontological
systems, and others. A logic is a formal language which
supports reasoning – natural and artificial. The logic has
precisely defined syntax and semantics. The logic is fully
independent of the domain of application. As a consequence,
representation of some things in logic may not be very natural.
Epistemic logic is a subfield of modal logic concerned with
reasoning about the knowledge. It was rediscovered in modern
form by Kripke in 1963. There are four basic modal operators
in epistemic logic, which may be digitized and applied in
a digital world: W – one knows that; S – one is certain
that, P – one supposes that, D – one admits that. The logic
defines/induces relations between the operators, and bases on
coherent axioms and rules, for example analogous to CPL
– classical propositional logic: A1. S(p → q) → (Sp → Sq);
A2. Sp → SSp; Df.W:Wp ↔ Sp ∧ p; A3. Wp → SWp; A4.
Sp → Pp; Df.D: Dp ↔∼S∼ p; A5. Pp → Dp. Using such
operators and relations between them it is possible to prove
theses. The used axiomatic method involves replacing a coherent body of propositions (like a mathematical theory) by
a simpler collection of propositions (like axioms). The axioms
are designed in such a way that the original theory can be
deduced from the axioms, using among others modus ponens
rule, semantic tables of Beth, literals, clauses, clause sets,
resolution rule. The general goal is to adapt and test the above
methods into epistemic logic, which may require research on
new theorems, and to incorporate epistemic logic into the ITC
engineering knowledge.
Virtual intelligent worlds are dynamic entities. Dynamic
epistemic logic embraces such fields as: epistemic logic,
public announcement logic, action models, expressivity and
probability. Dynamic epistemic logic is the logic of knowledge
change. DEL embraces by no means a single logical system. It
is an overhead for the whole family of logical systems. DEL is
expected to allow the artificial intelligence to specify static and
dynamic aspects of multi-agent systems. The language of DEL
bases on dynamic semantics, which is a subarea of semantics.
It distinguishes between de dicto and de re understanding of
sentences. It is concerned with the change of information and
the change in general. The main idea of dynamic semantics, in
information theory and artificial intelligence sense, is that the

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

meaning of a syntactic unit, irrespectively if it is a sentence
of a natural language or preferably a computer program, is
best described as the change it brings about in the state of
a human being or a computer. The meaningful information in
such an ontology is regarded as an entity (existence) that is
in functional relation to a particular subject. The subject has
a certain perspective in the real or/and virtual world. Such
a subject is defined as an agent. Epistemic logic trying to
describe the general relations of an agent may be written
formally: I know that p[Ka p]. He does not know that p[¬Kb p].
He knows whether p[Kb p Kb ¬p]. He knows that I know that
she does not know that p[Kb Ka ¬Kc p]. Epistemic models use
A. Kripke structures M=(S; R; V), where S is a nonempty
set of states, R gives an accessibility relation Ra ≤ SxS
for every a belonging to A; V:P → ρ(S). If all relations
Ra in M are equivalence relation, we call M an epistemic
model. Public announcement logic, important in virtual reality,
bases on the following statements. After announced that p,
everyone knows that p[(p)Ep]. After it I announced that
p, it is common knowledge that p[(p)Cp]. Example: After
it is announced that none of the programmers know the
newest programming language, all the programmers who do
not know this language know that they do not know this
language [(Λa∈A ¬Ka ma )Λa∈b Ka ma ]. The epistemic modeling of a real and/or virtual situation contains the following
components. Given is a description of a situation (for example
in the Internet based virtual reality). The designer/modeler tries
to determine: a set of relevant prepositions, a set of relevant
agents, a set of states, a nondistinguishability relation over
these worlds for each agent action models. Further step in
creation of an intelligent virtual world is a dynamic modeling.
There is given a description of a situation and an event
that takes place in this situation. The designer/modeler first
models the epistemic situation, and then tries to determine:
a set of possible events, preconditions for these events (for
example space and time coordinates, but not only, also more
complicated ones), a nondistinguishability relation over these
events for each agent. An action model M, in such intelligent
virtual reality, is a structure over a finite domain of action
points or events S, maintaining an equivalence relation on S,
denoted by operator ∼a , and fulfilling precondition function
pre: S→L that assigns a precondition to each s ∈ S.
Building of practical intelligent virtual environments (very
complex intelligent software systems basing on rich ontology,
we say ontology driven) requires the ability to represent the
knowledge. Knowledge is defined here simply as all what
I know, and is a symbolic description of the real world. It is
a set of messages from a specific area. The task of knowledge
representation is split to categorial objects representation. The
applications of categorial object representation are: categorial LISP, PROLOG, semantic networks, implementation of
machine engines in extended logics, and logics of categorial
objects, concepts and systems. The most objective (and popular in relevant research) Knowledge Representation Methods
(KRM) are independent of implementation. They base on
procedural representation (called production rules) and may
contain simple unification processes and could be used as
tensors of knowledge. KRM is a root of Semantic Networks

475

(now the Semantic Web) in the original linguistic M. Ross
Quillian and Allan M. Collins sense (semantic knowledge representation). Semantic network connects all possible notions
combined with an object or subject of the knowledge. This
may include: description, varieties, features, actions, relations,
applications, linkages, answers to such questions like: it has,
it is, made of, used for, where, why, what, etc. Modified
Quillian approach to KRM includes: simple and perceptive
ways to represent information, object oriented approach to
knowledge representation, natural representation, etc. However, yet the meaning of semantic networks is not always well
defined. Categorial Semantic Networks include the following
categories: T – things, F – features, S – situations, and C –
classes. The categories are related by the following operators:
Relatives Y/X = Y of X, relation T/T; Concatenations X^Y
=XY, relation F^T; Categories X, X1,. . ., Y,Y1,. . . categories
T, F, S, C; Hypostasis Y//X=Y arising from X, F//S being
X, T//C aggregate unit from the class; Operators of relation
and quantity: of, from, ^, ∼, =, a, the, very, some, all.
Using these formal tools any expression and then clause in
Categorial Networks is analyzed as a succession of operators
QTFSCA. Formal tools include categorial Lisp and Prolog.
Lisp consists of objects – entities. An object is composed on
n cells. Numerable cells represent categories. Classical Prolog
variables represent everything. During unification category
everything is substituted by formal variables. To infer a method
of resolution is used, which creates a very big solution space.
In extended Prolog, oriented for semantic networks, objects
are represented by cells with categories. During the unification
only the adequate categories are matched, and again a method
of resolution is used to infer. As a result a huge space of
possible solutions is significantly reduced. To find a solution,
much smaller space is searched, what is much faster.
Relatively simple (yet more and more complicated) examples of ontology driven virtual worlds are existing in some advanced PC games. Ontological drivers may concern separately
particular layers of the virtual reality like communication, bots
behavior, avatar’s personality, properties of the space/nature
and/or all the layers together. Ontological background concerns also the relations between the mentioned layers of the
virtual reality. A lot of different techniques is applied for
practical solution. One is interfacing Clojure with Pogamut
platform, an AI software exercise tried at Artificial Intelligence
Lab. of ISE PW. Clojure is a contemporary dialect of LISP language. It supports functional and multithreaded programming
and runs in JVM and CLR. Clojure: is succinct and factorial
functional language, integrates with and is embedded in Java,
can be integrated with pogamut, uses Java libraries, runs as
Java threads, allows for inline Java code, Clojure functions
are called by Java code, and is available as a clojure.jar Java
library. Pogamut, under constant upgrade, simplifies creation
of bots/agents, controls squads of bots and agents in virtual environments, enables flexible bot navigation, integrates
several Virtual Environments (like UT, UDK-UE, Defcon),
enables simple reinforcement learning and has ontology driven
communication. Squad control includes: coordination, communication, organizational structuring, maneuvers, formations,
planned and emergent behavior by influence vectors and team

476

controller. Learning enables gathering of experience. Ontology
driven communication bases on FIPA-ACL agent communication language. Integration of Clojure-Pogamut bases on
modular horizontal layered architecture (MHL) which consists
of layers and mediators. The mediators can execute multiple
layers. Each layer represents a simple or complex behavior and
has access to agent components like memory, communication,
body, behavior. Abstraction allows for simple Mediator change
according to strategy design pattern. MHL architecture enables
programming of each layer in a different language – Clojure,
Scala, Groovy and others. Clojure code can be called from
Java in three ways: a compiled jar file with class generated by
gen-class; a clj script file clojure.lang.RT or inline by string
object.
Modal Logics. Modal logics have been widely studied
for reasoning about knowledge and belief, usually based on
the monomodel logics S5 and KD45 for reasoning about
knowledge and belief respectively. Both logics have standard
axioms, which means that knowledge and belief both satisfy
positive and negative introspection. The logic S5 also has
axiom T, which means that knowledge is veridical, while
KD45 has also axiom D which mean that belief is consistent.
Kripke model was applied to build an algorithm of multimodal
logics of belief. Kripke model can be presented as a graph,
consisting of vertices – possible worlds, edges – transitions,
relations between possible worlds. The graph is complete and
directed with bidirectional edges. Each edge has a transition
factor which changes with time. If the transition factor is
above a defined threshold, the world pointed by this edge is
accessible, otherwise is not. After every event in the world,
the beliefs of an agent are updated in the Kripke model by
changing the transition factor. Each action taken by other
agents can be seen as positive or negative. Positive actions
increase transition factor, while negative actions decrease the
transition factor. Transition with factors below the threshold
are not updated and are considered inexistent – because
the worlds pointed by them are inaccessible. The model
can be efficiently implemented in a PC in server – client
architecture. The presented approach may facilitate or augment
building models of large and complex systems like global
communications or next generation of the future Internet.
Expanding Formal Languages. The research on expanding
formal languages is carried on in UKSW (CSWU) University,
in a group of prof. K. Świ˛etorzecka. The language is treated as
a family of n-languages which get some epistemic interpretation. The changeability is described consisting in adding new
expressions to the language to which they did not belong just
because an agent did not know them before so he could not
consider them. A formal basis of the proposed description is
the calculus LC. LC is extended by definition of two operators
G+ and G− . In the result the system LCG is obtained, in
which one can compare two types of changes described by
operators C and G. C is an operator of change and it reads: as
it changes, that. LCG calculus and LC logic has all tautologies
of classical sentential logic. The introduced G operators may
also be combined with some temporal notions. LC may be
extended by an operator N, to be read – next it is the case
that. . . LC extended by N is equivalent to temporal calculus

R. S. ROMANIUK

F of Prior which has an interpretation in discrete and linear
structure of time. The system is expected to be formally a basis
for algorithms researching the changeability of languages.
Inconsistency in logical systems: Inconsistency is a term
often associated with paradoxes or antinomies. It is a relationship between two sentences where one sentence is a negation
of the other. Internally inconsistent sentence is a conjunction
of two contradicted sentences or any other sentence which
could be transformed into such a conjunction. The subject of
research is automated proving of inconsistency by application
of IT. IT can be used for proving and checking correctness
of logical proofs. The implementations employ an indirect
proof technique – reduction ad absurdum. An example of
automated theorem proving is Prover9 software environment.
The consistency cannot be proved automatically, in contrast to
inconsistency which can be proved automatically. The proving
bases on the assumption that any syntactically correct formula
is derivable, as well as contradiction is derivable. A solution is
to determine the minimal set of axioms which, when accepted
all at once, bring inconsistency to the theory. Such algorithmic
methods are available and are used for inconsistency checking
and for redundancy checking of set of axioms (P. Orzeszek).
Automated formal concept analysis: Formal concepts analysis (FCA) is subject to normalized approach and research starting with philosophical background and ending in preparing
software environments. This concerns wide range of processes
spanning from social phenomena to extended and ultimately
complicated communication systems. A research on these
subjects are carried out at CSWU (M. Porwolik). Constituents
of thought are analogous to words, which are constituents of
a sentence. They are believed to be combinable. Categorization
is understood as primary tools by which one determines that
something falls into a category. Inference is understood as
follows. When someone applies the concept to some thing (and
makes categorization), someone else can use the concept to
infer facts about that object. Linguistic meaning is understood
as follows. It is the meanings of the words or components of
the meanings. Concepts are expressed when one uses words.
Concepts are not meanings but play a central role in the
epistemology of language. Representations refer to things and
are intermediaries between mind and world. Concepts would
express: its extensions, intentions as functions and properties,
and present referential points of view. An extension is a set
of actual objects that satisfy the concept. An intention is
a function that maps a possible world to the extension of the
concept in that world. Properties are the properties these object
share. FCA is data analysis based on the notion of a concept,
with its components like extension and intention. Data is
represented by a very basic data type and in a formal context.
This trem was introduced by R. Wille in 1984. FCA applies
lattice theory and order theory. The application is in data
and text mining, machine learning, knowledge management,
semantic web, etc. FCA is for analyzing of constituents of
thought, categorization, inference, finding linguistic meaning
and representation. Concepts in FCA are specified by extension and intention and can be concepts of individuals.
Inherence in logical systems: PowerLoom environment is
used for implementation of inherence calculus (M. Wachulski,

COMMUNICATIONS, MULTIMEDIA, ONTOLOGY, PHOTONICS AND INTERNET ENGINEERING 2012

ISE WUT). Inherence calculus s the calculus built on the top
of subsumption relation and inherence relation (relationship)
using formal language. It operates on abstracts. Abstracts
are all beings (concepts and individuals). Concrete abstracts
concern only individuals. PowerLoom technology is a successor of Loom System. It has more expressive representation
language, less arcane syntax, better scalability and portability
and extensibility, it is based on first-order predicate logic (not
description logic), and has pragmatic approach. There, the
usability is more important than theoretical formalism and
expressivity is more important than inferential completeness.
The query engine is used to retrieve asserted and inferable
statements. Prolog style backward inference is enhanced by
recursive subgoal detection, proper handling of negation, hypothetical reasoning, resource bounded depth-first or iterative deepening search, proof tree (justification) recording. It
features forward inference and simple constraint propagation,
equity and inequity reasoning, subsumption reasoning, relation
and instance classification, partial match for reasoning with incomplete information, WhyNot abductive inference for query
diagnostics, and extensible reasoning specialists architecture.
General idea is to put entities and relations in PowerLoom
module and supply the test module with the PowerLoom
results along with test data generator and queries. Power Loom
was shown during tests to be a feasible environment to build
rich information systems that exploit formal ontology to its
maximum.
XIII. C ONCLUSIONS , ACKNOWLEDGMENTS , F UTURE
WILGA Symposium from its beginning in the mid nineties
has produced more than 2500 articles, out of which over 1000
were published in Proc.SPIE. Several hundred of them are
associated with the research activities of the PERG/ELHEP
Research Group at ISE WUT. The Group is an initiator and
major organizer of the WILGA Symposia.
The WILGA 2012 meeting was a fruitful event gathering
young researchers from the fields of photonics and electronics
systems. Photonics and Web Engineering 2012, in its 30th
edition keeps the style of being a wide and friendly meeting
for young researchers, mainly Ph.D. students. The Wilga
Symposium proceedings are good memoirs of the development
of photonics, optoelectronics, metrology and some telecom
technologies in the academic laboratories in this country.
The author would like to acknowledge all the persons
involved in the organization of the conference on ‘Photonics
and Web Engineering’, Wilga 2012, in particular the Program
and Organization Committees. The author also would like to
thank all participants of Wilga Symposium for making the
event again and again a success. This paper was prepared
using the invited and contributed presentations debated during
Wilga 2012. Some fragments of the text were quoted from
these presentations and from session discussions
The 2013 Symposia on Photonics and Internet Engineering
will be held on 26–29 January at WEiTI PW building in
Warsaw and on 28.05–03.06 in Wilga Resort by PW. The
organizers warmly invite young researchers to present their
work. The WILGA Symposium web page is under the address:
http://wilga.ise.pw.edu.pl.

477

R EFERENCES
[1] R. S. Romaniuk and K. T. Poźniak, Eds., Wilga 2002. SPIE 5125;
Photonics Applications in Astronomy, Communications, Industry, and
High-Energy Physics Experiments 2002, 2003, 472 pages, 55 papers.
[2] R. S. Romaniuk, Ed., Wilga 2003. SPIE 5484; Photonics Applications
in Astronomy, Communications, Industry, and High-Energy Physics
Experiments 2003, 2004, 734 pages, 94 papers.
[3] ——, Wilga 2004. SPIE 5775; Photonics Applications in Astronomy,
Communications, Industry, and High-Energy Physics Experiments 2005,
2005, 710 pages, 92 papers.
[4] R. S. Romaniuk, S. Simrock, and V. M. Lutkovski, Eds., Wilga 2005
bis. SPIE 5948, Photonics Applications in Industry and Research 2005,
2005, 864 pages, 89 papers.
[5] R. S. Romaniuk, Ed., Wilga 2005. SPIE 6159; Photonics Applications
in Astronomy, Communications, Industry, and High-Energy Physics
Experiments 2005, 2006, 1244 pages, 172 papers.
[6] ——, Wilga 2006. SPIE 6347; Photonics Applications in Astronomy,
Communications, Industry, and High-Energy Physics Experiments 2006,
2006, 874 pages, 111 papers.
[7] ——, Wilga 2007. SPIE 6937; Photonics Applications in Astronomy,
Communications, Industry, and High-Energy Physics Experiments 2007,
2008, 1274 pages, 152 papers.
[8] R. S. Romaniuk and T. R. Woliński, Eds., Wilga 2008. SPIE 7124;
Photonics Applications in Astronomy, Communications, Industry, and
High-Energy Physics Experiments 2008, 2008, 312 pages, 35 papers.
[9] R. S. Romaniuk and K. S. Kulpa, Eds., Wilga 2009.
SPIE 7502;
Photonics Applications in Astronomy, Communications, Industry, and
High-Energy Physics Experiments 2009, 2009, 786 pages, 100 papers.
[10] R. S. Romaniuk, Ed., Wilga 2010. SPIE 7745; Photonics Applications
in Astronomy, Communications, Industry, and High-Energy Physics
Experiments 2010, 2010, 650 pages, 73 papers.
[11] ——, Wilga 2011. SPIE 8008; Photonics Applications in Astronomy,
Communications, Industry, and High-Energy Physics Experiments 2011,
2011, 500 pages, 70 papers.
[12] ——, Wilga 2012. SPIE 8454; Photonics Applications in Astronomy,
Communication, Industry,andHighEnergy Physics Experiments 2012,
2012, 600 pages, 90 papers.
[13] J. Dorosz and R. Romaniuk, “The role of regional developments in
optical fiber technology and photonics,” Proceedings of SPIE, vol. 5028,
pp. xi–xii, 2003.
[14] R. Romaniuk and K. Pozniak, “Foreword: Photonics and electronics for
astronomy and high energy physics experiments in Poland,” Proceedings
of SPIE, vol. 5125, pp. xiii–xxxiv, 2002.
[15] W. Woliński, Z. Jankiewicz, and R. Romaniuk, “Proceedings of SPIE
– The International Society for Optical Engineering: Introduction,”
Proceedings of SPIE, vol. 5230, pp. ix–x, 2003.
[16] R. Romaniuk, “Proceedings of SPIE – The International Society for
Optical Engineering: Introduction,” Proceedings of SPIE, vol. 5775, pp.
xxi–xxvii, 2005.
[17] ——, “Proceedings of SPIE – The International Society for Optical
Engineering: Introduction,” Proceedings of SPIE, vol. 5848, pp. xvii–
xxi, 2005.
[18] ——, “Proceedings of SPIE – The International Society for Optical
Engineering: Introduction,” Proceedings of SPIE, vol. 6347, pp. xxix–
xxxii, 2006.
[19] W. Wolinski, Z. Jankiewicz, and R. Romaniuk, “Proceedings of SPIE
– The International Society for Optical Engineering: Introduction,”
Proceedings of SPIE, vol. 6598, pp. ix–xii, 2007.
[20] R. Romaniuk, “Proceedings of SPIE – The International Society for
Optical Engineering: Introduction,” Proceedings of SPIE, vol. 6937, pp.
xxix–xli, 2008.
[21] W. Wolinski, Z. Jankiewicz, and R. Romaniuk, “Proceedings of SPIE
– The International Society for Optical Engineering: Introduction,”
Proceedings of SPIE, vol. 5229, pp. xi–xii, 2003.
[22] J. Dorosz, R. Romaniuk, and T. Wolinski, “Eleventh conference on
optical fibers and their applications,” Proceedings of SPIE, vol. 7120,
pp. xiii–xv, 2008.
[23] R. Romaniuk and K. Kulpa, “Photonics applications in Astronomy,
Communications Industry and High-Energy Physics Experiments 2009:
Introduction,” Proceedings of SPIE, vol. 7502, pp. xxiii–xxiv, 2009, art.
no. 750201.
[24] R. Romaniuk, “Photonics and Web Engineering in Poland, WILGA
2009,” Proceedings of SPIE, vol. 7502, 2009, art. no. 750202.
[25] ——, “WILGA Symposium on photonics applications,” Photonics Letters of Poland, vol. 1, no. 2, pp. 46–48, 2009.

478

[26] ——, “Wilga 2010, Photonics Applications,” Proceedings of SPIE, vol.
7745, pp. xiii–xviii, 2010.
[27] ——, “Wilga 2011, Photonics Applications,” Proceedings of SPIE, vol.
8008, pp. xii–xviii, 2011.
[28] ——, “Wilga 2012, Photonics Applications,” Proceedings of SPIE, vol.
8454, pp. vii–x, 2012.
[29] ——, “Wilga photonics and web engineering 2010,” Photonics Letters
of Poland, vol. 2, no. 2, pp. 55–57, 2010.
[30] ——, “Petabit photonic internet,” Photonics Letters of Poland, vol. 3,
no. 2, pp. 91–93, 2011.
[31] ——, “‘the Photonics Letter of Poland’ A new peer-reviewed internet
publication of the Photonics Society of Poland,” Photonics Letters of
Poland, vol. 1, no. 1, pp. 1–3, 2009.
[32] ——, “Wilga symposium on photonics applications,” Photonics Letters
of Poland, vol. 1, no. 2, pp. 46–48, 2009.
[33] ——, “POLFEL – A free electron laser in Poland,” Photonics Letters
of Poland, vol. 1, no. 3, pp. 103–105, 2009.
[34] R. S. Romaniuk, “Polish Debut,” Photonics Spectra, vol. 26, no. 5, pp.
10–12, May 1992.

R. S. ROMANIUK

[35] ——, “The photonics scene in the new Poland,” Photonics Spectra,
vol. 26, no. 4, pp. 64–65, April 1992.
[36] J. Modelski and R. Romaniuk, “Electronics and telecommunications
in Poland, issues and perspectives, Part I: Society and education,”
Proceedings of SPIE, vol. 7745, 2010, art. no. 774504.
[37] ——, “Electronics and telecommunications in Poland, issues and perspectives, Part II: Science, research, development, higher education,”
Proceedings of SPIE, vol. 7745, 2010, art. no. 774505.
[38] ——, “Electronics and telecommunications in Poland, issues and perspectives, Part III: Innovativeness, applications, economy, development
scenarios, politics,” Proceedings of SPIE, vol. 7745, 2010, art. no.
774506.
[39] S. Chatrchyan, G. Asova, K. Pozniak, R. Romaniuk, and W. Zabolotny
et al., “The CMS Collaboration, Observation of a new boson at a mass
of 125 GeV with the CMS experiment at the LHC,” Physics Letters B,
vol. 716, no. 1, pp. 30–61, 2012.

