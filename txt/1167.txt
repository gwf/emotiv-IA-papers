The Emotiv mind: Investigating the accuracy of the Emotiv
EPOC in identifying emotions and its use in an Intelligent
Tutoring System

HONOURS REPORT
by
Tegan Harrison

Supervised by Prof. Tanja Mitrovic

Department of Computer Science and Software Engineering
University of Canterbury
2013

1

Abstract
Emotion is one of the key elements involved in learning and education; it affects our decision
making, our communication and our ability to learn. Therefore in this project we proposed
the use of an electroencephalography (EEG) device called the Emotiv EPOC to detect a
user’s emotions via brainwaves in an Intelligent Tutoring System (ITS). We aimed to test the
accuracy and applicability of this device in an ITS called EER-Tutor. First we aimed to test
the EPOC’s accuracy in detecting emotions by comparing EPOC’s emotional values with
participants’ self-reported scores. Emotions were induced using the International Affective
Picture System (IAPS). Due to a lack of reliable data this study was then improved upon and
re-conducted. The improved study still resulted in no significant findings as a relationship
between self-report scores and EPOC emotional values could not be found. Therefore a
different approach was adopted to test the accuracy and applicability of the EPOC. In our
third study we tested the EPOC in EER-Tutor using a think-aloud protocol. The utterances of
participants were compared to the emotional values EPOC produced. These utterances were
defined by context of statement, action taken and timing and placed into categories. No
significant difference was found between categories. The variances of the readings for
various categories were also quite large, showing that individuals emotional responses can
vary greatly and a single emotional model cannot be developed for a group of people. Even
though no significant findings were found, certain limitations of the EPOC allowed us
conclude its unsuitability for use in ITSs.

2

Table of Contents
Abstract
Acknowledgements
Chapter 1:
1.1
1.2
1.3

Introduction

Motivation
Goals
Report Structure

Chapter 2: Background and Related Research
2.1
Emotion
2.2
Intelligent Tutoring Systems
2.3
Affective Computing
2.4
Electroencephalography (EEG)
2.5
Affective Computing and EEG
2.6
Emotiv EPOC
2.7
International Affective Picture System (IAPS)
2.8
Tobii Eye tracker
Chapter 3: Study 1: Evaluating the accuracy of the EPOC
3.1
Research Design
3.2
Method
3.3
Results and Analysis
3.4
Discussion
Chapter 4: Study 2: Evaluating the accuracy of the EPOC (improved)
4.1
Research Design
4.2
Method
4.3
Results and Analysis
4.4
Discussion
Chapter 5: Study 3: Implementation in EER-Tutor
5.1
Research Design
5.2
Method
5.3
Categorizing Data
5.4
Results
5.4
Discussion
Chapter 6: Conclusions
6.1
Discussion
6.2
Conclusions and Future Work
References

3

Appendix A:

Emotion Intensity Questionnaire

Appendix B:

Demographics Questionnaire

Appendix C:

Ethics Application for study 1

Appendix D:

Ethics Approval for study 1

Appendix E:

Ethics Application for study 3

Appendix F:

Ethics Approval for study 3

Appendix G:

Project Consent Form for study 1 & 2

Appendix H:

Project Description Form for study 1 & 2

Appendix I:

Project Consent Form for study 3

Appendix J:

Project Description Form for study 3

4

Acknowledgements
Thank you to Prof. Tanja Mitrovic and Moffat Matthews for their help and support
during this project. I also extend my thanks to the Computer Science Department
and ICTG, who have also been a great help.

5

Chapter1: Introduction
Emotions are one of the key factors for developing advanced educational systems that are
adaptive to a user’s needs. Emotions are essential in many areas of learning such as creative
thinking, inspiration, concentration and motivation [1, 2]. A large portion of the currently
available educational systems do not consider the affects a user’s emotions could have on
their learning. More recently new forms of technology have become available allowing an
emotional model to be incorporated into a system.
Electroencephalography (EEG) provides a way in which we can access and record neural
activity that can be used to detect emotional states. The use of electrical activity of the brain
as a form of emotion detection is becoming increasingly popular as it requires no physical
effort from the user. The use of an EEG also allows the system to build an emotional model
in real time without distracting the user from their current task.
EEG devices are usually very expensive and hard to obtain as they are mostly used for
medical research however, cheaper consumer-grade EEG devices have become available
such as the Emotiv EPOC. The Emotiv EPOC is a lightweight wireless headset that has a
significantly shorter setup time and allows for portability unlike original EEG devices.
The EPOC was not originally intended for research however is becoming increasingly
popular due its flexibility and wide range of suites which it offers. The EPOC comes with a
series of software suites that can detect user’s emotions, facial expressions and control
objects in a virtual world. There have been some applications that have used the EPOC and
its emotional detection suites in an ITS. There have been no studies investigating the
accuracy of the emotional channels of the EPOC in comparison to a user’s self-reported
emotional state. This report will investigate the accuracy and applicability of the Emotiv
EPOC in an ITS.

1.1

Motivation

The Intelligent Computer Tutoring Group (ICTG) is focused on improving students learning
experiences and enhancing their performance in many different fields of education. They
have developed a variety of intelligent tutoring systems which aim to achieve this goal by
providing a supportive learning environment. The learning effectiveness of a system is not
only dependant on the environment but also on the emotional state of the user. Therefore it
has been proposed that intelligent tutoring systems should utilize affective computing in
which systems can detect and recognize emotional information. This information is then used
to enhance learning environments by avoiding negative emotions such as boredom and
disengagement. Affective computing can be implemented in many different ways however
the use of EEG in intelligent tutoring systems looks promising.

6

The Emotiv EPOC is a lightweight inexpensive EEG device that allows greater flexibility
than traditional EEG devices. There has been some research on EPOC’s capability to detect
emotions; however, there has been no formative evaluation on how well the identified
emotions of EPOC correspond to self-reported states. We are therefore interested in
determining whether or not there is any relationship between the emotional states identified
by the EPOC and the emotional states reported by the user. We also interested as to whether
or not the EPOC is applicable and practical for use in an ITS such as EER-tutor to measure
emotional states with the future aim of making an affective model of the student.

1.2

Goals

Our aim was to investigate the EPOC’s accuracy in detecting the emotions of boredom,
frustration, meditation, excitement and engagement. We aimed to investigate which of these
emotions it most accurately measured in order to test its applicability for use in an ITS. The
research questions were as follows:





1.3

How accurately does the Emotiv EPOC detect emotions?
Which emotions does EPOC detect the best?
How difficult is it to use the EPOC when measuring emotions?
Is the EPOC useful/practical for measuring emotions in an ITS?

Report Layout

In the next sections we expand upon how our goals were accomplished and give a more
detailed explanation of the motivations behind this study. In Chapter two we provide a
comprehensive review of previous literature relevant to this study. In Chapter three, four and
five we describe the implementation and results of the studies conducted in order to achieve
our goals. Chapter six provides a discussion of results and overall conclusions of the project.

7

Chapter 2: Background and Related Work

In this Chapter we provide a summary of background research relevant to our project. We
define emotion and its importance computing and the techniques used to incorporate it in an
educational system. We also provide a detailed explanation on EEG, the Emotiv EPOC, IAPS
and the Tobii eye tracking system.

2.1

Emotions

Emotions influence the decisions people make, how effectively they learn and how they
communicate with others [3]. Psychologists define emotion as “a disorganized response,
largely visceral, resulting from a lack of effective adjustment” [4]. Three main theories are
defined for emotion and these are grouped into the following categories of physiological,
neurological and cognitive. These three categories differ based on the source of the emotions.
For example in the physiological approach the body is responsible for emotion whereas the
cognitive approach states that mental activates are accountable. In this study we are primarily
concerned with the neurological approach which states that electrical signals play an essential
role in forming emotions. There are also two theoretical approaches to studying emotion
consisting of the categorical and dimensional approaches. The categorical approach relies on
a discrete lexicon of emotional words to convey specific meaning. The dimensional approach
on the other hand assumes humans are in a constant affective state that can be represented by
the two underlying dimensions of valence and arousal [5]. Valence is the amount of positivity
or negativity which a person feels towards something. Arousal on the other hand is regarded
as what receives a person’s attention [6]. Emotions are extremely important in computing as
they can influence learning effectiveness. In this study we examine the types of situations
which give rise to emotions in a learning environment.

2.2

Intelligent Tutoring Systems

Intelligent tutoring systems (ITS) are designed to support student learning by providing
effective feedback and adapting to students individual needs [7]. ITSs have been found to
benefit users by significantly increasing learning via the use of cognitive models [8].
However many are yet to incorporate an affective model which has been found to have a
great impact on learning [8]. The use of an affective model in ITS will allow the system to
identify emotions and adjust the system accordingly avoiding emotions associated with a
decrease in learning such as boredom. Not all emotions are relevant in ITS; specific emotions
that are associated with learning are called “academic emotions” [10]. Pekrun et al. [10]
developed the Academic Emotions Questionnaire (AEQ) in order to study academic
emotions. They found in their studies that a rich diversity of emotions was observed in an
academic setting and positive emotions were described just as frequently as negative

8

emotions. Examples of the “academic emotions” included in AEQ were emotions such as
enjoyment, anger, boredom and anxiety.

2.3

Affective Computing

The importance of the study of emotions has prompted the development of many new
research areas such as affective computing. Affective computing is a person's interaction with
technology that relates to and arises from or deliberately influences emotions [4]. This
research is essential as people’s emotions affect their learning capabilities and their
interactional experiences with a system. If we can understand a user’s emotional state when
using a system then perhaps we can improve the system and enhance greater learning
avoiding negative emotions which have been found to decrease learning.
Considerable research has already been conducted demonstrating the importance and
effectiveness of affective computing. Affective systems have already been implemented in
some ITS [6]. Affective computing is a crucial element in using ITSs because learning new
concepts and tasks can be very frustrating thus impairing peoples’ performance because of
gaps in their knowledge. ITSs’ aim is to aid an individual to learn by providing customized
feedback and help without the need of a human tutor. Previous research has been conducted
in this area into systems that offer active support in helping users manage their emotions. An
ITS called EER-Tutor acknowledged its user’s emotions and responded with an avatar tutor
that appeared to be empathetic when a user made a mistake or congratulatory when he or she
gave the correct solution [11]. In this study valence and situational cues were used to
determine the level of affective response that was required. This was found to be an effective
technique and generally liked by users. Other research has been conducted into a system that
detects the affective state of users through the use of conversational cues, posture and facial
expressions [6]. These techniques were used in a tutor called AutoTutor [12].

2.4

Electroencephalography (EEG)

Different approaches to measuring emotions have been used in the past such as self-reporting,
annotation, webcams and physiological sensors [13] [14] [15] cited in [16]. However these
methods have been prone to human error, biases or have introduced unnecessary noise.
Therefore new techniques need to be investigated which provide greater accuracy in detecting
human emotions. In this project we propose of use EEG for measuring and detecting
emotions. EEG has been used in research and medical fields for around 80 years [17]. Until
recently EEG has been difficult to obtain and extremely costly. EEG works by detecting the
electrical signals released by the brain. In the human brain, each individual neuron
communicates with others by sending tiny electrochemical signals. When millions of these
neurons are activated the signal becomes strong enough that EEG devices can detect it [18]
[19] cited in [20]. EEG detects the electrical signals through a series of electrodes placed on
the head. These electrodes use the 10-20 system (see Figure 1). This system dictates that the
actual distances between adjacent electrodes are either 10 % or 20% of the total front-back or
right-left distance of the skull [17]. EEG detects four different types of brainwaves these

9

consist of: delta (0.1-3.5 Hz), theta (4-7.5 Hz), beta (14-30 Hz) and gamma (>30 Hz) waves.
Each of the waves represents certain states of the brain. Delta waves are used during sleep
whereas theta waves have been associated with meditation and arousal. Beta waves are
associated with concentration, high alertness and anxiety and gamma waves are associated
with focus and intellectual acuity [17].

Figure 1: The 10-20 System [21]
EEG devices are usually used in clinical research for stroke rehabilitation or for investigating
epilepsy. The use of EEG devices to detect the electrical activity of the brain is becoming
increasingly popular in other areas of research as it requires no physical effort from the user.
EEG devices can be time consuming to set up correctly as a large number of nodes need to be
precisely placed on the users head. These nodes are kept in place by a variety of gels and
saline solutions. EEG can be very costly and difficult to obtain however new inexpensive
EEG’s have been released that are easier to use and faster to set up such as the Emotiv EPOC.
EEG does have some limitations as the EEG signals can be very sensitive to noise and pick
up unwanted artefacts [22]. This noise introduced can be caused by many factors such as
facial movements of blinking or mouth movement [23].

2.5

Affective Computing and EEG

Research conducted by Inventado and colleagues [8] used EEG to detect a user’s emotional
state in order to identify the appropriate time to intervene with learning. An affective model
was also applied in order to measure feedback. Robison et al. [24] found certain emotions are
affected by feedback more than others and the time at which feedback is given is crucial in
avoiding negative states.
A large majority of researchers use a multimodal approach to studying a user’s affective
state. For example EEG data has been used along with other forms of detecting behaviours

10

such as mouse click behaviours. In a study conducted by Azcarraga et al. [25] they found that
brainwave data collected from an EEG had low accuracy of predicting emotions. However
when combined with mouse features this accuracy increased to 92 %. Another example of a
multimodal approach was a system called ABE [26]. ABE used a combination of the
EMOTIV EPOC, skin conductance, MIT mind reader system and pressure sensors to detect
emotions. This system was created to run in parallel to an already existing ITS with the aim
of providing a more empathic system [26] reducing the likelihood of negative emotions such
as frustration occurring. However this system was not implemented in real time as it only
analysed data after the participant had finished using the system.
The use of EEG to detect emotion has also been used in many other fields such as astronomy.
NASA developed an EEG-engagement index for switching between manual and auto-pilot
with the aim of improving performance. In their study they combined the use of physiological
sensors and EEG to record emotions and engagement [27]. They found that these factors
impact response time variability. This type of system could be applied to an ITS where the
system recognizes when it is appropriate to push users harder based on their emotional state
and engagement levels.

2.6 The Emotiv EPOC
The Emotiv EPOC (see Figure 2) was originally developed for gaming with a variety of
games and virtual environments available through the Emotiv app store. Gamers use the
EPOC device to control various aspects of a gaming environment. Over the past few years the
EPOC has been increasingly used in various areas of research. The EPOC can detect a user’s
thoughts, feelings and expressions by capturing the brainwaves produced by the wearer. The
EPOC has 14 sensors and 2 references following the 10-20 system (see Figure 1) [29]. The
signals received from the headset are transferred to a computer through a wireless USB
dongle. The electrodes on the EPOC are required to be slightly moistened with a saline
solution which also acts as a disinfectant, provided in the EPOC kitset [29]. These properties
along with it being lightweight make the EPOC extremely portable and easy to use (see
Figure 2).

Figure 2: The Emotiv EPOC headset [28]

11

Data is constantly collected by the EPOC. Upon collection the data is then converted into
digital data which is then wirelessly transmitted to the USB dongle. A post-processing
software called the Emotiv EmoEngine exposes the Emotiv detection results to the Emotiv
API and the Emotiv control panel [29]. The control panel displays the battery level, signal
quality and system status. It also provides a Graphical User Interface (GUI) to the
EmoEngine through the Emotiv API and demonstrates EmoEngine’s capabilities. The
EmoEngine provides a selection of built in detection suites including the Expressiv, Affectiv
and Cognitiv suites. The control panel has a tab for each of these suites with visual
representations of the real-time data. Other tools such as the EmoKey and EmoComposer are
also available. The Emokey is a tool which translates EmoStates given by the EmoEngine
into signals that emulate input devices such as a keyboard. The EmoComposer is an
EmoEngine emulator designed to speed-up the development of Emotiv-compatible software
applications. These two tools are not used in this study. The Emotiv SDK also allows
researchers to develop their own systems collecting data straight from the EPOC integrating
the SDK with both C++ and Java [29].
The Cognitive suite allows users to move objects in a virtual environment using just their
thoughts [29]. The Cognitive suite, unlike the other suites, requires extensive training in order
for it to be used effectively. The Expressiv suite on the other hand allows for the detection of
facial expressions such as blinking, winking, looking left or looking right. These suites are
not used in this study; however multiple suites can be used in parallel if desired.
This study is primarily concerned with the Affectiv suite which records real time changes in
emotion experienced by the user. Each person who wears the EPOC has their own user
profile and their data is rescaled over time in order to improve the EPOC’s accuracy [29].
How this rescaling is achieved by EPOC is unknown; it is necessary as one person’s high
levels of excitement may be another person’s low levels of excitement. Therefore
normalizing of data is used to account for variation. The EPOC can detect five different types
of emotions (channels): instantaneous excitement, long term excitement, meditation,
engagement and frustration. The EPOC scores each channel on a scale from zero to one
where a higher channel score corresponds to a greater intensity of the emotion. Instantaneous
excitement is defined as a “feeling or awareness of physiological arousal in a positive sense”
[29]. Engagement is defined as “the alertness experienced by a person and the conscious
direction of attention towards a task-relevant stimulus” [29]. The more engaged a person is
the higher their channel score is [29]. On the opposite end of the scale a lower score can be
attributed to feelings of boredom. Meditation represents how calm a person is: the higher the
score the calmer they are. A higher score on frustration channel represents a higher level of
frustration. The user does not require any training in order for the EPOC to be able to
successfully detect emotion. Emotiv has not publicly released their classification algorithms;
therefore it is unknown as to how these emotions are detected.
The EPOC data which we obtain in this study is collected from the Affectiv suite of the
EmoEngine on an emotional intensity scale from zero to one from each of the four channels

12

[29]. The emotion model of EPOC was built using data collected from over one hundred
volunteers. This was achieved by having volunteers play video games, watch movies and
view photos in order to invoke emotions. While participating, videos of the volunteers were
taken, along with data from physiological sensors and EEG data using the Emotiv EPOC.
After this volunteers filled out a questionnaire about their experience and psychologists
labelled the four emotional channels of: short term excitement, long term excitement,
frustration and meditation [9].
Previous studies have found that there are some limitations to the EPOC. For example, it has
been found that the device set up time can be extremely long especially if the participant has
long or thick hair. Some participants also noted discomfort when wearing the device [23]. It
was also found by Mampusti et al. [23] that the EPOC can be very sensitive to noise as noise
can be introduced by a weak signal or through the implementation of data collection from the
EPOC. Noise may also be introduced through the electronic amplifier, power line
interference or external interference [30] cited in [23].
NeuroSky is another inexpensive lightweight EEG that is available for public purchase.
NeuroSky has a slightly larger frequency range than EPOC with NeuroSky measuring 0.550HZ and EPOC with 0.2-43HZ [17]. NeuroSky measures attention, meditation, anxiety and
drowsiness. EPOC on the other hand measures long term excitement, short time excitement,
frustration, engagement/boredom and meditation. EPOC’s set of emotional detection
channels more accurately matches the academic emotions prevalent in learning, therefore is
more appropriate for academic research. EPOC also extends on the NeuroSky’s
functionalities by allowing facial expressions whereas NeuroSky only identifies blinking.
EPOC also follows the 10-20 system when NeuroSky does not [17], thus making it the more
superior inexpensive EEG for academic research.

2.7

International Affective Picture System (IAPS)

Many techniques for eliciting emotions have been used in the past, with videos and images
being two of the most commonly used ones. International Affective Picture System (IAPS)
[17] is a very important system as it acts as emotional stimuli for experimental investigation
of emotion and attention. IAPS is a set of colour images that includes many different objects
and situations which have been found to invoke an emotional response such as cupcakes,
snakes or riding a rollercoaster. The official evaluation of IAPS was conducted using a nonverbal picture assessment technique called Self-Assessment Manikin (SAM). In this study
images from IAPS were evaluated using SAM on a one to nine scale: one represents low
intensity of emotion and 9 represents a high intensity of emotion. SAM was then used to
measure the pleasure, arousal and dominance of each image from IAPS in association to a
person’s affective reaction [32]. A study [33] grouped IAPS images into categories which
were associated with specific emotions such as excitement, boredom, anger etc. For example,
images in relation to adrenalin correlated with the emotional category of excitement.

13

Previous studies have been conducted investigating IAPS and EEG brainwaves. One of these
studies was conducted by Heraz and Frasson [34], in which they used the IAPS along with an
EEG called Pendant in order to assess the three main dimensions in IAPS and its correlation
to brainwaves. They found that there was a high correlation between the three dimensions
measured by IAPS and the brainwaves detected by Pendant EEG. Delta brainwaves showed
negative correlation for pleasure and dominance. The theta and beta brainwaves showed
negative correlation with arousal and alpha had a significant negative correlation with arousal
and dominance [34]. To the best of our knowledge, there have been no previous studies
conducted directly mapping the EPOC channels to IAPS images.

2.8

Tobbi Eye tracker

The Tobii eye tracker uses infrared sensors and cameras built into a 17 inch computer
monitor [35]. Tobii has the ability to capture the screen, video the user while also tracking
their eye movements. Before using Tobii users have to complete a calibration test ensuring
there eye level is at the correct height. This calibration test involves tracking a circle that
appears on screen with their eyes [35]. D’Mello and colleagues [35] conducted a study using
Tobii in which they aimed to promote engagement with the aid of the Tobii. They achieved
this by having the ITS dynamically respond to student’s disengagement or boredom and
attempt to reengage the student. This was conducted using Tobii’s eye tracking technology to
determine when a participant looked away from the screen. Tobii is a useful tool for allowing
a multimodal approach to studying emotion by providing eye tracking along with video data.

14

Chapter 3: Study One: Accuracy of the EPOC

This Chapter describes the first study we conducted in order to determine the accuracy of the
EPOC. In this study ten participants viewed a series of images from IAPS in order for their
self-reported emotional scores to be compared to their corresponding EPOC values.
However, due to headset malfunctions only three participant’s data was obtained.

3.1 Study Design
Participants viewed images from IAPS and their emotions were detected using the Affectiv
suite provided by Emotiv. We chose to use IAPS to invoke emotions due to its proven
accuracy and validity in inducing emotional states. Forty eight carefully selected images were
chosen from IAPS to invoke the emotions of frustration, meditation, excitement and
engagement/boredom. Bradley and colleagues [30] classified the images of IAPS into
categories of emotions and we used these categories to select appropriate the images. For
example excitement photos were related to categories of extreme sport and food, whereas
categories such as anger related to images of pollution. Each of the different categories
corresponded to one of the emotional channels provided by EPOC (see Figure 3).
The photos were split into 4 blocks. Each of these blocks contained two images from each of
the six categories. The blocks are separated by a fifteen second rest period. We decided to
have these rest periods in-between blocks in order to inhibit fatigue.
Azcarraga et al. [23] suggested that the EPOC needs a small amount of time to normalize a
user’s data . In their study they told the participants to relax for a period of three minutes in
order for the EEG to create a baseline for EEG data. Therefore in this study participants are
instructed to play an online game for 5 minutes before starting the study. Our participants
viewed each of the images for 5 seconds. After viewing the image, the participant specified
their current emotional state in regards to each of the six categories: engagement, excitement,
irritated, anger, calm and boredom. The questionnaire consisted of a 5 point scale on the
intensity of emotion.
We decided to prompt participants on screen immediately after each of the photos were
shown in order to decrease the delay between viewing the photo and self-reporting. A timer
of fifteen seconds was also shown on screen in order to force users to self-report as instantly
as possible. This 15 second time frame was decided based on the amount of time which was
used during the SAM assessment of IAPS.

15

EPOC Channel

Emotional Category

Engagement

Engagement

Excitement

Excitement

Frustration

Irritated

Frustration

Anger

Meditation

Calm

Boredom

Boredom

Figure 3: EPOC channel to emotional category mapping.

3.2

Method

In this study 10 participants were recruited. All participants were healthy and none had any
previous experience with EEG or IAPS. Participation in this study was rewarded with a $20
Reboot voucher.
Participants were sat comfortably in front of the Tobii computer screen. The experimenter sat
to the side watching the participant’s actions and monitoring the status of the headset from
another screen. The EPOC headset was fitted with moist sensor nodes and connected up to
the Tobii computer.
The Emotiv control panel (see Figure 4) was used to ensure that good sensor contact was
established. The experimenter constantly monitored the control panel on another computer
during the experiment to ensure good signal quality was maintained. To control the
experiment a program was written in Java that displayed the images and questionnaire and
recorded the user’s responses. Another program was also written to collect data from the
EPOC. This program used the Emotiv SDK to integrate a Java application with the
EmoEngine in order to read values directly from the EPOC headset. The program constantly
polls for new states, and once a new state is found it then records the emotional values
associated with that state. The rate at which polling is conducted varies greatly as it can have
a sampling rate of multiple polls per second or one every few seconds.

16

Figure 4: EPOC Control Panel [26]
Participants first filled out an emotional intensity questionnaire (Appendix A) and
demographic questionnaire (Appendix B). Each of the sensor nodes are moistened with a
saline solution and then clicked into place on the headset. Once the headset was ready, it is
was then placed upon a user’s head. The control panel is then used to ensure that each of the
sensor nodes have a good signal. Obtaining a good signal can be quite difficult and very time
consuming if a participant has long or thick hair. Once all the sensor nodes had a good signal,
the participants then played an online game of Tetris for 5 minutes while the EPOC
normalizes their data. Participants then completed a Tobii calibration test. Calibration
consisted of participants following a red ball on screen with their eyes ensuring that their eyes
can be accurately tracked. Once calibration is completed participants then start the
experiment. Participants first viewed two trial images and filled out the questionnaire
allowing them to get familiar with the procedure of the study. Upon completion of the trials
participants were given 15 seconds to rest, after which the experiment started. Participants
viewed the first block of images, consisting of 12 images viewed for 5 seconds each. After
viewing an image, the participant then had 15 seconds to answer a six level self-report
questionnaire regarding their emotional state. Upon completion of this questionnaire, they
viewed the next photograph and so forth. Once the first block is complete participants then
received another 15 second rest period. After this rest period finished they then completed the
second block and so forth. There were 4 blocks to complete and this took participants around
30 minutes.

3.3 Results and Analysis
In this study only three participants data was used to analyse the accuracy of the EPOC. This
lack of data was caused by headset failure as two of the sensors failed to receive any signal
after the third participant. The experiment continued as normal however the data obtained

17

from participants after the headset failure was deemed invalid and unreliable as the processes
involved in collecting the emotional data from the headset was unknown. For all images
viewed by participants EPOC values were obtained. There were no significant findings of this
study and the results remain inconclusive. However there were two significant correlations:
of self-reported anger with EPOC meditation and self-reported boredom with EPOC
meditation (Table 1). A larger data sample needs to be obtained in order to justify any
significant findings as these were the results of only three participants.

Table 1: EPOC values versus Self-Reported (SR) Scores
EPOC
Excitement
SR
Engagement
SR
Excitement

SR Irritated

SR Anger

SR Bored

SR
Meditation

Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N

EPOC
EPOC
EPOC
EPOC
Engagement Frustration Meditation Boredom

0.168

0.133

0.226

-0.923

0.416

0.892
3

0.915
3

0.855
3

0.251
3

0.727
3

0.168

0.133

0.226

-0.923

0.416

0.892
3

0.915
3

0.855
3

0.251
3

0.727
3

0.805

0.783

0.838

-0.932

0.930

0.405
3

0.428
3

0.367
3

0.26
3

0.239
3

0.482

0.45

0.533

-0.998

0.691

0.68
3

0.703
3

0.642
3

*0.039
3

0.514
3

0.461

0.429

0.513

-0.996

0.674

0.695
3

0.718
3

0.657
3

*0.054
3

0.529
3

0.352

0.318

0.406

-0.979

0.581

0.771
3

0.794
3

0.734
3

0.130
3

0.606
3

* Correlation is significant at the 0.05 level (2-tailed)

We also compared the EPOC and self-reported scores for each of the categories to the image
categories valence and arousal scores depicted by IAPS. This comparison produced some
interesting findings (Table 2). It was found that calm photos valence values had a significant
correlation with the EPOC scores of boredom (p <0.05). It was also found that anger photos
arousal had a strong positive correlation with EPOC frustration values. Suggesting that as

18

arousal levels increased in images proven to induce anger, EPOC frustration values also
increased.
Table 2: Other Significant Findings

Category One

Category Two

Pearson’s Correlation

P value

N

Calm Valence
Anger Arousal
Engagement Arousal
EPOC Excitement
EPOC Excitement

EPOC Boredom
EPOC Frustration
EPOC Meditation
EPOC Frustration
EPOC Engagement

-0.998
0.999
-0.999
0.988
0.99

0.035
0.025
0.031
0.038
0.023

3
3
3
3
3

We found EPOC meditation and engagement arousal to have a strong correlation (Table 2).
This is an interesting finding, as meditation and engagement are quite different emotions.
Interestingly we also found in this study that two of the EPOC channels have a strong
positive correlation (Table 2). We found that excitement values from the EPOC had a strong
positive correlation with the engagement values. This is an interesting finding as it suggests
that as participants’ excitement increased their engagement also increased. This is to be
expected as excitement and engagement often occur in parallel. However, another positive
relationship was found between EPOC frustration values and EPOC excitement values. This
relationship also suggests that perhaps frustration is strongly linked with arousal, as it would
seem unlikely that a person is excited and frustrated at the same time.

3.4

Discussion

The aim of this study was to determine if the EPOC headset accurately measured
participants’ emotions. This was achieved by participants viewing a series of emotionally
invoking images from IAPS and self-reporting their own emotional responses while wearing
the EPOC. The self-report scores were then compared to the values obtained from the EPOC.
Although there were some significant correlations, the overall results of the study were
deemed inconclusive due to the small size of the data set. Therefore due to a lack of reliable
data, the results from this initial study were inconclusive and no findings could be reached.
The relationship between EPOC excitement and EPOC engagement was found to have a high
correlation; this was expected as both involve high levels of arousal. However, the
relationship between the excitement and frustration channels is an unexpected result. Given
that anger arousal was also found to have a relationship with frustration this suggests that
perhaps the frustration channel is related to arousal and not frustration. However these are
only preliminary results and further testing needs to be conducted to justify these findings.

19

We did notice some limitations during this study that could be improved upon in order to
potentially achieve significant results. First, we would need to obtain more participants in
order to get a more representative sample and acquire another headset which is fully
functioning. Second, we need to adjust our emotional scale to account for more variability in
emotions as 5 point does not have enough range. Therefore we propose to use a similar scale
to one that has already been used in studies such as the 9-point scale used in SAM. Another
improvement to be made would be to have fewer images, as participants mentioned that
wearing the EPOC headset is uncomfortable for an extended period of time. This could affect
their emotional state in a negative way therefore fewer images will be used to decrease the
overall time which participants wears the headset. In this study the initial setup for the EPOC
seemed to be very time consuming in regards to connecting the headset to the computer. In
the second study this will be improved upon by using another computer which is closer to the
dongle and has faster USB ports hopefully decreasing setup time.

20

Chapter 4: Study Two: Accuracy of the EPOC (improved)

This Chapter describes the second study we conducted in order to determine the accuracy of
the EPOC. This study improved on the limitations of the first study as fewer images were
used, a larger sample size was obtained and a larger scale was constructed. In this study seven
participants viewed a series of images from IAPS in order for their self-reported emotional
scores to be compared to their corresponding EPOC values.

4.1

Study Design

Upon receiving inconclusive results due to a lack of reliable data, some improvable aspects of
the previous study were noted. It was decided that in a second study new headsets needed to
be purchased in order obtain reliable and accurate results. These headsets were then used in
this study and in the following study. There is no significant difference between the headset
previously used and the one currently obtained. The scale which we previously used was up
upgraded to a nine point scale. This allowed for a more diverse range of emotions and was
based on the IAPS SAM scale. Fewer images were also used in this study. During the
previous study some images were found to invoke more emotional responses than others,
with some images having no effect on any participants at all. Therefore in this second study
only images which held the strongest emotional response were used. In this study two sets of
eight images and one set of nine were used and this took around 20 minutes to complete.

4.2

Method

This study recruited seven participants aged 18-29. This study consisted of all male
participants. The ethnicities of participants were New Zealand European, Chinese, Korean,
South African and Taiwanese (see Appendix B). In this study one participant had viewed
IAPS before in the previous study. Participation in this study was rewarded with a $20
Reboot voucher.
The physical setup of this experiment was identical to that of the first experiment except the
EPOC was connected to a Sony laptop running Windows 7 with 2 GB RAM instead of the
Tobii machine. This was due to the USB port’s unreliability on the Tobii machine. The
software used in this study is identical to the software used in the first study. However the
Java program controlling the experiment was adjusted to display fewer images and the
questionnaire was adjusted to a nine point scale instead of a five point scale (see Figure 5).

21

Figure 5: Improved questionnaire with timer (9-point scale)

4.3 Results and Analysis
In this experiment the EPOC data for images was grouped into the categories of excitement,
engagement, frustration, meditation and boredom based their predefined image category (see
Figure 3). We managed to obtain data for all seven participants for the 25 images viewed.
However, not all participants self-reported data could be accurately mapped to EPOC data.
For example only five out of seven participants had usable EPOC values for mapping
engagement photos to the engagement category. All participants had usable EPOC data for
mapping excitement images to the excitement category. Unfortunately due to noise and
disengaging channels, some samples collected were also deemed unusable. For the
engagement category, 91% of samples for participants within that category were usable. For
excitement images, 100% of samples were usable. For irritated and angry images only 88%
of samples were usable and for boredom only 66% of samples are used in analysis (see Table
3). Meditation is not represented in analysis as only one image in the reduced set of images
corresponded to this emotion as it is not one of the main emotions focused on in this study.

22

Table 3: Usable Data
Engagement

Excitement

Irritated

Bored

Angry

Participants

5

7

5

4

5

Total Participants

7

7

7

7

7

Percentage Used %
Samples (for
participants)
Unusable Samples

71%

100%

71%

57%

71%

35

43

36

13

36

3

0

4

5

4

Percentage Used %

91%

100%

88%

61%

88%

In this study we wanted to determine the accuracy of the EPOC in detecting emotions. In
order to achieve this, we compared participants’ self-reported scores with their corresponding
EPOC scores by inducing emotions using IAPS. However, even after recruiting more
participants and improving the experimental design, there were still no significant findings
and the results remain inconclusive. Although a significant relationship was found between
self-reported meditation and EPOC engagement (Table 4), this finding is inconsistent with
previous results.
We also compared the EPOC and self-reported scores for each of the categories to the image
categories valence and arousal scores depicted by IAPS. This comparison produced some
interesting findings. It was found that arousal for anger and irritated images strongly
correlated to self-reported anger (Table 5). However some other relationships were found
that were not expected. For example it was also found that valence for anger and irritated
images strongly correlated with EPOC excitement and EPOC engagement (Table 5). It was
also found that EPOC excitement and EPOC engagement had a strong negative relationship
(Table 5). This result contradicts the findings of the previous study in which EPOC
excitement and EPOC engagement had a strong positive relationship. This contradiction
demonstrates the unreliability of collected data from the device.
The values for the emotional intensity questionnaire (see Appendix A) varied greatly between
participants for some emotions and remained reasonably similar for others. For example, for
the statement “I am easily excited” values varied from five (neutral) to seven (agree).
Whereas the statement “I am easily bored” values varied from three (disagree) to eight
(strongly agree), showing how differently people rate their own emotions.

23

Table 4: EPOC Scores versus Self-Report (SR) Scores
EPOC
EPOC
EPOC
Engagement Excitement Frustration
SR
Engagement
SR
Excitement

SR Irritated

SR Anger

SR Bored

SR
Meditation

Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N
Pearson
Correlation
P value
N

EPOC
Meditation

EPOC
Boredom

0.039

0.089

-0.505

0.087

0.747

0.934
7

0.849
7

0.247
7

0.913
4

0.147
5

-0.541

0.409

-0.586

0.944

-0.064

0.21
7

0.362
7

0.167
7

*0.056
4

0.919
5

-0.139

-0.256

-0.537

0.122

0.183

0.766
7

0.58
7

0.214
7

0.878
4

0.769
5

-0.433

-0.079

-0.365

0.539

-0.444

0.332
7

0.867
7

0.421
7

0.461
4

0.454
5

-0.312

-0.32

-0.808

-1.00

-0.389

0.609
7

0.599
7

0.098
7

0
4

0.517
5

0.885

-0.76

0.474

0.22

0.437

*0.046

0.136
7

0.42
7

0.778
4

0.712
5

7

* Correlation is significant at the 0.05 level (2-tailed)

Table 5: Other Significant Results
Category One
Self-report Anger
Self-report Anger
Valence Anger/ Irritated
Valence Anger/ Irritated
Valence Engagement
Engagement EPOC

Category Two
Arousal Anger
Arousal Irritated
EPOC Excitement
EPOC Engagement
EPOC Frustration
Excitement EPOC

Pearson’s Correlation
0.851
0.851
-0.789
-0.783
0.775
-0.847

P value
0.015
0.015
0.035
0.037
0.041
0.016

N
7
7
7
7
7
7

4.4 Discussion
This study was an adaption of the previous study in which participants viewed a series of
emotionally invoking images from IAPS, while the Emotiv EPOC recorded the emotional

24

data collected from measuring brainwaves. Participants also recorded their emotional
responses in the form of a self-reported questionnaire. These two items were then compared
and it was found that there were two significant relationships between the participants’ selfreported scores and the EPOC’s values. These correlations did not indicate any relationship
between EPOC and self-reported scores as the correlations produced did not make sense. For
example, self-reported excitement had a high correlation with EPOC meditation and selfreported meditation had a high correlation with EPOC engagement.
Some significant findings were found such as self-reported anger having a strong positive
relationship with IAPS images of irritation and anger arousal values. This is an interesting
finding as it suggests that as arousal increases in angry or irritating images participants
recorded higher levels of frustration. This suggests that perhaps our self-report questionnaire
is accurate; however this finding alone is not sufficient to justify any significance between
IAPS values and the self-report questionnaire. Other relationships were found in analysis;
however they are disregarded as isolated findings as they do not allow for any meaningful
interpretation. It is interesting to note, that in both studies the channels of engagement and
excitement are highly correlated. However the first study found a positive correlation
between the two channels and the second study found a negative correlation. Therefore
although there is a relationship, due to this contradiction no conclusions can be made from
these results.
Not all EPOC data was included in analysis due to several factors. The first factor being that
the sampling rate of the EPOC was not consistent, given that some seconds there were several
samples and sometimes there were no samples for up to 30 seconds. This meant that some of
the images and self–reported scores could not be mapped to EPOC values as mappings are
reliant on timestamps. Thus not all images were accounted for in this study. This
inconsistency could be caused by the change of computer as the second study used a different
computer, to record the data in order to improve the signal and decrease set up time. Emotiv
does not state any factors which could effect data collection; therefore there is no way of
diagnosing whether or not this was a factor. Another factor which affected the amount of
usable data was the channels reliability. The engagement channel for example would
sometimes lose its signal and get deactivated without the experimenter’s knowledge and the
values projected by the EPOC for this period of time would be invalid. This was deactivation
of the channel was identified through a constant string of identical values in the data over a
large period of time as our emotional states vary rapidly. The last factor that affected the
amount of samples was the sensors contact quality. If the device was worn for a large period
of time the participant’s sensors would lose their moisture and lose their quality of signal,
causing channels to become disengaged or increase noise.

25

Chapter 5: Study Three: Implementation in a tutoring system

This Chapter describes the third study we conducted in order to determine the applicability of
the EPOC in an ITS. In this study eleven participants used the think-aloud protocol to
determine if users emotional statements had any relationship with EPOC values. User’s
statements were grouped into a series of categories based on context of statement, action
taken and timing.

5.1

Research Design

Participants in this study used EER-Tutor1 [36] while wearing the EPOC headset in order to
determine its accuracy and applicability in an ITS. EER-Tutor is tutoring system in which
students learn to design databases from textual problem descriptions [37]. We choose to use
EER-Tutor because our participant pool was accustomed to it. Participants were instructed to
attempt one easy and one difficult task of their choosing. This was designed so that it would
be possible to compare the emotions experienced during both difficult and easy tasks and
determine if there were any differences.
Since our previous studies produced inconclusive results, we proposed to use a different
approach for users to identify their emotions. We used the “think-aloud” protocol [38] in
which participants’ state everything they are thinking and/or feeling aloud. This approach
was implemented using Tobii eye tracker to record on screen events, track participants eyes
and record a video of the participant. We determined all three of these techniques would be
needed to determine the context in which the emotions were stated. These techniques also
allowed us to segregate the emotional statements made by participants into context specific
categories as explained in the results section.

5.2

Method

In this study 11 participants were recruited, from the COSC-265 database course and
throughout the computer science department. Some of the participants participated in the first
study and had used the EPOC device before. Participants received a $20 Reboot voucher for
participation.
The physical setup of this experiment was identical to that of the second study except an
additional microphone was placed in front of the participant to record participant dialogue.
The Emotiv control panel (see Figure 3) was used to ensure that good sensor contact was
established. The experimenter constantly monitored the control panel on another computer
1

http://ictg.cosc.canterbury.ac.nz:8005/

26

during the experiment to ensure good signal quality was maintained. The Tobii studio
software suite was used to record the participants screen, the participant his/herself and their
dialogue. The Tobii also tracked the participant’s eye movements. EER-tutor was then used
to assess EPOC’s applicability in an ITS and this was accessed via the internet. The program
which was used to collect data in the previous two studies was also used in this study. This
program used the Emotiv SDK to integrate a Java application with the EmoEngine in order to
read values directly from the EPOC headset. The program constantly polls for new states and
once a new state is found, it then records the emotional values associated with that state. The
rate at which polling is conducted varies greatly as it can have a sampling rate of multiple
polls per second or one every few seconds.

Figure 6: Tobii Studio Eye Tracking, Video & Sound
First the EPOC is setup in the same manner as described in the previous studies. Once the
EPOC is on the participants head and a good signal quality is achieved, participants then
completed a calibration test with Tobii, and then log onto EER-tutor. Participants were then
asked to attempt one easy and one difficult task of their choosing. Here they were also
instructed on the concept of the think aloud protocol. Participants were told to say everything
which they are thinking and or feeling aloud. Once participants had a clear understanding of
the protocol, they were then instructed to start the first task. Participants were not given any
specific amount of time to complete each of these tasks however an overall time limit of 20
minutes was given. Once they produced the correct answer, they were then prompted to select
a second task from the menu bar. Participants were able to determine the difference between
a difficult and easy task by his/herself, as the tasks are categorized in this fashion in the
menu. However, if a participant was unsure of task selection, the experimenter then instructed
them to choose a difficult task above question thirty and an easy task below question ten.
While participants are completing their tasks the researcher is watching the EPOC’s signal

27

quality to make sure the signal remains strong and is available for any questions which the
participants may have.

5.3

Categorizing data

Before any results could be analysed, the data collected from the think-aloud protocol had to
be categorized in a meaningful way. Significant events were determined by listening to the
audio and watching the screen recordings of each participant. Significant events could be
statements, gestures or expressions. These events were then placed into a range of categories.
These categories are determined by the context of the statement (whether it is positive,
negative, or confused), the action taken (such as performing a task, reading a question,
reading feedback or obtaining a correct answer), and the timing involved such as before, after
or while committing an action.
Each statement that elicits some emotion is classified as positive, negative, or confused
depending on the context of the statement. Positive statements are usually said in a positive
tone, for example “Yah” or “Awesome”. Neutral statements are not included in any of the
following categories. Negative statements are usually stated in a higher tone and include
swearing and exasperation such as “What?!?” or “I am extremely frustrated”. A statement is
only included in the confused statement category if the participant explicitly states they are
confused “I am really confused” or “I don’t understand”.
For each statement that is recorded, a participant is known to be performing some action. The
first action is performing a task: any action that is associated with constructing the diagram
(see Figure 8). Another action which could be taken by a participant is reading the question
or reading feedback. These actions are found by looking at the eye tracking data generated by
Tobii (see Figure 9 & Figure 10). Another significant action is receiving a correct answer;
this is determined by the screen recording of each participant. Another action was also
created called the “no action taken” action. This action was when participants said a
statement, usually of confusion without taking any action. This action category was
determined by no mouse movement on screen, and stable or no eye tracking data.
After mapping each of the statements to a particular action, we then had to determine whether
the statement happened before, during or after the action was taken. This was executed by
listening to the participant’s statements while viewing the screen shots and determining what
timing category each mapping was placed in. If a significant event was gesture or expression,
actions and timing were also paired with the event. In some categories not enough samples
were obtained, thus categories had to be combined and consisted of gesture/expression and
action. Thirteen categories were created (see Figure 7).

28

Category

Category (Full Name)

Frequency

PSACA

Positive Statement After Correct Answer

5

TWRQ

Touching Face While Reading Question

NSARQ

Negative Statement After Reading Question

9
9
6

NSAPT

Negative Statement After Reading Question

12

CSWPT

Confusion Statement While Performing Task

3

CSWTNA

Confusion Statement While Taking No Action

6

SLRQ

Smiling/Laughing Reading Question

4

SLPT

Smiling/Laughing Performing Task

3

NSWPT

Negative Statement While Performing Task

8

CSARQ

Confusion Statement After Reading Question

5

PSAPT

Positive Statement After Performing Task

5

PSWPT

Positive Statement While Performing Task

4

CSWRQ

Confusion Statement While Reading Question

3

Figure 7: Categories

Figure 8 shows the eye tracking data for a
particular participant performing a task.
Participants express many different responses
when performing a task such as smiling,
laughing or stating frustration “I am feeling
annoyed”.

Figure 8: Tobii data for performing a task

Figure 9 shows the eye tracking data for a
participant reading a question. Participants
often stated confusion “That doesn’t make any
sense” or touch their face when reading text.

Figure 9: Tobii data for reading a question

29

5.4 Results and Analysis
From this study we managed to obtain eleven participants data. However not all participants
“think-aloud” statements could be accurately mapped to EPOC data. As depicted in Table 6
some participants had lower percentages of usable data. This was caused by one of several
factors such as loss of signal, noise or channel deactivation. Categories which only had one or
two samples were not included in analysis due to lack of data, whereas some categories were
combined such as smiling/laughing.
The emotional intensity data (see Appendix A), and the demographic questionnaire data (see
Appendix B) for this study was the same as study two as each participant participated in both
studies.
Table 6: Usable data
Participant

Events obtained

One
Two
Three
Four
Five
Six
Seven
Eight
Nine
Ten
Eleven

21
10
33
14
4
11
13
18
10
8
25

Events successfully mapped
to EPOC
17
5
13
10
4
7
12
14
7
6
9

% Used
81%
50%
39%
71%
100%
63%
92%
78%
70%
75%
36%

Table 7: Variance in each sample
Category
PSACA
TWRQ
NSARQ
NSAPT
CSWPT
CSWTNA
SLRQ
SLPT
NSWPT
CSARQ
PSAPT
PSWPT
CSWRQ

Number of
participants
4
4
3
8
3
5
4
3
5
4
4
3
3

Variance
EX
0.0165
0.034
0.008
0.12
0.02
0.02
0.02
0.0003
0.06
0.071
0.041
0.087
0.088

Variance
EN
0.009
0.003
0.0002
0.014
0.001
0.003
0.014
0.0008
0.002
0.002
0.005
0.024
0.013

30

Variance
FR
0.028
0.009
0.009
0.07
0.008
0.09
0.062
0.06
0.04
0.027
0.083
0.0099
0.00026

Variance
ME
0.0006
0.0002
0.0002
0.0007
0.00009
0.0003
0.00003
0.0004
0.0002
0.0027
0.00046
0.000001
0.00009

In order to determine whether the values for participants in each of the categories varied
greatly, we calculated the variance for each of the channels within the specific categories. As
shown in Table 7 the variance for each of the categories is big. For example in category
“NSAPT” excitement has a variance of 0.12. This value demonstrates the large variability
found in excitement levels when stating negative dialogue. However, the large variability also
suggests that some negative statements were mapped to high levels of excitement, which
seems slightly contradictory. Meditation has the least variance out of all the channels and
remains reasonably stable over all the categories. However the larger amount of variance
shown in the other remaining channels demonstrates individual differences of emotional
reactions to various learning situations. This result was to be expected as participants
emotional responses vary greatly and demonstrates how one emotional model will not fit all
participants.
We compared the EPOC data for various categories with the Kruskal-Wallis non parametric
test. We found no significant differences between the categories on any of the four channels
of excitement (p = 0.068), engagement (0.275), frustration (p=0.236) and meditation (p =
0.139). This finding was unexpected as it was predicted that certain emotional statements of a
positive, negative or confused context would depict a trend in EPOC emotional values.
However this insignificance could be attributed to inaccurate readings from the device or due
to large variances found in each of the categories.
Table 8: Mann-Whitney Results
Category One

Category Two

SLRQ
(Frustration)
CSWPT
(Engagement)
CSARQ
(Engagement)
PSACA
(Meditation)

SLPT
(Frustration)
CSWTNA
(Engagement)
CSWPT
(Engagement)
PSWPT
(Mediation)

Mann-Whitney Value

P value

0.000

0.050

1.000

0.053

0.000

0.034

0.000

0.034

In order to determine if any of the channels of a specific category were significantly different
to the other categories channels, a Mann-Whitney test was conducted. Four significant
differences were found. SLRQ frustration was found to be significantly different than SLPT
frustration p<0.05 (Table 8). This suggests that smiling and laughing caused different
amounts of frustration measured by EPOC. CSWPT engagement and CSWTNA engagement
were also found to be significantly different p < 0.05 as did CSARQ engagement with
CSWPT engagement. This suggests that for confused statements, engagement levels were
quiet different for each of the three categories. No significant differences were found for any
of the other channels. PSACA meditation was also found to be significantly different than
PSWPT meditation p<0.05 (Table 8). This is an interesting finding as little variance was

31

displayed in the meditation channel over all categories (see Table 7). More analysis needs to
be conducted in order to determine whether or not these are isolated findings.
Table 9: Comparison of Statements
Category One

Category Two

Confusing Statements
(Excitement)
Confusing Statements
(Frustration)
Confusing Statements
(Engagement)
Confusing Statements
(Excitement)

Negative Statements
(Excitement)
Negative Statements
(Frustration)
Negative Statements
(Engagement)
Negative Statements
(Frustration)

Pearson’s Correlation

P value

N

0.889

0.007

7

0.87

0.01

7

0.889

0.007

7

0.758

0.048

7

Each of the existing categories were then regrouped into simply positive, negative or
confused statement groups, without actions or timing. Only participants who were in all of
these three groups were included. Each of the groups were then compared using a correlation
test to determine if there were any interesting relationships. The positive statement group did
not have any significant relationships with any of the other two groups. However there were
some interesting relationships between the negative and confused group. For example
excitement in the confused group had a strong relationship with negative excitement p<0.05.
Frustration and engagement also had a strong positive relationship p<0.05. Interestingly
excitement in the confusion group also had a strong positive relationship with negative
frustration p<0.05 (see Table 9). These findings suggest that there was a strong relationship
between confusion statements and negative statements and perhaps these were heavily linked.

5.5

Discussion

In this study users adopted the think-aloud protocol while using EER-Tutor and wearing the
EPOC in order to determine its accuracy and applicability in an ITS. In this study participants
stated everything they were thinking and feeling aloud. This dialogue was then grouped into
categories based on the context of the statement, the action taken and the timing of the action.
We found the differences between categories were not significant. The lack of difference
between the categories suggests that alike the previous research the EPOC was not accurate
in capturing the user’s emotions, as some noticeable difference between the emotions of
positive and negative statements should have been observed. However this result could also
be attributed to the large variance found in each of the categories. The large amounts of
variance in channels such as frustration and excitement suggest that the individual difference
in emotional responses is too great to be mapped to a single model. People respond very

32

differently when making a positive or negative statement and these differences make it very
difficult to apply emotion recognition to learning systems.
There was also a relationship found between confusion categories engagement channels,
however this finding alone is not meaningful as engagement had a small variance over all
categories. We also found confusion statements EPOC channel scores and negative
statements EPOC channel scores to have a very strong relationship for multiple channels.
This is an interesting finding as it suggests that perhaps confusion and negativity are strongly
related, however more research needs to be conducted to explore and verify this relationship.
The limitations of the device experienced in previous studies were again experienced in this
study such as long, laborious setup, noise, channel disengagement, participant discomfort and
poor signal quality. These limitations experienced reaffirms the conclusion that the device is
unsuitable for use in an ITS.

33

Chapter 6: Conclusions

This Chapter provides a discussion and comparison of the results of all three studies. We also
discuss the limitations which we have found with the device and its applicability in an ITS.
We also provide a section discussing future work and overall conclusions of the study.

6.1 Discussion
While previous studies have shown that using the EPOC for detecting emotions is possible,
they did not attempt to investigate its accuracy and validity. In our studies we have found that
the EPOC is not reliable in detecting emotions as no significant relationships were found
between participants’ self-reported emotional states and EPOC emotional values. Other
studies such as [32] and [24] have used multimodal approaches when using EPOC increasing
its accuracy. For example, [24] found that the EEG had a low accuracy rate for predicting
emotions but when combined with another form of detection such as mouse click behaviours
this increased dramatically. However in our third study we also used a multimodal approach
with the use of eye tracking and cameras and still found no significant findings.
We also found EPOC to be very inconsistent in its outcomes for the first two studies as we
obtained conflicting results. For example, in the first study, EPOC channels excitement and
engagement were found to have a positive relationship, which is to be expected due to high
amounts of arousal caused by both emotions. However, in the second study engagement and
excitement had a negative relationship, suggesting that as engagement increased excitement
decreased. These conflicting results suggest that the EPOC is not very reliable. The first study
also found that frustration had a significant relationship with excitement. This is an
unexpected finding as excitement and frustration are opposite emotions. This finding suggests
that perhaps the frustration channel is highly linked to arousal and perhaps frustration is
incorrectly labelled by Emotiv.
In the third study we found no significant difference between each of the event categories we
defined. This was attributed to either inaccurate data obtained from the EPOC or due to the
large amount of variance found in each of the different categories. However this variance was
to be expected due to individuals having different emotional responses. Although significant
relationships were found between the EPOC channels in negative statements and confusion
statements suggesting that perhaps these emotions are strongly related. However more
research needs to be conducted to explore this relationship further. Although we achieved
insignificant results, the “think-aloud” study allowed us to discover certain aspects of EERTutor which participants found frustrating or difficult to understand. For example, some
participants who had not used the tutor in a while had difficulty in determining how to set
labels. Therefore we suggest that if a user new to the system then the tutor should display

34

instructional information on how to complete this task with the aim of reducing participants
perceived frustration.
In this study we also experienced several challenges when using the device. We found the
EPOC signal to be very temperamental, as it would constantly fade in and out of signal and
disengage the channels during the study. Secondly, we also found the set up time for the
device was extensively long, such as 15 minutes if the participant had long or thick hair. We
also experienced device malfunctions; after conducting three successful sessions during the
first study, the headset failed to function and two of the sensor nodes could not receive any
signal. After much investigation, this failure was attributed to the build-up of copper
corrosion on the headset caused by moistening the sensor nodes. This is a design flaw of the
device and reduces its longevity suggesting that a new device need to be acquired yearly.
Participants in our studies also noted discomfort after wearing the headset for an extended
period of time. Therefore, we tried to limit the amount of time which participants wore the
headset to reduce this discomfort. This discomfort is most likely due to the sideways pressure
that holds the headset in place. Other studies also noted participants experienced discomfort
[28]. This limitation alone make the headset unsuitable for use in an ITS as tutorial sessions
last for around 2 hours. We also found that the wireless signal received from the headset was
extremely noisy and if too much noise was detected then channels such as engagement
became useless. Noise could be introduced in many ways such as facial movements or
environmental noise. This makes the data less accurate if too much noise is introduced. Given
these limitations and the lack of conclusive findings we would deem this device to be
unsuitable for use in an ITS.

6.2 Conclusions and Future Work
The Emotiv EPOC certainly has advantages over traditional EEG, being less expensive,
portable and easy to obtain. It also comes with a variety of software suites which can be used
with other programs in order to obtain raw or processed data straight from the EPOC headset.
While investigating the EPOC our main focus was to determine whether its emotional values
accurately map to participants self-reported emotional state. In the first two studies we
performed, participants viewed a series of IAPS images to invoke emotions allowing a direct
comparison between self-reported scores and EPOC emotional values. We found that there
was no significant relationship between self-reported scores of each of the channels and the
emotional values obtained from the EPOC. This suggests that the EPOC is not accurate in
identifying users’ emotional values as none of the channels correlated with their
corresponding self-reported categories.
Our second focus was to determine its use in an ITS. Since we found no significant findings
in our previous studies, we adopted a different approach in determining users’ emotions. In
the third study detailed in this report, we used the think-aloud protocol in which participants
stated everything they were thinking and feeling aloud while using EER-Tutor. We found no
significant differences between categories. The lack of difference could be attributed to

35

inaccuracy of the EPOC or the large variances found within the categories. However, these
variances showed the potential individual differences when expressing emotion and
demonstrated how people cannot be mapped to one affective model.
A large number of limitations were found when using the device and these limitations
prevented the device from being applicable in an ITS. However, future researchers should
explore the practicality of other lightweight EEG to determine their use in ITS and their
accuracy in depicting emotions. Even though the EPOC produced inconclusive findings and
its limitations make it currently unsuitable for an ITS, it does have its advantages. EPOC’s
low cost, portability and capabilities still make the it a useful research tool even though it
might not be applicable in this context. If the EPOC’s limitations were resolved and it
produced consistent and meaningful results then perhaps it could be usable in ITS as affective
computing still needs to be accurately incorporated more effectively in ITS. Raw data can
also be obtained straight from the headset instead of using the provided software suites. This
would allow us to use our own classification and noise reduction algorithms; however this
was outside the scope of this project. Future researchers could attempt to replicate the studies
conducted to determine if using raw data increased accuracy and consistency.

36

References
[1]

J. P. Guilford, & R. Höpgner, “The Analysis of Intelligence,” McGraw-Hill Book Company,
New-York, 1971.

[2]

A. M. Isen, “Positive affect and decision making,” Handbook of emotions, 2000.

[3]

S. R. Quartz, “Reason, emotion and decision-making: risk and reward computation with
feeling,” Trends in Cognitive Sciences, vol. 13, no. 5, pp. 209-215, 2009.

[4]

R. Picard, “Affective Computing,” Massachusetts Institute of Technology, 1997.

[5]

R. Sun, & E. Moore, “Empirical study of dimensional and categorical emotion descriptors in
emotional speech perception,” Proceedings of FLAIRS 2012, pp. 104-109, 2012.

[6]

K. Zakharov, A. Mitrovic, & L. Johnston, “Towards Emotionally-Intelligent Pedagogical
Agents. Intelligent Tutoring Systems,” Intelligent Tutoring Systems, pp. 19-28, 2008.

[7]

B. P. Woolf, J. Beck, C. Eliot, & M. Stern, “Growth and maturity of intelligent tutoring
systems: A status report,” Smart machines in education, Cambridge, 2001.

[8]

B. Woolf, W. Burleson, I. Arroyo, T. Dragon, D. Cooper, & R. Picard, “Affect-aware tutors:
Recognising and responding to student affect,” International Journal of Learning
Technology, vol. 4, no. 3/4, pp. 129-164, 2009.

[9]

P. S. Iventado, R. Legaspi, M. Suarez, & M. Numao, “Predicting student emotions resulting
from appraisal of ITS feedback,” Research & Practice in Technology Enhanced Learning,
vol. 6, no. 2, pp. 107, 2001.

[10]

R. Pekrun, T. Goetz, W. Titz, & R. Perry, “Academic Emotions in Students Self-Regulated
Learning and Achievement: A Program of Qualitative and Quantitative Research,”
Educational Psychologist, vol. 37, no. 2, pp. 91-105, 2002.

[11]

K. Zakharov, A. Mitrovic, & L. Johnston “Pedagogical Agents Trying on a Caring Mentor
Role,” Proceedings of AIED, pp. 59-66, 2007.

[12]

A. C. Graesser, “Learning, thinking, and emoting with discourse technologies,” American
Psychologist, vol. 66, pp. 743-757, 2011.

[13]

I. Arroyo, D. G. Cooper, W. Burleson, B. P. Woolf, K. Muldner, & R. Christopherson,
“Emotion sensors go to school,” Proceedings of Artificial Intelligence in Education, pp. 1724, 2009.

[14]

J. Bailenson, E. Pontikakis, I. Mauss, J. Gross, M. Jabon, C. Hutcherson, C. Nass, & O. John,
“Real-Time Classification of Evoked Emotions Using Facial Feature Tracking and
Physiological Responses,” International Journal of Human-Computer Studies, vol. 65, no. 5,
pp. 303-317, 2008.

37

[15]

A. Heraz, R. Razaki, & C. Frasson, “Using Machine Learning to Predict Learner Emotional
State from Brainwaves,” Proceedings of Advanced Learning Technologies, pp. 853-857,
2007.

[16]

P. Inventado, R. Legaspi, & M. Suarez, “Predicting Students Appraisal of Feedback in an ITS
using Previous Affective States and Continuous Affect Labels form EEG data,” Research and
Practice in Technology Enhanced Learning, vol. 6, no. 2, pp. 107-133, 2011.

[17]

C. Hondrou, & G. Caridakis, “Affective, natural interaction using EEG: sensors, application
and future directions,” Proceedings of SETN’12, pp. 331-338, 2012.

[18]

D. S. Cantor, “An overview of quantitative EEG and its applications to neuro feedback in
Introduction to Quantitative EEG and Neurofeedback,” in Press, pp. 3-27, 1999.

[19]

M. F. Bear, B.W. Connors, & M. A. Paradiso, “Neuroscience: Exploring the Brain,”
Lippincott Williams & Williams, Baltimore, 2001.

[20]

A. Heraz, & C. Frasson, “Predicting the three major dimensions of the learner’s emotions
from brainwaves,” World Academy of Science and Technology, vol. 25, ISSN 1307-6884,
2007.

[21]

Trans Cranial Technologies Ltd., “Trans Cranial Technologies: 10-20 system positioning
manual,” Wanchai, Hong Kong, 2012.

[22]

M. Murugappan, N. Ramachandran, & Y. Salzail, “Classification of human emotion from
EEG using discrete wavelet transform,” J. Biomedical Sci. and Eng, vol. 3, no.4, pp. 390-396,
2010.

[23]

E. T. Mampusti, J. S. Ng, J. J. I. Quinto, & G. L. Teng, “Measuring academic affective states
of students via brainwave signals,” KSE, pp. 226-231, 2001.

[24]

J. Robison, S. Mcquiggan, & J. Lester, “Developing empirically based student personality
profiles for affective feedback models,” ITS 2010, pp. 285-295 2010.

[25]

J. Azcarraga, J. F. Ibañez, I. R. Lim, N. J. Lumanas, R. Trogo, & M. T. Suarez, “Predicting
Academic Emotion based on Brainwaves Signals and Mouse Click Behavior,” Asia-Pacific
Society for Computers in Education, Chiang Mai, Thailand, 2001.

[26]

J. Gonzales-Sanchez, M. E. Chavez-Echeagaray, R. Atkinson, & W. Burleson, “ABE: an
agent-based software architecture for a multimodal emotion recognition framework,” WICSA
2011, pp.187-193, 2011.

[27]

A. T. Pope, E. H. Bogart, & D. S. Bartolome, “Biocybernetic system evaluation indices of
operator engagement in automated task,” Biological psychology: vol. 40, no. 9, 1995.

[28]

Technovelogy.com, http://www.technovelgy.com/ct/Science-Fiction-News.asp?NewsNum=
2715, Date accessed: 15th October 2013.

[29]

Emotiv Research Edition SDK v1.0.0.5-Premium, Emotiv SDK User Manual.

38

[30]

R. Lievesley, M. Wozencroft, & D. Ewins, “The Emotiv EPOC neuroheadset: an inexpensive
method of controlling assistive technologies using facial expressions and thoughts?,” Journal
of Assistive Technologies, vol. 4, no. 2, pp. 67-82, 2011.

[31]

M. Murugappan, M. Rizon, R. Nagarajan, & S. Yaccob, “Lifting scheme for human emotion
recognition using EEG,” Int.Symp.Information Technology, pp. 1-7, 2008.

[32]

P. J. Lang, M. M. Bradley, & B. N. Cuthbert, “International affective picture system (IAPS):
Affective ratings of pictures and instruction manual,” Technical Report A-6. University of
Florida, Gainesville, 2005.

[33]

M. M Bradley, & P. J. Lang, “The International Affective Picture System (IAPS) in the study
of emotion and attention,” in Press, pp. 29-46, 2007.

[34]

A. Heraz, & C. Frasson, “Predicting the three major dimensions of the learners emotions from
brainwaves,” CICIS 2010, pp. 33-43, 2010.

[35]

S. D’Mello, A. Olney, C. Williams, & P. Hays, “Gaze tutor: A gaze-reactive intelligent
tutoring system,” International Journal of Human-Computer Studies, vol. 70, no. 5, pp. 377398, 2012.

[36]

EER-Tutor, http://ictg.cosc.canterbury.ac.nz:8005, Date Accessed: May 2013-October 2013.

[37]

A. Mitrovic, “Fifteen years of Constraint-Based Tutors: What we have achieved and where
we are going,” User Modeling and User-Adapted Interaction, vol. 22, no. 1-2, pp. 39-72,
2012.

[38]

K. Ericsson, & H. Simon, “Protocol Analysis: Verbal Reports as Data (2nd ed.),” Boston:
MIT Press, 1993.

39

Appendix A:

I am easily excited:
Strongly Disagree
1

2

Neutral
3

4

5

Strongly Agree
6

7

8

9

I am easily engaged:
Strongly Disagree
1

2

Neutral
3

4

5

Strongly Agree
6

7

8

9

I am easily calmed:
Strongly Disagree
1

2

Neutral
3

4

5

Strongly Agree
6

7

8

9

I am easily bored:
Strongly Disagree
1

2

Neutral
3

4

5

Strongly Agree
6

7

8

9

I am easily irritated:
Strongly Disagree
1

2

Neutral
3

4

5

Strongly Agree
6

7

8

9

I am easily angered:
Strongly Disagree
1

2

Neutral
3

4

5

Strongly Agree
6

40

7

8

9

Appendix B:

Questionnaire:

What is your age? (Please circle)
18-23

24-29

30-35

36-41

42-47

48+

What is your gender? (Please circle)
Female

Male

What is your ethnicity? (Please circle)
New Zealand/European
Other

Maori

Asian

Pacific Island

If you circled “other”, please specify:

Have you ever participated in a study using the IAPS before? (Please circle)
Yes

No

Have you ever participated in a study using the EPOC headset before? (Please circle)
Yes

No

41

Appendix C:
FOR STUDENT RESEARCH UP TO AND INCLUDING MASTERS LEVEL
ETHICAL APPROVAL OF LOW RISK RESEARCH INVOLVING
HUMAN PARTICIPANTS REVIEWED BY DEPARTMENTS
Please read the important notes appended to this form before completing the sections below
1

RESEARCHER’S NAME: Tegan Grace Harrison

2

NAME OF DEPARTMENT OR SCHOOL: Department of Computer Science and Software
Engineering

3

EMAIL ADDRESS: tgh28@uclive.ac.nz

4

TITLE OF PROJECT: The Emotiv mind: Investigating the accuracy of the Emotiv EPOC in
identifying emotions and its use in Intelligent Tutoring System.

5

PROJECTED START DATE OF PROJECT: 29/05/2013

6

STAFF MEMBER/SUPERVISOR RESPONSIBLE FOR PROJECT: Prof. Tanja Mitrovic

7

NAMES OF OTHER PARTICIPATING STAFF AND STUDENTS: Moffat Mathews

8

STATUS OF RESEARCH: (e.g. class project, thesis) Honours Research Project

9

BRIEF DESCRIPTION OF THE PROJECT:
Please give a brief summary (approx. 300 words) of the nature of the proposal in lay language,
including the aims/objectives/hypotheses of the project, rationale, participant description, and
procedures/methods of the project: The objective of this project is to investigate the use of the Emotiv EPOC and the software that
comes with it as a way to determine if the device is accurate in detecting user’s emotions. The
EPOC is a small, wireless consumer grade “brain-computer-interface” device that rests on the
head. It is designed for gaming and allows for a user’s emotions to be detected through their
thoughts. Our aim is to investigate how accurate this device is in detecting user’s emotions.
Volunteers will receive a $20 café voucher rewards for participation. The study will be done in
the Department of Computer Science and Engineering in the Erskine building in room 200. This
study is open to anyone over 18 willing to take part in this research. Participants will typically be
400 level computer science students. We aim to have approximately 10 participants. A short
introduction to the research will be given at the start of the experiment, and participants will
receive a project description and a consent form which they will read and sign, should they wish
to participate in the study. Participants will then be told that they can elect to opt out at any point
during the experiment. Participants will then be given a demographics questionnaire to fill in and
an emotional 5 point Likert scale questionnaire. The demographic questionnaire allows factors
such as ethnicity to be measured as previous research has indicated that the expression of
emotions is culturally dependant. This consists of 5 questions asking participants to self-report
their how easily they feel certain emotions such as engagement, excitement, frustration etc. The
participants will then be equipped with the device: this is simply a non-invasive headset with 14

42

sensors that sit on the head. Participants will then be required to play an online game (Angry
birds, Tetris or Bejewelled) for approximately 15 minutes in order for the device to normalize
their emotions. Participants will then be placed in front of a Tobii eye tracker computer.
Participants will then be required to complete a couple of eye calibration tests, where the
participant is told to look at certain parts of the screen in order for the computer to track their eyes
accurately. Once this is completed participants will then be shown a series of 48 images from the
International Affective Picture System (IAPS). These images will be separated into 5 blocks of 12
images. These images are known to correlate with the following emotional responses of
excitement, engagement, calm, boredom, neutral, anger and irritation. We have carefully selected
photos from the IAPS that will not induce stress or offend participants and that are only relevant
to the 7 emotional categories mentioned above. Each image will be displayed on screen for
approximately 5 seconds. The image will then disappear and the participant will be asked to selfreport their emotional state as a questionnaire appears on screen. Participants will receive 15
seconds to complete the questionnaire. Once they have done so the participant will then view the
next image and so forth. Once a block has been completed participants will receive a 15 second
rest period and told how many blocks they have left to go. While participants are viewing the
photos the Emotiv EPOC is measuring their emotional state and saving this in correlation to each
image viewed. The Tobii eye tracker is also measuring their eye movements when viewing the
images, in order to determine whether or not participants are actually looking at the images for
the full 5 seconds. This experiment should take no longer than 50 minutes. After the experiment,
a short questionnaire will be administered to find out their perceptions of the system.
10 WHY IS THIS A LOW RISK APPLICATION?
Description should include issues raised in the Low Risk Checklist.
Please give details of any ethical issues which were identified during the consideration of the
proposal and the way in which these issues were dealt with or resolved. :This is a low risk application because it does not raise any issue of deception, threat, invasion of
privacy, mental, physical or cultural risk or stress, and does not involve keeping personal
information of sensitive nature about individuals. All participants will be given an information
sheet describing the research project and will give consent for their results to be used for
analysis. The participants will be fully aware that they can withdraw themselves or their results
from the research at any stage. All the data will be kept in a secure environment and only those
listed above allowed access to it.
11 PROVIDE COPIES OF INFORMATION & CONSENT FORMS FOR PARTICIPANTS
These forms should be on University of Canterbury Departmental letterhead. The name of the
project, name(s) of researcher(s), contact details of researchers and the supervisor, names of who
has access to the data, the length of time the data is to be stored, that participants have the right to
withdraw participation and data provided without penalty, and what the data will be used for
should all be clearly stated. A statement that the project has been reviewed approved by the
appropriate department and the University of Canterbury Human Ethics Committee Low Risk
Approval process should also be included.

43

Please ensure that Section A, B and C below are all completed
APPLICANT’S SIGNATURE:

Date

...16/05/2013..............................

A

SUPERVISOR DECLARATION:
1
2
3

I have made the applicant fully aware of the need for and the requirement of seeking HEC
approval for research involving human participants.
I have ensured the applicant is conversant with the procedures involved in making such an
application.
In addition to this form the applicant has individually filled in the full application form
which has been reviewed by me.

SIGNED (Supervisor):

B

Date ...............................

.

Date .16.5.2013

SUPPORTED BY THE DEPARTMENTAL/SCHOOL RESEARCH COMMITTEE:

Name .....Neville Churcher......................................................................

Signature: ......
2013..........
C

....

APPROVED BY HEAD OF DEPARTMENT/SCHOOL:

Name .....Tim Bell......................................................................

44

Date ...20th May

Signature: .............................................................

Date .23 May 2013...........

SUBMISSION OF APPLICATION:
 Please attach copies of any Information Sheet and Consent Form.
 Forward two hard copies to: The Secretary, Human Ethics Committee, Okeover House.

NOTES ON PROCEDURE:
 The Chair of the University of Canterbury Human Ethics Committee will review this
application.
 In normal circumstances queries will be forwarded via email to the applicant within 7 days.
 Please include a copy of this form as an appendix in your thesis or course work.

45

__________________________________________________________________________________
________

ACTION TAKEN BY HUMAN ETHICS COMMITTEE:



Added to Low Risk Reporting Database



Referred to University of Canterbury HEC



Referred to another Ethics Committee - please specify:
...............................................................................................

Reviewed by: ......................................................................
..................................................

46

Date

Appendix D:
HUMAN ETHICS COMMITTEE
Secretary, Lynda Griffioen
Email: human-ethics@canterbury.ac.nz

Ref: HEC 2013/67/LR

31 July 2013

Tegan Harrison
Department of Computer Science and Software Engineering
UNIVERSITY OF CANTERBURY

Dear Tegan
Thank you for forwarding your Human Ethics Committee Low Risk application for your research
proposal “The Emotiv mind: investigating the accuracy of the Emotiv EPOC in identifying
emotions and its use in intelligent tutoring system”.
I am pleased to advise that this application has been reviewed and I confirm support of the
Department’s approval for this project.
With best wishes for your project.
Yours sincerely

Lindsey MacDonald
Chair, Human Ethics Committee

47

Appendix E:
FOR STUDENT RESEARCH UP TO AND INCLUDING MASTERS LEVEL
ETHICAL APPROVAL OF LOW RISK RESEARCH INVOLVING
HUMAN PARTICIPANTS REVIEWED BY DEPARTMENTS
Please read the important notes appended to this form before completing the sections below
1

RESEARCHER’S NAME: Tegan Grace Harrison

2

NAME OF DEPARTMENT OR SCHOOL: Department of Computer Science and Software
Engineering

3

EMAIL ADDRESS: tgh28@uclive.ac.nz

4

TITLE OF PROJECT: The Emotiv mind: Investigating the accuracy of the Emotiv EPOC in
identifying emotions and its use in Intelligent Tutoring System.

5

PROJECTED START DATE OF PROJECT: 06/08/2013

6

STAFF MEMBER/SUPERVISOR RESPONSIBLE FOR PROJECT: Prof. Tanja Mitrovic

7

NAMES OF OTHER PARTICIPATING STAFF AND STUDENTS: Moffat Mathews

8

STATUS OF RESEARCH: (e.g. class project, thesis) Honours Research Project

9

BRIEF DESCRIPTION OF THE PROJECT:
Please give a brief summary (approx. 300 words) of the nature of the proposal in lay language,
including the aims/objectives/hypotheses of the project, rationale, participant description, and
procedures/methods of the project: The objective of this project is to investigate the use of the Emotiv EPOC and the software that
comes with it as a way to determine if the device is accurate in detecting user’s emotions. The
EPOC is a small, wireless consumer grade “brain-computer-interface” device that rests on the
head. It is designed for gaming and allows for a user’s emotions to be detected through their
thoughts. Our aim is to investigate how accurate this device is in detecting user’s emotions when
using the intelligent tutoring system SQL-tutor. Volunteers will receive a $20 café voucher
rewards for participation. The study will be done in the Department of Computer Science and
Engineering in the Erskine building in room 200. This study is open to anyone over 18 willing to
take part in this research. Participants will be COSC265 students. We aim to have approximately
10 participants. A short introduction to the research will be given at the start of the experiment,
and participants will receive a project description and a consent form which they will read and
sign, should they wish to participate in the study. Participants will then be told that they can elect
to opt out at any point during the experiment. The participants will then be equipped with the
device: this is simply a non-invasive headset with 14 sensors that sits on the head. Participants
will then be placed in front of a Tobii eye tracker computer. Participants will then be required to
complete a couple of eye calibration tests, where the participant is told to look at certain parts of
the screen in order for the computer to track their eyes accurately. Participants will then be
instructed that this is a “think-aloud” study in which they are required to state their current

48

emotional state out loud. Once this is completed participants will then be asked to log into SQLtutor and complete a couple of problems of their choosing. While participants are working on
problems they will be saying out loud what they are thinking and how they are feeling. While
participants are doing this the Emotiv EPOC will be measuring their emotional state and saving
this into a data file. The Tobii eye tracker is capturing their voice, facial expressions and
measuring their eye movements. This experiment should take no longer than 30 minutes.
10 WHY IS THIS A LOW RISK APPLICATION?
Description should include issues raised in the Low Risk Checklist.
Please give details of any ethical issues which were identified during the consideration of the
proposal and the way in which these issues were dealt with or resolved. :This is a low risk application because it does not raise any issue of deception, threat, invasion of
privacy, mental, physical or cultural risk or stress, and does not involve keeping personal
information of sensitive nature about individuals. All participants will be given an information
sheet describing the research project and will give consent for their results to be used for
analysis. The participants will be fully aware that they can withdraw themselves or their results
from the research at any stage. All the data will be kept in a secure environment and only those
listed above allowed access to it.
11 PROVIDE COPIES OF INFORMATION & CONSENT FORMS FOR PARTICIPANTS
These forms should be on University of Canterbury Departmental letterhead. The name of the
project, name(s) of researcher(s), contact details of researchers and the supervisor, names of who
has access to the data, the length of time the data is to be stored, that participants have the right to
withdraw participation and data provided without penalty, and what the data will be used for
should all be clearly stated. A statement that the project has been reviewed approved by the
appropriate department and the University of Canterbury Human Ethics Committee Low Risk
Approval process should also be included.

49

Please ensure that Section A, B and C below are all completed
APPLICANT’S SIGNATURE:

A

SUPERVISOR DECLARATION:
1
2
3

I have made the applicant fully aware of the need for and the requirement of seeking HEC
approval for research involving human participants.
I have ensured the applicant is conversant with the procedures involved in making such an
application.
In addition to this form the applicant has individually filled in the full application form
which has been reviewed by me.

SIGNED (Supervisor):

B

Date ..26/07/2013

.

Date .26.7.2013

SUPPORTED BY THE DEPARTMENTAL/SCHOOL RESEARCH COMMITTEE:

Name: Prof. Krys Pawlikowski

Signature:
C

Date 26.07.2013

APPROVED BY HEAD OF DEPARTMENT/SCHOOL:

Name ...Prof. Tim Bell.....................................................................

Signature:

Date 27 July 2013

SUBMISSION OF APPLICATION:
 Please attach copies of any Information Sheet and Consent Form.
 Forward two hard copies to: The Secretary, Human Ethics Committee, Okeover House.

NOTES ON PROCEDURE:

50





The Chair of the University of Canterbury Human Ethics Committee will review this
application.
In normal circumstances queries will be forwarded via email to the applicant within 7 days.
Please include a copy of this form as an appendix in your thesis or course work.

51

__________________________________________________________________________________
________

ACTION TAKEN BY HUMAN ETHICS COMMITTEE:



Added to Low Risk Reporting Database



Referred to University of Canterbury HEC



Referred to another Ethics Committee - please specify:
...............................................................................................

Reviewed by: ......................................................................
..................................................

52

Date

Appendix F:
HUMAN ETHICS COMMITTEE
Secretary, Lynda Griffioen
Email: human-ethics@canterbury.ac.nz

Ref: HEC 2013/67/LR

31 July 2013

Tegan Harrison
Department of Computer Science and Software Engineering
UNIVERSITY OF CANTERBURY

Dear Tegan
Thank you for forwarding your Human Ethics Committee Low Risk application for your research
proposal “The Emotiv mind: investigating the accuracy of the Emotiv EPOC in identifying
emotions and its use in intelligent tutoring system”.
I am pleased to advise that this application has been reviewed and I confirm support of the
Department’s approval for this project.
With best wishes for your project.
Yours sincerely

Lindsey MacDonald
Chair, Human Ethics Committee

53

Appendix G
Department of Computer Science and Software Engineering
Telephone: +64 3578422
Email: tgh28@uclive.ac.nz

The Emotiv mind: Investigating the accuracy of the Emotiv EPOC
in identifying emotions and its use in Intelligent Tutoring System.
Project Consent Form for participants:
I have been given a full explanation of this project provided in the project description and
have had the opportunity to ask questions. I understand what is required of me if I agree to
take part in the research. I understand that participation is voluntary and I may withdraw at
any time without penalty. Withdrawal of participation will also include the withdrawal of
any information I have provided should this remain practically achievable.
I understand that any information or opinions I provide will be kept confidential to the
researcher and also the researcher’s supervisor Prof. Tanja Mitrovic and the fellow
researcher on this project, Dr. Moffat Mathews and that any published or reported results
will not identify the participants. I understand that a thesis is a public document and will be
available through the UC library.
I understand that all data collected will be kept in locked and secure facilities and in
protected password electronic form and will be destroyed after five years. I understand the
risks associated with taking part and how they will be managed. I understand that I am able
to receive a report on the findings of the study by contacting the researcher at the
conclusion of the project.
I understand that I can contact Tegan Harrison, tgh28@uclive.ac.nz or Prof. Tanja
Mitrovic, tanja.mitrovic@canterbury.ac.nz for further information. If I have any complaints,
I can contact the Chair of the University of Canterbury Human Ethics Committee, Private Bag
4800, Christchurch (human-ethics@canterbury.ac.nz).
By signing below, I agree to participate in this research project.
Name:
Signed:
Date:
Please return this consent form to the researcher,
Tegan Harrison

54

Appendix H

Department of Computer Science and Software Engineering
Telephone: +64 3578422
Email: tgh28@uclive.ac.nz
28th May 2013

The Emotiv mind: Investigating the accuracy of the Emotiv EPOC in identifying
emotions and its use in Intelligent Tutoring System.
Project Description for participants:
I am Tegan Harrison and I am an honours student majoring in computer science at the
University of Canterbury. This research project is part of my honours research project in
order to determine the effectiveness of the Emotive EPOC headset in detecting emotions.
Thank you for participating in this study. This study will take approximately 50 minutes to
complete. Firstly you will be expected to fill out a short demographic questionnaire and
emotional survey. Then you will be fitted with the Emotiv EPOC headset device. This device
consists of 14 sensors that measure your brainwaves in order to detect your emotions. This
is a wireless device and perfectly safe to use. Once the device has been fitted you will be
seated before a screen and will be required to complete some short eye calibration tests.
Then you will be shown a series of photographs on screen in blocks of 12 at a time. After
each block you will receive a short 15 second rest. There will be 5 blocks in total. After
viewing each photo you will be asked to record your current emotional state by selecting an
answer on screen. You will be asked to select at least one type of emotion but may select
more than one. Once you have finished viewing the photos you will then be asked to fill out
a short post-test questionnaire. You may receive a copy of the results by contacting the
researcher at the conclusion of the project.
There are no foreseeable risks in participating in this research project. Participant of this
research is voluntary and you have the right to withdraw at any stage without penalty. If
you withdraw, I will remove information relating to you once the data is made anonymous.
The results of the project may be published, but you may be assured of the complete
confidentiality of data gathered in this investigation: your identity will not be made public
without your prior consent. To ensure anonymity and confidentiality, you will be assigned a
random id number by which each participant will anonymously be identified. Tegan
Harrison will have access to the data as well as Prof. Tanja Mitrovic and Dr Moffat Mathews;

55

the data will be stored on a secured machine that requires authenticated password to
access. The data will be destroyed after five years. A thesis is a public document and will be
available through the UC Library
This project is being carried out by Tegan Harrison, who is postgraduate student at the
University of Canterbury. This study is being conducted for her honours research project
under the supervision of Dr Tanja Mitrovic (tanja.mitrovic@canterbury.ac.nz). She will be
pleased to discuss any concerns you may have about participation in the project.

This project has been reviewed and approved by the University of Canterbury Human Ethics
Committee, and participants should address any complaints to The Chair, Human Ethics
Committee, University of Canterbury, Private Bag 4800, Christchurch (humanethics@canterbury.ac.nz).
If you agree to participate in the study, you are asked to complete the consent form and
return it to Tegan.
Tegan Harrison

56

Appendix I
Department of Computer Science and Software Engineering
Telephone: +64 3578422
Email: tgh28@uclive.ac.nz

The Emotiv mind: Investigating the accuracy of the Emotiv EPOC in identifying
emotions and its use in Intelligent Tutoring System.
Project Consent Form for participants:
I have been given a full explanation of this project provided in the project description and
have had the opportunity to ask questions. I understand what is required of me if I agree to
take part in the research. I understand that participation is voluntary and I may withdraw at
any time without penalty. Withdrawal of participation will also include the withdrawal of
any information I have provided should this remain practically achievable.
I understand that any information or opinions I provide will be kept confidential to the
researcher and also the researcher’s supervisor Prof. Tanja Mitrovic and the fellow
researcher on this project, Dr. Moffat Mathews and that any published or reported results
will not identify the participants. I understand that a thesis is a public document and will be
available through the UC library.
I understand that all data collected will be kept in locked and secure facilities and in
protected password electronic form and will be destroyed after five years. I understand the
risks associated with taking part and how they will be managed. I understand that I am able
to receive a report on the findings of the study by contacting the researcher at the
conclusion of the project.
I understand that I can contact Tegan Harrison, tgh28@uclive.ac.nz or Prof. Tanja
Mitrovic, tanja.mitrovic@canterbury.ac.nz for further information. If I have any complaints,
I can contact the Chair of the University of Canterbury Human Ethics Committee, Private Bag
4800, Christchurch (human-ethics@canterbury.ac.nz).
By signing below, I agree to participate in this research project.
Name:
Signed:
Date:
Please return this consent form to the researcher,
Tegan Harrison

57

Appendix J
Department of Computer Science and Software Engineering
Telephone: +64 3578422
Email: tgh28@uclive.ac.nz
28th May 2013

The Emotiv mind: Investigating the accuracy of the Emotiv EPOC in identifying
emotions and its use in Intelligent Tutoring System.
Project Description for participants:
I am Tegan Harrison and I am an honours student majoring in computer science at the
University of Canterbury. This research project is part of my honours research project in
order to determine the effectiveness of the Emotive EPOC headset in detecting emotions
when using an intelligent tutoring system.
Thank you for participating in this study. This study will take approximately 30 minutes to
complete. Firstly you will be fitted with the Emotiv EPOC headset device. This device
consists of 14 sensors that measure your brainwaves in order to detect your emotions. This
is a wireless device and perfectly safe to use. Once the device has been fitted you will be
seated before a screen and will be required to complete some short eye calibration tests.
You will then receive a description on how this is a “think-aloud” study in which you are
required to say how you are feeling and what you are thinking about out loud. You will then
log onto SQL-tutor and work on a couple of questions of your choosing. While you are
working on the problems you will be required to state how you are feeling and what you are
thinking out loud and your voice will be recorded. You may receive a copy of the results by
contacting the researcher at the conclusion of the project.
There are no foreseeable risks in participating in this research project. Participation of this
research is voluntary and you have the right to withdraw at any stage without penalty. If
you withdraw, I will remove information relating to you once the data is made anonymous.
The results of the project may be published, but you may be assured of the complete
confidentiality of data gathered in this investigation: your identity will not be made public
without your prior consent. To ensure anonymity and confidentiality, you will be assigned a
random id number by which each participant will anonymously be identified. Tegan
Harrison will have access to the data as well as Prof. Tanja Mitrovic and Dr Moffat Mathews;
the data will be stored on a secured machine that requires authenticated password to

58

access. The data will be destroyed after five years. A thesis is a public document and will be
available through the UC Library
This project is being carried out by Tegan Harrison, who is postgraduate student at the
University of Canterbury. This study is being conducted for her honours research project
under the supervision of Dr Tanja Mitrovic (tanja.mitrovic@canterbury.ac.nz). She will be
pleased to discuss any concerns you may have about participation in the project.
This project has been reviewed and approved by the University of Canterbury Human Ethics
Committee, and participants should address any complaints to The Chair, Human Ethics
Committee, University of Canterbury, Private Bag 4800, Christchurch (humanethics@canterbury.ac.nz).
If you agree to participate in the study, you are asked to complete the consent form and
return it to Tegan.
Tegan Harrison

59

