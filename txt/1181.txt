Special Issue on 3D Medicine and Artificial Intelligence

Brain Computer Interface for Micro-controller Driven
Robot Based on Emotiv Sensors
Parth Gargava1, Krishna Asawa2 *
Centre for Development of Advanced Computing (India)
Jaypee Institute of Information Technology (India)

1
2

Received 31 August 2016 | Accepted 1 November 2016 | Published 23 December 2016

Abstract

Keywords

A Brain Computer Interface (BCI) is developed to navigate a micro-controller based robot using Emotiv sensors.
The BCI system has a pipeline of 5 stages- signal acquisition, pre-processing, feature extraction, classiﬁcation
and CUDA inter- facing. It shall aid in serving a prototype for physical movement of neurological patients who
are unable to control or operate on their muscular movements. All stages of the pipeline are designed to process
bodily actions like eye blinks to command navigation of the robot. This prototype works on features learning
and classiﬁcation centric techniques using support vector machine. The suggested pipeline, ensures successful
navigation of a robot in four directions in real time with accuracy of 93 percent.

Emotiv, Brain Computer
Interface, Support
Vector Machine, Robot,
Microcontroller.

I. Introduction

B

rain computer interfaces are systems which act as a communication
channel between the human brain and an external device. In the ﬁrst
international meeting on Brain Computer Interface (BCI) technology,
BCIs have been de- ﬁned as an aid to users’ communication and also
control channels which do not vary on the brain’s normal output of
peripheral nerves and muscles [1]. Brain Computer Interfaces is a
growing ﬁeld which has added a new dimension to Human Computer
Interaction. BCI development comes mainly from the concern of
creating a novel interaction channel between users, especially those
who are unable to control or operate on their muscular movements.
The motivation for this proposal is to work for people who have
neurological disorder such as Amyotrophic Lateral Sclerosis (ALS)
[2], referred to as Lou Gehrig’s disease, Dystonia and Ataxia. Patients
suffering from such severe disorders are unable to perform any
muscular movement and in order to help them, we have devised this
prototype which shall help the patients in physical movements in the
form of a wheel chair [3]. Brain Computer Interface systems acquire
signals in the form of energy potential (electroencephalogram signals)
which are processed and sent to an external device. Previously, noninvasive brain computer interfaces’ systems have signiﬁcantly reduced
labor and cost, such as in BCI2000 which facilitates applications for
various domains such as biomedical engineers, computer scientists,
environment and investigators [4]. Another system has been developed
which is a software controller that is matched with the individual’s
residual motor abilities [5]. It was a rehabilitation program carried out
in a house-like simulation. In current progress, F.Gallan, M. Nuttin and
E. Lew have developed an asynchronous and non-invasive which tested
patients through experiments to work on brain computer interface in

* Corresponding author.
E-mail addresses: parthstein@gmail.com (Parth Gargava),
krishna.asawa@jiit.ac.in (Krishna Asawa).

DOI: 10.9781/ijimai.2017.457

an environment which is complex and aids sound analysis [6]. In a
recent developed system of brain computer interface in the University
of Bremen, the makers of the project made a human machine interface
(HMI)semi-autonomous robot by the name of FRIEND II which was
executed and compiled by the MASSiVE control architecture [7].
Learning from these projects, this paper present a brain computer
interface system where we are applying machine learning technique,
support vector machine to the visual patterns of the signals to navigate
a robot using an Arduino Duemilanove microcontroller [8]. In order
to extract signals for the brain computer interface, we use an Emotive
Headset which captures neuro-signals [9]. After fetching signals
from the brain, we de-noise or ﬁlter the noise and unwanted signal
interruptions from the input signals. Later we apply Support Vector
Machine algorithm to classify signals to intents of our choice in
order to navigate a robot. We used an Arduino Duemilanove microcontroller to transfer the output signals in order to control the robot
using imagined movements.

II. Related Work
A brief overview of the work and research done on brain computer
interface systems helps us gauge the current development.
It has been used in 2003 as a clinical application to study patients
with motor impairment [10]. Their objective was to see whether
a patient with a severe cerebral palsy was able to control a brain
computer interface system. The research concluded with the patients
being able to produce distinct EEG patterns with an accuracy of 70
percent. This technique based on EEG biofeedback helped improve
actual levels of communication ability for patients with physical
disorders. This application with the use of ’tele monitoring- assisted’
BCI facilities served as a motivation to complete this project. In 2000,
University of California, San Diego conducted a research which
explored imagination on mu rhythms and readiness potentials which
yielded to major classiﬁcation using pattern recognition techniques
[11].The authors of the research concluded that mu rhythm can not

- 39 -

International Journal of Interactive Multimedia and Artificial Intelligence, Vol. 4, Nº 5
only be changed or modulated by self-generated movement but by
virtually imagining the movement. The study also concludes that
self-generated limb actions have distinct properties from single limb
movements. Such kind of practical BCI systems use identiﬁcation and
its classifying with pattern recognition techniques.
In the initial days of brain computer interface, volunteers were
asked in 2004 to navigate through a 2 dimensional maze using their
thought processes which was further aided using high ﬁeld MRI
scanner [12]. The authors understood real time fMRI to construe the
spatial distribution of brain functions as commands of brain computer
interface. With a high ﬁeld MRI scanner, activities of the brain were
segregated into 4 distinct functional tasks which were translated
into commands for four directional cursors. This work helped us in
understanding how to divide signals in four patterns.
In 2003, Georgia State university brainLab validated brain computer
interface interactions with respect to real life ap- plications such as in
communication, creative expression and environment [13]. The authors
used methods of Human Com- puter Interaction to train brain computer
interface systems to real world scenarios wherein they also discuss
challenges of having cumulative BCI outputs with human computer
interface paradigms to achieve best interaction. This research used
and tested various domains to validate interactions completed by BCI
systems. This paper motivated us to improve and enhance the quality
of life for those with neurological disorders such as ALS.
BCI2000 was a system developed in 2004 which could conduct
multiple applications using one system [4]. Such an application varies
on comparisons of different brain signals, algorithms and its output
formats. BCI2000 can successfully aid biomedical engineers, computer
scientists and people from various professions in the practical usage
of an online operation of brain computer interface. Its documentation
helped us understand the multi-domain features of existing brain
computer interface.
Initially, two macaque monkeys were trained to perform movements
related to limbs [14]. Using bilateral electromyography, the authors
concluded the presence of a neural representation which can be
successfully out in a brain-machine interface. Monkeys have been used
in other projects too where they have completed tasks successfully
using Brain computer interface systems [15].

to the other Emotiv Control Panel tabs. Poor contact quality will
result in poor quality EEG signals
3. Emotiv test bench: Real-time display of the Emotiv head- set
data stream, including EEG, contact quality, FFT, gyro (if ﬁtted
custom option), wireless packet acquisition/loss display, marker
events, headset battery level Record and replay ﬁles in binary
EEGLAB format. Command line ﬁle converter included to
produce .csv format. Deﬁne and insert timed markers into the
data stream, including on- screen buttons and deﬁned serial port
events. Markers are stored in EEG data ﬁle. It is used to check
EEG quality by verifying eye blinks and alpha rhythms.
4. Open ViBe [17]: The signals acquired from the sensors are then
ﬁltered in Open ViBe scenarios which helps in processing of
the input. Open ViBe acts as a base for signal processing of the
fetched input.
5. MATLAB [18]: After processing signals we used support vector
machine algorithm using LIBSVM library to pro- cess the input
signals to intents of our choice. The intents were classiﬁed to
numerals which help in providing input to Arduino micr-controller.
6. LIBSVM [19]: LIBSVM is an open source machine learning
library which is written in C++, and it implements SMO algorithm
for kernelised support vector machines for use of classiﬁcation and
regression. An SVM also uses a discriminant hyperplane to identify
classes. However, concerning SVM, the selected hyperplane is the
one that maximizes the margins, i.e., the distance from the nearest
training points (see Figure 1). Maximizing the margins is known
to increase the generalization capabilities. As RFLDA, an SVM
uses a regularization parameter C that enables accommodation
to outliers and allows errors on the training set. Such an SVM
enables classiﬁcation using linear decision boundaries, and is
known as linear SVM. This classiﬁer has been applied, always
with success, to a relatively large number of synchronous BCI
problems. However, it is possible to create nonlinear decision
boundaries, with only a low increase of the classiﬁers complexity,
by using the kernel trick. It consists in implicitly mapping the data
to another space, generally of much higher dimensionality, using a
kernel function K(x, y). The kernel generally used in BCI research
is the Gaussian or Radial Basis Function (RBF) kernel.

In the review of the ﬁrst international meeting on brain computer
interface technology, the members discussed that the pivotal element
in every brain computer interface system is the algorithm which
converts or translates electro physiological input from a given set of
users to control external devices. Existing BCI systems work on the
transformation rate of 10- 25 b/min [1] and such achievement can be
enhanced using only improved signal processing, user training and
more efﬁcient algorithms.

III. Tools and Technology Used
1. Emotiv headset [16]: A sensor headset by Emotiv is used for
practical and commercial usage in research applications for brain
computer interface. Emotiv heaset in our project is used to detect
libraries such as mental commands, and facial expressions. The 14
EEG channel locations are AF3, F7, F3, FC5, T7, P7, O1, O2, P8,
T8, FC6, F4, F8, AF4. In our project we used Emotiv headset to
measure facial expression in the form of eye blinks.
2. Emotiv control panel: The Headset Setup panel is dis- played by
default when starting Emotiv Control Panel. The main function
of this panel is to display contact quality feedback for the
neuroheadsets EEG sensors and provide guidance to the user in
ﬁtting the neuroheadset correctly. It is extremely important for the
user to achieve the best possible contact quality before proceeding
- 40 -

Fig. 1. Support Vector Machine Hyperplane.

7. Arduino Duemilanove [8]: The Arduino Duemilanove is a microcontroller which is powered by any USB connection or with
external supply. These kits help in developing digital devices for
the physical world. In our project we have developed the microcontroller to build a robot for its navigation using imagined
movements.

Special Issue on 3D Medicine and Artificial Intelligence

Fig. 2. Arduino micro-controller.

8. CUDA (Compute Uniﬁed Device Architecture)[20]: We used a dll
ﬁle of CUDA where the code was imported. The CUDA platform
was used to enhance the efﬁcacy of the system while using in real
time and remove time lag. Our project of a BCI system followed
the given chronology for execution:
• Fetching of signals from Emotiv Epoch sensors:
Here we use sensors to acquire brain signals (in the form of
EEG) which also includes articulates, noise and unwanted
elements [21].
• Filtering and signal processing:
In order to utilise data received from brain, it is imperative that
we remove all noise from the signals which may have come
possibly due to sweat, heart beats or hair on the scalp. This
achieved using ﬁlters in the Open ViBe platform [22].
• Extraction of signiﬁcant features:
This allows signiﬁcant parts of signals acquired to be
discriminated and differentiated from others as features in our
Brain Computer Interface system [23].
• Classiﬁcation of signals:
In this process a set of features are assigned classes which can
be in correspondence to the navigation required from brain
signals.
• CUDA processing and interfacing:
In this step we interface all platforms to get the application
working, and enhance the speed of the application by
interfacing the ﬁnal end user application on a CUDA platform
[20].

and multiple channeled neuro-head-gear which has 14 sensors to detect
electric signals detected from brain. It is used to acquire EEG signals.
The Emotiv headset follows International10-20 locations: AF3, F7, F3,
FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4 [26]. A BCI simulator
is conﬁgured to present the visual stimulus to the user while the EEG
data is being collected. The simulator speciﬁed the number classes and
trials for each class, along with the limits for the duration of trials. For
training the classiﬁer, eye movements were deﬁned as the classes and
30 trials were conducted for each class in a random sequence, with
each trial lasting from 1.5-3.5 seconds. For collecting labelled EEG
data Graz motor Imagery BCI simulator is used which displays certain
arrows according to the number of classes deﬁned and on each arrow
an intent has to be performed. The acquisition client collected the EEG
data while the stimulus was presented to the user using the simulator.
Event markers indicate the time instants to mark the beginning and end
of trials. The data acquired was passed to a visualizer and also written
into a generic stream writer module that dumped the OpenVibe stream
to a binary ﬁle, with markers indicating the start and end of a trial, the
appearance of the arrow and the ﬁxation cross.

B. Pre-processing
For de-noising of data, the emotiv provides software which
completes basic signal processing at a threshold frequency of 85Hz,
which is then continued by applying a high pass ﬁlter with a threshold
value of 0.15 Hz. For pre- processing the EEG signals, the Emotiv
headset provides basic signal processing which includes low pass
ﬁltering of the EEG data with a cut-off frequency at 85 Hz, followed
by a high pass ﬁlter with a cut-off at 0.16 Hz. Finally, a notch ﬁlter
is applied at 50-60 Hz is applied to remove the noise due to supply
lines interference. Further, our application involves identiﬁcation of
the following intents: eye blinks, right eye wink and left eye wink.
These intents are identiﬁed by EEG activity in the alpha frequency
band, hence we perform band-pass ﬁltering of the EEG signals in the
range of 1 to 4 Hz.
In our application, butterworth ﬁlter is used for band- passing the
signals generated.
A Butterworth ﬁlter of order n at threshold frequency D0 has the
frequency as depicted in Equation 1.

1

Various challenges were faced by us in developing an application
based on Brain Computer Interface system such as:
• Acquisition of brain signals from the sensor device Emotiv
Epoch requires an expert who has used such sensors before.
A key challenge for us was to set up the device in order to get
valuable output [9].
• The delay in transmission of messages after classifying was
more than 2 seconds (before applying CUDA) which made
it difﬁcult for us to read the ﬁnal output since they were not
corresponding to the input we were giving in a real time
scenario [24].

IV. Brain Computer Interface System Processing Steps to
End User Application

(1)
C. Feature Extraction and Classification
This section discusses our feature extraction techniques where we
use band power of signals to differentiate signals from each other. The
reason why we used band power features and visual patterns against
traditional features is given in [27].
Logarithmic value of band power features is taken to derive the EEG
signals as X, the feature matrix vector as FV of a size of [channelsX1]
is calculated as given in Equation 2.

(2)

In this section, the paper discusses various steps of a brain computer
interface pipeline.
We discuss fetching and acquisition of brain signals in sub-section IVA, followed by de-noising methods in sub- section IV-B, extraction of
valuable features and its classiﬁcation in sub-section IV-C, subsequent
classiﬁcation of signals in sub-section IV-D.

A. EEG Signal Acquisition Paradigm
Signals are fetched by an Emotiv EPOC [25] which is a wireless

The band-power of the EEG signal is used as a fea- ture, which
is simple to compute and computationally inexpensive. Computation
of band-power as a feature has been previously used in BCI systems.
We compute the band-power features from EEG signals of a trial,
represented as a matrix X of size [channelsXsamples] (where channels
represent the number of channels of the recorded EEG data and samples
is determined by the sampling rate of the EEG acquisition device
multiplied by the length of the time window in which the trial data
is taken) After obtaining features we used LIBSVM for classiﬁcation

- 41 -

International Journal of Interactive Multimedia and Artificial Intelligence, Vol. 4, Nº 5
which is an open-source implementation of Support Vector Machine
[28]. There is comparison of classiﬁcation accuracies using two
classiﬁers: LDA and SVM. In case of multi-class LDA the accuracy is
low and so to improve multi-class classiﬁcation accuracy SVM is used.
The trained classiﬁer is used to identify eye blinks in an online session
for the subject.

• Acquisition of signals from device- The input signals were fetched
from Emotiv Epoch sensor by 10 users where each one of them
took 20 readings each of every intent (left eye blink, right eye
blink, both eyes blink, neutral state). Out of the 800 recordings, we
were successfully able to catch 785 recordings, which gives us the
accuracy of 98 percent.

D. Transferring Signals to a Robot

• De-noising and ﬁltering of data- In this step, we took all 800
signals received by multiple users and converted into meaningful
data by applying band pass ﬁlter. All signals were successfully denoised giving us an accuracy of 100 percent.

Once we fetched the signals from an acquisition device, and process
it on MATLAB, we interface it on a CUDA platform to remove data
lag and then navigate a simple robot using Arduino, an open source
development plat- form [29]. We accelerated our C++ code by moving
the computationally intensive portions of code to an NVIDIA GPU.
In addition to providing drop-in library acceleration, we were able to
efﬁciently access the massive parallel power of a GPU with a some
syntactic elements and calling functions from the CUDA Runtime
API such as global kernel function. In this step we also assigned the
ﬁnal output of the classiﬁer to control an external device. We sent the
output to an Arduino which is used to control the movement of a servo
motor. After obtaining processed signals from Matlab (with the use
of OpenViBe), we use the data to move a simple robot with the help
of arduino, an open source development platform . In this project, an
Arduino Due Milanove was used with two motors, wheels and wires.
The robot followed the output which was fed through MATLAB into
the Arduino once a connection was built using a USB cable.

• Feature extraction and classiﬁcation- Using band power features,
we applied support vector machine algorithm using LIBSVM to
classify intents. out of the 800 signals, we were successfully able to
classify 730 input signals which gives us an accuracy of 91 percent.
• Output on Arduino platform through MATLAB- After processing
and classiﬁcation, out of the 730 signals which were given as input
to the Arduino Due Milanove micro-controller, only 681 signals
were converted into successful actions by the robot for navigation.
The basic challenge in this step was the interfacing with a
considerable time lag ( 6 seconds) of MATLAB, Arduino platform
and Open ViBe in a real time system. This challenge was met by
integrating on a CUDA platform which using parallel processing
made the system faster by reducing the time lag to 0.5 seconds. The
accuracy achieved in this step is 93 percent.

The results and test cases are shown in Table I.

• Over all accuracy of the system is the sum average of all above
mentioned accuracies ie. 95 percent. This result is discussed in
Table 1.

TABLE I
Results and Accuracy
TESTs

MODULE

SUCCESS (in %)

0

Acquisition of signals from device

98

1

De-noising and filtering data

100

2

Feature extraction and classification

91

3

Output on Arduino platform through MATLAB

93

5

Overall system

93

VI. Conclusion and Future Work
This project provides a real-time, easy to use imple- mentation
of a Brain Computer Interface system which can identify multiple
states without a time lag of more than 0.5 seconds. This system does
not require individual training, and has high accuracies in achieving
classiﬁca- tion. Its future work involves development of medical aid
for those with neurological diseases based on EEG signals for different
intents.

Acknowledgment
This work would not have been possible without the support of
Centre for Development of Advanced Computing, Noida for their help
in the usage of Emotiv headset.

References
[1]

[2]
[3]

Fig. 3. Process implementation and framework.
[4]

V. Result and Accuracy
In this section, we discuss the tests undertaken at each step of the
BCI pipeline and detailed summary of test cases:

[5]

- 42 -

Jonathan R Wolpaw, Niels Birbaumer, William J Heetderks, Dennis
J McFarland, P Hunter Peckham, Gerwin Schalk, Emanuel Donchin,
Louis A Quatrano, Charles J Robinson, Theresa M Vaughan, et al.Braincomputer interface technology: a review of the ﬁrst international meeting.
IEEE transactions on rehabilitation engineering, 8(2):164–173, 2000.
Amyotrophic Lateral Sclerosis ALS. Amyotrophic lateral sclero- sis (als).
2007.
John T Dimos, Kit T Rodolfa, Kathy K Niakan, Laurin M Weisenthal,
Hiroshi Mitsumoto, Wendy Chung, Gist F Croft, Genevieve Saphier,
Rudy Leibel, Robin Goland, et al. Induced pluripotent stem cells
generated from patients with als can be differentiated into motor neurons.
science,321(5893):1218–1221, 2008.
Gerwin Schalk, Dennis J McFarland, Thilo Hinterberger, Niels
Birbaumer, and Jonathan R Wolpaw. Bci2000: a general-purposebraincomputer interface (bci) system. Biomedical Engineering, IEEE
Transactions on, 51(6):1034–1043, 2004.
Febo Cincotti, Donatella Mattia, Fabio Aloise, Simona Bufalari, Gerwin
Schalk, Giuseppe Oriolo, Andrea Cherubini, Maria Grazia Marciani, and

Special Issue on 3D Medicine and Artificial Intelligence

[6]

[7]

[8]
[9]

[10]

[11]

[12]

[13]
[14]

[15]
[16]

[17]

[18]
[19]
[20]
[21]

[22]
[23]

[24]
[25]

Fabio Babiloni. Non-invasive brain–computer interface system: towards
its application as assistive technology. Brain research bulletin, 75(6):796–
803, 2008.
Ferran Galan,´ Marnix Nuttin, Eileen Lew, Pierre W Ferrez, Gerolf
Vanacker, Johan Philips, and J del R Millan´. Abrain-actuated wheelchair:
asynchronous and non-invasivebrain–computer inter- faces for continuous
control of robots. Clinical Neurophysiology, 119(9):2159–2169, 2008.
Diana Valbuena, Marco Cyriacks, Ola Friman, Ivan Volosyak, and Axel
Graser. Brain-computer interface forhigh-level control of rehabilitation
robotic systems. In Rehabilitation Robotics, 2007. ICORR 2007. IEEE
10th International Conference on, pages 619– 625. IEEE, 2007.
Alicia M Gibb. New media art, design, and the Arduino micro- controller:
A malleable tool. PhD thesis, Pratt Institute, 2010.
Matthieu Duvinage, Thierry Castermans, Mathieu Petieau, Thomas
Hoellinger, Guy Cheron, and Thierry Dutoit. Perfor- mance of the emotiv
epoc headset for p300-based applications. Biomed Eng Online, 12:56,
2013.
C Neuper, GR Muller,¨ A Kubler,¨ N Birbaumer, and G Pfurtscheller.
Clinical application of an eeg-based brain– computer interface:
a case study in a patient with severe motor impairment. Clinical
neurophysiology, 114(3):399–409, 2003.
Jaime A Pineda, BZ Allison, and A Vankov. The effects of self- movement,
observation, and imagination on rhythms and readi- ness potentials (rp’s):
toward a brain-computer interface (bci). Rehabilitation Engineering, IEEE
Transactions on,8(2):219–222, 2000.
Seung-Schik Yoo, Ty Fairneny, Nan-Kuei Chen, Seh-Eun Choo, Lawrence
P Panych, HyunWook Park, Soo-Young Lee, and Ferenc A Jolesz. Brain–
computer interface using fmri: spatial navigation by thoughts.
Neuroreport, 15(10):1591–1595, 2004.
Melody M Moore. Real-world applications for brain-computerinterface
technology. Neural Systems and Rehabilitation Engi- neering, IEEE
Transactions on, 11(2):162–165, 2003.
Karunesh Ganguly, Lavi Secundo, Gireeja Ranade, Amy Ors- born,
Edward F Chang, Dragan F Dimitrov, Jonathan D Wallis, Nicholas M
Barbaro, Robert T Knight, and Jose M Carmena. Cortical representation
of ipsilateral arm movements in monkey and man. The Journal of
Neuroscience, 29(41):12948–12956, 2009.
Michele Baum. Monkey uses brain power to feed itself with robotic arm.
Pitt Chronicle. http://www. chronicle. pitt. edu, 2008.
Matthieu Duvinage, Thierry Castermans, Thierry Dutoit, M Petieau,
T Hoellinger, C De Saedeleer, K Seetharaman, and G Cheron. A p300based quantitative comparison between a medical eeg device and emotive
headset.
Yann Renard, Fabien Lotte, Guillaume Gibert, Marco Congedo,
Emmanuel Maby, Vincent Delannoy, Olivier Bertrand, and Ana- tole
Lecuyer´. Openvibe: an open-sourcesoftware platform to design, test, and
use brain-computerinterfaces in real and virtual environments. Presence:
teleoperators and virtual environments, 19(1):35–53, 2010.
Duane Hanselman and Bruce C Littleﬁeld. Mastering MATLAB 5: A
comprehensive tutorial and reference. Prentice Hall PTR, 1997.
C Chang and C Lin. fLIBSVMg: a library for support vector machines
(version 2.3). 2001.
CUDA Nvidia. Compute uniﬁed device architecture programming guide.
2007.
Benjamin Blankertz, Guido Dornhege, Matthias Krauledat, Klaus- Robert
Muller,¨ and Gabriel Curio. The non-invasive berlin brain– computer
interface: fast acquisition of effective performance in untrained subjects.
NeuroImage, 37(2):539–550, 2007.
Ali Bashashati, Mehrdad Fatourechi, Rabab K Ward, and Gary E Birch. A
survey of signal processing algorithms in brain– computer interfaces based
on electrical brain signals. Journal of Neural engineering, 4(2):R32, 2007.
Vladimir Bostanov. Bci competition 2003-data sets ib and iib: feature
extraction from event-related brain potentials with the con- tinuous
wavelet transform and the t-value scalogram. Biomedical Engineering,
IEEE Transactions on, 51(6):1057–1061, 2004.
Gopal Santhanam, Stephen I Ryu, M Yu Byron, Afsheen Afshar, and
Krishna V Shenoy. A high-performancebrain–computer interface.
nature, 442(7099):195–198, 2006.
Robert Lievesley, Martin Wozencroft, and David Ewins. The emotiv
epoc neuroheadset: an inexpensive method of controlling assistive

[26]
[27]

[28]
[29]

technologies using facial expressions and thoughts?Journal of Assistive
Technologies, 5(2):67–82, 2011.
Richard W Homan, John Herman, and Phillip Purdy. Cerebral location of
international 10–20 system electrode placement. Elec- troencephalography
and clinical neurophysiology, 66(4):376–382, 1987.
George Townsend, Bernhard Graimann, and Gert Pfurtscheller. A
comparison of common spatial patterns with complex band power features
in a four-class bci experiment. Biomedical Engineering, IEEE Transactions
on, 53(4):642–651, 2006.
Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM Transactions on Intelligent Systems and
Technology (TIST), 2(3):27, 2011.
Michael Margolis. Make an Arduino-controlled robot. ” O’Reilly Media,
Inc.”, 2012.
Parth Gargava
Parth Gargava is Research and Development Engineer at
Centre for development of Advanced Computing. He is
also Vice President from Jaypee Institute of Inormation
Technology, Student Council. He got is bachelor degree in
Technologi at Jaypee institute of information technology.
He is currently working in his doctoral thesis.
Dr. Krishna Asawa

Dr. Krishna Asawa working with Jaypee Institute of
Information Technology, Noida, India since 2003. During
her teaching career of about 16 years, she pursued and
experienced teaching subjects like Data Structures,
Algorithm Analysis and Design, Artificial Intelligence,
Soft Computing, Distributed Databases, Computer
Architecture and Organization, Logic Programming,
Theory of Computation, Database System, Human Computer Interaction,
Cryptography, Management Information System. Apart from teaching, she is
an active researcher and currently advisor of 4 research scholars. She is also a
member of technical program committee of referred Journals and Conference
Proceedings. She is actively involved in curriculum planning, syllabus design,
editing and reviewing text books at an engineering graduate level. She has been
an organizing member of workshops and conferences, organized by the CSE
department of JIIT. She has also served as an external examiner and adjunct
instructor at the other Universities.

- 43 -

