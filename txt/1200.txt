Journal of Computer Science
Original Research Paper

A Novel Approach to Wheelchair Design and Operation Using
Multi-Function Controller
Jamal I. Al-Nabulsi
Department of Medical Engineering, Al-Ahliyya Amman University, Amman, Jordan
Article history
Received: 05-05-2020
Revised: 04-07-2020
Accepted: 23-07-2020
Email: j.nabulsi@ammanu.edu.jo

Abstract: An electric multi-function controlled wheelchair is designed
and fully tested. This wheelchair is controlled by several physiological
variables namely; voice, head movement, finger bending, breathing
pressure and Electrooculography (EOG). The patient has the choice to
use any of these variables to control the wheelchair. The voice
command is recorded by a voice recognition module with its
microphone, whereas, the head and finger motion operate through the
gyro accelerometer and flex sensors. A pressure sensor is used to
determine the force of breathing and EOG signals are used to control
the wheelchair movement. All of the inputs are processed using a
microcontroller. Testing of the wheel chair using the mentioned
variables is carried out successfully with accuracy between 88 to 96%
for various control modules and safety consideration as a primary goal.
Keywords: Wheelchair, Breathing-Pressure, EOG, Head-Motion, Voice
Recognition Module, Body Gestures

Introduction
For the past few decades, the need for assistive
devices has increased numerously because of the
different injuries that could occur to people globally
starting from work related injuries to war related
injuries. The wheelchair is one of the most commonly
used assistive devices for enhancing personal
mobility. Wheelchairs assist people with disabilities
to become productive members of their communities.
According to the US census, around 10% of the global
population has disabilities and require some kind of
assistive device (Taylor, 2018), where in Europe, according
to several statistics is estimated to be around 1%. These
estimates reach between 5 to 10% in countries like
Cambodia or Afghanistan (Philip, 2012). The current
formal estimate of disability incidence in Jordan is more
than 10% as reported in (Thompson, 2018).
The first commercially manufactured electric
wheelchair was simply heavy-duty powered by leadacid batteries, engines, drive belts and pulleys (Mann
et al., 2002). Advances in batteries and electric system
management which covers engines and batteries
enabled important mechanical advances in electric
wheelchairs, where the power base divided the electric

wheelchair into two components: The base providing
the mobility and the seating system providing the
support for the posture, while there was a change from
a standard power wheelchair to a power-base
wheelchair, (Ebrahimi et al., 2016) reported important
developments in electronic devices. Chow and Levy
(2011) have shown some of these mechanical and
electrical advances included the capacity to add
energy tilt and recline systems, as well as
programmable performance settings such as forward
speed, turning velocity and velocity.
New control techniques were developed that surpass
using joystick to control a powered wheelchair; most
common solution’s is a control using the chin or elbow,
but expressing directions using head movements is
also used as shown in (Gomes et al., 2019). This can
replace wheelchairs that are operated by joysticks
which is impossible to control for people with
disabilities in their upper extremities. Craig and
Nguyen (2006) presented the first head motion
movement system that uses a telemetric head motion
tool to control a powered wheelchair.
Alam et al. (2019; Noman et al., 2018) proposed
the development of electrical wheelchair that detects
the gesture of hands and finger motion using a mobile

© 2020 Jamal I. Al-Nabulsi. This open access article is distributed under a Creative Commons Attribution (CC-BY) 3.0
license.

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

phone and microcontroller. The microcontroller uses
touch sensor and smartphone gesture function to
control an electric wheelchair.
In previous works, authors used the automatic
speech recognition technique for electric wheelchair
control where the most common type of voice control
is based on detecting various keywords that function
as commands to accomplish a specific motion as
reported in (Oleiwi and Alkhalid, 2019; Sinyukov et al.,
2014). The voice-controlled method allows the user to
use specific commands which can move the
wheelchair to different locations rather than saying
each command verbally.
Electroencephalogram (EEG) is a method that
enables the brain's bioelectric activity to be recorded
through electrodes applied to the scalp. This technique
could be used as a possible control system coupled with
the use of algorithms that classify distinct patterns in the
recorded brain activity (Wang et al., 2014). Barriuso et al.
(2018) stated that EEG signals can be used to control the
movement of an electric wheelchair inside a closed
environment with minimal modification to its
structure which is also safe and minimally low cost.
Jameel et al. (2020) proposed a system to control
wheelchair by using human head motion detected by
an instrument called EMOTIV Insight.
Abiyev et al. (2016; Al-Aubidy, 2019) presented
Wavelet Transform (WT) method to be used for the
extraction of the mental tasks. WT coefficients
provide the highest discrimination in the appropriate
frequency band between wheelchair instructions. The
system which binds brain activity to external
operations such as multiple microcontrollers that can
operate an electric wheelchair is called the Brain
Computer Interface (BCI) and involves both invasive
and non-invasive methods where there are several
primary non-invasive modalities, including EEG and
other parameters could be used. There are possible
four commands, two generated from the prefrontal
cortex and two from the motor cortex by configuring
functional Near Infrared Spectroscopy (fNIRS) and
EEG to generate each control signal from its
associated brain region (Khan et al., 2014;
Khan and Hong, 2017).
Various methods of controls are reported, such as
breathing pressure that causes the wheelchair to move,
another method proposed by (Caiza et el., 2020) is used
the tracking eye movement, blink detection and eye gaze
tracker where it tracks the eyes causing the wheelchair to
move in response to its motion. In such devices there is
no need to use a coupling medium to extract signals
(Nguyen and Jo, 2012).

Taher et al. (2016; Mohd et al., 2020) have shown
that the development of eye detection technology
gives the chance to incorporate eyes to be used in
assistive devices. This advancement in technology
gives the ability of detecting eye signals and eyetracking, so it can be used as an input signal for many
application systems as reported in (Çetinel et al.,
2017; Ma’touq et al., 2014).
Eye-tracking is accomplished by using a camera to
obtain user images and evaluate user intent, where a
camera is set up to capture the user's face in real time
and then data handling module extracts horizontal and
vertical eye positions which can be determined from
the pupil major coordinates (Li et al., 2019; Veerati et al.,
2018). For safety reasons, an emergency stop is
triggered when the electric wheelchair user does not
concentrate his eyes consistently in one direction for a
specified time.
The rest of this paper is divided as follows: Section II
introduces the background, section III describes the
methodology, section IV presents the results, section V
discusses these results and section VI concludes the
findings of the paper.

Background
Cooper et al. (2006) concluded that a convenient
well-designed wheelchair in general has several features
that contribute to the rehabilitation of the user such as
seat height, seat width, backrest height, seat and back
angles, armrests, foot-rests, rear-wheel camber, leg rests,
anti-tip devices, tilt and recline.
Various types of microcontrollers are used to control
processes, among them is the Arduino, which is an opensource microcontroller that can be programmed to give
output voltage in a specific time and magnitude
(Components 101, 2020). Power wheelchairs are
similarly structured having the following basic parts:
Chair with wheels, motor, battery, drive system and
controller. The lightweight power wheelchair uses a 2pole motor, while the heavy-duty power wheelchair uses
a 4-pole motor. The 4-pole motor offers more carrying
power and allows for additional important options.
Since Controlling the wheelchair with a joystick isn’t
convenient to many individuals with severe motordisabilities such as cerebral palsy, high level spinal cord
injury and poliomyelitis, multiple controls can be
manipulated such as.

A. Voice Control
To remove the need to move one or more limbs to
command the seat, (Malik et al., 2017) proposed a

1030

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

voice controlled system to help the wheelchair user to
maintain a convenient location within his seat frame.
It is more compatible yet more convenient to talk
rather moving the user limbs to control the wheelchair
according to the results obtained by (Bramhe et al.,
2017; Agarwal et al., 2019).

B. Head Motion Control
Machangpa and Chingtham (2018) reported that
the head development is one of the most
straightforward and normally followed signals to
control a wheelchair. Other researchers (Prasad et al.,
2017; Ankit et al., 2018) concluded that the
wheelchair control systems with the head requires
high concentration and precise movement of the head
which could cause neck pain in rare cases.

C. Sip and Puff Control
The Sip-and-puff controller is an assistive
technology that sends signals using air pressure by
Sipping or puffing through a tube in order to move the
electric wheelchair which is perfect for individuals
who have restricted limbs movements to operate the
wheelchair and for people affected with severe motor
disabilities, as manipulating a standard switch can be
difficult, tiring and in some cases impossible
(Riman, 2018).

D. Finger Control
Finger switch is a type of electric wheelchair control
which could enable people who suffer from a disability
to move freely from one place to another by using their
fingers as a joystick where the user has to bend his
fingers to move a wheelchair (Megalingam et al., 2017).

E. Eye Control
Electromyogram is the signal produced by the
movement of the eye and can be used to control
mobility aids. EOG is captured by placing 5
electrodes around the eyes. Two electrodes are placed
to the right and left of the outer canthi and two
electrodes are placed to detect horizontal movements
and one reference electrode. Eye movement could be
detected by using various methods as reported in
(Abraham et al., 2019; Jafar et al., 2019).
In this study, a novel approach to wheelchair
design, control and operation is proposed and
demonstrated. The new approach uniquely combines
and correlates five physiological variables to control
the wheelchair activities namely; voice, breathing
pressure, head-movements, finger gestures and eye

movements. These variables are selected because they
are the least damaged parameters after having an
accident that could hinder the mobility of the body.
These variables are most critical and sensitive for a
balanced control of the wheelchair motion and also
conserve much needed energy compared with pushing
the rims manually. The built wheelchair proved to be
inexpensive with ability to apply various controlling
algorithms to improve both the comfortability and the
safety of the patient.

Methodology
The system is designed and tested using the flowchart
in Fig. 1 as follows.
When the push button is not pressed, the counter
value is 0, which indicates that the voice control is
activated. If the push button is pressed once, the
counter value is one, which indicates that breathing
pressure control is activated. If the push button is
pressed twice, the counter value becomes two, which
indicates that head movement control is activated. If
the push button is pressed a third time, the counter
value becomes three, which indicates that the finger
flex control is activated. If the push button is pressed
a fourth time, the counter value becomes four, which
indicates that the EOG control is activated. Finally, if
the push button is pressed a fifth time, the counter
value resets back to 0, which means that voice control
is activated.
The final operational system, proposed by the author
comprises the modules as presented in Fig. 2.

A. Voice Control System
The voice control system is composed from a
microcontroller, Voice Recognition Module (VRM)
and H-bridge. This control begins when the patient
provides a voice command to the VRM which is
mainly used to match the voice commands given by
the user and the nature of voice command recorded in
the VRM, then the signal is conveyed from the VRM
to the microcontroller, which interprets the electrical
signals into mechanical motion of the motors through
the H-bridge, resulting in the wheelchair movement.

B. Head Movement System
The head movement system is designed using
accelerometer (MPU-6050), transceiver (nRF24L01),
microcontroller and H-bridge. When the patient tilts his
head in a particular degree and direction, the
accelerometer senses the direction of tilting and
translates it into wheelchair movement through the
microcontroller and the H-bridge.

1031

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

Start

Is push button value =
0 (Not pressed)

Activate voice control

Is push button value
= 1 (Pressed once)

Activate breathing pressure control

Is push button value
= 2 (Pressed twice)

Active head movement control

Is push button value = 3
(Pressed third time)

Active finger flex
control

Is push button value =
4 (Pressed fourth time)

Active EOG control

Is push button value =
5 (Pressed 4 time)

Fig. 1: System’s flowchart

1032

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

Fig. 2: Overall integrated system to control a wheelchair

C. Breathing Pressure System
The breathing pressure system consists of pressure
sensor (MPX50GP), pressure tube, microcontroller and
H-bridge. The initiation of this module starts when a
patient applies breathing pressure into a tube that is
connected to the pressure sensor which is then translated
to electrical signal to control wheelchair movement
through the microcontroller and the H-bridge.

system, with detailed measurements presented in
Tables 1 to 4.
Figure 4 shows the response time against velocity
graph, while Fig. 5 shows the response for eye
movement control. Figure 6 shows the wheel controls,
whereas Fig. 7 shows wheel chair system prototype, with
Fig. 8 to 10 presenting the carried out reliability tests
under the control circuit in Fig. 11 which includes.

D. Finger Flexing System

Voice Control

The finger flex system is constructed from flex sensor,
transceiver (nRF24L01), microcontroller and H-bridge,
where four fingers indicate different wheelchair directions
with a finger flex sensor on each one. When flexing a finger
the flex senor converts this mechanical signal into an
electrical signal which will be translated into wheelchair
movement through the microcontroller and the H-bridge.

The VRM recognizes the voice command which is
then processed by the main microcontroller. The motor
drivers operate depending on the voice confirmed by the
microcontroller and then the motors function depending
on the sent voice command.

E. Eye Control System
The eye control system is developed using an
instrumentation amplifier, a microcontroller, a motor driver
and a low pass filter. Where different eye movements
indicate different wheelchair movements. When blinking or
moving the eye in any direction, the circuit converts the
mechanical signal to electrical signal capable to control the
wheelchair movement through the microcontroller.

Results
Figure 3 shows the results of the signal peak
values testing of the EOG for the designed wheelchair

Head Movement Control
The accelerometer reads the change in the orientation
of the head on the x-y axes, then this data is transmitted
into a receiver module and delivered to the main
microcontroller, which commands the motors to operate
depending on the tilting of the head.

Breathing Pressure Control
The pressure sensor receives the data acquired by
breathing pressure which is then processed by the main
microcontroller. Then motor driver operates depending
on the pressure volume confirmed by the main
microcontroller. Finally the motors function depending
on how much pressure is applied.

1033

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

Table 1: voice control. Results of the
Command Voice control
Go
Right
Left
Break
Back

Result
Forward.
Right.
Left.
Stop.
Backward.

Table 2: Results of the head control.
Command Head movement control (X and Y orientation)
X = N/R, Y = Y<135
X = N/R, Y = Y>150
X = X> 365, Y = N/R
X = X<345, Y = N/R
X = 360>x>345, Y = 150>y>140

Result
Forward.
Backward.
Left.
Right.
Stop

Table 3: Results of the pressure control.
Command Pressure control (Kpa)
35> = Pressure value> = 20
3> = Pressure value> = 2
17> = Pressure value> = 13
9> = Pressure value> = 8

Result
Forward.
Left.
Right.
Stop.

Table 4: Test results of the finger flexing control
Command Finger flexing control (Flexing Range)
Flexing of thumb finger (20 to 30)
Flexing of index finger (30 to 50)
Flexing of middle finger (40 to 50)
Flexing of ring finger (40 to 50)
None (51 to 80)

Result
Backward.
Left.
Forward.
Right.
Stop.

(a) Movement of the eyeball up and down

(c) Eyeball movement to the right

(b) Eyeball movement up

(d) Eyeball movement to the left

(e) Blinking movement

Fig. 3: Eye movements tracking signals

1034

Time required
0.8 sec
1.3 sec
1.2 sec
0.6 sec
1.5 sec

Time required
1 sec
1.2 sec
1.3 sec
1.3 sec
1.5 sec
Time required
0.9 sec
1.2 sec
0.8 sec
1.1 sec
Time required
1.7 sec
1.3 sec
1.5 sec
1.1 sec
0.8 sec

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

400
350

Velocity (m/s)

300
250
Voice
Head
Pressure
Finger

200
150
100
50
0
0

0.5

1

1.5

2

Time (s)

Fig. 4: Response time against velocity graph

Angular velocity (°/s)

Response for eye movement control
1000
800
600
400
200
0

0

10

20

30

40

50

60

70

80

90

Time (ms)

Fig. 5: Response time against angular velocity graph of eye movement control

Head motion control
Eye movement control

Voice control
Pressure control

Finger flex control

Fig. 6: System controls

1035

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

Fig. 7: Wheelchair prototype

Reliability of the control variable

30
25

25

24

25

23

25

23

25
22

20
15
10
5
0

3

1

2

2

Head control

Voice control

Finger control

Number of true trails

Number of false trails

Pressure control

Total number of trails

Fig. 8: System reliability test

16

15

15

14

14

12

13

12
10
8

Distance measured in cm

6
4
2
0
Trail 1

Trail 2

Trail 3

Trail 4

Trail 5

Fig. 9: System safety with ultrasound sensor at 90 degrees

1036

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

16

15

14
13

14

12

12

12
10
8

Distance measured in cm

6
4
2
0
Trail 1 Trail 2

Trail 3

Trail 4

Trail 5

Fig. 10: System safety with ultrasound sensor at 45 degrees

Vertical
channel
electrodes

Flex
sensors

MUC1

Horizontal
channel
electrodes

Transceiver
model 1

Instrumentation
amplifier

Low pass
filter (40Hz)

Pressure
sensor
Main
transceiver
module

Transceiver
module 2

Main
MCU

Voice
recognition
module

Driver
motor

Motors

5 LED
indicators

MUC 2
Selector switch
Accelerometer
sensor

Fig. 11: System control block

Finger Flex Control

EOG Control

Bending the fingers in certain degrees gives different
electrical resisting values acquired by the flex sensor, the
data is then transmitted from transceiver module to the
main microcontroller where the data is processed. Then
motor driver operates the motors and the wheelchair is
moved accordingly.

The electrical signal is acquired using electrodes
placed on the eyes that are amplified using an
instrumentation amplifier. The amplified signal is then
filtered using low pass filter with a frequency of 40 Hz.
The filtered signal is then processed in the main
microcontroller, depending on the signal, the motor

1037

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

driver operates and the motors function depending on the
eye signal extracted.
The ultrasound sensor is used as the safety aspect
where it cuts off the movement of the wheelchair when
an obstacle or stairs come across the wheelchair path.

Discussion
The designed and developed wheelchair prototype
with various interfacing modules was accomplished in
this study, the movement of wheelchair was tested using
different physiological variables. The results showed that
the wheelchair moved according to the voice command;
forward when the user said (go), to the right when the
user said (right), to the left when the user said (left) and
stopped when the user said (break) as shown in Table 1.
The head movement control system was designed and
tested. The wheelchair moved depending on the angular
orientation of the head as follows; moved forward when
the user tilted his head forward only for a maximum of 8
sec for the safety of the user because if the patient suffers
a stroke, his head may tilt forward which will result in
the wheelchair keeping to move forward, hence,
avoiding neck injury. The wheelchair moved right when
the user tilted his head right for a maximum of 5 sec,
moved left when the user tilted his head left for a
maximum of 5 sec and stopped when the user centered
his head in a certain x-y point. The readings of the sensor
used to detect the head movements are converted to
digital data and entered the microcontroller which
translates this data to orders to control the motors of the
wheelchair. The system programmed and calibrated in a
manner that right and left movements depend on the
change in the chosen X values, whereas forward and
backward movements depend on the chosen Y values.
The calibrated values and results are presented in Table
2. As example if X value less than 150, the
microcontroller gives order to the motors to move the
wheelchair forward.
The wheelchair functionality is further improved
through the detection of shaking of the head, eye, fingers
and abnormal breathing which can be achieved by using
Bessel low pass filter that allows the normal action
movement of the head, eye, fingers and normal
breathing which will cause the system not to respond
to abnormal action of shaking source of control and
the wheelchair will stop.
The breathing control circuit accomplished what it
was intended for. When a high breathing pressure is
applied, the wheelchair moves forward, when a lower
breathing pressure is applied, the wheelchair moves to
the right, when the user applies a pressure into his lungs
the wheelchair moves to the left and when the user stops
breathing into the tube, the wheelchair stops moving.
The pressure sensor used in this design detects the
change in pressure during snipping and puffing of the

disabled individual in the tube and converts this change
to digital data to be entered to the microcontroller and
then to be translated into a certain values according to
the programming of the microcontroller. These values
and thresholds determine the direction of the wheelchair
movement. These control values and the results of
testing breath control circuit are shown in Table 3. From
this table, it's clear that if the pressure value between 2035, the microcontroller gives an appropriate signal to the
motors of the wheelchair, via H-bridge, to move forward.
The finger flexing control system functioned
properly. The test results of this system are shown in
Table 4. The wheelchair moved depending on the flexing
of the fingers of the impaired individual. When the
impaired flexes his thumb the wheelchair moves
backwards for a maximum of 5 sec for the safety of
patient because if the user gets a seizure, the
wheelchair doesn’t keep moving. When the index
finger is flexed, it moves left for a maximum 5 sec,
when the middle finger is flexed it moves forward for
a maximum 8 sec and finally when the patient flexes
his ring finger the wheelchair moves right for a
maximum 5 sec.
The EOG control operated as required, the
wheelchair moved depending on the eye movement of
the patient. When the patient moves his eyes UP the
wheelchair moves forward, if the patient moves his eyes
RIGHT the wheelchair moves right, if the patient moves
his eyes LEFT the wheelchair moves left and finally, if
the patient moves his eyes DOWN the wheelchair moves
backward, as shown in Fig. 3.
Figure 4 shows that the velocity of voice is the
fastest with an average of 334 m/s. The air pressure
flow velocity comes second with an average velocity
of 131 m/s. The finger flexing average velocity is 97
m/s. The head movement average velocity is 60 m/s.
The minimum response time was 0.8 sec and the
maximum is 1.7 sec.
The graph in Fig. 5, shows the measured angular
velocity of the eye in (°/s). The angular velocity of the
eye is between 750°/s and 900°/s
Comparing with the manual wheelchair the proposed
electric wheelchair performs the same functionality more
efficiently. This wheelchair serves impaired individuals
with various spinal cord injuries using five different
controls as shown in Fig. 6. The proposed system's
prototype is shown in Fig. 7.
Testing the system's reliability was one of the main
objectives, so the system is validated through intensive
experimental work. Each control method is put into a
trail of 25 times with a 5 min waiting interval between
each experiment and the correct response per experiment
is recorded. The results are shown in Fig. 8. The
accuracy of the system is calculated from the obtained
results in Fig. 8. It is found to be 88% for pressure

1038

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

control module, 92% for both finger control and voice
control and 96% for head movement control.
The safety system in the wheelchair consisted of two
ultrasound sensors; one is positioned 90 degrees from the
ground on the front side of the wheelchair (position 1)
and the second is positioned 45 degrees from the ground,
so it can stop the wheelchair from moving when it is
close to stairs (position 2). The experiment (trail) was
repeated five times for each position and the wheelchair
stopped at distance (12-15) cm. away from the obstacle
in all experiments, for both positions. The results of
these experiments are depicted in Fig. 9 and 10.

Acknowledgement

Conclusion

There is no conflicts of interest regarding the
publication "A novel approach to wheelchair design and
operation using multi-function controller".

In conclusion, a major challenge is to help physically
disabled people, increase their mobility and rely on
themselves. This challenge is increased when even the
upper limbs are disabled. In this case, an ordinary joystick
controlled electric wheelchair is not suitable to provide
autonomous user movements. The proposed powered
wheelchair was developed to help individuals with different
spinal cord injuries and illnesses of poliomyelitis that are
widespread worldwide. A novel approach to wheelchair
design, control and operation is proposed and demonstrated.
The new approach uniquely combine and correlate five
physiological variables to control the wheelchair activities
namely; voice, breathing pressure, head-movements, finger
gestures and eye movements. These variables are selected
because they are the least damaged parameters after having
an accident that could hinder the mobility of the body. The
proposed wheelchair is a multicontrol device with ability to
choice the optimum method of control according to health
condition of the disabled individual easily and smoothly.
The designed wheelchair is cost effective which makes it
affordable for the community and the user.
The future direction of the proposed work could be
concentrated in adding other methods of wheelchair's
control by using various physiological variables such as
Electromyogram (EMG) and EEG controls, where
brain activity could be used to control the electric
wheelchair by means of multiple microcontrollers
known as BCI. Also it will be valuable to implement
WT to extract different features of EEG signal,
especially the mental ones that rise upon thinking
about certain action to accomplish namely thinking
about the wheelchair movement.
Also, an automatic selection control mode of the
wheelchair, according to the user needs, could be added
in the future work.
The five chosen controls in this study should cover
all possible cases of disability. Furthermore, the
wheelchair is simple, user-friendly. Depending on the
degree of injury, the chosen variables can be calibrated
so it responds accordingly.

This work is supported and sponsored by Al-Ahliyya
Amman University. The experimental work was carried
out by: Talal Altabba, Mahmoud Qamhieh, Mostafa
Abuthuaba and Momen Alkhawandi.

Funding Information
The experimental work and publication is partially
funded by Al-Ahliyya Amman University.

Ethics

References
Abiyev, R. H., Akkaya, N., Aytac, E., Günsel, I. and
Çağman, A. (2016). Brain-computer interface for
control of wheelchair using fuzzy neural networks.
BioMed research international, 2016.
Abraham, R., Thaliath, R. M., Davies, S., Jacob, T.
and Johny, T. (2019). Eye Controlled Wheelchair
with Asthenopia Detection. In 2019 3rd
International
conference
on
Electronics,
Communication and Aerospace Technology
(ICECA) (pp. 502-505). IEEE.
Agarwal, T., Bhalgat, S., Khandhar, C. and Khetan, B.
(2019). Swheel: Low-Cost Smart Wheelchair with
Wireless Control. In 2019 International Conference
on Communication and Electronics Systems
(ICCES) (pp. 1021-1025). IEEE.
Alam, M. E., Kader, M. A., Hany, U., Arjuman, R.,
Siddika, A. and Islam, M. Z. (2019). A multicontrolled semi-autonomous wheelchair for old and
physically challenged people. In 2019 1st International
Conference on Advances in Science, Engineering and
Robotics Technology (ICASERT) (pp. 1-6). IEEE.
Al-Aubidy, K. M. (2019). Wheelchair Neuro Fuzzy
Control Using Brain Computer Interface. In 2019
12th International Conference on Developments in
eSystems Engineering (DeSE) (pp. 640-645). IEEE.
Ankit, K., Vikas, C. Kuldeep, S. Avinash Raj, Y. Ishwar,
Y. Pankaj D. and Somkuwar. S. (2018). HeadMotion Controlled Wheel Chair Direction using
ATMega328p Microcontroller. International Journal
of Advanced Research in Computer and
Communication Engineering, 7 (4), 61-65.
Barriuso, A. L., Pérez-Marcos, J., Jiménez-Bravo, D. M.,
Villarrubia Gonzalez, G. and De Paz, J. F. (2018).
Agent-based intelligent interface for wheelchair
movement control. Sensors, 18(5), 1511.

1039

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

Bramhe, M. V., Vijay, N., Rao, K. B., Bisen, P.,
Navsalkar, R. and Bajganiya, T. (2017). Voice
Controlled Wheelchair for Physically Disabled
Person. International Journal of Advanced Research
in Electrical, Electronics and Instrumentation
Engineering, 6(2), 940-948.
Caiza, G., Reinoso, C., Vallejo, H., Albarracín, M. and
Salazar, E. P. (2020). Semi-automatic Eye
Movement-Controlled Wheelchair Using Low-Cost
Embedded System. In World Conference on
Information Systems and Technologies (pp. 755764). Springer, Cham.
Çetinel, G., Sevda, G. Ü. L., Tiryaki, Z., Enes, K. U. Z.
U. and Milligüney, M. (2017). Move Your
Wheelchair with Your Eyes. International Journal of
Applied Mathematics Electronics and Computers,
(Special Issue-1), 5-8.
Chow, J. W. and Levy, C. E. (2011). Wheelchair
propulsion biomechanics and wheelers' quality of
life: An exploratory review. Disability and
Rehabilitation: Assistive Technology, 6(5), 365-377.
Components 101, (2020) a resource dedicated
for electronics design engineers 2018, Arduino Uno
schematic
diagram..https://components101.com/microcontrolle
rs/arduino-uno
Cooper, R. A., Ohnabe, H. and Hobson, D. A. (Eds.).
(2006).
An
introduction to rehabilitation
engineering. CRC Press.
Craig, D. A. and Nguyen, H. T. (2006). Wireless realtime head movement system using a Personal
Digital Assistant (PDA) for control of a power
wheelchair. In 2005 IEEE Engineering in
Medicine and Biology 27th Annual Conference
(pp. 772-775). IEEE.
Ebrahimi, A., Kazemi, A. and Ebrahimi, A. (2016).
Wheelchair Design and Its Influence on Physical
Activity and Quality of Life Among Disabled
Individuals. Iranian Rehabilitation Journal, 14(2),
85-92.
Gomes, D., Fernandes, F., Castro, E. and Pires, G.
(2019). Head-movement interface for wheelchair
driving based on inertial sensors. In 2019 IEEE 6th
Portuguese Meeting on Bioengineering (ENBENG)
(pp. 1-4). IEEE.
Jafar, F., Fatima, S. F., Mushtaq, H. R., Khan, S.,
Rasheed, A. and Sadaf, M. (2019). Eye
Controlled Wheelchair Using Transfer Learning.
In 2019 International Symposium on Recent
Advances in Electrical Engineering (RAEE) (Vol.
4, pp. 1-5). IEEE.
Jameel, H. F., Mohammed, S. L. and Gharghan, S. K.
(2020). Wheelchair Control System based on
Gyroscope of Wearable Tool for the Disabled.
MS&E, 745(1), 012091.

Khan, M. J. and Hong, K. S. (2017). Hybrid EEG–
fNIRS-based eight-command decoding for BCI:
Application to quadcopter control. Frontiers in
neurorobotics, 11, 6.
Khan, M. J., Hong, M. J. and Hong, K. S. (2014).
Decoding of four movement directions using hybrid
NIRS-EEG brain-computer interface. Frontiers in
human neuroscience, 8, 244.
Li, Y., Huang, Q., Zhang, Z., Yu, T. and He, S. (2019).
An EEG-/EOG-Based Hybrid Brain-Computer
Interface: Application on Controlling an Integrated
Wheelchair Robotic Arm System. Frontiers in
Neuroscience, 13, 1243.
Ma’touq, J., Al-Nabulsi, J., Al-Kazwini, A., Baniyassien,
A., Al-Haj Issa, G. and Mohammad, H. (2014). Eye
blinking-based method for detecting driver
drowsiness. Journal of Medical Engineering &
Technology, 38(8), 416-419.
Machangpa, J. W. and Chingtham, T. S. (2018). Head
Gesture Controlled Wheelchair for Quadriplegic
Patients. Procedia computer science, 132, 342-351.
Malik, M. I., Bashir, T. and Khan, O. F. (2017). Voice
controlled wheel chair system. International Journal
of Computer Science and Mobile Computing, 6(6),
411-419.
Mann, W. C., Goodall, S., Justiss, M. D. and Tomita, M.
(2002). Dissatisfaction and nonuse of assistive
devices among frail elders. Assistive Technology,
14(2), 130-139.
Megalingam, R. K., Sreekanth, S., Govardhan, A., Teja, C.
R. and Raj, A. (2017). Wireless gesture controlled
wheelchair. In 2017 4th International Conference on
Advanced Computing and Communication Systems
(ICACCS) (pp. 1-5). IEEE.
Mohd, A. A. Q., Khairudin, M. F. M., Basirun, L. H. and
Jailani, R. (2020). Motorised Wheelchair with
Multicontrol System. In 2020 IEEE 10th
Symposium on Computer Applications & Industrial
Electronics (ISCAIE) (pp. 233-238). IEEE.
Nguyen, Q. X. and Jo, S. (2012). Electric wheelchair
control using head pose free eye-gaze tracker.
Electronics Letters, 48(13), 750-752.
Noman, A. T., Khan, M. S., Islam, M. E. and Rashid, H.
(2018). A New Design Approach for Gesture
Controlled
Smart
Wheelchair
Utilizing
Microcontroller. In 2018 International Conference
on Innovations in Science, Engineering and
Technology (ICISET) (pp. 64-68). IEEE.
Oleiwi, B. K. and Alkhalid, F. F. (2019). Smart
Autonomous Wheelchair Controlled by Voice
Commands-Aided by Tracking System. Iraqi
Journal OF Computers, Communication and Control
and Systems Engineering, 19(1), 82-87.
Philip, E. (2012). Eubserver, 2012. Disability in Figures.
https://euobserver.com/disability/118249

1040

Jamal I. Al-Nabulsi / Journal of Computer Science 2020, 16 (7): 1029.1041
DOI: 10.3844/jcssp.2020.1029.1041

Prasad, S., Sakpal, D., Rakhe, P. and Rawool, S. (2017).
Head-motion controlled wheelchair. In 2017 2nd
IEEE International Conference on Recent Trends in
Electronics, Information & Communication
Technology (RTEICT) (pp. 1636-1640). IEEE.
Riman, C. F. (2018). Multi-Controlled Wheelchair for
Upper Extremities Disability.
Sinyukov, D. A., Li, R., Otero, N. W., Gao, R. and Padir,
T. (2014). Augmenting a voice and facial expression
control of a robotic wheelchair with assistive
navigation. In 2014 IEEE International Conference
on Systems, Man and Cybernetics (SMC) (pp. 10881094). IEEE.
Taher, F. B., Amor, N. B. and Jallouli, M. (2016). A self
configured and hybrid fusion approach for an
electric wheelchair control. In 2016 IEEE 8th
International Conference on Intelligent Systems (IS)
(pp. 729-734). IEEE.

Taylor, D. M. (2018). Americans with disabilities: 2014.
US Census Bureau.
Thompson, S. (2018). The current situation of persons
with disabilities in Jordan.
Veerati, R., Suresh, E., Chakilam, A. and Ravula, S. P.
(2018). Eye Monitoring Based Motion Controlled
Wheelchair for Quadriplegics. In Microelectronics,
Electromagnetics and Telecommunications (pp. 4149). Springer, Singapore.
Wang, H., Li, Y., Long, J., Yu, T. and Gu, Z. (2014). An
asynchronous wheelchair control by hybrid EEG–
EOG
brain–computer
interface.
Cognitive
neurodynamics, 8(5), 399-409.

1041

