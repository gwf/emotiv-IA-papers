VOL. 10, NO. 17, SEPTEMBER 2015

ARPN Journal of Engineering and Applied Sciences

ISSN 1819-6608

©2006-2015 Asian Research Publishing Network (ARPN). All rights reserved.

www.arpnjournals.com

A REAL TIME PERSONALIZED SELF ASSISTIVE TECHNOLOGY FOR
THE DISABLED PEOPLE BASED ON VOICE AND EOG
Archana JS. and K. Gangadharan
Department of Computer Science, Mohandas College of Engineering & Technology, Anad, Nedumangad, Thiruvananthapuram, India
E-Mail: archanajsmtech@gmail.com

ABSTRACT
Electrooculography (EOG) is a technology for recognition of eye movements. This technology is based on the
principle of recording the polarization potential or corneal-retinal potential (CRP), which is the resting potential between
cornea and retina. This potential difference between cornea and retina creates an electric field, and the electrical signal that
can be measured is called Electrooculogram EOG is a very small electrical potential that can be detected by placing
electrodes on the forehead around the eyes.
The EOG signals can be used for various activities, which include controlling rehabilitation aids. This paper
presents about the automatic wheel chair which is controlled by EOG signals and voice. This paper aims to develop a
personalized real time technology based on EOG and voice to control a wheel chair. The personalization is achieved with
the help of neural network. This paper discuss about various types of techniques used to control a wheelchair,
electrooculography, block diagrams and working of a prototype wheel chair , shows the result of voice recognition etc.
Keywords: wheelchair, hand-free control, electrooculography.

INTRODUCTION
Mobility is very much important to every
creature. Wheelchairs are used by the people who cannot
walk due to physiological or physical illness, injury or any
disability. Wheel chair is a chair with wheels. People with
physical disabilities and partial paralysis always find it
difficult to navigate through their habitat or their home
without the assistance of someone. Often after paralysis or
physical disability the wheelchair is the most common
means of locomotion for such people. But to navigate
through one’s own house by taking help of someone at
every time can be inconvenient.
Conventional wheel chairs were manually
controlled one. But recent development promises a wide
scope in developing smart wheelchairs, with least
interaction of human.
The section II presents the study of various types
of automatic and intelligent wheel chairs. Section III gives
brief idea about EOG and voice technique. Section IV
explains about the experiment and results. Section V is the
conclusion and future work.
LITERATURE REVIEW
Various types of controlling techniques are
implemented on wheel chairs. Some of them are:
 Manually propelled
 Electric powered
 Gestures based

hand

Head

Eye

tongue
 Voice based
 Thought controlled

Manually Propelled
Manual wheelchairs are those that require human
power to move them. There are handles behind the seat for
someone else to do the pushing
The advantage of this type of wheel chair is it is
very cheap. Disadvantage is that always need the help
from other people.
Electric Powered
An electric-powered wheelchair is a wheelchair
that is moved via the means of an electric motor and
navigational controls, usually a small joystick mounted on
the armrest, rather than manual power.
The advantage of this one over manually
controlled one is that there is no need of a second party.
The disadvantage is it is not suitable for the patients who
can’t move their hands. And there is no intelligence for the
wheel chair; Socollision avoidance is responsibility of
disabled person. The patient should be more careful all the
time.
Gestures Based
Gestures are used for non verbal communication.
Gesture is the movement of part of the body, especially a
hand or the head; to express an idea or meaning. Here in
the case of wheel chairs we are using the gesture to move
the wheel chair. The advantage is that there is no need of
any second party other than the disabled person. Usually
some kind of sensors will be connected to the wheel chair
for collision detection. So that the wheel chair will become
more intelligent. Usually gestures of hand, head, eye,
tongue etc. will be taken and their combinations also.


Hand Gesture
The wheel chair will move according to the
movement of hand. The intention behind this idea is to

7533

VOL. 10, NO. 17, SEPTEMBER 2015

ARPN Journal of Engineering and Applied Sciences

ISSN 1819-6608

©2006-2015 Asian Research Publishing Network (ARPN). All rights reserved.

www.arpnjournals.com
enable the physically handicapped people to navigate and
move around their home without the help of anyone.
The recognition of hand gestures requires both
hand`s detection and gesture`s recognition. The hand
gesture module has been prepared by using a triple axis
accelerometer sensor (ADXL 335). The accelerometer
sensor senses the accelerating force (acceleration due to
gravity or g) and thus gives a particular voltage for the x,
y and z coordinate orientation [1].
This method is challenging, mainly due to the
variability of the possible hand gestures and very difficult
to detect in dynamic environments with cluttered
backgrounds and variable illumination.

There are two ways to place sensors:
 Some systems mount sensors and circuits in a dental
retainer for use within the mouth and wirelessly
sending the signals [4].


Use separate devices, placing sensors on a headband
to bring them closer up to the cheeks, and then put on
the tongue a magnet(piercing the tongue) [5][6].
There are so many advantages of this technique;
it is useful for the patients with a minimum capacity of
movement or practically nil. The disadvantage is the
sensors should be kept in the mouth, which will be
uncomfortable the patients. But we can’t say this as a
severe disadvantage.



Head Gesture
Traditional electric-powered wheelchairs are
normally controlled by users via joysticks, which cannot
satisfy the needs of elderly and disabled users who have
restricted limb movements caused by some diseases such
as Parkinson’s disease and quadriplegics. Therefore a
hands-free control system for intelligent wheelchairs
based on visual recognition of head gestures is used.
EEG sensor called Emotiv EPOC, is used to
detect head movements [2].Direction of wheelchair
movement is determined by direction and amount of head
tilt. For example, in [3] the movement of wheel chair is
determined as follows:
 When person tilt his head in forward direction above
20 degree angle chair will move in forward
direction.


If person tilt his head in backward direction above
20 degree angle chair will move in backward
direction.



If person tilt his head in left direction above 20 degree
angle chair will move in left direction.



If person tilt his head in right direction above 20
degree angle chair will move in right direction.



If person tilt his head at 45degree forward priority
will be given to forward direction.



Eye Movement Based
This method can be used even by patients who
are paralyzed from shoulder downward. The advantage is
that simple eye movements don’t require too much
energy. The mobility of eye movements can be
recognized by various techniques such as based on
Electrooculography, capturing video of eye movements
etc. One of the disadvantages of this method is use of eye
movement demand too much concentration.



Using Tongue Movement
A wheel chair which uses the movements of the
patient's tongue to operate it is invented for the people
who cannot move their body parts, including their head. A
sensor will be there to take the movements of tongue.

Voice Based
Wheelchair control has been guided by voice
commands delivered through speech recognition, motor
control, a user interface, and central processor modules.
Such systems usually require the user to record functional
oral commands, e.g., “Forward” makes the wheelchair
move forward and “Stop” makes the wheelchair stop.
In this method first the patient has to mount the
wheel chair. Then the patient can give voice commands
via a head phone. Voice commands are: Forward, Reverse
Left, Right, stop etc.
The disadvantage of this method is different
people have different slang. So recognition is little bit
tough, background noise, gender etc will affect the
performance.
Combined Techniques
The combinations of the above techniques are
also used to control the wheelchair, like voice and vision
based. This is the most commonly used combined
mechanism.
Two driving methods are implemented as voice
and vision. In vision method the wheel chair will
automatically move to a particular direction as the patient
moves his/her eye in that particular direction. The eyeball
sensor basically works with the infrared sensors to detect
the movement of the eye ball direction [7]. In voice
driving mode, the wheelchair is controlled by means of
various voice commands such as forward, backward, left,
right, stop, and send.
Another kind of combined technique is to control
the wheel chair by head movements and facial expression
[2]. Three head movements are used to stop the wheelchair
and display the turning commands in the graphical
interface (GI) of the human machine Interface (HMI),
while two facial expressions are employed to move
forward the wheelchair and confirm the execution of the
turning command displayed on the GI of the HMI.
Thought Control Wheel Chair
The wheelchair can be directed by brain signals
detected using a cap fitted to the user. [8]. Audeo, a device
which reads nerve impulses in the neck to help people
speak and even control an electronic wheelchair. Designed

7534

VOL. 10, NO. 17, SEPTEMBER 2015

ARPN Journal of Engineering and Applied Sciences

ISSN 1819-6608

©2006-2015 Asian Research Publishing Network (ARPN). All rights reserved.

www.arpnjournals.com
to help people suffering from diseases like ALS which
erode muscle control over time, Audeo has received
numerous awards. Audeo Basic, the system which allows
someone to speak only using nerve signals, is already
available on a limited basis for trials [9].
ELECTROOCULOGRAPHY AND VOICE
TECHNIQUE
The Electrooculogram (EOG) is the electrical
signal that corresponds to the potential difference between
the retina and the cornea of the eye [10]. This difference is
because of the fact that occurrence of metabolic activities
in the cornea region is higher than that in the retinal
region. Usually the cornea maintains a voltage of +0.40 to
+1.0 milli volts which is higher than the retina. When the
eyes are rolled upward or downward, positive or negative
pulses are generated. As the rolling angle increases, the
amplitude of the pulse also increases and the width of the
pulse is in direct proportion to the duration of the eyeball
rolling process.
The EOG is the electrical recording
corresponding to the direction of the eye and makes the
use of EOG for applications such as HCI very attractive.
EOG-based techniques are very useful for patients with
severe cerebral palsy or those born with a congenital brain
disorder or those who have suffered severe brain trauma
[4].
EOG is a bio signal, in which human do not have
much control over it. Because of this reason in this work a
non bio signal that is, voice is going to combine with
EOG. For the direction control EOG signals are used and
for the motion control voice based commands is used.
This technology is a personalized one, which
means that once after started using this technique the
system will self adjust in such a way that it will be more
suitable and reliable for individuals.
EOG signals will be different for individuals.
Through calibration a particular potential value will be set
in the system at the very first time. But that value may not
be the exact one for the user. So in this work this value
will be get adjusted as there is a difference between actual
value and the desired value. Neural network concept is
made use for this.
In the case of voice recognition the difficulty lies
in slang, gender, style etc. These will be different for
different people. So speech recognition is a challenging
task. In this work neural network based voice recognition
technique is used. The trained voices commands will be
get adjusted with actual commands produced by the user.
Since the system is self adjusting according to the
users, it is entitled as `personalized`.

Block Diagrams

Figure-1. Basic block diagram of EOG.

Figure-2. Block diagram of voice recognition.
The basic block diagram of EOG technique is as
shown in Figure-1. The EOG signals from the user are
measured by placing electrodes on the region surrounding
the eye. They were recorded from two separate regions:
horizontal and vertical. Horizontal electrodes were for
detecting horizontal eye movements (left and right eye
movement) and vertical electrodes were for detecting
vertical movements (up and down cornea movements).
After processing the captured signal the next step
followed is bio signal to control signal generation.
Separate eye movements and duration between individual
eye movements can be formulated to produce individual
control signals. These control signals were used to control
the movement of a wireless controlled wheelchair.
These control signals were parallel in nature; the
encoder took parallel data, packaged it into serial format
and then transmitted it with the help of the RF transmitter
module. At the receiver end, the decoder received the
signal through the RF receiver module, decoded the serial
data and reproduced the original data in the parallel

7535

VOL. 10, NO. 17, SEPTEMBER 2015

ARPN Journal of Engineering and Applied Sciences

ISSN 1819-6608

©2006-2015 Asian Research Publishing Network (ARPN). All rights reserved.

www.arpnjournals.com
format. The microcontroller controlled the direction of
wheel chair motors depending on the incoming data.
The basic block diagram of voice recognition is
as shown in Figure-2. The control signals are already
recorded, that is already trained the system with some
control commands. The sound wave from the user is
captured. The application captures user speech using the
MATLAB wav record function (up to 2 seconds after
recording started). The captured data is then processed for
classification. Classifiers were trained using the previously
discussed data. All data was used to train since the user of
the translator is thought of as the test set (translator has not
seen user speech prior to use).
Once after the classification of captured data the
control signals are generated. These control signals are
passed to the rehabilitation aid, in our case it is a wheel
chair.
EXPERIMENTS AND RESULTS
The voice is stored as in the form of matrix. The
system will be trained by using commands such as ‘start’,
‘stop’ etc. Then captured voice commands in real time are
also represented as matrix. Then correlations between
these two are calculated. That is correlation between
trained command and real time command. Figure-3 and
4.3 shows the correlation values between trained
commands and tested commands. The first value
corresponds to ‘start’ command and second value
corresponds to ‘stop’ command.
Figure-3 shows the values which are tested by
‘start’ command against trained ‘start’ and ‘stop’
commands. The first value, 27.3565 shows the correlation
between trained ‘start’ command and tested ‘start’
command. The second value, 13.5585 shows the
correlation between trained ‘stop’ command and tested
‘start’ command. From this it is clear that the command
‘start’ is correctly recognized.
The Figure-4 shows the values in graphical form.
First line (at point 1) shows the correlation of ‘start’
command and second line (at point 2) shows the
correlation of ‘stop’ command. From the Figure-4, it is
clear that the tested ‘start’ command is more correlated to
trained ‘start’ command itself.

Figure-3. Correlation values when tested by ‘start’
command.

Figure-4. Graphical representation of correlation value
when tested by ‘start’ command.

Figure-5. Correlation values when tested by ‘stop’
command.
Figure-5 and 6 shows the values when tested by
using ‘stop ’command.

7536

VOL. 10, NO. 17, SEPTEMBER 2015

ARPN Journal of Engineering and Applied Sciences

ISSN 1819-6608

©2006-2015 Asian Research Publishing Network (ARPN). All rights reserved.

www.arpnjournals.com
IEEE proceedings 2012.
[3] Rakhi A. Kalantri, D.K. Chitre. 2013. “Automatic
Wheelchair using Gesture Recognition” International
Journal of Engineering and Innovative Technology
(IJEIT) ISSN: 2277-3754, Vol. 2, No. 9, March.
[4] Malik Q A and Ahmad J. 2007. “Retina Based Mouse
Control (RBMC)”, World Academy of Science,
Engineering and Technology, No. 7, pp. 318-321.

Figure-6. Graphical representation of correlation value
when tested by ‘stop’ command.
Figure-5 shows the values which are tested by
‘stop’ command against trained ‘start’ and ‘stop’
commands. The first value, 38.8640 shows the correlation
between trained ‘start’ command and tested ‘stop’
command. The second value, 70.4201 shows the
correlation between trained ‘stop’ command and tested
‘stop’ command. From this it is clear that the command
‘stop’ is correctly recognized.
The Figure-6 shows the values in graphical form.
First line (at point 1) shows the correlation of ‘start’
command and second line (at point 2) shows the
correlation of ‘stop’ command. From the Figure-6, it is
clear that the tested ‘stop’ command is more correlated to
trained ‘stop’ command itself.
CONCLUSION AND FUTURE WORK
This paper presents briefly about various
techniques used for controlling a wheel chair
automatically. Each method has its own advantages and
disadvantages. Some techniques use combination of
methods, so that it can take the advantages of several
methods.
This paper presented about EOG and voice based
wheel chair. The paper consists of basic block diagrams
and brief working.
The future work can be a combination of other
two techniques instead of voice and vision, so that we can
make use of the advantages of those two.

[5] Poonam S. Gajwani and Sharda A. Chhabria.
2010.”Eye Motion Tracking for wheelchair Control”,
International Journal of Information Technology and
Knowledge Management July-December, Vol. 2, No.
2, pp. 185-187.
[6] Amberlay Ruíz-Serrano, Rubén Posada-Gómez,
Albino Martínez-Sibaja, ”Development of a magnetic
control system for an electric wheelchair using the
tongue” IEEE 2013.
[7] Xueliang Huo, Jia Wang and Maysam Ghovanloo.
2008. “Introduction and preliminary evaluation of the
Tongue Drive System: Wireless tongue-operated
assistive technology for people with little or no upperlimb function”, Journal of Rehabilitation Research
&Development, Vol. 45, No. 6, pp. 921–930.
[8] Behnaz Yousefi, Xueliang Huo and Maysam
Ghovanloo. “Preliminary Assessment of Tongue
Drive System in Medium Term Usage For Computer
Access And Wheelchair Control”.
[9] G Uday Kiran, N. Nithesh Chakravarthi, K. R.
Radhakrishnan. 2013.
“Voice
and Vision
Controlled Wheelchair for Disabled” International
Journal of Engineering Research & Technology
(IJERT) ISSN: 2278-0181 Vol. 2 No. 6, June.
[10] Nolan Y. M. 2005. “Control and communication for
physically disabled people, based on vestigial signals
from the body”, PhD thesis paper submitted to
National University of Ireland, Dublin, pp. 7-18.
[11] Kohei Arai and Ronny Mardiyanto. “A Prototype of
electric Wheelchair controlled by Eye-Only for
Paralyzed User”.

REFERENCES
[1] Shreedeepgangopadhyay, Somsubhramukherjee and
Soumyachatterjee. “Intelligent Gesture Controlled
Wireless Wheelchair for the Physically Handicapped”
in Proceedings of Fifth IRAJ International
Conference, 15th September 2013, Pune, India, ISBN:
978-93-82702-29-0.
[2] Ericka Janet Rechy-Ramirez, Huosheng Hu and Klaus
McDonald-Maier,” Head movements based control of
an intelligent wheelchair in an indoor environment”

[12] GundaGautam, GundaSumanth, Karthikeyan K. C,
Shyam Sundar, D. Venkataraman. 2014. “Eye
Movement Based Electronic Wheel Chair For
Physically Challenged Persons, International Journal
Of Scientific & Technology Research ,ISSN 22778616, Vol. 3, No. 2, February.
[13] Rakhi A. Kalantri, D.K. 2013. “Automatic
Wheelchair using Gesture Recognition” International
Journal of Engineering and Advanced Technology

7537

VOL. 10, NO. 17, SEPTEMBER 2015

ARPN Journal of Engineering and Applied Sciences

ISSN 1819-6608

©2006-2015 Asian Research Publishing Network (ARPN). All rights reserved.

www.arpnjournals.com
(IJEAT) ISSN: 2249 – 8958, Vol. 2, No. 6, August.
[14] Diksha Goyal and S.P.S. Saini. 2013.“Accelerometer
Based Hand Gesture Controlled Wheelchair” in
International Journal on Emerging Technologies, Vol.
4, No. 2, pp. 15-20.
[15] P. Jia, H. Hu, T. Lu and K. Yuan. “Head Gesture
Recognition for Hands-free Control of an Intelligent
Wheelchair”.
[16] Kohei Arai and Ronny Mardiyanto. 2011. “Eyes
Based Eletric Wheel Chair Control System”
(IJACSA) International Journal of Advanced
Computer Science and Applications, Vol. 2, No. 12.
[17] ToonGoedem´e, MarnixNuttin, TinneTuytelaars and
Luc Van Gool. “Vision Based Intelligent Wheel Chair
Control: the role of vision and inertial sensing in
topological navigation”.
[18] Pedro Ponce, Arturo Molina and Rafael Mendoza
“Wheelchair and Virtual Environment Trainer by
Intelligent Control”.
[19] http://www.bbc.com/news/science-environment12490048
[20] http://singularityhub.com/2009/11/16/audeo-lets-youtalk-or-control-wheelchair-with-your-thoughts-video/

7538

