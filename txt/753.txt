Int. J. Computers in Healthcare, Vol. 2, No. 1, 2014

43

A method for assessing the usability of an on screen
display for a brain-computer interface
Melanie P. Ware, Gaye Lightbody*,
Paul J. McCullagh and Maurice D. Mulvenna
School of Computing and Mathematics,
University of Ulster,
Jordanstown Campus, Shore Road,
Newtownabbey, Co. Antrim, BT37 0QB, UK
Fax: +44(0) 28-90366216
E-mail: melanie.ware@investni.com
E-mail: g.lightbody@ulster.ac.uk
E-mail: pj.mccullagh@ulster.ac.uk
E-mail: md.mulvenna@ulster.ac.uk
*Corresponding author

Suzanne Martin
Health and Rehabilitation Sciences Research Institute (HRSRI),
School of Health Sciences,
University of Ulster,
Jordanstown Campus, Shore Road,
Newtownabbey, Co. Antrim, BT37 0QB, UK
E-mail: s.martin@ulster.ac.uk

Eileen Thomson
Cedar Foundation,
31 Ulsterville Avenue,
Belfast, Co. Antrim, BT9 7AS, UK
E-mail: e.thomson@cedar-foundation.org
Abstract: Brain-computer interfaces (BCIs) aim to provide a mechanism by
which individuals with severe forms of paralysis can communicate their wishes
to others or gain control over devices in their environment. This paper
describes the evaluation of an interface designed for use with BCI, which is
dedicated to providing a flexible and extensible framework for controlling
multiple devices within a domestic environment. A structured format for
assessing the usability aspects of this interface is presented. The
goal-question-metric (GQM) approach derived from measurement in software
engineering was used, providing a mechanism that can be readily adapted to
assess other interfaces in assistive technology. The assessment was based on
interaction of five participants currently using assistive technology who
provided an initial representation of the target user group. Their input, in
addition to guidance from a lead user, was used to influence subsequent design
choices and directions and highlight the importance of user-centred design
within BCI development.
Copyright © 2014 Inderscience Enterprises Ltd.

44

M.P. Ware et al.
Keywords: human computer interfaces; brain-computer interface; BCI;
steady-state visually evoked potential; SSVEP; assistive technology; usability
principles; user-centred design; goal-question-metric; GQM; computers in
healthcare.
Reference to this paper should be made as follows: Ware, M.P., Lightbody, G.,
McCullagh, P.J., Mulvenna, M.D., Martin, S. and Thomson, E. (2014) ‘A
method for assessing the usability of an on screen display for a brain-computer
interface’, Int. J. Computers in Healthcare, Vol. 2, No. 1, pp.43–67.
Biographical notes: Melanie P. Ware was a Research Fellow at the School
of Computing and Mathematics in the University of Ulster working on the
BRAIN project. Prior to this, she was a Researcher at the Centre for Software
Process Technologies at the University with experience on large-scale software
projects in the areas of data warehousing, finance, commercial and local
government applications. She possesses a BA in Philosophy, and an MSc and
PhD in Computing and Information Systems. She currently works for Invest
Northern Ireland.
Gaye Lightbody received her MEng (1995) and PhD (2000) in Electrical and
Electronic Engineering from Queen’s University of Belfast. She is a Lecturer
within the School of Computing and Mathematics in the University of Ulster.
Her research interests include high performance hardware design, FPGAs,
biomedical signal processing, data mining and brain computer interfaces.
Paul J. McCullagh received his BSc (1979) and PhD (1983) in Electrical
Engineering from Queen’s University of Belfast. His research interests include
biomedical signal and image processing, data mining, brain computer interface,
and assisted living applications. He is interested in advancing the education and
professionalism of biomedical engineering and health informatics.
Maurice D. Mulvenna researches pervasive healthcare technologies and
the application of information and computing technologies to support social
inclusion. He served on Ofcom’s Advisory Committee on Older and Disabled
People and serves on the editorial boards for several journals. He is a Fellow of
the British Computer Society and a senior member of the IEEE and ACM.
Suzanne Martin is an Occupational Therapist and Reader in Health Sciences.
She has worked on a number of international research projects, with a focus on
healthcare technology to deliver services for people with complex needs living
at home. She is a trainer and contributor to the Cochrane Library and a
co-Director of the TRAIL Living Lab.
Eileen Thomson is the Deputy Chief Executive of the Cedar Foundation, a
non-governmental organisation, which delivers services to people with
complex disabilities. She is a Queens University graduate with Master’s in
Social Work, Master’s in Organisational and Management and NVQ Level V in
Strategic Management. Her background is in social work in the statutory sector
in both practitioner and management posts before joining Cedar Foundation in
1984. She has extensive experience working on European projects, including
the BRAIN research project.

A method for assessing the usability of an on screen display for a BCIs

1

45

Introduction

Individuals can experience a variety of conditions which result in lack of motor function
and severe levels of paralysis, e.g., brain injury, stroke and cerebral palsy. Social
interactions which we take for granted may not be achievable or are extremely exhausting
and may not be fully effective. The opportunity to circumvent physical forms of control
and communication may enable an alternative form of interaction (Schalk et al., 2004;
Vaughan et al., 2003; Wolpaw et al., 2003). Millán et al. (2010) stated that this has
become a demonstrated reality using a brain-computer interface (BCI). BCIs operate by
recognising ‘distinct mental processes from the electroencephalogram (EEG)’ (Neuper
et al., 2003); thereby, harnessing this brain activity into control signals for computerised
applications and devices (Scherer et al., 2007), such as, spellers (Cecotti, 2011; Friman
et al., 2007a), web-browsers (Mugler et al., 2010), domestic controls embedded into
smart-home environments (Lin et al., 2014; McCullagh et al., 2010), games (Kapeller
et al., 2012; Nijholt, 2009), art-packages and media players (George et al., 2010; Teo
et al., 2006; Todd et al., 2012), musical devices (Miranda and Brouse, 2005) and robotic
devices, wheelchairs, and prosthetics (Acharya et al., 2007; Valbuena et al., 2007).
BCIs use a variety of strategies to implement channels of communication (Mason
et al., 2007). These include using stimuli to illicit an exogenous neurological
phenomenon which can be detected within the EEG and classified accordingly. The
forms of stimuli regularly employed include steady-state visually evoked potentials
(SSVEP) (Friman et al., 2007a, 2007b; McCullagh et al., 2010; Parini et al., 2009; Zhu
et al., 2011) or surprise events manifested as a positive wave response in the EEG around
300ms post stimulus, referred to as the P300 response (Bayliss, 2003). Typically, the
P300 response relates to visual events (Teo et al., 2006; Valsan et al., 2009) and auditory
events (Schreuder et al., 2009). Alternatively, self-paced control strategies, commonly
using imagined movement result in potentially recognisable endogenous components of
the EEG (Hinterberger et al., 2003, 2005; Kus et al., 2012; Neuper et al., 2003). Other
task classifications may be applied to signals received after the subject has performed
activities such as mental arithmetic (Millán, 2003).
A survey of BCI technology found that “99% of the designs were targeted at
individuals with some form of severe motor disability” (Mason et al., 2007). However, in
reality, commonly, these conditions are not solely typified by a loss of muscle control.
The impact upon individuals is therefore varied and their full mental and physical
condition can be regarded as both complex and unique. For instance, “ALS patients as a
group often show cognitive impairment in working memory, attention response
inhibition, naming and other functions” (Hinterberger et al., 2005). Differences in
cognitive ability need to be accounted for when devising the operational context of a BCI
and screening tools and assessment procedures have been proposed (Cheng et al., 2002;
Felton et al., 2012; Pasqualotto et al., 2011). Also, some users require high levels of
support and consequently, BCI may be judged more feasible for some rather than others;
hence mechanisms for assessing whether to accept a subject onto a programme or
otherwise have been proposed (Neumann and Kübler, 2003) and attention to defining and
specifying user needs within the BCI context has been addressed, (Zickler et al., 2009).
Similarly in the realm of assistive technology it has been recognised for some time that
devices and interfaces need to be carefully selected and highly tailored to the individual
(Blain-Moraes et al., 2012; de Jonge et al., 2007; Mauri et al., 2007). This suggests that

46

M.P. Ware et al.

all aspects of a BCI application would therefore need to be suited to an individual in
order for the application as a whole to provide for that person’s individual needs
(Vaughan et al., 2003). The important role of user centred design within BCI has been
highlighted recently by the inclusion of target users within the research and development
(Holz et al., 2013; Hoogerwerf et al., 2010; Lightbody et al., 2010), thereby informing the
research community on the variability and scope of individual needs providing greater
insight and understanding on how BCI can operate under real conditions; a necessity for
BCI to function beyond the laboratory setting.
Typically, when BCIs are used for control purposes an onscreen interface is presented
to the user representing the semantic context of the interaction sequence (McCullagh
et al., 2010). Such BCI applications can be considered as having two cooperating
human-computer interfaces, one relating to signal acquisition and classification, and one
relating to the onscreen display of information and the associated control mechanisms. To
provide a single effective and usable form of control, both interfaces need to facilitate the
user. Usability can be said to be an examination of “the fit between the technology and
the person, the tasks and environment” (de Jonge et al., 2007). ISO standard 9241
(International Standards Organization, 2010) defines key usability measures as
effectiveness, efficiency and satisfaction in a specified context of use. Hix and Hartson
(1993) extend the definition, relating to ease of learning, speed of task performance, low
error rate, and facilitating knowledge retention over time. In relation to BCI, aspects of
usability have traditionally addressed factors such as, physical size and sensors (Miranda
and Brouse, 2005), providing usable commands in real-time (Valsan et al., 2009) or on
how onscreen features facilitate BCI operation. A report on the usability of a P300
application presented results which examined the relationship between the required
repetitions of stimuli and achieved percentage command classification accuracy levels
(Holzner et al., 2009). In this study, other usability factors were implied; the number of
on screen items represented, the mapping mechanism for on screen stimuli, and the
associated font size. These studies tend not to encompass an application-based usability
assessment. However, it is recognised that to be successful BCI advances need to be fully
integrated into the realm of assistive technology offering the user operability which is at
least as good as any alternative device and providing a satisfying and rewarding user
experience. In the BCI community this has been the focus of formal discussion (TOBI
Workshop, 2010a, 2010b) and has been addressed in The Future BNCI Project (2012)
roadmap.
To this end, this paper investigates the usability of a BCI application developed as
part of the BRAIN Project (2014), EU FP7-2007-ICT-2-7.2. The focus is on an interface
for domestic control which was designed to be as flexible as possible; thereby attempting
to meet the user’s requirements, and to provide an architectural framework which can
accommodate emerging user requirements or can accommodate updates in the technology
which would represent an enhanced user experience (Allison, 2009).The interface was
subjected to usability assessment using application specific measures which were derived
from well accepted usability principles (Nielsen, 1994; Nielsen and Levy, 1994). At the
initial stage of evaluation, the interface was controlled using mouse activation in order to
assess the user interaction prior to the added complexity and uncertainty of the BCI
technology.
Five people with a range of complex conditions consented to participate as test
subjects for the assessment of the BCI application interface. The process was guided by
ethical approval (University of Ulster REC/09/0034 BRAIN – BCIs with Rapid

A method for assessing the usability of an on screen display for a BCIs

47

Automated Interfaces for Nonexperts). They represented both technically confident and
naïve computer users; all were familiar with a range of existing assistive technologies
which were used in their smarthomes accommodation provided by the Cedar Foundation
(2014). The usability results are assessed and recommendations are derived and
discussed. The measurement method is presented in detail so that the principles of the
technique can be adopted and applied to other BCI applications.
Section 2 presents the application design and compares it to other current BCI
interfaces. Section 3 discusses usability principles, presents our measurement framework
and suggestions concerning its use in the field. The findings are discussed in Section 4,
with conclusions, future work and recommendations presented in Section 5.

2

Background

Different BCI paradigms facilitate different forms of on screen control mechanism due to
the nature of the predefined communication strategy. The basic representational context
of each paradigm is presented in Figure 1. An example P300 interface for spelling uses a
matrix of 36 characters (Holzner et al., 2009). In this scenario, the rows and columns
flash in a random manner with the subject focussing on some item of semantic interest,
either a single letter or instance of grammar. The relative probability of this item flashing
compared to the other items is low and as such its unexpected flash can elicit a detectable
endogenous positive potential within the subject’s EEG at 300 ms post stimulus. This is
referred to as the oddball paradigm (Sutton et al., 1965). With SSVEP a flashing icon or
light can be used to evoke a response in the subject’s EEG that matches the frequency of
the flash rate of the stimulus (and its harmonics). By employing multiple stimuli flashing
at different rates, thereby each with a unique EEG response, a number of control choices
can be made. Practically, there are limits on the number of distinct frequency responses
that can be differentiated (Gao et al., 2003), and is also subject dependent (Müller-Putz
and Pfurtscheller, 2008). Furthermore, an SSVEP system using light emitting diode
(LED) stimulation and an onscreen user interface is not able to effectively discriminate
between a large set of potential stimuli due to the problem of placing an excessive
cognitive load upon the user when attempting to map onscreen representations to an
associated but distinct LED control matrix. A more effective control mechanism can be
achieved by using four LEDs. This is achieved by placing the flexibility of operation into
the corresponding menu structure (Ware et al., 2010a, 2010c) or visual representation
which is then manipulated using four directional controls (up, down, left, and right)
(Maggi et al., 2006; Parini et al., 2009). Martinez et al. (2007) used SSVEP with four
directional chequer board stimuli as controls embedded within the onscreen display
thereby regarded as additional to the context of the information displayed on the screen.
Alternatively, using the imagined movement paradigm, also known as event-related
resynchronisation/event-related synchronisation (ERD/ERS), there is no need for the
application to accommodate a stimulus mechanism as the control is achieved by the user
performing a set task such as imagining moving a hand or foot (Kus et al., 2012).
Typically, two or three way control mechanisms can be possible but training is needed.
The controls can be represented as semantic prompts on the screen which manipulate a
menu structure or visual representation.

48
Figure 1

M.P. Ware et al.
Example of on screen representations for SSVEP (see online version for colours)

Note: Low and high frequency SSVEP, P300 and intended movement or ERD/ERS BCI
paradigms.

The aim of the BRAIN Project was to develop a BCI system that enabled users to control
a range of domestic devices and multimedia applications from one consistent interface
and application framework. The project sought to place the user at the centre of the
design; this was done by involving test subjects from the target user community from an
early stage and thereafter throughout each development cycle (Lightbody et al., 2010).
Furthermore, the project sought to enhance the user experience by adopting
enhancements to hardware and software as they became available and to accommodate as
many prospective users as possible by incorporating various BCI paradigms (Garcia and
Mihajlovic, 2010; Zhu et al., 2011). To this end, the design team sought to build usability
and accessibility into the architecture of the application (Bass et al., 2004; Bosch and
Juristo, 2003). A general purpose interface was devised with a flexible architecture
(McCullagh et al., 2010), as depicted in Figure 2, which can accommodate various
implementations of SSVEP (Friman et al., 2007a, 2007b; Garcia and Mihajlovic, 2010)
or ERD/ERS interactions (Kus et al., 2012). The purpose was to devise an interface
which could be used across paradigms, thereby allowing the user flexibility in BCI
interaction whilst maintaining the context of use on screen. Recent developments in
hybrid BCI aim to offer a range of possible mechanisms to improve user interaction
(Allison et al., 2012; McCullagh et al., 2013; Müller-Putz et al., 2013).
The menu structure is designed to represent to the user their domestic environment.
At the highest level in the menu hierarchy, the rooms of the house are displayed and at
the lower levels the devices associated with the specific rooms. The menu icon
representations are based upon photographs or icons. Only three icons/images are
displayed at a time with the central icon representing the current option. For SSVEP four
LEDs provide the stimuli and are placed on the edges of the screen. The user regards the
LED which corresponds to the required menu action. When a command classification is
made this is confirmed to the user by the appropriate prompt turning red as the command
fires and the interface responds appropriately. The horizontal selections facilitate the
carousel action of the menu items whilst the vertical prompts facilitate traversal of menu
levels. At the lowest node, the downward selection causes the enacting of the appropriate
command, for example, switching a light on or off, or control of a media application.

A method for assessing the usability of an on screen display for a BCIs
Figure 2

Four-way command interface for a BCI using SSVEP, incorporating icons which
carousel horizontally, a hierarchical menu structure, four directional command prompts,
and LEDs associated with the prompts for use as stimuli, (a) hierarchical menu structure
with photographs (b) LEds mounted on screen (see online version for colours)

(a)
Figure 3

49

(b)

IGUI interaction with BCI applications, domestic devices and applications (see online
version for colours)

The interface module was placed at the core of the application, labelled IGUI in Figure 3.
This intuitive graphical user interface (IGUI) displays the hierarchical menu which is
defined in a separate extensible markup language (XML)-based structure. The IGUI
accepts unified command classifications from BCI acquisition modules (resulting from
the user’s interaction with the interface), providing navigation decisions which were then
used to process the menu options, such as scrolling through icons or making a selection to

50

M.P. Ware et al.

activate a device. Device and application interactions are in turn handled through separate
modules which are placed under the control of a wrapper known as the universal
application interface (UAI) (see UAI wrapper in Figure 3). This acts to keep the
complexity of the device and application interaction from the user interface. As
appropriate, device interactions are relayed from the IGUI to the UAI and vice-versa.

3

Method

Research has been undertaken to establish how well the components of the BRAIN
project conform to end user expectations and to establish criteria by which the
appropriateness of a design may be judged (Ware et al., 2010b, 2010d). Concerning the
design of BCI technology it has been identified by Adams et al. (2008) that assistive
technology has sometimes been ‘functionally valuable but aesthetically inadequate’. They
proceed to say that BCI systems of the future need to have ‘sophisticated industrial
design that meets functionality needs and user requirements whilst affording usability,
accessibility, aesthetics and personae’ with companies such as Emotiv (2014) creating
sleek headsets this is very much becoming a reality.
ISO 9241 and ten commonly accepted principles of usability defined by Nielsen and
Levy (1994) were used to provide a systematic framework by which to assess the
interface. An additional principle was added (Hix and Hartson, 1993) to determine if the
application is acceptable to the personae of the individual (Adams et al., 2008).
Furthermore, in deriving applicable measures an approach from measurement in software
engineering was used, goal-question-metric (GQM) (Basili and Rombach 1988), which
formalises the statement of the measurement goal, the definition of questions in keeping
with the goal, ensuring that the responses can be expressed in quantifiable terms and that
appropriate metrics are selected, see Figure 4. This approach has been used successfully
in the field of software quality and process engineering (Tvedt et al., 2002; Ware et al.,
2008). It uses a standard template into which context relevant terminology is inserted,
resulting in a structured expression of purpose. From the resulting expression of the goal,
questions appropriate to the domain are devised, from these questions appropriate
measurement methods are identified, i.e., observable quantifiable attributes related to
suitable classification of indicator. Therefore, in this context, each usability principle is
substituted in the template in turn and related to the context of the BCI software, thereby
informing the derivation of the BCI interface specific questions and the context of
measurement – see Tables 1 to 10 in the Appendix.
The elicitation process was carefully planned and handled, aiming to be sensitive to
and focused upon the particular needs of the user in order to ensure that a good flow of
information was established (Sustar et al., 2008):
•

A structured conversational approach was adopted with mutual self-disclosure used
as a prompt in discussion.

•

Interviews were conducted in their residential setting of the Cedar Foundation.

A method for assessing the usability of an on screen display for a BCIs

51

•

All users had physical impairments ranging from: cerebral palsy, stroke,
hydrocephalus with associated physical impairment and brain injury.

•

The group consisted of four females and one male.

•

Two of the group were computer users and the others were not; all used various
forms of assistive technology which was part of the infrastructure of the living
environment provided by Cedar.

•

The subjects knew the researchers involved and were familiar with the purpose of the
application.

•

In some circumstances, the users had physical difficulties in operating the interface
therefore where necessary care givers were used to operate the equipment according
to the expressed wishes of the individual and one subject submitted their own
responses in writing.

•

Where opinions were sought but could not be given due to speech inhibition the
attitudes of the individual expressed as actions or as eloquent body language were
taken to be representative, sometimes the questions were adjusted so that the user
could demonstrate rather than explain their understanding.

Figure 4

The GQM applied to the BCI interface evaluation (see online version for colours)
10 commonly accepted
principles of usability

1: acceptability to the
personae of the individual

Deriving applicable measures
Goal, Question, Metric

Goal
Measurement goal

Question
Applicable to domain
with quantifiable

Metric
Measurable

Assessment

Analysis of results and refinement of interface

The operation of the BCI in terms of obtaining EEG signals and classifications was not
used at this stage. This has been independently assessed from the user perspective
elsewhere (Ware et al., 2010a, 2010b), and the users contributing to this assessment were
familiar with its operation. In this initial instance the desire was to focus solely upon user
interaction with the interface alone so to gauge its suitability for the target user group
before adding the extra layer of complexity in terms of BCI control. Detailed assessments

52

M.P. Ware et al.

of the end-to-end use of BRAIN software had been made with non-disabled users but at
this stage BCI studies for users with a physical disability was being performed as an
ongoing incremental process within the project (Garcia and Mihajlovic 2010; Kus et al.,
2012; Ware et al., 2010a, 2010b). The evaluation process is depicted in Figure 5.
Figure 5

Evaluation process (see online version for colours)

Figure 6

Feedback pane within the IGUI (see online version for colours)

A method for assessing the usability of an on screen display for a BCIs
Figure 7

4

53

IGUI interface with Wigit images indicating a UAI interaction error (see online version
for colours)

Results

Once all users’ interaction with the interface had been assessed the responses were
tabulated and summarised for each usability principle (see Tables 1 to 10 in the
Appendix). The general observations derived from these results are listed below.

4.1 User computer literacy
The sample group of 5 was small and therefore not statistically significant. However, the
answers did reveal a divide between users who felt confident in the use of the application
and those that did not. Two users were technically confident and this tended to be
reflected in their interaction capabilities, one user was somewhat less confident and two
users tended to interact with technology less and maybe their cognitive capabilities
impinged on their ability to engage with and effectively operate this interface. However,
the computer naïve users were able to give voice to their concerns and from the input of
these individuals’ serious issues were raised and explored. It was felt that this would not
have been the case had assessments merely been based upon healthy and technically
competent users from the university environment.

4.2 User interface
It was found that the users could understand the high level objective of operating
domestic controls through the interface. For some users they were satisfied that they
could interact with and navigate the system effectively, for others, the difficulty came in
how this was achieved. The arrow-based prompts to the LEDs can be used directly as
controls to the interface when the BCI element of the system is not in operation. The

54

M.P. Ware et al.

technical users were confident with the function of these arrows understanding that the
directional input activated the control of the menu icons (Table 1).

4.3 Visibility of system status and match to the real world:
(usability principles 1 and 2)
When asked, the users could perform various tasks (navigate to, turn device on/off,
cancel a command, etc.) with variable degrees of confidence. They also exhibited
transferable skills – learning an action in one context and transferring the same action to a
new context, and they exhibited use of shortcuts such as moving left once rather than
multiple rights (Tables 1 and 2).

4.4 User control and freedom and consistency and standards: (usability
principles 3 and 4)
To a varying extent and in different ways the three naïve computer users tended to exhibit
confusion concerning the control interaction sequence, the function of the directional
prompts, directional orientation, and the purpose of the menu icons. This was expressed
on the user’s part as:
•

A lack of confidence in the outcome of arrow operation.

•

An inability to plan an action sequence for a purpose.

•

An inability to activate an arrow in order to manipulate a menu position with a
degree of intent.

•

Confusion concerning the relationship between the directional arrow and the menu
item that they intended to manipulate. They felt like they should be reaching directly
for the menu icon.

Levels of displacement between prompts and menu icons appeared to be the cause of this
confusion, suggesting that this would be even more the case if a third element and level
of displacement was introduced, i.e., when the LEDs were used to activate commands.
In addition, for one user the use of the up/down arrows to transverse each level of
hierarchy in the interface as well as the rotation of the carousel icons left and right was
confusing. In essence, concerning the interaction of these onscreen elements, it is fair to
suggest that this interface in its present form failed to support computer naïve users or
users who require additional cognitive support and assistance. The revised interface
should walk the user through their task and should aim to minimalise the user’s
dependence upon directional manipulation, and provide additional status and location
information within the menu structure.
Consideration is also needed regarding the match between the system and real world.
All the users expressed satisfaction with the use of photographs for menu icons and as
conceptual representations. However, clearly there was confusion between onscreen
items and their functions so perhaps the representation could be improved. When asked,
users were more confident with device representations than with room representations, in
a sense this is curious because device representations are more abstract, i.e., showing the
same illuminated light bulb to indicate the light in each room of the house. Subsequent
advice from experts in the field of speech and language therapy suggests that the use of

A method for assessing the usability of an on screen display for a BCIs

55

simplified drawings/symbols “requires different levels of visual, cognitive and language
processing than those required to process the information contained within a composite
picture; as such simplified drawings/symbols are frequently used to access other assistive
interfaces” (Blaney, 2010), for example (Millar, 2014).

4.5 Help error recognitions and diagnostics and error prevention (usability
principles 5 and 6)
Aside from confusion over navigation the interface exhibited no formal error. However,
one user inadvertently exited the application and expressed dissatisfaction as this
interrupted their interaction. Accuracy rates for command classifications using
SSVEP-based BCI do not tend to exceed 80% (Ware et al., 2010b). Therefore, it is
possible that other erroneous commands can cause the same disruption, suggesting that a
confirmatory dialogue would provide a more graceful mechanism for exiting the
application.

4.6 Recognition over recall and flexibility, efficiency of user and aesthetic or
minimalist design (usability principles 7 to 9)
The photographs used in the interface were supplemented with labels, which most users
were satisfied with. However, some felt that both font and image size should be bigger in
order to accommodate short sight. One felt that the label was redundant. Generally, the
users expressed appreciation of the aesthetics of the interface; they tended to like the
black background feeling that it was not obtrusive. However, it was acknowledged by
one user that a lighter colour background may improve the visibility of the on screen
components. One suggested that if the layout were to be modified then a tiled
representation similar to a solitaire game may be applicable.

4.7 Help and documentation (usability principle 10)
Concerning additional support, the reaction was mixed. The technically competent users
felt that no additional training was required, other users that it might be useful, whilst
computer naïve users who had expressed dis-satisfaction felt that it would not make a
difference. Where support was felt to be a good idea this should take the form of training
and personal assistance. Written documentation was generally thought by the users to be
of use if a care assistant was on hand to mediate the knowledge.

5

Conclusions

An assistive interface for use by people with physical disability was implemented and
tested prior to integration into a domestic BCI. The architecture of the application
attempted to support a flexible approach to emerging technologies and to exhibit
characteristics which supported the usability of the application. A systematic means of
assessing the interface for its ability to provide a nurturing and effective environment for
the target users was devised. The assessment method was based upon recommendations
in ISO 9124 and commonly accepted principles of usability combined with the GQM

56

M.P. Ware et al.

technique. These elements were brought together to provide a comprehensive elicitation
process which was conducted in accordance with recommendations from other usability
studies, some focusing upon other physically vulnerable individuals. The process of
constructing the study is detailed in full so that other studies may adopt this approach or
elements of the approach should they have to assess their applications. By providing a
structured framework for usability assessment which can be tailored to a specific context
it is hoped that the same techniques can be duplicated across studies thereby providing a
uniform format for eliciting results and facilitating the contrasting of findings across
research projects and user groups.
The study was conducted at the premises of the Cedar Foundation with the
cooperation of five residents who all have complex needs. Informed consent was
obtained at the outset after careful explanation of the procedures. The findings proved to
be informative. Technically minded users were able to navigate the interface in an
efficient and effective manner. They did have recommendations to make concerning
additional facilities and services that they would like to see; for instance, the inclusion of
a status bar and the ability to personalise the colour scheme. For naïve computer users
there were fundamental elements of the operation of the interface which proved to be too
complex for them, making interaction confusing and obscuring the user’s ability to
conduct a series of tasks. This finding concurs with a study performed by Blain-Moraes
et al. (2012, p.6) in which they investigated barriers to and mediators of BCI. They found
that those with technical knowledge expressed confidence in “their ability to learn to
autonomously use and operate a BCI”. However, those uncomfortable with technology
found the complexity of the BCI systems overwhelming, with one user stating “how can
this be made accessible to the computer illiterate or technologically illiterate …?”
[Blain-Moraes et al., (2012), p.6)]. Without existing technology competence the naïve
user felt that the BCI operation was beyond their capacity for independent use.
In relation to the complexity of the IGUI, this finding is also significant because fourway command interfaces are used by other projects working with the SSVEP paradigm. It
also suggests that for a significant number of users with a disability an interface needs to
be devised which will be expressly tailored to supporting the user’s cognitive abilities as
well as using BCI to compensate for difficulties with physical interaction. This finding
raises questions concerning the levels of complexity to be found in an interface, what is
required in order to support different BCI paradigms, and how this is to be implemented
if the interface is also providing a supportive and facilitating environment for the user.
The NASA Task Load Index (TLX) has been used by Felton et al. (2012) to provide a
measure of the mental workload during BCI training comparing disabled and able bodied
users. Using a BCI imagery paradigm to move a cursor in one dimension they found little
difference between computer literate disabled and able bodied users. However, with
being used on such a simple interface it would be interesting to see the outcome of the
NASA TLX used on the BRAIN interface for SSVEP. The key aspects of the NASA
TLX are mental demands, physical demands, temporal demands, performance, effort and
frustration – a numerical value being determined for each and an overall value calculated.
Pasqualotto et al. (2011) also used the NASA TLX to measure cognitive workload for
BCI, combined with system usability scale for usability.
Using a structured means electing responses based upon usability principles ensured
that the assessment was both systematic and directed. This provided a context within
which the users could base their responses and give more detailed feedback concerning
their use of the interface than might otherwise have been the case.

A method for assessing the usability of an on screen display for a BCIs

57

Concerning the interface for the BRAIN project and the underlying architecture of the
IGUI, it is clear that the current four-way command interface is, with some modifications,
capable of supporting some members of our target user group. It was therefore
maintained as part of the application and upgraded to incorporate the suggestions made
by these users. However, it is equally clear that a cognitively assistive interface is also
required. An additional interface should therefore be supplied which is specifically
devised to assist the users towards their objectives in any interaction sequence.
Improvements to the IGUI have been undertaken in response to the users’ feedback and
the key changes are listed in Table 11.
The advantage of the IGUI and BCI system architecture is its ability to support an
additional interface without significant modification thereby proving the flexibility and
adaptability of the original systems design.

Acknowledgements
The research leading to these results has received funding from the European
Community’s Seventh Framework Programme under grant agreement no. 224156.

References
Acharya, S., Aggarwal, V., Tenore, F., Shin, H.C., Etienne-Cummings, R., Schieber, M.H. and
Thakor, N.V. (2007) ‘Towards a brain-computer interface for dexterous control of a multifingered prosthetic hand’, in Neural Engineering, 2007. CNE‘07. 3rd International
IEEE/EMBS Conference on, pp.200–203, IEEE.
Adams, R., Bahr, G. and Moreno, B. (2008) ‘Brain computer interfaces: psychology and pragmatic
perspectives for the future’,. Society for the Study of Artificial Intelligence and the Simulation
of Behaviour, Proceedings, Vol. 5, pp.1–6, ISBN: 1 902956 64 8.
Allison, B. (2009) ‘The I of BCIs: next generation interfaces for brain-computer interface systems
that adapt to individual users’, in Human-Computer Interaction. Novel Interaction Methods
and Techniques, pp.558–568, Springer, Berlin Heidelberg.
Allison, B., Brunner, C., Altstätter, C., Wagner, I., Grissmann, S. and Neuper, C. (2012) ‘A hybrid
ERD/SSVEP BCI for continuous simultaneous two dimensional cursor control’, Journal of
Neuroscience Methods, Vol. 209, No. 2, pp.299–307.
Basili, V. and Rombach, H. (1988) ‘The TAME project: towards improvement-oriented software
environments’, IEEE Transactions on Software Engineering, Vol. 14, No. 6, pp.758–773.
Bass, L., John, B.E., Juristo, N. and Sanchez-Segura, M.I. (2004) ‘Usability-supporting
architectural patterns’, in Proceedings of the 26th International Conference on Software
Engineering, pp.716–717, IEEE Computer Society, May.
Bayliss, J. (2003) ‘Use of the evoked potential P3 component for control in a virtual apartment’,
IEEE Transactions on Neural Systems and Rehabilitation Engineering, Vol. 11, No. 2,
pp.113–116.
Blain-Moraes, S., Schaff, R., Gruis, K., Huggins, J. and Wren, P. (2012) ‘Barriers to and mediators
of brain-computer interface user acceptance: focus group findings’, Ergonomics, Vol. 55,
No. 5, pp.516–525.
Blaney, B. (2010) Private Communication by Email, Communication Matters, Communication
Advice Centre, Belfast Health and Social Care Trust, Regional Disablement Services,
Musgrave Park Hospital, Stockmans Lane, Belfast, Received 14 October 2010.

58

M.P. Ware et al.

Bosch, J. and Juristo, N. (2003) ‘Designing software architectures for usability’, in the Proceedings
of the 25th, International Conference on Software Engineering, pp.757–758.
BRAIN Project (2014) EU project BRAIN – Brain-project.org. [online] http://brain-project.org
(accessed 29 July 2014).
Cecotti, H. (2011) ‘Spelling with non-invasive brain-computer interfaces – current and
future trends’, Journal of Physiology-Paris, Vol. 105, Nos. 1–3, pp.106–114 [online]
http://dx.doi.org/10.1016/j.jphysparis.2011.08.003 (accessed 29 July 2014).
Cedar Foundation (2014) Disability Charity | Northern Ireland | Disabled People | Cerebral Palsy
| The Cedar Foundation | Disability | Disabled [online] http://www.cedar-foundation.org/
(accessed 29 July 2014).
Cheng, M., Gao, X., Gao, S. and Xu, D. (2002) ‘Design and implementation of a brain-computer
interface with high transfer rates’, IEEE Transactions on Biomedical Engineering, Vol. 49,
No. 10, pp.1181–1186.
De Jonge, D., Scherer, M. and Rodger, S. (2007) Assistive Technology in the Workplace, 1st ed.,
Mosby Elsevier, St. Louis, Mo.
Emotiv (2014) Emotiv EPOC Neuroheadset, Neurotechnology | High Resolution EEG |
Neuroimaging [online] http://www.emotiv.com/apps/epoc/299/ (accessed 29 July 2014).
Felton, E., Williams, J., Vanderheiden, G. and Radwin, R. (2012) ‘Mental workload during
brain-computer interface training’, Ergonomics, Vol. 55, No. 5, pp.526–537.
Friman, O., Luth, T., Volosyak, I. and Graser, A. (2007a) ‘Spelling with steady-state visual evoked
potentials’, in the Proceedings of the 3rd. IEEE International Conference on Neural
Engineering, Hawaii, pp.354–357.
Friman, O., Volosyak, I. and Graser, A. (2007b) ‘Multiple channel detection of steady-state visual
evoked potentials for brain-computer interfaces’, IEEE Transactions on Biomedical
Engineering, Vol. 54, No. 4, pp.742–-750.
Gao, X., Xu, D., Cheng, M. and Gao, S. (2003) ‘A BCI-based environmental controller for the
motion-disabled’, IEEE Transactions on Neural Systems and Rehabilitation Engineering,
Vol. 11, No. 2, pp.137–140.
Garcia, G. and Mihajlovic, V. (2010) ‘Spatial filters to detect steady-state visual evoked potentials
elicited by high frequency stimulation: BCI application’, BiomedizinischeTechnik/Biomedical
Engineering, Vol. 55, No. 3, pp.173–182.
George, H., Hosle, A., Franz, D. and Kubler, A. (2010) ‘Brain painting – BCI meets patients and
artists in the field’, in Integrating Brain-Computer Interfaces with Conventional Assistive
Technology, TOBI Workshop, p.43.
Hinterberger, T., Mellinger, J. and Birbaumer, N. (2003) ‘The thought translation device: structure
of a multimodal brain-computer communication system’, in the Proceedings of the 1st
International IEEE EMBS Conference on Neural Engineering, pp.603–606.
Hinterberger, T., Wilhelm, B., Mellinger, J., Kotchoubey, B. and Birbaumer, N. (2005) ‘A device
for the detection of cognitive brain functions in completely paralyzed or unresponsive
patients’, IEEE Transactions on Biomedical Engineering, Vol. 52, No. 2, pp.211–220.
Hix, D. and Hartson, H.R. (1993) Developing User Interfaces: Ensuring Usability: Through
Product and Process, pp.51–52, John Wiley & Sons, New York.
Holz, E., Kaufmann, T., Desideri, L., Malavasi, M., Hoogerwerf, E. and Kubler, A. (2013) ‘User
centred design in BCI development’, in Towards Practical Brain-Computer Interfaces,
pp.155–172, Springer.
Holzner, C., Guger, C., Edlinger, G., Gronegress, C. and Slater, M. (2009) ‘Virtual smart home
controlled by thoughts’, in the Proceedings of the 18th IEEE Workshop on Enabling
Technologies: Infrastructures for Collaborative Enterprises, pp.236–239.
Hoogerwerf, E., Mongardi, S., Staiger-Sälzer, P. and Zickler, C. (2010) ‘BCI research and user
involvement: the role of independent AT centres in the TOBI project’, in Assistive
Technology-Technology Transfer, Proc of the AAATE Workshop, pp.32–35.

A method for assessing the usability of an on screen display for a BCIs

59

International Standards Organization (2010) Ergonomics of Human-System Interaction – Part 100:
Introduction to Standards Related to Software Ergonomics, PD ISO/TR 9241-100:2010,
Geneva, Switzerland.
Kapeller, C., Hintermüller, C. and Guger, C. (2012) ‘Augmented control of an avatar using an
SSVEP based BCI’, Proceedings of the 3rd Augmented Human International Conference,
p.27.
Kus, R., Valbuena, D., Zygierewicz, J., Malechka, T., Graeser, A. and Durka, P. (2012)
‘Asynchronous BCI based on motor imagery with automated calibration and neurofeedback
training’, IEEE Transactions on Neural Systems and Rehabilitation Engineering, Vol. 20,
No. 6, pp.823–835.
Lightbody, G., Ware, M., McCullagh, P., Mulvenna, M., Thomson, E., Martin, S., Todd, D.,
Medina, V. and Martinez, S. (2010) ‘A user centred approach for developing brain-computer
interfaces’, 2010 4th International Conference on Pervasive Computing Technologies for
Healthcare (PervasiveHealth), pp.1–8.
Lin, C-T., Lin, B-S., Lin, F-C. and Chang, C-J. (2014) ‘Brain computer interface-based smart living
environmental auto-adjustment control system in UPnP home networking’, Systems Journal,
June, Vol. 8, No. 2, pp.363–370, IEEE, doi: 10.1109/JSYST.2012.2192756.
Maggi, L., Parini, S., Piccini, L., Panfili, G. and Andreoni, G. (2006) ‘A four command BCI system
based on the SSVEP protocol’, in the Proceedings of the 28th IEEE EMBS Annual
International Conference, pp.1264–1267.
Martinez, P., Bakardjian, H. and Cichocki, A. (2007) ‘Fully online multicommand brain-computer
interface with visual neurofeedback using SSVEP paradigm’, Computational Intelligence
and Neuroscience, Article ID 94561, pp.1–9, doi:10.1155/2007/94561 [online]
http://dl.acm.org/citation.cfm?id=1321361 (accessed 1 October 2014).
Mason, S., Bashashati, A., Fatourechi, M., Navarro, K. and Birch, G. (2007) ‘A comprehensive
survey of brain interface technology designs’, Annals of Biomedical Engineering, Vol. 35,
No. 2, pp.137–169.
Mauri, C., Granollers, T. and Solanas, A. (2007) ‘On the assessment of the interaction quality of
users with cerebral palsy’, in the Proceedings of the Second International Conference on
Availability, Reliability and Security, pp.799–805.
McCullagh, P., Galway, L. and Lightbody, G. (2013) ‘Investigation into a mixed hybrid using
SSVEP and eye gaze for optimising user interaction within a virtual environment’, Lecture
Notes in Computer Science, pp.530–539, Springer.
McCullagh, P.J., Ware, M.P., Lightbody, G., Mulvenna, M.D., McAllister, H.G. and Nugent, C.D.
(2010) ‘Brain computer interfaces for inclusion’, in the Proceedings of the 1st International
Conference of the Augmented Human.
Millán, J. (2003) ‘Adaptive brain interfaces’, Communications of the ACM, Vol. 46, No. 3,
pp.74–80.
Millán, J., Rupp, R., Müller-Putz, G., Murray-Smith, R., Giugliemma, C., Tangermann, M.,
Vidaurre, C., Cincotti, F., Kubler, A., Leeb, R. et al. (2010) ‘Combining brain-computer
interfaces and assistive technologies: state-of-the-art and challenges’, Frontiers in
Neuroscience, Vol. 4, Article 161, pp.1–45, doi: 10.3389/fnins.2010.00161 [online]
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2944670/ (accessed 1 October 2014).
Millar S. (2014) A Guide to Picture and Symbol Sets for Communication [online]
http://www.callscotland.org.uk/resources/quick-guides/AAC/A-Guide-to-Picture-and-SymbolSets-for-Communication/ (accessed July 2014).
Miranda, E. and Brouse, A. (2005) ‘Interfacing the brain directly with musical systems: on
developing systems for making music with brain signals’, Leonardo, Vol. 38, No. 4,
pp.331–336.
Mugler, E., Ruf, C., Halder, S., Bensch, M. and Kubler, A. (2010) ‘Design and implementation of a
P300-based brain-computer interface for controlling an internet browser’, IEEE Transactions
on Neural Systems and Rehabilitation Engineering, Vol. 18, No. 6, pp.599–609.

60

M.P. Ware et al.

Müller-Putz, G. and Pfurtscheller, G. (2008) ‘Control of an electrical prosthesis with an
SSVEP-based BCI’, IEEE Transactions on Biomedical Engineering, Vol. 55, No. 1,
pp.361–364.
Müller-Putz, G.R., Leeb, R., Millán, J.D.R., Horki, P., Kreilinger, A., Bauernfeind, G., Allison, B.,
Brunner, C. and Scherer, R. (2013) ‘Principles of hybrid brain-computer interfaces’, in
Towards Practical Brain-Computer Interfaces, pp.355–373, Springer, Berlin, Heidelberg.
Neumann, N. and Kübler, A. (2003) ‘Training locked-in patients: a challenge for the use of
brain-computer interfaces’, IEEE Transactions on Neural Systems and Rehabilitation
Engineering, Vol. 11, No. 2, pp.169–172.
Neuper, C., Müller, G., Kübler, A., Birbaumer, N. and Pfurtscheller, G. (2003) ‘Clinical application
of an EEG-based brain-computer interface: a case study in a patient with severe motor
impairment’, Clinical Neurophysiology, Vol. 114, No. 3, pp.399–409.
Nielsen, J. (1994) ‘Enhancing the explanatory power of usability heuristics’, in the Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems: Celebrating
Interdependence, pp.152–158.
Nielsen, J. and Levy, J. (1994) ‘Measuring usability: preference vs. performance’, Communications
of the ACM, Vol. 37, No. 4, pp.66–75.
Nijholt, A. (2009) ‘BCI for games: a ‘state of the art’ survey’, Entertainment Computing-ICEC
2008, Springer, pp.225–228.
Parini, S., Maggi, L., Turconi, A. and Andreoni, G. (2009) ‘A robust and self-paced BCI system
based on a four class SSVEP paradigm: algorithms and protocols for a high-transfer-rate direct
brain communication’, Computational Intelligence and Neuroscience, 2009, Hindawi
Publishing Corporation.
Pasqualotto, E., Simonetta, A., Gnisci, V., Federici, S. and Belardinelli, M. (2011) ‘Toward a
usability evaluation of BCIs’, International Journal of Bioelectromagnetism, Vol. 13, No. 3,
pp.121–122.
Schalk, G., McFarland, D., Hinterberger, T., Birbaumer, N. and Wolpaw, J. (2004) ‘BCI2000: a
general-purpose brain-computer interface (BCI) system’, IEEE Transactions on Biomedical
Engineering, Vol. 51, No. 6, pp.1034–1043.
Scherer, R., Schloegl, A., Lee, F., Bischof, H., Janvsa, J. and Pfurtscheller, G. (2007) ‘The
self-paced Graz brain-computer interface: methods and applications’, Computational
Intelligence and Neuroscience, 2007, Hindawi Publishing Corporation.
Schreuder, M., Tangermann, M. and Blankertz, B. (2009) ‘Initial results of a high-speed spatial
auditory BCI’, Int. J. Bioelectromagnetism, Vol. 11, No. 2, pp.105–109.
Sustar, H., Pfeil, U. and Zaphiris, P. (2008) ‘Requirements elicitation with and for older adults’,
Software, IEEE, Vol. 25, No. 3, pp.16–17.
Sutton, S., Braren, M., Zubin, J. and John, E. (1965) ‘Evoked-potential correlates of stimulus
uncertainty’, Science, Vol. 150, No. 3700, pp.1187–1188.
Teo, E., Huang, A., Lian, Y., Guan, C., Li, Y. and Zhang, H. (2006) ‘Media communication center
using brain computer interface’, in the Proceedings of the 28th IEEE EMBS Annual
International Conference, pp.2954–2957.
The Future BNCI Project (2012) Future BNCI: A Roadmap for Future Directions in
Brain/Neuronal Computer Interaction [online] http://bnci-horizon-2020.eu/images/bncih2020/
FBNCI_Roadmap.pdf (accessed 8 October 2014).
TOBI Workshop (2010a) TOBI Workshop 2010 | TOBI: Tools for Brain-Computer Interaction,
Integrating Brain-Computer Interfaces with Conventional Assistive Technology [online]
http://www.tobi-project.org/tobi-workshop-2010 (accessed 29 July 2014).
TOBI Workshop (2010b) TOBI Workshop ll | TOBI: Tools for Brain-Computer Interaction,
Translational Issues in BCI Development: User Needs, Ethics, and Technology Transfer
[online] http://www.tobi-project.org/TOBI-workshop-2 (accessed 29 July 2014).

A method for assessing the usability of an on screen display for a BCIs

61

Todd, D., McCullagh, P., Mulvenna, M. and Lightbody, G. (2012) ‘Investigating the use of
brain-computer interaction to facilitate creativity’, Proceedings of the 3rd Augmented Human
International Conference, p.19.
Tvedt, R., Costa, P. and Lindvall, M. (2002) ‘Does the code match the design? A process for
architecture evaluation’, in the IEEE Proceedings of the International Conference on Software
Maintenance. pp.393–401.
Valbuena, D., Cyriacks, M., Friman, O., Volosyak, I. and Graser, A. (2007) ‘Brain-computer
interface for high-level control of rehabilitation robotic systems’, 10th International
Conference on Rehabilitation Robotics, IEEE, pp.619–625.
Valsan, G., Grychtol, B., Lakany, H. and Conway, B. (2009) ‘The strathclyde brain computer
interface’, in the Proceedings of the 31st Annual International Conference of the IEEE EMBS,
pp.606–609.
Vaughan, T.M., Heetderks, W.J., Trejo, L.J., Rymer, W.Z., Weinrich, M., Moore, M.M.,
Kübler, A., Dobkin, B.H., Birbaumer, N., Donchin, E., Wolpaw, E.W. and Wolpaw, J.R.
(2003) ‘Guest editorial. brain-computer interface technology: a review of the second
international meeting’, in the IEEE Transactions on Neural Systems and Rehabilitation
Engineering, Vol. 11, No. 2, pp.94–109.
Ware, M., Lightbody, G., McCullagh, P., Mulvenna, M., Martin, S. and Thomson, E. (2010a)
‘Assessing the accessibility of a domestic Brain Computer Interface and the implications for
future design choices’, in the IEEE International Conference on Information Technology and
Applications in Biomedicine, November, pp.1–5.
Ware, M., McCullagh, P., McRoberts, A., Lightbody, G., Nugent, C., McAllister, G.,
Mulvenna, M., Thomson, E. and Martin, S. (2010b) ‘Contrasting levels of accuracy in
command interaction sequences for a domestic brain-computer interface using SSVEP’, in the
Proceedings of the 5th Cairo International Conference on Biomedical Engineering, Sponsored
by the IEEE Engineering in Medicine and Biology Society (EMBS), pp.150–153.
Ware, M., McCullagh, P., Mulvenna, M., Nugent, C., McAllister, H. and Lightbody, G. (2010c) ‘A
universal command structure for multiple domotic device interactions’, in the Proceedings of
the TOBI Workshop on Integrating Brain-Computer Interfaces with Conventional Assistive
Technology, p.41.
Ware, M.P., Lightbody, G., McCullagh, P.J. and Mulvenna, M.D. (2010d) ‘Temporal and
representational complexity in the semantics of a brain-computer interface’, in the
Proceedings of the TOBI Workshop on Translational Issues in BCI Development: User Needs,
Ethics, and Technology Transfer, December.
Ware, M., Wilkie, F., Shapcott, M. and Lester, N. (2008) ‘The use of product measures in tracking
code development to completion within small to medium sized enterprises’, in the
Proceedings of the 2008 Sixth International Conference on Software Engineering Research,
Management and Applications, pp.263–270.
Wolpaw, J., McFarland, D., Vaughan, T. and Schalk, G. (2003) ‘The Wadsworth Center
brain-computer interface (BCI) research and development program’, IEEE Transactions on
Neural Systems and Rehabilitation Engineering, Vol. 11, No. 2, pp.1–4.
Zhu, D., Garcia-Molina, G., Mihajlovi’c, V. and Aarts, R. (2011) ‘Online BCI implementation of
high-frequency phase modulated visual stimuli’, in Stephanidis, C. (Ed.): Universal Access in
Human-Computer Interaction Users Diversity, Vol. 6766, pp.645–654, Springer-Verlag,
Berlin, Heidelberg.
Zickler, C., Di Donna, V., Kaiser, V., Al-Khodairy, A., Kleih, S., Kübler, A., Malavasi, M.,
Mattia, D., Mongardi, S., Neuper, C. et al. (2009) ‘BCI applications for people with
disabilities: defining user needs and user requirements’, Assistive Technology from Adapted
Equipment to Inclusive Environments, AAATE 25 Assistive Technology Research Series,
pp.185–189.

62

M.P. Ware et al.

Appendix
Usability results
Table 1

Usability results – principle 1

Usability principle 1: visibility of system status
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing the provision of
appropriate feedback] with respect to [action taken] from the viewpoint of [the user] in the
[context of regular use].
Questions:
Q1.1

Can the user describe how they know that an arrow has been activated?

Q1.2

On activation of a menu navigation command can the user describe the action that has
been taken?

Q1.3

On issuing a command can the user describe what has taken place?

Measurable metrics:
M1.1

Three users know how to activate a command arrow, two do not.

M1.2

Three are confident that they know what will happen once pressed; two were not.

M1.3

One user found the directional aspect of menu traversal confusing (i.e., rotating the
icons).
One user found the hierarchical structure of the menu confusing, muddling the higher
and lower levels of the menu with the upstairs and downstairs of a house.

Table 2

Usability results – principle 2

Usability principle 2: match between the system and the real world
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing images and command
arrows] with respect to [representation ability] from the viewpoint of [the user] in the [context
of regular use].
Questions:
Q2.1

Do the photographs of a home and domestic items convey to the user the command
context?

Q2.2

Does the use of command arrows convey to the user the actions required to enact a
command?

Q2.3

Are the labels used on the pictorial images appropriate or useful?

Measurable metrics:
M2.1

There were no complaints concerning the use of photographs for device control icons.

M2.2

Two of the users, who have prior technology use, were confident concerning the
operation of the command arrows.
However the other three exhibited confusion between the photographic icons and the
command arrows which are used to manipulate menu selections represented by the
icons.

M2.3

Four users were happy with the use of icon labels; one felt that they were not needed.
A common opinion felt that the font should be bigger.

A method for assessing the usability of an on screen display for a BCIs
Table 3

63

Usability results – principle 3

Usability principle 3: user control and freedom
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing user control and
freedom] with respect to [freedom of navigation] from the viewpoint of [the user] in the [context
of regular use].
Questions:
Q3.1

Can you describe how you would select a menu item?

Q3.2

Can you describe how you would re-locate from one menu selection to another?

Q3.3

Can you describe how you would turn a device off having first turned it on?

Q3.4

Can you describe how you would back out of a menu selection?

Q3.5

Can you describe how you would exit the application?

Measurable metrics:
M3.1,
M3.2

Only one user was confident enough to navigate to a selected menu item without
assistance. Due to the confusion between directional command arrows and the
manipulation of menu icons the other users found it difficult when trying to navigate to
a specified item.

M3.3,
M3.4

The three users who felt at home with technology were confident that they could
successfully turn devices off and on, and that they could cancel menu selections.
However the other two users less familiar with technology were not sure.

M3.5

Most users were not confident that they could exit the application, this was not intuitive
and they were not familiar with this action.

Table 4

Usability results – principle 4

Usability principle 4: consistency and standards
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing images and command
arrows] with respect to [consistent representational ability] from the viewpoint of [the user] in
the [context of regular use].
Questions:
Q4.1

Can you describe what the high level menu items represent?

Q4.2

Can you describe what the device menu items represent?

Q4.3

Can you describe the operation of the command arrows?

Q4.4

Can you describe any exceptions to the operation of the command arrows?

Measurable metrics:
M4.1

The users found the use of generic photographs to represent the rooms of a house
confusing.

M4.2

However in contrast the users did not complain about the photographic representation
of devices in the application. These tended to be more abstract but did represent single
items (see also M2.1).

M4.3,
M4.4

The two technically competent users were confident with the operation and purpose of
the command arrows. The other users were less confident. No user had noted any
exception to the operation of the command arrows.

64

M.P. Ware et al.

Table 5

Usability results – principle 5

Usability principle 5: help error recognition and diagnostics
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing error handling] with
respect to [informing] from the viewpoint of [the user] in the [context of regular use].
Questions:
Q5.1
Can you describe anything un-expected during interaction with the application?
Q5.2
Can you describe your understanding of the problem?
Measureable metrics:
M5.1
M5.2

No technical exceptions (errors) occurred during the interaction sequences – this was
unfortunate as a BCI operation would be more vulnerable to error commands.
Where the users were experiencing difficulty understanding the operation and function
of the navigational command arrows they tended to feel that they could not describe the
difficulties that they were experiencing.

Table 6

Usability results – principle 6

Usability principle 6: error prevention
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing error recovery] with
respect to [smooth operation] from the viewpoint of [the user] in the [context of regular use].
Questions:
Q6.1
Can you describe anything that did not work during your interaction with the
application?
Q6.2
Did this cause you a problem when interacting with the application?
Measureable metrics:
M6.1 As the users had not encountered error conditions they tended not to be asked this
question.
M6.2 However, one technically competent user reported a problem due to un-expectedly
exiting the application. It was felt that this disrupted interaction with the application.
Table 7

Usability results – principle 7

Usability principle 7: recognition over recall
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing intuitive understanding]
with respect to [navigation and use] from the viewpoint of [the user] in the [context of regular
use].
Questions:
Q7.1

On first introduction to the application can the user learn navigation techniques from
one room/device and apply the same principle to navigation to a second room/device?

Q7.2
Does the user become lost in the application?
Q7.3
Can the user readily understand the use of the command arrows?
Measureable metrics:
M7.1 The two technically competent users demonstrated transferrable skills when navigating
the menu structure.
M7.2,
M7.3

The other three users felt lost in the application and did not readily recognise the
function of the navigational arrows.

A method for assessing the usability of an on screen display for a BCIs
Table 8

Usability results – principle 8

Usability principle 8: flexibility and efficiency of use
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing speed controls] with
respect to [user preference] from the viewpoint of [the user] in the [context of regular use].
Questions:
Q8.1

Is the user prepared to employ speed controls?

Measureable metrics:
M8.1

Two of the technically minded users used menu shortcuts, navigating left/right
depending upon the shortest route.

Table 9

Usability results – principle 9

Usability principle 9: aesthetic or minimalist design
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [assessing the interface] with
respect to [complexity] from the viewpoint of [the user] in the [context of regular use].
Questions:
Q9.1

Does the user feel the need for any additional features on the on screen interface, i.e.,
status bar, more icons?

Q9.2

Does the user feel the need to remove any items from the interface?

Findings:
M9.1

The users had varying degree of confidence with the existing interface. Suggested
enhancements include a status bar displaying menu location, device interaction details,
larger or more simplified icons.

M9.2

They did not feel the need to remove any items from the screen display.

Table 10

Usability results – principle 10

Usability principle 10: help and documentation
Expression of goal:
Analyse the [BRAIN command interface] for the purpose of [help facilities] with respect to
[provision] from the viewpoint of [the user] in the [context of regular use].
Questions:
Q10.1

Does the user feel the need for additional training in the use of the application?

Q10.2

If documentation was provided would the user use it?

Q10.3

Is there anything that the user feels that they need to know about the application?

Findings:
M10.1,
M10.2,
M10.3

Where users were not confident with the system they did not feel that additional
information would make a difference. Where a desire for additional information was
expressed then training was the preferred route to knowledge. Where a desire was
expressed for written documentation it was felt that this would require the assistance
of an additional person.

65

66

M.P. Ware et al.

Table 11

Changes to the IGUI in response to the user feedback

Usability principle: visibility of system status
Changes:
Colour coding of command arrows indicating arrow activation:
SSVEP – ambient resting arrow green, on activation red.
ERD/ERS – ambient resting arrow green, primary selection amber, on firing the command the
arrow turns red
Use of textural prompts as on screen feedback relating to confirmation of menu interactions.
The IGUI is flexible in this matter being capable of personalisation in three significant respects:
1

Offering an appropriate number of command options (2 to 4 choices) that can be suited to
user capability.

2

Tailoring operation according to user accuracy. Accepting a variable number of inputs prior
to definitive command classification in an attempt to reduce the impact of spurious
classification of a user’s neurological activity. The interface gives feedback to the user
concerning command classification in the form of incremental ‘chips’ on the command
arrow. When the appropriate number of chips have accrued the command fires.

3

The ability to tailor menu content in accordance with the users BCI capability. The menu
can therefore be simplified in respect to size and complexity and number of hierarchical
layers.

Usability principles: Match between the system and the real world
Consistency and standards
Aesthetic or minimalist design
Changes:
The use of photographs of a home as icons for controls tended to be well received. However, a
taxonomy of communication strategies (Millar, 2014) was also used to assess the icons used in
the IGUI interface. This taxonomy suggested that although photographic representation could be
100% specific and fully personalised they can also be considered to be too ‘busy’, possibly
poorly composed, poorly lit, and visually difficult to process, for some users. The taxonomy
suggested the use of symbol sets which are specially produced for communication purposes. The
Widgit symbol set (http://www.widgit.com/) was compatible with IGUI implementation
requirements: uniform representation, uniform size and layout, succinct individual and
accessible icons, offering sufficiently relevant content for a domestic context. The classification
of photographic representation was not entirely negative, therefore, it was decided that an
alternative menu representations should be offered as a choice rather than a substitution.
Usability principles: User control and freedom
Recognition over recall
Flexibility and efficiency of use
Help error recognition and diagnostics
Help and documentation
Changes:
Provision on screen has been made for feedback mechanisms which reflect both the state of the
home environment and recent interaction with the IGUI interface. The user needs to know where
they are located within the IGUI menu structure and where possible the status of the devices
within their immediate control.
In addition the feedback mechanism should confirm to the user the last command issued, so that,
if for instance, the device fails to respond the user knows that the IGUI part of the command
mechanism was enacted, and that any likely fault lies elsewhere.

A method for assessing the usability of an on screen display for a BCIs
Table 11

67

Changes to the IGUI in response to the user feedback (continued)

Usability principle: visibility of system status
Changes:
A sample of the emerging feedback pane is given in Figure 6. The breadcrumb trail to the left of
the panel indicates that the user has navigated to the home cinema via the living room and media
player options. The icon to the right of the screen indicates that the user has selected the node
level command to play the ‘James May’s Big Ideas’ film clip. This was the last command issues
evidenced by the down arrow, also to the right of the screen.
Given the context of use of a BCI and the slow and unreliable form of interaction effective
feedback in the interface is desirable. The IGUI attempts to compensate for the lack of explicit
device feedback or for the potential unreliability of device interaction. The first step is to help
the user distinguish between the issuing of spurious commands or confirm the issuing of
deliberate commands. Therefore, each time a command classification is received the appropriate
command arrow responds with a colour change from green to orange. On command enactment
the arrow turns to red. Having issued a command further feedback is given. If the command
relates solely to menu navigation a directional indicator is echoed back on the screen and a
breadcrumb is displayed for the purpose of confirming the user’s position in the menu hierarchy.
On the successful sending of a command to the UAI, the IGUI displays a graphical status
indicator in the bottom right of the screen. The user therefore knows that a command enactment
has taken place and the probability of a successful outcome. Where an error status indicator is
available this is also displayed. Should the device not respond appropriately the user has a
chance of defining where in the call chain an error has occurred, Figure 7.
Usability principle: Error prevention
Changes:
The exit option was placed at the top of the hierarchy at the same level as the choice of rooms.
Previously, a multiple repeat of the upward arrow eventually exited the system. The updated
method made a more robust system.
Usability principle: Accommodate the individual user
Changes:
In addition to the aforementioned changes, enhanced size of command arrows.

