MATEC Web of Conferences 252, 06004 (2019)
CMESâ€™18

https://doi.org/10.1051/matecconf/201925206004

Controlling the industrial robot model with the hybrid BCI based
on EOG and eye tracking
Arkadiusz Kubacki1,*, and Arkadiusz Jakubowski1
1Institute

of Mechanical Technology, Poznan University of Technology, 3 Piotrowo St., 60-965 Poznan, Poland

Abstract. The article describes the design process of building a hybrid brain-computer interface based on
Electrooculography (EOG) and centre eye tracking. In the first paragraph authors presented theoretical
information about Electroencephalography (EEG), Electrooculography (EOG), and Eye. Authors prepared an
overview of the literature concerning hybrid BCIs. The interface was built with use of bioactive sensors
mounted on the head. Movement of industrial robot model was triggered by a signal from eyes movement by
EOG and eye tracking. The built interface has been tested. Three experiments were carried out. In all
experiments, three people aged 25-35 were involved. 30 attempts per scenario were recorded. Between each
attempt, a respondent had a 1-minute break. The investigators attempted to move cube from one table to the
other.

1 Introduction
Nowadays, computers can be found everywhere around
us. They serve us both for work and fun. Non-disabled
people have no problems using devices such as a mouse
or keyboard to communicate with them. Despite this
fact, people are constantly looking for new interfaces
to communicate with the computer. Especially, the part
dealing with the interaction of people with disabilities
with a computer seems to be particularly interesting.
For instance, systems based on vision systems. An
interface was created using the Kinect camera, which
is usually used as a play tool. Thanks to it, people with
paresis can issue commands by looking at the screen
[1]. Using the same camera, an interface for sign
language was created [2]. Another type of interface is
the voice recognition system, implemented nowadays
in almost every telephone [3, 4]. Currently, a very
interesting part of human-computer interfaces are these
based on data coming straight from the human body.
There are two main sources of these signals. The most
commonly used ones are these from muscles. Using
electromyography (EMG) hand gestures can be
recognised [5]. Other interfaces are based on signals
coming from the human brain. This is especially
important for bedridden people. Due to a significant
drop in the price of devices, the most common method
of
obtaining
signals
from
the
brain
is
Electroencephalography [6].

2 Theoretical background
2.1 Brain-Computer Interface (BCI)
Electroencephalography is a non-invasive method of
recording a bioelectric activity of the brain straight
from a subject's skull. Electrodes are usually placed in
*

the â€œ10-20â€ system [7]. Thanks to the use of this
system, the influence of the skull size of the examined
person is omitted. EEG is the basic method used in
constructing the brain-computer interface. Each of the
BCI methods is limited to the number of commands
that can be generated. Therefore, more and more often
hybrid BCI are used. The division based on [8] is
presented in Fig.1. The main purpose of using more
than one interface is to increase the number of
commands and improve the classification of existing
ones.
Hybrid BCI

Combination of two
BCIs

Parallel work with
the same function

BCI + HCI

Different functions

Fig. 1. Classification of hybrid BCIs [8].

2.2 Electrooculography EOG
During the EEG test, artefacts can be observed.
Artefacts are all signals that do not come directly from
the brain [9]. Artefacts can come from the environment
as a change in the electrodes, movement of the
electrodes, related to the bioelectric activity of the skin,
as well as from the human body like muscle activity,

Corresponding author: arkadiusz.kubacki@put.poznan.pl

Â© The Authors, published by EDP Sciences. This is an open access article distributed under the terms of the Creative Commons Attribution License 4.0
(http://creativecommons.org/licenses/by/4.0/).

MATEC Web of Conferences 252, 06004 (2019)
CMESâ€™18

https://doi.org/10.1051/matecconf/201925206004

induced eye movements and facial gestures. Artefacts
are usually considered useless and are removed from
the result. Authors used AF3, AF4, F7 and F8
electrodes to do very simple electrooculography test.
Electrooculography (EOG) is a non-invasive
diagnostic method applied near eyeballs. An eye is an
electric dipole. Changes in the position of the eyeball
result in changes in the electrical charge. This change
is registered by electrode [10].

Table 1. D-H parameters of Mitsubishi RV-12SL.

2.3 Eye Tracking
There are many ways to track eye movements. In this
article, authors used an algorithm based on Means of
Gradients [11]. This algorithm has been implemented
in an eyeLike library. The library in the first place
recognises the face in a picture. For this purpose, the
Viola-Jones method based on Haar wavelets [12] was
used. On the basis of the contrast between the iris and
the sclera, a group of colours was isolated. The optimal
centre câˆ— of a circular object in an image:

a [mm]

Î± [rad]

d [mm]

ğœ½ [rad]

0

150

0

-

-

1

0

ğœ‹
2

-450

ğœƒ1

2

560

0

0

3

80

4

0

5

0

6

X3

K3

Z3

ğ‘

âˆ’

ğœ‹
+ ğœƒ2
2

ğœ‹
2
ğœ‹
âˆ’
2
ğœ‹
2

0

ğœƒ3

-670

ğœ‹ + ğœƒ4

0

ğœ‹ + ğœƒ5

-

97

ğœƒ6

670

97

Y3

X6

(1)
80

Y6
Y4
Z4
K4
X4

(2)

where: g i is a gradient vector at position xi, di
normalised displacement vector.
Weight was introduced because dark centres are more
likely than bright centres. This is due to the fact that
eyelids and eyelashes or wrinkles are also visible on the
image, which are dominant.

Y1 X1
K1
Z1

X2

450

(3)

ğ‘¤ğ‘ = I âˆ— (ğ‘ğ‘¥ , ğ‘ğ‘¦ )

(4)

K2

Z2

ğ‘

1
2
ğ‘ âˆ— = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘ [ âˆ‘ ğ‘¤ğ‘ (ğğ“ğ¢ ğ  ğ¢ ) ]
ğ‘

K6
Z6

X5
K5
Y5 Z5

560

1
2
ğ‘ âˆ— = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥ğ‘ [ âˆ‘(ğğ“ğ¢ ğ  ğ¢ ) ]
ğ‘
ğ‘–=1
ğ±ğ¢ âˆ’ ğ‘
ğ’…ğ’Š =
, âˆ€ğ‘– : ||ğ  ğ¢ ||2 = 1
||ğ± ğ¢ âˆ’ ğ‘||2

i

Y2

Y0 X0
K0
Z0

ğ‘–=1

150

Where ğ‘¤ğ‘ is the grey value at (ğ‘ğ‘¥ , ğ‘ğ‘¦ ). Results are
shown in Fig. 2.

Fig. 3. Structure of Mitsubishi RV-12sl.

Finding joint angles of the robot by having the position
and orientation of the end of effector is the main
problem [13]. This problem can be solved by inverse
kinematics. The angle of the first joint can be
calculated by the projection of a vector from origin of
frame ğ¾0 to origin of frame ğ¾4 on the X-Y plane. This
0
vector is âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
ğ‘ƒ . The target transformation matrix relative
4 ğ¾0

to the base is ğº0ğ‘‡ .
Fig. 2. Evaluation of (1) for an exemplary pupil [11].

2.4 Mitsubishi RV-12sl robot kinematics
Mitsubishi RV-12sl is a six-axis industrial robot. In
order to solve the inverse kinematics, a D-H parameter
table should be created. It is presented in Tab. 1. These
parameters describe the structure of the robot shown in
Fig. 3.

0
ğºğ‘‡

=

0
ğº ğ‘‡11
0
ğº ğ‘‡21
0
ğº ğ‘‡31

[ 0

0
ğº ğ‘‡12
0
ğº ğ‘‡22
0
ğº ğ‘‡32

0

0
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
6ğ‘ğ¾0 =

2

0
ğº ğ‘‡13
0
ğº ğ‘‡23
0
ğº ğ‘‡33

0

0
ğº ğ‘‡13
[ ğº0ğ‘‡23 ]
0
ğº ğ‘‡33

0
ğº ğ‘‡14
0
ğº ğ‘‡24
0
ğº ğ‘‡34

1 ]

â‡’
(5)

MATEC Web of Conferences 252, 06004 (2019)
CMESâ€™18

https://doi.org/10.1051/matecconf/201925206004

Then,

For joint 5:
ğœƒ3

0
0
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
6ğ‘ƒğ¾0 = ğ‘‘6 Ã— 6ğ‘ğ¾0
0
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
6ğ‘ƒğ¾0

0
ğº ğ‘‡14
[ ğº0ğ‘‡24 ]
0
ğº ğ‘‡34

=

{

=

so,
ğœƒ1

= {

0
0
4
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
â‡’ âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
4ğ‘ƒğ¾0 = 6ğ‘ƒğ¾0 âˆ’ 6ğ‘ƒğ¾0

4
6ğ‘…

âˆ’ğ‘4 ğ‘5 ğ‘6 âˆ’ ğ‘ 4 ğ‘ 6
= [âˆ’ğ‘ 4 ğ‘5 ğ‘6 + ğ‘4 ğ‘ 6
ğ‘ 5 ğ‘6

0
0
ğº ğ‘‡14 âˆ’ ğ‘‘6 ğº ğ‘‡13
0
âŒŠ ğº ğ‘‡24 âˆ’ ğ‘‘6 ğº0ğ‘‡23 âŒ‹
0
0
ğº ğ‘‡34 âˆ’ ğ‘‘6 ğº ğ‘‡33

ğ´ğ‘¡ğ‘ğ‘›2 ( ğº0ğ‘‡24 âˆ’ ğ‘‘6 ğº0ğ‘‡23 , ğº0ğ‘‡14 âˆ’ ğ‘‘6 ğº0ğ‘‡13 ) + ğœ‹

= {ğœ‹ğœ‹ âˆ’+ ğ›·ğ›· âˆ’âˆ’ ğœ€ğœ€

Where,

(7)

0

0

(

2
(|âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
4ğ‘ƒğ¾0 | âˆ’

ğ‘™21

ğ‘22

âˆ’

)
2
+ |âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
4ğ‘ƒğ¾0 |

2

2
2 |âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
4ğ‘ƒğ¾0 |

+ğ´ğ‘ ğ‘–ğ‘›

(9)

)

ğ‘2
(

)

ğœ€ = ğ´ğ‘¡ğ‘ğ‘›2(âˆ’ğ‘‘4 , ğ‘3 )

For joint 2:
ğœƒ2

=

âˆ’ğ‘4 ğ‘ 5
âˆ’ğ‘ 4 ğ‘ 5 ]
âˆ’ğ‘5

(15)

2.5 State of the Art
In the article [14] authors built a hybrid brain-computer
interface. They combined steady-state visual evoked
potential (SSVEP) and P300 paradigm. Thanks to that,
authors increased the precision of command detection.
Authors of articles [15] proposed hybrid interface
which
fused
electromyographic
(EMG)
with
electroencephalographic (EEG). Using this system
facilitates a controlling process. This system may be used
even by people tired throughout the day.
Another hybrid interface was presented by authors of
the article [16]. ERS method was used as switch to
activate functions of four-step orthosis. To open and close
this orthosis was used SSVEP.
Authors of articles [17] presented a combination of
tasks that could inspire BCI systems that are more
accurate than conventional BCIs, especially for users who
cannot attain accuracy adequate for effective
communication.

2

âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
2
2 | 4ğ‘ƒğ¾ | ğ‘™1

ğ‘4 ğ‘5 ğ‘ 6 âˆ’ ğ‘ 4 ğ‘6
ğ‘ 4 ğ‘5 ğ‘ 6 + ğ‘4 ğ‘6
âˆ’ğ‘ 5 ğ‘ 6

(8)

âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
2
(ğ‘™12 âˆ’ ğ‘22 + | 4ğ‘ƒğ¾ | )
ğ›· = ğ´ğ‘ ğ‘–ğ‘›

(14)

In which ğ‘4 is corresponding to ğ‘ğ‘œğ‘ (ğœƒ4 ) and ğ‘ 4 is ğ‘ ğ‘–ğ‘›(ğœƒ4 ).
For brevity:
4
4
4
6ğ‘…11
6ğ‘…12
6ğ‘…13
4
4
4
4
ğ‘…
=
[
(16)
ğ‘…
ğ‘…
6
6 21
6 22
6ğ‘…23 ]
4
4
4
6ğ‘…31
6ğ‘…32
6ğ‘…33
So,
4
4
(17)
ğœƒ4 = ğ´ğ‘¡ğ‘ğ‘›2 (âˆ’ 6ğ‘…23 , âˆ’ 6ğ‘…13 )
4
4
(18)
ğœƒ6 = ğ´ğ‘¡ğ‘ğ‘›2 (âˆ’ 6ğ‘…32 , âˆ’ 6ğ‘…31 )

By using the same method and a simple geometric rule an
angle for 3 can be also calculated.
ğœƒ3

âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—

To obtain ğœƒ4 and ğœƒ6 , rotation matrix 46ğ‘… is used.
(6)

ğ´ğ‘¡ğ‘ğ‘›2( ğº0ğ‘‡24 âˆ’ ğ‘‘6 ğº0ğ‘‡23 , ğº0ğ‘‡14 âˆ’ ğ‘‘6 ğº0ğ‘‡13 )

âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—

= ğœ‹ âˆ’ ğ´ğ‘ğ‘œğ‘  ( 04ğ‘ğ¾0 âˆ™ 06ğ‘ğ¾0 )

ğœ‹
âˆ’ (|ğ›½1 | + ğ›½2 )
2
{ğœ‹
+ (|ğ›½1 | âˆ’ ğ›½2 )
2

(10)

(11)

3 Software and Equipment

Where:
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
2
2
ğ›½1 = ğ´ğ‘ ğ‘¡ğ‘ğ‘›2 ( 4ğ‘ƒğ¾ , 4ğ‘ƒğ¾ )
2ğ‘¥

3.1 Emotiv EPOC+ Headset
Emotiv EPOC+ Headset (Fig. 4) is low cost neuroheadset.
This headset can record data from a range of 0.16 to 43Hz.
It has 14 EEG electrodes and two additional reference
electrodes which are arranged according to the 10-20
system. The 16-bit ADC can record data with sampling
rate equal 256 samples per second. For measurements in
this headset, wet electrodes based on saline were used
[18].

(12)

2ğ‘¦

2

âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
2
(ğ‘™12 + ğ‘22 âˆ’ | 4ğ‘ƒğ¾ | )
0

ğ›½2 = ğ´ğ‘ ğ‘–ğ‘›

2ğ‘™1 ğ‘2
(

(ğ‘™1 âˆ’
+ğ´ğ‘ ğ‘–ğ‘›

)
ğ‘™21

+

ğ‘22

2

2
âˆ’ |âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
4ğ‘ƒğ¾0 |
)
2ğ‘™1

(13)

2
|âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—âƒ—
4ğ‘ƒğ¾0 |

(

)

3

MATEC Web of Conferences 252, 06004 (2019)
CMESâ€™18

https://doi.org/10.1051/matecconf/201925206004

Fig. 6. View of the first scenario.
Fig. 4. Emotiv EPOC+ headset [18].

In the second scenario, the investigators attempted to
move cube from one virtual table to another table in the
shortest possible time (Fig. 7). All tests have been completed
successfully. The fastest attempt lasted 15.6 seconds. The
longest attempt lasted 27.7 seconds. The average duration of
trials was 23.8 seconds. Fig. 8 shows the path of the end point
of robotic arm in the all axes of moves for the one of the
attempt.
In the third scenario, the investigators attempted to move
cube from one virtual table to another table in the shortest
possible time (Fig. 9). In this scenario, a wall between tables
was added. With this experiment, not all attempts were
successful. Approximately 85% of the tests were completed
with success. The fastest attempt was 19.2 seconds. The
longest attempt lasted 38.7 seconds. The average trial
duration was 37.8 seconds.

3.2 Software
Two programs in C# were written for the purpose of
this paper (Fig. 5). First in Visual Studio to receive
signal from EOG from Emotiv EPOC+ using Xavier
SDK. The same program recognises the centre of eye
using camera and eyeLike library. In this program, a
user can change influence of signal from EOG and
camera to results and change value of a threshold.
The second program was written in Unity 3D. In
this program, kinematics model and gravitation were
implemented.

Fig. 5. Program to reveive data from EOG and camera.

In the same program all tests were carried out. Both
programs exchange data with Memory Mapped Files.
Fig. 7. View of the second scenario.

4 Tests and results
In all experiments, three people aged 25-35 were
involved. Each of them participated in 5 to 10 attempts
of each scenario. Between each attempt, the respondent
had a 1-minute break. The eye movement to the right
or left triggered the movement of the robot. To change
the direction of movement, the tested person was
closing and opening eyes twice in no more than 2
seconds.
In the first scenario, the tested person practiced using
the system (Fig. 6). The threshold was also adapted to
the person being examined. The subject was to raise the
cube using an industry robot model. The cube was
connected to the end of the manipulator at the moment
of its contact to simulate the gripper.

Fig. 8. Movement of the end point of robotic arm in the second
experiment.

4

MATEC Web of Conferences 252, 06004 (2019)
CMESâ€™18

https://doi.org/10.1051/matecconf/201925206004

errors in the third scenario resulted from human
mistakes. There were no incorrect classifications in the
trails. The least intuitive is the way the axes movement.
The selection in the loop is ambiguous. The specific
axis cannot be chosen. When changing from the Y axis
to X there is a need for a double change. This moment
was crucial for the times obtained. Authors want to add
additional interface based on SSVEP method and
control real industrial robot.
The work described in this paper was funded from
02/22/DSMK/1458.
Fig. 9. View of the third scenario.

References

Fig. 10 shows the path of the end point of robotic arm in
the all axes of moves for the one of the attempts.

1.

C. Hennessey, J. Fiset, Proc ETRA, 1, 249 (2012)

2.

Z. Zafrulla, H. Brashear, T. Starner, H. Hamilton, P.
Presti, Proc ICMI, 1 (2011)

3.

L. R. Rabiner, Proc IEEE 77, 2 (1989)

4.

A. Varga and H. J. M. Steeneken, Speech Commun
12, 3 (1993)

5.

Z. Arief, I. A. Sulistijono, R. A. Ardiansyah, IES,
2015, 11 (2015)

6.

L. Bi, X. a Fan, K. Jie, T. Teng, H. Ding, Y. Liu, IEEE
T Intell Transp, 15, 959 (2014)

7.

V. K. Varadan, S. Oh, H. Kwon, P. Hankins,
J. Nanotechnol. Eng. Med, 1, 3, (2010)

8.

A. Cudo, E. Zabielska, B. BaÅ‚aj, KUL, (2011).

9.

Oâ€™Regan, S., S. Faul, EMBC (2010)

Fig. 10. Movement of the end point of robotic arm in the third
experiment.

10. Andrew Duchowski, Springer, 2007, (2007)

5 Conclusion

12. P. Viola and M. Jones, IEEE CVPR 1 (2001)

The article describes design process of building the
hybrid
brain-computer
interface
based
on
Electrooculography (EOG) and centre eye tracking.
Authors have proved that it is possible to obtain
satisfactory results in controlling the industrial robot
model only from electrodes already placed as standard
in the Emotiv EPOC + headset and standard web
camera. It was proved that moving objects with the
robot and performing more complicated tasks such as
avoiding obstacles is possible. It is also noticeable that
the length of trials was getting shorter with practice.
The method of controlling the industrial robot is
intuitive, which confirms the obtained results. Most

13. A. Khatamian, Proc CSC (2015)

11. Timm, Fabian, and Erhardt Barth, Visapp 11 (2011):

14. E. Yin, Z. Zhou, J. Jiang, F. Chen, Y. Liu, and D. Hu,
J. Neural Eng. 10, 2 (2013)
15. R. Leeb, H. Sagha, R. Chavarriaga, and J. del R.
MillÃ¡n, Proc EMB (2010)
16. G. Pfurtscheller, T. Solis-Escalante, R. Ortner, P.
Linortner, and G. R. Muller-Putz IEEE T Neur Sys
Reh 18, 4 (2010)
17. B. Z. Allison, C. Brunner, V. Kaiser, G. R. MÃ¼llerPutz, C. Neuper, and G. Pfurtscheller J. Neural Eng.
7, 2 (2010)
18. Emotiv EPOC+ Specification, http://emotiv.com/

5

