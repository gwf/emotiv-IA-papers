IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 23, NO. 2, MARCH 2015

159

Quantitative Evaluation of a Low-Cost Noninvasive
Hybrid Interface Based on EEG and Eye Movement
Minho Kim, Byung Hyung Kim, and Sungho Jo, Member, IEEE

Abstract—This paper describes a low-cost noninvasive
brain-computer interface (BCI) hybridized with eye tracking. It
also discusses its feasibility through a Fitts' law-based quantitative evaluation method. Noninvasive BCI has recently received
a lot of attention. To bring the BCI applications into real life,
user-friendly and easily portable devices need to be provided. In
this work, as an approach to realize a real-world BCI, electroencephalograph (EEG)-based BCI combined with eye tracking is
investigated. The two interfaces can be complementary to attain
improved performance. Especially to consider public availability,
a low-cost interface device is intentionally used for test. A low-cost
commercial EEG recording device is integrated with an inexpensive custom-built eye tracker. The developed hybrid interface is
evaluated through target pointing and selection experiments. Eye
movement is interpreted as cursor movement and noninvasive BCI
selects a cursor point with two selection conﬁrmation schemes.
Using Fitts' law, the proposed interface scheme is compared with
other interface schemes such as mouse, eye tracking with dwell
time, and eye tracking with keyboard. In addition, the proposed
hybrid BCI system is discussed with respect to a practical interface
scheme. Although further advancement is required, the proposed
hybrid BCI system has the potential to be practically useful in a
natural and intuitive manner.
Index Terms—Electroencephalograph (EEG)-based brain-computer interface (BCI), eye tracking, Fitts' law, noninvasive hybrid
interface.

I. INTRODUCTION

N

ONINVASIVE brain-computer interfaces (BCIs) have
been of huge interest because of their potential [1], [2].
A noninvasive recording procedure is safer and its recording
device is less expensive than for invasive procedures. Furthermore, it is relatively easy to apply and large human population
tests are possible. Many interesting studies have examined
the feasibility of noninvasive BCI for applications for either
physically disabled or healthy people: reactive BCIs using
P300 potential [3]–[5] or steady state visually evoked potential
(SSVEP) [6]–[8], and active BCIs based on sensorimotor
rhythm [9]–[14]. Although noninvasive BCI techniques have
been signiﬁcantly improved, their practical real-world usage
Manuscript received August 13, 2013; revised July 02, 2014; accepted October 25, 2014. Date of publication November 05, 2014; date of current version
March 05, 2015. This work was supported by the Basic Science Research Program through the National Research Foundation of Korea funded by the Ministry of Education under Grant 2013R1A1A2009378.
The authors are with the Department of Computer Science, Korea Advance
Institute of Science and Technology, Daejeon 305-701, Republic of Korea
(e-mail: minokino@ kaist.ac.kr; bhyung@ kaist.ac.kr; shjo@ kaist.ac.kr).
Color versions of one or more of the ﬁgures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identiﬁer 10.1109/TNSRE.2014.2365834

has yet to be realized. This is because current noninvasive BCIs
are still of limited frequency range and are usually restricted to
binary commands. In addition, the information transfer rate is
low for communication.
To extend its applicability, even with the limitations, attention has been paid to hybrid BCI [15]–[17]. The concept of
hybrid BCI suggests using different brain signal protocols together or even combining nonbrain signals. In this way, more
extended control capacity is realized. Among the various possible combinations of signals, integrating simple interfaces together, for which each requires relatively little training, can be
thought to enhance the convenience with respect to practical interface, while increasing the number of commands. There are
some implementations of hybrid BCIs which combine EEGbased protocol with other protocols: fusion of EEG and electromyographic (EMG) activities [18], fusion of joystick and
BCI controls [19] and so on.
The development of eye tracking-based control interfaces has
a long history and has made signiﬁcant progress [20]. Eye movement input requires no intensive training and it operates impressively fast [21]. In addition, eye movement can naturally
express the user's focus of attention. It also makes some tasks
performable for the physically disabled as well as older people.
Technical improvements have made it possible to obtain reasonable performance, even with low-cost eye-tracking systems
[22], [23]. In addition, a low-cost binocular eye tracker was proposed for 3-D gaze estimation [24]. However, eye tracking is not
yet popular for use in real-life HCI applications, because using
one's hands to move a mouse is still much more convenient than
that of using an eye to move a cursor. Intended or unintended inputs are hard to be distinguished so unintended input can cause
unintended pointing or selection. A critical issue is that it is difﬁcult to ﬁnd a proper algorithm for the selection operation. Most
eye-tracking solutions rely on the dwell time. A user's gaze has
to remain ﬁxed to a target for a certain period to conﬁrm its
selection. However, this raises the question of determining the
optimal dwell time. Due to the complexity of various tasks, it
is almost impossible to optimally ﬁx the dwell time [25]. Furthermore, while in the case of binary decisions, an eye-based
method would possibly be better, by proving that BCI and eye
interfaces can coexist for simple binary transactions, future BCI
protocols such as P300 or SSVEP could be used to allow more
than two commands, which are much more difﬁcult to do with
eye movements alone.
The concept of the hybrid BCI and the potential of eye
tracking motivate the creation of a hybrid BCI system which
combines eye tracking with a simple protocol of noninvasive
BCI. The hybridization of the two is expected to complement

1534-4320 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

160

IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 23, NO. 2, MARCH 2015

each other's deﬁciencies and to establish a better performing interface. Gaze estimation from eye tracking yields impressively
higher information transfer rates than current BCIs and is adequate for performing search tasks. Meanwhile, an appropriate
noninvasive BCI protocol can be responsible for the selection
operation. A simple BCI protocol is expected to demand less
training and to achieve faster selection operations than using
dwell time with eye tracking. In addition, BCI-based selection
is a more natural and intuitive scheme than dwell time-based
selection.
In fact, Zander et al. argued that a multimodal interface combining eye gaze and a BCI can be a robust and intuitive device for touchless interaction [25]. However, their eye tracker
and BCI device were too high-end and expensive to accommodate use by the general public. They did not take into account real-world applicability. In addition, their analysis focused solely on the accuracy of correct selections and task completion time.
Recently, inexpensive products to measure EEG signals noninvasively have been commercialized. They provide the potential to extend BCI to practical HCI applications. Commercialization is an important aspect of BCI technology [26]. In this
respect, the currently available devices need to be tested. One
direction of BCI among its research issues is to become a part of
daily life as a convenient approach to interfacing with the environment [17], [26]. This work intentionally uses a user-friendly
system to contribute in this aspect.
This work aims to evaluate the promising hybrid interface
scheme of eye movement and noninvasive BCI, particularly,
with an inexpensively built and comfortable system. Instead of
focusing on advancing the performance of this hybridization,
this work attempts to evaluate its feasibility as a potential
approach to real-world applications through quantitative performance comparison of the developed interface with other
pointing interfaces. Fitts' law-based assessment has been a great
theoretical tool for the innovation of interface designs and the
Fitts' law has been widely adopted in HCI as a description of
a frequent elemental task such as pointing and target selection
as well as a predictive model to estimate the response time
[27]. Therefore, it has been used to validate the effectiveness of
computer pointing devices such as a mouse, joystick, touchpad
and eye tracker as a standard tool (ISO9241-9) [27]. In most
BCI studies, evaluation was conducted based on metrics such
as accuracy and information transfer rate [3]–[14]. However,
those may not be enough to express synthetic assessment,
especially with respect to practical HCI. Previously, an investigation applied Fitts' law for BCI evaluation [28]. However,
no attempt of quantitative evaluation of a hybrid BCI case has
been reported. This work uses Fitts' law for overall assessment
of the proposed hybrid BCI system.
II. METHODS
A. Hybrid BCI System
This work uses a low-cost BCI and eye-tracking system:
Emotiv Epoc EEG recording headset [29] and custom-built
eye-tracking equipment. The EEG recording headset consists
of 14 electrode channels plus CMS/DRL references around

the sensorimotor cortex as shown in Fig. 1. The headset was
designed as a personal interface system for HCI. It is relatively much cheaper than standard EEG recording systems.
Its potential as a reliable recording system has been studied
in some previous reports [3], [30], [31]. Fig. 1 also shows the
eye-tracking system. The total cost to build it was less than
$40 USD. An eye tracker was built based on previous studies
[22], [23]. It consists of two components, an infrared camera
and light-emitting diodes (LEDs). Five LEDs are ﬁxed around
the lens of the infrared camera that is connected to a glasses
frame at about 8 cm away from a left eye. When a subject
wears the glasses frame, the camera is pointed toward the left
eye to capture its image. LEDs illuminate the eye to enhance
the contrast between the pupil and the iris. The eye-tracking
system applies standard image processing methods for pupil
detection based on the open source [32], [33].
The combination of the low-cost BCI and eye-tracking systems comprise the hybrid BCI system used for this study (see
Fig. 1). Each system's modules for data acquisition and processing were developed separately, and synchronous operation
of the two systems was implemented using the .NET framework. Performance measurement and calibration modules for
each system were also implemented. However, due to the use of
open source libraries, the effort required to implement our hybrid system was minimized, reducing the total cost of the system
further. For use in this study, the eye-tracking system detects
eye movements, which is interpreted to be a cursor pointing trajectory while the BCI system provides explicit commands for
selection operation.
B. Data Acquisition
This pilot study originally involved ten healthy subjects
without any prior experience with eye tracking and EEG-based
BCI. They all gave written informed consent. The KAIST Institutional Review Board approved the proposed experimental
protocol of this study. However, one of them failed to acclimate
to both eye tracking and BCI protocols. Hence, the data from
nine subjects (age 24.44 3.02 (mean SD) years) excluding
the previously mentioned subject were used for evaluation.
Each participant was seated comfortably in a chair facing the
monitor screen, which was placed about 1 m in front of the subject on a table. Each subject wore the hybrid BCI system. The
head-mounted eye tracker captured images of eye movement
with a spatial resolution of 640 320 pixels at sampling rate of
60 Hz. The EEG recording headset is known to record data at
sampling frequency of 128 Hz from a 14–channel layout (AF3,
F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4).
C. Tasks
This work evaluates the proposed hybrid system through multidirectional pointing and selection task experiments. A subject wearing the hybrid interface looks at the 48-in display as
shown in Fig. 2(a). At the beginning of each trial, a cursor is
placed at the center of the screen and no target is visible. Among
the 12 possible circular target placements in a circular arrangement, an arbitrary one appears in red. Then, a subject should
move the cursor within the target circle as quick as possible.

KIM et al.: QUANTITATIVE EVALUATION OF A LOW-COST NONINVASIVE HYBRID INTERFACE

161

Fig. 1. Proposed noninvasive hybrid interface system.

Fig. 2. (a) Interface system with a subject during a task. (b) Pointing and selecting task procedure (dotted target circles are invisible). (c) Task parameters: distance
to a target and its size.

This is the pointing task mode [see Fig. 2(b)]. Then, each subject should select the target. This is the selecting task mode [see
Fig. 2(b)]. A successful target selection is complete when any
point in the target circle is selected within a limited time of 6 s
since a trial begins. When a selection is successfully ﬁnished,
the circular target disappears immediately. If selection is not
performed within the time limit, the trial is considered a failure.
In Fig. 2(c), and indicate the diameter of a circular target and
the distance from the center of the circular arrangement to the

center of the target, respectively. Three different sizes (large,
middle, small) of circular targets, and two different distances
(short, long) are assigned so that the six tasks are differentiated:
pixels (7.8, 5.2, 2.6 cm), and
pixels (20.8, 31.2 cm).
The targets are selected in a random order during a sequence
of 12 trials, therefore each subject tries all 12 target selections
one time per trial. This work pays attention to overall performance rather than speciﬁc individual performances to evaluate

162

IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 23, NO. 2, MARCH 2015

the interface protocols. Thus, 108 trials per each scenario are
acquired.
For the purposes of evaluation, four interface protocols are
compared:
1) Mouse: point and select with mouse;
2) Eye Manual: point with eye tracker and select with keyboard;
3) Eye Dwell: point with eye tracker and select with eye
tracker using dwell time;
4) hBCI (Short, Long): point with eye tracker and select with
noninvasive BCI.
(1) Conventional interface, (2) eye tracking with manual
input, (3) eye tracking only, and (4) hybrid BCI are selected
for comparison. The proposed hybrid BCI system is used in
case (4). In the hybrid case, two BCI selection conﬁrmation
schemes (hBCI Short, hBCI Long) are applied as explained in
Section II-D.
D. Signal Processing
1) BCI: The BCI protocol is used to select a point within
a circular target. Subjects are asked to concentrate on the target
point. The BCI-based selection is designed to be a binary classiﬁcation problem. The two mental states indicate selecting state
(through concentration) and pointing state during eye tracking,
respectively. When a focus on concentration is detected, the
system interprets a subject’s intent to select the target point. To
discriminate the two classes, a popular technique for feature extraction in EEG-based BCI, the Common Spatial Patterns (CSP)
algorithm [34], is used to ﬁnd spatial ﬁlters which extract features for classiﬁcation. Let
and
denote a set of EEG
signals representing concentrated state and neutral state, respectively, from the th trial. Suppose that, to obtain data for each
state, EEG signals are recorded from channels and the number
of samples is . Then, each
is transformed to a feature
vector,

CSP ﬁnds spatial ﬁlters
tion:

that extremize the following func-

where
,
is a set of feature vectors as a
matrix and denotes the number of trials.
Each
indicates the spatial covariance matrix of
an associated class assuming a zero mean for EEG signals. The
zero mean assumption is met by preprocessing when the EEG
signals are band-pass ﬁltered (1–50 Hz). Using the Lagrange
multiplier method, the optimization problem is transformed to
be a standard eigenvalue problem. The eigenvectors of
,
which corresponds to its largest and lowest principal eigenvectors, respectively, are selected as the spatial ﬁlters for extremization. EEG signals from all of the channels are projected onto
the ﬁlters. The power spectrum of projected signals is estimated
using the Burg method based on an autoregressive (AR) model.

The model order of the autoregressive model is ﬁxed as 16 based
on the result of a previous study [35]. The power values between 11 and 19 Hz are assigned to be extracted features. The
frequency band is selected based on the observation of preliminary tests from subjects. Then, an optimal classiﬁer based on
the features acquired from EEG signals is determined by the
Support Vector Machines (SVM) algorithm. SVM was selected
because it is known to have good generalization properties and
insensitivity to overtraining [36]. The properties would be effective for the use of the low cost BCI system. The linear kernel
SVM was implemented.
For robust conﬁrmation of selection, the following procedure
is implemented. In real-time, data points in a 1 s time window
with 125 ms increment are used to classify a selection state.
Two BCI conﬁrmation schemes are tested. With a short selection conﬁrmation scheme, selection is conﬁrmed if two sequential selection states are classiﬁed the same (hBCI Short). Hence,
a short selection conﬁrmation takes 250 ms, theoretically, if the
selection succeeds at once. Whereas, a long selection conﬁrmation scheme requires four sequential selection state detections
by classiﬁcation to be identical (hBCI Long). Thus, a long selection conﬁrmation takes 500 ms theoretically.
2) Eye Tracking: Points forming the contour in between the
pupil and iris from each binarized image are extracted. The
points are then ﬁtted to an ellipse to estimate the center of the
pupil. The RANSAC algorithm is applied to eliminate outliers
among the extracted points. Using a second-order polynomial
for the horizontal and vertical axes, a gaze point is interpolated.
The coefﬁcients of the polynomials are calculated through the
calibration procedure. In the case of using eye tracking alone
for the control interface, the dwell time was used to determine
the target selection. The selection is conﬁrmed if a gaze point
stays within a small circle for a certain time period, which is the
dwell time. It is extremely difﬁcult for the human gaze to stay
perfectly still for any amount of time. Thus, a small circle is assigned and a gaze point is considered static as long as the point
stays within the circle for the speciﬁed dwell time. As soon as
an experiment begins, an initial gaze point becomes a possible
selection position. If the gaze stays within a small circle whose
center is the initial point for the dwell time, the selection would
be conﬁrmed at the initial point. However, if a subject moves his
or her eye toward a target circle, the gaze point will leave the
small circle too quickly before the dwell time expires. At this
point the gaze point just outside of the small circle becomes the
possible selection spot, and a small circle around the new position is considered in the same manner as before. This procedure is repeated until a selection point is conﬁrmed by the dwell
time. In this work, the dwell time is set to be 1 s empirically
after repeated trials with different dwell time values. The size
of the small circle to indicate a static gaze position is ﬁxed to be
40 pixels (2.6 cm) in diameter. The size was selected through
a simple experiment. Subjects were asked to focus a point with
no eye movement for a while. Then, the diameter of a minimal
circle to include actual gaze trajectory was extracted.
E. Training
Each subject was allowed to become acclimated to both the
eye tracking and BCI systems for a certain amount of time be-

KIM et al.: QUANTITATIVE EVALUATION OF A LOW-COST NONINVASIVE HYBRID INTERFACE

cause the subjects had no prior experience. The amount of time
spent acclimating to the eye tracker varied from about 10 minutes to 1 hour depending on the subject. Subjects become familiar with the BCI module relatively quickly, within 20 to 30
minutes. Whether subjects were ready for the experiments was
determined by checking the accuracies of using the modules
with simple calibration protocols.
To ascertain the accuracy of the eye-tracking module, the calibration routine of the eye-tracking module requested each subject to point at a series of nine point marks displayed on the
computer monitor 1 m away from the subject's eyes.
To obtain a speciﬁc classiﬁcation of the BCI module per subject, the following procedure was used. Each subject looked at a
target circle when it appeared at a random position on the computer screen. The subject was asked to concentrate on it when
colored red and remain in the neutral state (without concentrating) when colored yellow. At six target locations, scenarios
of concentration and nonconcentration were repeated six times,
respectively. Hence, in total, 36 trial training data were collected
per subject where a trial includes one concentrated and one nonconcentrated states. The most appropriate SVM classiﬁer was
then set per subject using the data.
F. Performance Evaluation
To evaluate the interface systems, this work uses the Fitts'
index of difﬁculty (ID) which describes the relative difﬁculty of
a particular movement used in a task, based on two factors [27].
In the general Fitts' law, movement time is set to a function of
ID. In this work, the designed task consists of point movement
and point selecting tasks. Hence, the complete task time is the
sum of the target pointing and selection task times. Previous
studies reported the point movement task's performance relies
on two factors [21], [28]: the distance ( ) from a starting point
to a target point, and the diameter ( ) of the circular target.
Furthermore, this work presumed that the target selecting task
performance is affected at least by the target size , based on a
previous report [37].
For this work, the following Fitts' law formulation is used:

163

TABLE I
ACCURACY OF EACH OF EYE TRACKING AND BCI SYSTEM

This work used Fitts' law to evaluate performances with
different interfaces and repeated measures analysis of variance
(RMANOVA) which provides a statistical interpretation for
comparison as well.
G. User Study
A simple survey about the proposed hybrid interface was requested to be completed by participating subjects after the experiment. They answered four questions by comparing their performance with that of using a mouse.
Q1 : Score your ability to move a cursor by eye tracking
between 0 (much different) and 10 (exactly the same).
Q2 : Score your ability to select a target through BCI between 0 (much different) and 10 (exactly the same).
Q3 : Score how convenient the hybrid interface is between
0 (much different) and 10 (exactly the same).
Q4 : Score your satisfaction level during task performance
between 0 (much different) and 10 (exactly the same).
III. RESULTS
A. System Accuracy

where CT indicates a total time taken to complete a target
pointing and selection task.
In addition, the information transfer rate (ITR) (bits/s) is calculated by taking the reciprocal of the slope in the linear regression equation

Using Fitts' law, it can be shown how quickly tasks at different
difﬁcult levels can be performed and how much information
an operating interface can transfer. Comparison of the results
across subjects and interfaces is used for evaluation. The task
difﬁculty increases as decreases or increases.

Before conducting experiments, the eye tracking and BCI
systems were evaluated to estimate their accuracy with the determined settings. Table I summarizes the evaluation results.
Over all of the subjects, an averaged error of 0.54 0.11 cm was
attained at eye-tracking calibration. This error is translated into
the angular error of 0.30 0.06 degrees. The estimated error is
on par with commercially available eye-tracking systems. For
instance, the angular accuracy of the EyeLink II ($25,000 USD),
a binocular eye tracker, was reported to be 0.5 degrees [24]. A
custom-built eye tracker reported in [22] achieved accuracies of
0.6–1.71 cm positioned about 60 cm away. Based on the evaluation, it was judged that the subjects could use the eye-tracking
system properly.
The accuracy of the BCI classiﬁcation is also shown in
Table I. To compute the accuracy, 12 target locations were
randomly shown with either red or yellow color on the screen.

164

IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 23, NO. 2, MARCH 2015

TABLE II
SUCCESS RATES OF FIVE INTERFACES AVERAGED ACROSS SUBJECTS (%)

TABLE III
TASK COMPLETION TIMES OF FIVE INTERFACES
AVERAGED ACROSS SUBJECTS (S)

Fig. 3. Task completion times over the target sizes with four interfaces that use
cm).
eye tracking for pointing at a ﬁxed distance (

hybrid interfaces were faster than eye-tracking interfaces over
all difﬁculty levels except
(bits). In this case they were
slower than using the mouse. At the easier difﬁculty levels, the
differences of CT between the proposed hybrid interfaces and
mouse were less than 1 s. As the difﬁculty level increased, the
difference tended to get larger. MTs of hBCI Short and hBCI
Long were less than two times of MT of the mouse at
(bits), but about two times of the mouse at
(bits).
Subjects were instructed to imagine pushing a corresponding
target and stay neutral when its color was red and yellow,
respectively. As a result, the classiﬁcation accuracy was
above 94% on average. Six subjects achieved 100% accuracy
and three subjects misclassiﬁed two times among 12 cases.
Based on the subjects' performances with both systems, it was
assumed that subjects were sufﬁciently proﬁcient using the
interfaces. Then, task experiments were conducted.
B. Success Rates and Task Completion Times
Overall success rates and completion times of the tasks are
indicated in Tables II and III, respectively. The task implementation mentioned in Section II-B by the four different interface
protocols are compared. The task difﬁculty (ID) ranged from
2.4 to 4.6 (bits). The success rate was computed by counting
the total successful tasks out of 108 trials, 12 directional tasks
across subjects. CT indicates how fast a task was completed.
At the most difﬁcult level of 4.6 (bits), the proposed hybrid
interface with long and short conﬁrmations resulted in 74.03%
and 81.48% on average, respectively. In the case of CT, the interface reported 2.92 and 2.91 s.
At the easiest task level of 2.4 (bits), the proposed hybrid interface achieved a success rate of about 97% with both conﬁrmation schemes and took 1.78 and 2.01 s with short and long
conﬁrmations.
From these results, the average success rates of interfaces
other than the mouse tended to decrease as the task difﬁculty
increased. However, as the difﬁculty level increased, in most
cases the standard deviation tended to increase except in the
case of the mouse. As shown in Table III, CT tended to generally increase as the difﬁculty level increased. The proposed

C. Fitts' Law Evaluation
The Fitts' law in our study considers the target size and distance. It is intuitive that they affect pointing performance, and
previous studies veriﬁed it [21], [28]. Furthermore, it is presumed that the target size affects selecting performance based
on a previous study [36]. To prove this assumption, the relationship between task completion time and the target size is investigated. In reality, it is difﬁcult to identify pointing and selecting
times separately because subjects do not always perform a selection after pointing. In some cases, selecting the right point failed
and the subject tried again. Because four interface systems use
the same eye tracking for target pointing, pointing performances
are not signiﬁcantly different over the four interface systems
when conditioned on the same distance. Therefore, the examination of task completion times over different target sizes can
indirectly point out whether selecting time relies on the target
size. Fig. 3 shows that task completion times taken across different target sizes at the same distance (
cm) to see if
performance using each interface system is different depending
on the target sizes. It is observed that task completion time decreases as the target size increases. Statistical analysis veriﬁes
that task completion time relies signiﬁcantly on the target sizes
[eye tracking with keyboard (
,
),
eye tracking with dwell time (
,
),
hBCI Short (
,
, and hBCI Long (
,
)].
Based on this result, the Fitts' law relationship between
ID and CT during pointing and selecting tasks is evaluated.
Fig. 4 shows the Fitts' law relationships averaged across the
subjects. Their estimated linear regressions are also included.
The CTs of hBCI Short and hBCI Long showed no pairwise

KIM et al.: QUANTITATIVE EVALUATION OF A LOW-COST NONINVASIVE HYBRID INTERFACE

Fig. 4. Relationships between ID and MT averaged across subjects using the
correlation coefﬁcient).
four interfaces (

signiﬁcant difference (
,
) although
the averaged CTs of hBCI Short were a bit lower than those
of hBCI Long. The different interpretation between statistical
analysis and comparison of the average values of hBCI Long
may be due to the standard deviation difference as shown in
Table III. Relatively high standard deviation in hBCI Short may
imply that subjects' performances were less consistent over
IDs. Using hBCI Short tended to complete a task quicker than
hBCI Long as expected, but was less robust. Therefore, there is
no signiﬁcant difference within a statistical view.
Their ranges of CT were comparable to that of eye tracking
with keyboard and quite less than that of eye tracking with dwell
time. While CTs of the proposed hybrid interfaces were signiﬁcantly different than those of a mouse (
,
) and eye tracking with dwell time (
,
), they were not signiﬁcantly different than that of
eye tracking with a keyboard (
,
).
This may imply that BCI selection mode is comparable to the
keyboard clicking in terms of task time. Averaged CTs of the
proposed hybrid interfaces apparently tended to increase a bit
more than those of the other interfaces as ID increases (slopes
). With the mouse, ID did not signiﬁcantly affect CT
(
,
). Meanwhile, CT tended to rely
on ID when using eye tracking with a keyboard (
,
), eye tracking with dwell time (
,
), hBCI Short (
,
),
and hBCI Long (
,
).
The overall performance was well ﬁtted with linear regressions as shown in correlation coefﬁcient values. The Fitts' law
linear regression resulted in
(bits/s) with
for hBCI Short, and
(bits/s) with
for hBCI Long.
D. BCI Classification Analysis
To perform the selecting task, subjects were asked to mentally concentrate when the hBCI interface was used. This section
attempts to analyze whether the BCI protocol was reasonably
implemented. Fig. 5 illustrates typical brain activity difference
between the selecting and pointing states with respect to power
spectrum density (PSD) per subject in the frequency band, between 11 and 19 Hz, in which feature vectors were assigned.
Each plot was obtained by averaging the brain activity of each

165

state. Each density difference at a particular electrode spot is visualized. The selected frequency band overlaps both the alpha
wave (8–13 Hz) and beta wave (13–30 Hz) bands, more specifically low-to-mid beta wave band [38], [39].
Although this work does not attempt any detailed neurophysiological analysis, a potential interpretation on this observation
is possible. According to literature [38], [39], active concentration or alertness results in an increase of activity in the beta
wave band and a reduction of activity in the alpha wave band.
The beta activity is distributed symmetrically and observed over
various areas, but prominently in frontal areas. The alpha wave
occurs over the entire lobe, but strongly at the occipital lobe
with eyes closed. Although the electrode channels of the current headset do not cover the entire scalp, they seem to detect
the alpha wave attenuation on the parietal or occipital area and
the beta wave ampliﬁcation as shown in Fig. 5. In cases of subjects S1, S2, S5, and S8, brain activities over the occipital area
attenuate during the selecting state at relatively low frequencies (11 to 13 Hz) while brain activity increases over the frontal
area especially at relatively high frequencies (16 to 19 Hz). The
rest of the subjects show stronger activities over the electrodes
during selecting rather than when pointing, which may indicate
the beta wave ampliﬁcation.
E. Eye Movement Analysis
The proposed hybrid interfaces incorporated two modules, eye tracking and BCI-based selection. To evaluate each
module's performance during these tasks, Fig. 6 illustrates the
cursor trajectories recorded during the experiments. Black and
red dots represent trajectory components from eye tracking and
BCI selection modes, respectively. Most subjects realized almost straight cursor trajectories while eye tracking to all the 12
directions. In some cases, overshot trajectory patterns appeared
right before selection. They are probably due to the speed of the
eye being too fast. Some wobbles were sometimes observed in
trajectories. Instant information loss by eye blinking or the device failing to detect a gaze caused this unsteadiness. However,
all directional performances through the proposed interface
were visually uniform overall. The directional indifference is
statistically veriﬁed (
,
).
F. User Study
Table IV shows the survey result. Based on statistical analysis using the Student's -test under 95% conﬁdence interval,
subjects expressed a bit higher conﬁdence in cursor movement
control than selection operation. As for the BCI selection mode,
some capability differences between subjects were expressed.
The hybrid interface was convenient overall in their opinions
and subjects felt about 73% satisﬁed.
IV. DISCUSSION AND CONCLUSION
In this study, nine subjects successfully performed the tasks
with proposed interfaces in at least 11 trials among 12 trials at
lowest difﬁculty level and in at least seven trials at the highest
difﬁculty level. Maintaining a cursor within a circular target
while selecting it seemed to be the most difﬁcult part of the
experiment based on the inspection of the cursor trajectories.
This difﬁculty is based on the ability of the human eye to point

166

IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 23, NO. 2, MARCH 2015

Fig. 5. Visualization of brain activities (selecting state– pointing state) per electrode over frequencies.

TABLE IV
SURVEY RESULT

Fig. 6. Examples of cursor trajectories collected from subjects during tasks.

and the eye tracker's precision of detection. Blinking of the
eyes also affected the cursor trajectory. Even while such limitations exist, eye tracking still possesses good advantages such as
quickness and omnidirectional controllability. Although there
was a clear difference in the adeptness of using the system, the
subjects could, most of the time, complete tasks successfully

KIM et al.: QUANTITATIVE EVALUATION OF A LOW-COST NONINVASIVE HYBRID INTERFACE

within the time limit. Subjects tended to select their targets at
the very center of targets rather than as soon as getting inside
the targets. This tendency can be explained because the subjects
should keep their eye inside the targets during necessary dwell
time for successful selection. In other words, the subjects can
reach their targets faster and reduce the completion time when
they would make the selection at the edge of their targets. However, in this case, they usually failed the selection task because
their eye positioning was out of the targets and did not satisfy
the dwell time.
The performances of pointing and selection tasks were statistically comparable with that of the eye tracking with a keyboard.
The proposed hybrid interface system achieved an overall ITR
of 2.02–2.27 bit/s. It was lower than that of the mouse interface (7.61 bits/s) and a bit less than that of eye-tracking interfaces (2.58–3.64 bits/s). However, it was higher than that (0.541
bits/s) of the BCI-only interface reported in [28]. Furthermore,
in terms of task speed, CTs of using the proposed hybrid interfaces were still faster than that of using eye tracking with dwell
time and comparable with that of using eye tracking with a keyboard. CT of eye tracking with dwell time might be lowered
further by reducing the dwell time interval, but its robustness
would then suffer. The results indicate that BCI-based selection
is competitive with keyboard clicking and eye dwell time when
it is concurrently used with eye tracking.
An interesting comparison is with the EMG interface. Although EMG-based methods achieve a higher accuracy rate than
EEG-based BCIs, the EMG-based methods still require muscle
activation and translating the detection of this activation into
certain kinds of interface commands. This intentional activation
still presents an artiﬁcial, indirect mapping between intent and
commands. It is the purpose of EEG-based BCI to eradicate this
mapping in future computer interface systems. EEG-based BCIs
enable the perception of subjects' intent directly from their brain
and reﬂect the intent in controlling and communicating with machines. According to a report in [40], the overall ITR of the
EMG interface was 1.299 bits/s, which is lower than those of the
proposed hybrid interfaces. In addition, the range of ID in this
work was from 2.4 to 4.6 (bits). It is similar to that of EMG interface in [40], but much greater than that of the BCI only interface
in [28]. The report [28] mentioned that subjects missed the 3.7
bit ID commonly with a pure BCI interface scheme. Therefore,
it had a limitation on performing high ID tasks. In the case of
the EMG interface [40], the cursor movement was restricted to
only horizontal and vertical directions. Multidirectional cursor
movement is not yet reasonably achievable by either EMG or
EEG only interfaces. Currently these interfaces are limited not
by the physiological or psychophysical component, but rather a
pragmatic one. In practice, some pragmatic limitations such as
transition time between brain activities and EEG/EMG signals
cannot be avoided perfectly. In order to overcome the current
limitations, the proposed hybrid interface is derived with some
techniques such as a sliding windows method. Developing these
techniques increases the likelihood that brain signals-based discrimination method will be faster than a simple manual response
in future. It is a valid remark that the proposed hybrid interface
took advantage of eye tracking for moving cursors in terms of
both movement speed and accuracy, and BCI-based selection

167

was comparable to keyboard clicking. BCI-based selection may
be natural and intuitive for disabled people or even for healthy
people while their hands are busy.
In comparison with the mouse, the performances of all of the
other interfaces tended to be less consistent over subjects (lower
correlation coefﬁcients). However, the developed interface relies on a simple BCI protocol, which a user can easily adapt to
without a serious training session. Individual adaptation will be
improved by gaining further experience to attain more robust
and consistent performances.
This study presented important implications for future work
on developing hybrid interface devices, especially using low
cost equipment. Evaluation of the low cost hybrid interface is
necessary for future work on developing real-world BCI interfaces. The hybrid eye tracking and BCI interface system will
be effective to any one with no degradation in the ocular-motor
system and motor planning and decision making areas of the
brain. In addition, its operation is natural and intuitive. The proposed interface system should be tested with people with motor
disabilities in the future to further conﬁrm its feasibility.
ACKNOWLEDGMENT
The authors appreciate Y. Chae for his technical assistance,
T. Morse for his editorial assistance, and anonymous reviewers
for their valuable comments.
REFERENCES
[1] D. McFarland and J. Wolpaw, “Brain-computer interfaces for communication and control,” Commun. ACM, vol. 54, pp. 60–66, 2002.
[2] G. G. Dornhege, J. del R Millan, T. Hinterberger, D. McFarl, and K.-R.
Müller, Eds., Toward Brain-Computer Interfacing Cambridge, MA,
USA, MIT Press, 2007.
[3] A. T. Campbell et al., “NeuroPhone: Brain-mobile phone interface
using a wireless EEG headset,” in Proc. ACM SIGCOM Workshop Networking, Systems, and Applications on Mobile Handhelds, 2010, pp.
3–8.
[4] C. J. Bell, P. Shenoy, R. Chalodhorn, and P. N. Rao, “Control of a
humanoid robot by a noninvasive brain-computer interface in humans,”
J. Neural Eng., vol. 5, pp. 214–220, 2008.
[5] J. Park, K. E. Kim, and S. Jo, “A POMDP approach to P300-based
brain-computer interfaces,” in Proc. Int. Conf. Intell. User Interfaces
(IUI), 2010, pp. 1–10.
[6] G. R. Müller-Putz and G. Pfurtscheller, “Control of an electrical prosthesis with an SSVEP-based BCI,” IEEE Trans. Biomed. Eng., vol. 55,
no. 1, pp. 361–364, Jan. 2008.
[7] G. R. Müller-Putz, R. Scherer, C. Brauneis, and G. Pfurtscheller,
“Steady-state visual evoked potential (SSVEP)-based communication:
Impact of harmonic frequency components,” J. Neural Eng., vol. 2,
no. 4, 2004.
[8] E. C. Lalor et al., “Steady-state VEP-based brain-computer interface
control in an immersive 3D gaming environment,” EURASIP J. Applied Signal Processing, pp. 3156–3164, 2005.
[9] Y. Chae, J. Jeong, and S. Jo, “Toward brain-actuated humanoid robots:
Asynchronous direct-control using an EEG-based BCI,” IEEE Trans.
Robot., vol. 28, no. 4, Apr. 2012.
[10] Y. Chae, J. Jeong, and S. Jo, “Noninvasive brain-computer interfacebased control of humanoid navigation,” in Proc. IEEE/RSJ Int. Conf.
Intell. Rob. Syst., 2011.
[11] J. R. Wolpaw and D. J. McFarland, “Control of a two-dimensional movement signal by a noninvasive brain-computer interface
in humans,” Proc. Nat. Acad. Sci. (PNAS), vol. 101, no. 51, pp.
15849–17854, 2004.
[12] A. S. Royer and B. He, “Goal selection versus process control in a
brain-computer interface based on sensorimotor rhythms,” J. Neural
Eng., vol. 6, 2009.
[13] G. Pfurtscheller et al., “Current trends in Graz brain-computer interface
(BCI) research,” IEEE Trans. Rehabil. Eng., vol. 8, no. 2, pp. 216–219,
Apr. 2000.

168

IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING, VOL. 23, NO. 2, MARCH 2015

[14] G. Pfurtscheller, C. Brunner, A. Schlogl, and F. H. Lopes da Silva, “Mu
rhythm (de)synchronization and EEG single-trial classiﬁcation of different motor imagery tasks,” NeuroImage, vol. 31, no. 1, pp. 153–159,
2006.
[15] G. Pfurtscheller et al., “The hybrid BCI,” Front. Neurosci., vol. 4, no.
30, pp. 1–11, 2010.
[16] B. Z. Allision et al., “Toward smarter BCIs: Extending BCIs through
hybridization and intelligent control,” J. Neural Eng., vol. 9, 2012.
[17] J. del R Millan et al., “Combining brain-computer interfaces and assistive technologies: State-of-the-art and challenges,” Front. Neurosci.,
vol. 4, no. 161, 2010.
[18] R. Leeb, H. Sagha, R. Chavarriaga, and J. Millan, “A hybrid braincomputer interface based on the fusion of electroencephalographic and
electromyographic activities,” J. Neural Eng., vol. 8, no. 2, 2011.
[19] A. Kreilinger et al., “Switching between manual control and braincomputer interface using long term and short term quality measures,”
Front. Neurosci., vol. 5, no. 145, 2012.
[20] R. Jacob and K. S. Karn, “Eye tracking in human-computer interaction
and usability research: Ready to deliver the promises,” Mind, 2003.
[21] R. Vertegaal, “A Fitts law comparison of eye tracking and manual input
in the selection of visual targets,” in Proc. Int. Conf. Multimodal Interact. (ICMI), 2008, pp. 241–248.
[22] D. Li, J. Babcock, and D. J. Parkhurst, “OpenEyes: A low-cost
head-mounted eye-tracking solution,” in Proc. Symp. Eye Tracking
Res. Appl. (ETRA), 2006, pp. 95–100.
[23] J. San Agustin, H. Skovsgaard, J. P. Hansen, and D. W. Hansen, “Lowcost gaze interaction: Ready to deliver the promises,” in Proc. Int.
Conf. Extended Abstracts on Human Factor in Computing Systems CHI
EA'09 (New YorkL ACM), 2009, pp. 95–100.
[24] W. W. Abbott and A. A. Faisal, “Ultra-low-cost 3D gaze estimation:
An intuitive high information throughput compliment to direct brainmachine interfaces,” J. Neural Eng., vol. 9, no. 4, 2012.
[25] T. O. Zander, M. Gaertner, C. Kothe, and R. Vilimek, “Combining eye
gaze input with a brain-computer interface for touchless human–computer interaction,” Int. J. Hum.-Comput. Interact., vol. 27, no. 1, pp.
38–51, 2010.
[26] P. Brunner, L. Bianchi, C. Guger, F. Cincotti, and G. Schalk, “Current
trends in hardware and software for brain-computer interfaces (BCIs),”
J. Neural Eng., vol. 8, 2011.
[27] R. W. Soukoreff and I. S. MacKenzie, “Towards a standard for pointing
device evaluation, perspectives on 27 years of Fitts' law research in
HCI,” Int. J. Hum.-Comput. Studies, vol. 61, pp. 751–789, 2004.
[28] E. A. Felton, R. G. Radwin, J. A. Wilson, and J. C. Williams, “Evaluation of a modiﬁed Fitts law brain-computer interface target acquisition
task in able and motor disabled individuals,” J. Neural Eng., vol. 6, no.
5, 2009.
[29] EmotivSystems. Emotiv – Brain Computer Interface Technology, [Online]. Available: http://emotiv.com
[30] P. Bobrov et al., “Brain-computer interface based on generation of visual images,” PLoS ONE, vol. 6, no. 6, p. e20674, 2011.
[31] Y. Liu et al., “Implementation of SSVEP based BCI with emotive
EPOC,” in Proc. IEEE Int. Conf. Virtual Environ. Human-Comput. Int.
and Measurement Syst. (VECIMS), 2012, pp. 34–37.
[32] J. San Agustin et al., “Evaluation of a low-cost open-source gaze
tracker,” in Proc. Symp. Eye-Tracking Res. Appl. (ETRA), 2010, pp.
77–80.
[33] ITU Gaze Tracker, [Online]. Available: http://www.gazegroup.org/
downloads/23-gazetracker
[34] F. Lotte and C. Guan, “Regularizing common spatial patterns to improve BCI designs: Uniﬁed theory and new algorithms,” IEEE Trans.
Biomed. Eng., vol. 58, no. 2, pp. 355–362, Feb. 2011.
[35] D. J. McFarland and J. R. Wolpaw, “Sensorimotor rhythm-based braincomputer interface (BCI): Model order selection for autoregressive
spectral analysis,” J. Neural Eng., vol. 5, no. 2, pp. 155–162, Jun. 2008.

[36] F. Lotte, M. Congedo, A. Lécuyer, F. Lamarche, and B. Arnaldi, “A
review of classiﬁcation algorithms for EEG-based brain-computer interfaces,” J. Neural Eng., vol. 4, pp. R1–R13, 2007.
[37] C. Ware and H. H. Mikaelian, “An evaluation of an eye tracker as a device for computer input,” in Proc. SIGCHI/GI Conf. Human Factors in
Computing Systems and Graphics Interface (CHI), 1987, pp. 183–188.
[38] E. R. Kandel, J. H. Schwartz, and T. M. Jessell, Principles of Neural
Science, 4th ed. New York, NY, USA: McGraw-Hill, 2001, p. 916.
[39] D. Purves et al., Neuroscience, 3rd ed. Sunderland, MA, USA: Sinauer Associates, 2004, pp. 668–670.
[40] C. Choi, S. Micera, J. Carpaneto, and J. Kim, “Development and quantitative performance evaluation of a noninvasive EMG computer interface,” IEEE Trans. Biomed. Eng., vol. 56, no. 1, pp. 188–191, Jan.
2009.
[41] J. Wobbrock et al., “An error model for pointing based on Fitts' law,”
in Proc. SIGCHI/GI Conf. Human Factors in Computing Systems and
Graphics Interface (CHI), 2008, pp. 1613–1622.

Minho Kim received the B.S. and M.S. degrees in
computer science from Korea Advanced Institute of
Science and Technology, Daejeon, Korea in 2012 and
2014, respectively.
His research interests include wearable brain-computer interface systems and their applications.

Byung Hyung Kim received the B.S. degree in
computer science from Inha University, Incheon,
Korea, in 2008, and the M.S. degree in computer
science from Boston University, Boston, MA, USA,
in 2010. He is currently working toward the Ph.D.
degree at Korea Advanced Institute of Science and
Technology, Daejeon, Korea.
His research interests include machine learning,
computer vision, brain-computer interfaces, and
neuroimaging analysis.

Sungho Jo (M’09) received the B.S. degree from the
School of Mechanical and Aerospace Engineering,
Seoul National University, Korea, in 1999, and the
M.S. degree in mechanical engineering and the Ph.D.
degree in electrical engineering and computer science
both from the Massachusetts Institute of Technology
(MIT), Cambridge, MA, USA, in 2001 and 2006, respectively.
While pursuing the Ph.D. degree, he was with the
Computer Science and Artiﬁcial Intelligence Laboratory and the Laboratory for Information Decision
and Systems. From 2006 to 2007, he was a Postdoctoral Researcher with the
MIT Media Lab. Since December 2007, he has been with the Department of
Computer Science, Korea Advanced Institute of Science and Technology, Daejeon, Korea, where he is currently an Associate Professor. His research interests include brain-machine interface, muscle-computer interface, and wearable
computing.

