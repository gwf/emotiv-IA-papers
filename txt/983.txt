A Game-based Learning Framework for Controlling Brain-Actuated
Wheelchairs
Rolf-Magnus Hjørungdalú , Filippo Sanfilippo† , Ottar L. Osenú , Adrian Rutle‡ and Robin T. Byeú
ú Software and Intelligent Control Engineering Laboratory
Faculty of Engineering and Natural Sciences
Norwegian University of Science and Technology
NTNU in Ålesund, Postboks 1517, NO-6025 Ålesund, Norway
Email: rolf.hjdal@gmail.com, {robin.t.bye,ottar.l.osen}@ntnu.no
† Department of Engineering Cybernetics
Norwegian University of Science and Technology
NO-7491 Trondheim, Norway
Email: filippo.sanfilippo@ntnu.no
‡ Department of Computing, Mathematics and Physics
Bergen University College
P.O. Box 7030, Bergen, Norway
Email: adrian.rutle@hib.no

K EYWORDS
Brain-computer interface, electroencephalography, virtual reality, low-cost commercial-off-the-shelf products.
A BSTRACT
Paraplegia is a disability caused by impairment in motor
or sensory functions of the lower limbs. Most paraplegic
subjects use mechanical wheelchairs for their movement,
however, patients with reduced upper limb functionality
may benefit from the use of motorised, electric wheelchairs. Depending on the patient, learning how to control
these wheelchairs can be hard (if at all possible), timeconsuming, demotivating, and to some extent dangerous.
This paper proposes a game-based learning framework
for training these patients in a safe, virtual environment.
Specifically, the framework utilises the Emotiv EPOC EEG
headset to enable brain wave control of a virtual electric
wheelchair in a realistic virtual world game environment
created with the Unity 3D game engine.
I NTRODUCTION
The ability to move around, explore our surroundings
and being able to transfer to other places in order to take
part in daily activities is an essential quality in human life.
People with disabilities may lack this ability due to their
illness. With the aid of prostheses or wheelchairs, many
disabled people can be become more mobile. However, it
may be very difficult or even impossible for tetraplegic
patients, or paraplegic patients with reduced upper limb
functionality, to control an electric wheelchair via a joystick or other manual control devices. For these patients, an
electric wheelchair that can be operated solely by the mind
Corresponding author: Robin T. Bye.

could provide a formidable improvement in the quality of
life.
Prior to the development of brain-actuated wheelchair,
several factors must first be considered. In order to map
out the needs and shortcomings of the available technology
it is desirable to test the existing technology in a virtual
environment. This enables the exploration, testing and
development of the user interface and brain-computer
interface (BCI) functionality.
This paper presents the development of an open-source
framework for disabled people such as tetraplegics or
sufferers of amyotrophic lateral sclerosis (ALS) to control
a brain-actuated wheelchair in a virtual environment. The
framework is realised by exclusively adopting low-cost
commercial off-the-shelf (COTS) components and tools.
In particular, an electroencephalography (EEG) headset,
the Emotiv EPOC, is chosen for monitoring the subject’s
brain waves. These signals are then used as inputs for
controlling a wheelchair in a simulated environment. To
achieve this goal, the Unity 3D game engine is selected
as an efficient integration platform. The adopted design
choices make the proposed framework very flexible and
extremely low-cost.
In the following sections, we first provide a background
of game-based learning, EEG technology, related work,
and our motivation and aim. We proceed with describing our game-based methodology before presenting the
framework architecture, including the Emotiv headset and
library and the Unity environment, the interface between
the headset and Unity, design of the game environment,
and a preliminary artificial neural network (ANN) (Yegnanarayana, 2009) for converting raw EEG brain waves
into control signals. Finally, we present the results of our
work and a discussion, including future work.

Paraplegic
subject

EEG headset
Game based learning
framework
Virtual
Environment

Real Environment

Figure 1: Game-based learning framework for paraplegic
subjects to control brain-actuated wheelchairs.
BACKGROUND
There is a rapidly growing body of empirical evidence
on the effectiveness of using video and computer games
to provide instruction (Tobias et al., 2014). We experience
pleasure when actively engaged in games, especially in
coming to understand how a new system works. This is
true whether the game is considered “entertainment” or
“serious” (Susi et al., 2007). Here, we adopt a game-based
learning approach, where a ”serious” game is developed to
engage disabled subjects in learning how to control their
wheelchair by using brain waves. We believe our approach
is especially suitable for this kind of training, because
mastering brain wave control can be difficult and involve
tedious and repetitive tasks and thus be demotivating for
the subject. Figure 1 illustrates the main idea and a high
level view of the components involved in the game-based
learning framework.
EEG Technology
EEG is an electrophysiological monitoring method that
measures the natural electric potential on the scalp (Niedermeyer and da Silva, 2005). Physiologically, EEG power
reflects the number of neurons that discharge synchronously (Klimesch, 1999). This electric potential is a result
of brain activity and behaves in a periodic, wavelike
fashion referred to as brain waves and can be recorded
with a portable EEG headset such as the Emotiv EPOC
EEG (e.g., Duvinage et al., 2013).
Traditionally, EEG technology has been a diagnostic
tool for medical professionals in order to diagnose neurological disorders such as epilepsy, brain tumors and more
(Vaque, 1999). In recent years, EEG technology has seen
a commercialisation which has resulted in affordable EEG
equipment for both researchers and end-consumers. One
such device is the Emotiv EPOC headset, which comes
with several pieces of software developed for BCI. The
included software enables the user to both record and
investigate their brain wave activity in real time with
the brainwave signals split into the conventional EEG
frequency bands, namely the delta (< 4 Hz), theta (4–7

Hz), alpha (8–15 Hz), beta (16–31 Hz), and gamma (> 32
Hz) bands, of which the alpha band, which is active during
an alert and cognitive state of the patient (Klimesch, 1999),
and the beta band, which is closely related to purposive
movement (Niedermeyer and da Silva, 2005), are the most
important with respect to BCI.
One of the features of this software is the ability to
store specific brain wave patterns as commands. Up to
four different user-defined commands can be stored (e.g.,
up, down, left, right), and the proper software for mapping
and training these commands are provided. The application
programming interface (API) provided by Emotiv enables
software developers to access these commands and use
them in their software, achieving BCI as a result.
Related Work
The possibility of using EEG technology for medical
assistant applications has been studied in the literature.
In particular, the possibility of enabling disabled subjects
to control their wheelchairs by using brain waves has
been investigated by several research groups. For instance,
an attempt to use brain signals to control mechanical
devices such as wheelchairs was presented by Tanaka et al.
(2005). To achieve this goal, a recursive training algorithm
to generate recognition patterns from EEG signals was
developed. Relevant experimental results demonstrated the
potential of the proposed system. In particular, the system
was tested in a real experimental scenario where subjects
were required to approach target positions by repeating
movements. However, this experimental setup required
external assistance for the subjects because the wheelchair
was supposed to be stopped during EEG detection and
pattern matching, since the processing time was very
slow. Successively, thanks to different advances in this
technology, a real-time EEG classification system was
presented by Craig and Nguyen (2007), with the goal
of enhancing the control of a head-movement controlled
power wheelchair for patients with chronic spinal cord
injury (SCI).
One of the main challenges that characterises most of
these previous works is that developing and testing brainactuated wheelchairs in a real-world environment is very
difficult because of the extensive training that is required
for the paraplegic subjects to safely operate the systems.
In this perspective, even though numerous research efforts
have been performed to develop brain-actuated wheelchairs, to the best of our knowledge there exists only a few
integrated frameworks for effectively training paraplegic or
tetraplegic subjects in controlling their wheelchairs. For
instance, the possibility of a tetraplegic for using brain
waves to control movements of his wheelchair in virtual
reality was first studied by (Leeb et al., 2007). In this
study, a tetraplegic subject was able to generate bursts of
beta oscillations in the EEG by imagination of movements
of his paralyzed feet. These beta oscillations were used for
self-paced (asynchronous) BCI control based on a single
bipolar EEG recording. The subject was placed inside a
virtual street populated with avatars. Even though the use

of a visual-rich virtual environment was proved to be a
very effective approach for improving efficiency of virtual
training, the possibility of adding some elements of gamebased learning was not deeply considered.
Motivation and Aim
As described above, development and testing of brainactuated wheelchairs for paraplegic patients in a real-world
environment is challenging due to safety concerns. Only
recently did researchers attempt to use virtual reality training to overcome this challenge but failed to include the advantage of game-based learning aspects. Moreover, using
a virtual environment enables an adaptive and incremental
learning paradigm, where training exercises are matched
with the level of skill attained by the users. Finally, virtual
environments are easy to modify and extend in software
compared to their physical counterparts.
Motivated by these factors in an emerging field of
research, our aim is to propose a game-based learning
framework for brain-actuated wheelchairs, developed in
only a few months by a small group of people, and
involving only low-cost COTS components, that incorporates many of the advantages a virtual training environment
can offer.
M ETHOD
This section describes the implementation details of
our framework, and the rationale for some of our design
decisions.
Game-based Methodology
The aim of implementing game-based learning concepts
is to create a training environment in such a manner that
the trainee is learning while at the same time enjoying the
game aspects of the exercises. The skills and knowledge
provided by the experience of playing the game can then
later be applied in real-life scenarios.
Several aspects of game-based learning have been implemented in our virtual training environment in order to
enhance learning, including
• safe risks, where users can experience consequences
from their mistakes in a safe environment;
• goal-based tasks, where an on-screen prompt informs
the user about the current task;
• incremental learning, where the user is prompted
to complete more challenging tasks as the game
progresses; and
• timed events, where users can compare themselves
with their previous times.
Learning to consistently switch between input commands is imperative to safely operate a brain-actuated
wheelchair. In our virtual environment, the user can explore and test the wheelchair functionality in a setting
free from risk in order to improve their BCI skills. As
a result, a patient utilizing a brain-actuated wheelchair in
the future may greatly benefit from the experience gained
in the game-based learning environment.

Users can also benefit from competing against themselves, for example trying to reduce their completion time
for a particular game level. A shorter completion time
would likely be due to better ability to switch input
commands.
We have attempted to address some of the advantages
of a game-based methodology in our implementation. For
example, we have included the abovementioned game
completion timer that accompanies an onscreen description of the task at hand where it is relevant. Furthermore,
we have created an environment with an emphasis on
step-by-step self-paced incremental learning, where users
complete game levels with progressively more difficult
tasks, exploring different aspects of the skills needed to
operate an electric wheelchair in a real-world environment.
The proposed game-based training methodology follows
the game levels depicted in Figure 2. In Level 1, users
begin by concentrating on only learning and practicing a
single input command, namely that of moving forward in
a “drag race,” in which the task is to drive the wheelchair
straight forward from the starting line to the finish line.
After this level has been completed, the player can choose
to repeat the level in order to improve completion time,
or to proceed to more challenging tasks involving several
input commands. Specifically, in Level 2, the user will
learn to switch between movement commands in order
to navigate a labyrinth. In Level 3, users learn to handle
safety mechanisms, whereas Level 4 offers more difficult
and advanced tasks for improving navigation skills. Finally, Level 5 provides users with a realistic real-world
scenario, where users must navigate the wheelchair in an
urban area, using all the skills they have learnt previously.

Framework Architecture
The framework consists of two main components: the
EEG headset and the game engine. The choice of these two
components is crucial for the success of the framework. In
this section, each of the main components are explained
and finally the interfacing between the game engine and
the EEG device is described in detail.
Emotiv Headset and Library: The EEG device connects the user’s brain to the virtual environment by converting EEG brain waves measured from one or several
electrodes positioned on the user’s scalp into BCI commands. There were two factors that we considered the most
important when choosing the EEG hardware to achieve our
goals, namely convenience and accuracy.
In terms of convenience, the device must be simple for
the end user to equip and use. In addition the device should
have a good API in order to make it easy for the developer
to create software.
In terms of accuracy, it is important that the EEG device
provides enough accuracy to differentiate between several
mental states for control, e.g. “forward,” “left,” and “right.”
This is necessary in order for the user to able to move
around freely in an open, unconstrained environment such
as the real world.

EEG headset

Level 1
move
forward

Disabled subject

Drag race

Level 2

Level 3

switch
between
commands

safety
mechanisms

Labyrinth

Ramp

Level 4
Improve
navigation
skills

Labyrinth

Level 5
Real-life
scenario

Street

Figure 2: The proposed game-based training methodology. A training sequence of different levels is adopted.
Compared to most conventional medical equipment, the
Emotiv EPOC is very convenient to use as it is intended
for end-consumers and does not require professional expertise. Considering the nature of how EEG is measured,
convenience is not compatible with accuracy and some
sacrifices must be made in order to ensure the quality
of the measured signals. Nevertheless, compared to other
COTS EEG equipment that solely uses dry electrodes,
the Emotiv EPOC is designed to be used with a saline
solution that is applied to electrodes, thus significantly
improving the connection between the capacitive sensors
and the scalp compared to dry electrodes.
Furthermore, the numerous electrodes of the Emotiv
EPOC along with its advanced software for brain wave
pattern recognition enables the device to recognise up to
four different mental states that can be used simultaneously for EEG brain control. Whilst the source code of
Emotiv software is proprietary and hidden, it is clear that
the recognition of these mental states is presumably done
by means of some advanced machine learning algorithm
such as an ANN and is conveniently available to programmers via the API.
Also convenient is the availability of an official plugin
compatible with Unity 4.6 that connects the Emotiv API
to Unity. This enables software developers to implement
in Unity the features provided by the headset with ease.
Hence, considering these facts, the Emotiv EPOC provides
a solid middle ground between convenience and accuracy.
Unity Environment: A number of 3D game engines for
developing virtual environments exist, with perhaps some
of the most popular being Unity, Unreal Engine, CryEngine, and Source 2. While each of these have their own
strengths and weaknesses, we wish to highlight that the
engine of our choice, Unity, is very quick to learn whilst
simultaneously being quite advanced, thus enabling very
fast prototyping of virtual game environments. Another
strength of the Unity game engine is the cross-platform
focus of the engine, which potentially enables portability
between platforms such as tablets or smart phones in
future development. Finally, as stated above, Emotiv has
developed an official Unity plugin which further simplifies
the relationship between the headset and the game engine.
By having access to a readily available plugin to the
Emotiv API, choosing the Unity engine ensures that a
minimal amount of time is used to interface the EEG

device to the game engine and more time can be used
to develop the virtual environment.
An illustration of the framework architecture is shown in
Figure 3. The Emotiv “EmoEngine” handles the EEG signals from the headset and interfaces with the Emotiv API,
enabling programmers to access both raw and processed
EEG signal, thus utilizing BCI functionality. The Unity
plugin provides an interface between the Emotiv API and
the Unity game engine, which is used for creating the virtual training environment. More details on the components
and their interaction are provided in the following sections.
Interfacing Emotiv EPOC with Unity
Before the EPOC headset can be used as a BCI, the
user must record at least one mental state associated with
a “BCI command.” This is done via the bundled Emotiv
Control Panel. This software stores the user profiles and
their relationship between mental states and BCI commands. When the configuration process begins, the user is
prompted for a “neutral” state of mind for a short amount
of time in order for the algorithm to have a neutral base
where no command is active. After the neutral state is
stored, the user may proceed to configure up to four other
commands, in our case labelled by the Emotiv software as
“push,” “pull,” “left,” and “right.” These four commands
can later be accessed programmatically from within Unity
via the plugin. After the desired BCI commands have been
recorded, the user can attempt to activate them again inside
the wheelchair framework. Importantly, these commands
can be mapped to have different meanings in different
setting, e.g., “push” can be used as a “forward” command
in one particular setting, or mode, while being used as a
“choose” command in another mode.
When the headset measures an EEG state similar to a
previously recorded EEG state, the EPOC determines how
closely it matches the original recording, and assigns it
a number in the interval [0, 1], where 1 means a perfect
match. Algorithm 1 shows how this information can be
accessed in Unity. This particular piece of code is executed
at a rate of 60 times per second, and queries the Emotiv
plugin for updated information.
When recording the BCI commands it may be beneficial
for the user to mentally associate the commands with physical movements. For example, one may associate “push”
with walking, and “left” and “right” with movement of the

EmoEngine(EDK.dll)
EmoState Buﬀer

EEG and Gyro postprocessing

Unity
plugin

EmoState and
EmoEvent query
handling

Control Logic

Emotiv EPOC EEG headset

Unity

Input script

Emotiv
API

Asynchronous
input

Autopilot

forward

choose

rotateLeft

down

rotateRight

up

toggle

toggle

Figure 3: The proposed framework architecture. Some elements of this figure are credit to Emotiv (Emotiv, 2014).
void CognitivActionUpdate(){
int count=0;
foreach (float f in EmoCognitiv.
CognitivActionPower) {
//only the current active command can be
greater than zero
if(f>0f) {
CurrentCognitivPower = f;
CurrentCognitivAction =
EmoCognitiv.cognitivActionList[count].
ToString();
}
else count+=1;
}
}

Algorithm 1: Method of obtaining cognitive data from the
plugin.

left and right arms, respectively, thus making it easier to
reproduce a particular BCI command. However, in cases
where the person never had the ability to walk or make
arm movements, slightly more creative approaches must be
made. For example, some testing was done using various
mental images unrelated to bodily movement. An example
of this could be to imagine a cube suspended by rubber
bands inside one’s own head. To imagine the movement or
rotation of such a cube will require mental concentration,
which in turn affects the EEG state of the user that can
be used as a control signal.
Design of Game Environment
This section describes how the environment was created
together with the design choices underneath the surface.
The aim was to create an open game world where the
user can roam freely around the environment whilst having
terrain, trees, rubble and buildings limit movement to a
reasonable degree. This is done in order to both create an
intuitive understanding of what to do and where to go, and
at the same time avoid the artificial feeling one might get
from a minimalistic design using invisible borders.

The world is divided into five game levels as described previously (see Figure 2), each testing various
BCI commands and implemented functions as illustrated
in Figure 3.
The wheelchair has two modes of operation that the
user can switch between at any time, a manual self-paced
asynchronous mode (as opposed to cue-based synchronous
mode) and an autopilot mode. We mainly adopt the term
“asynchronous” in this paper since this term is commonly
used in the literature but for most purposes the term
is equivalent to “manual,” meaning that the user is not
limited by any cues or overridden by an autopilot, but is
free to make movements at will.
In the asynchronous mode, the user can move around
freely while learning and practicing BCI commands by
completing tasks prompted by a context-sensitive graphical user interface (GUI) (an example is shown in Figure 4).

Figure 4: Context-sensitive GUI describing the current
objective.
The autopilot mode enables the user to travel between
five predefined geographical locations in the virtual environment. The autopilot is realized by utilizing the A*algorithm (Hart et al., 1968) for pathfinding, and is accessed via the GUI. For real-word purposes, the autopilot
would have to incorporate real maps and a means to
select target locations, for example by freely available map

services online. Great care must also be taken to ensure
that the chosen path is safe and accessible for an electric
wheelchair.
Being limited to four degrees of freedom impose some
challenges when designing how the user should interact
with the application, when there are two modes of operation involved. Several designs were considered prior to
choosing the design illustrated in Figure 3. The diagram
illustrates the inner workings of the Emotiv software
engine, and how the virtual environment is connected. The
“EmoEngine” receives pre-processed EEG and gyro data
from the headset which will then be post-processed into
an “EmoState”-structure that contains the currently active
BCI command. These structures can be accessed via an
“EmoEvent” query which will return the current state. In
this case, the querying for an “EmoEvent” is handled by
the Emotiv API and the Unity plugin (Emotiv, 2014).
The number of mental commands that the Emotiv
software can learn and store for a particular user is limited
to four. However, by using modes, we can use these four
commands for many different purposes. Specifically, the
function activated by a BCI command depends on which
mode is currently active, and whether the command’s
threshold value has been exceeded.
In asynchronous mode, forward, rotateLeft, rotateRight
and toggle are activated by the BCI commands “push,”
“left,” “right,” and “pull,” respectively. The purpose of the
first three commands is to move the wheelchair forward
or rotate it to the left or right, whilst the last command is
used to switch to autopilot mode and back.
In autopilot mode, the wheelchair is operated in a
similar fashion, with the BCI command “push” mapped
to choose, “left” is mapped to down, “right” is mapped to
up, and “pull” is again mapped to toggle. The user uses the
up and down commands to navigate a list of destinations,
and then selects it using the choose command.
Importantly, when having two or more modes in the
game, every mode must include a command for changing
modes. Here, we use a toggle but for more complex structures, possibly involving many modes and even submodes,
a better command could be back, which is well know to
users of smartphones and tablets.

Here, an experiment was performed to investigate
whether an ANN was able to classify two types of EEG
states, namely “meditation” and the command “push.“
We collected 200 raw EEG data samples comprised of
these two different cognitive states. The first 100 samples
were sampled while the user was meditating with closed
eyes, trying to become completely relaxed. The other
100 samples were sampled while the user was trying
to generate the cognitive action “push,” which can be
considered a polar opposite to “meditation.” We used
the Neural Network Toolbox in Matlab (Mathworks, Inc.,
2015) to implement the ANN and perform the experiment.
During the EEG data collection for the experiment, the
user controlled the wheelchair in Unity to ensure that the
correct cognitive state was activated, meaning that if the
wheelchair was not moving, the “push” sample would be
discarded. Measuring the quality of meditation is usually
not a straightforward process because the nature of high
quality meditation remains subjective. As long as the eyes
are closed, less beta activity in the brain can be normally
expected. For these reasons and in order to ensure good
reliability and accuracy for the considered data set, each
data sample was sampled with a duration of 10 seconds
by the same user during the same day.
As inputs to the ANN, the mean power spectral density
from seven EEG channels was used. The power spectrum
was further divided into six frequency bands, namely the
delta, theta, and alpha bands, and the low, medium, and
high subbands of the beta band. Using some rules-ofthumb and trial-and-error, the number of hidden neurons
was set to 21 for best results. Summarising, the ANN
therefore consisted of a 6 ◊ 7 inputs, 21 hidden neurons,
and an output categorising the input as either “meditation”
or “push“, as depicted in Figure 5. The sample size of the
training set was 140, whereas both the validation set and
testing set was set to 30 samples.

Preliminary Artificial Neural Network (ANN)
Whilst the Emotiv software for brain wave pattern recognition is a powerful tool for the framework we describe
here, it is proprietary and closed source. This may limit
the functionality of the framework and also forces it to be
compatible with current and future versions of the Emotiv
software. We therefore decided to implement a preliminary
ANN for EEG pattern recognition and classification. In
machine learning and cognitive science, ANNs are a
family of models inspired by biological neural networks
and are used to estimate or approximate functions that
can depend on a large number of inputs and are generally unknown (Yegnanarayana, 2009). This technology is
particularly relevant for the application recognising mental
BCI commands, with a large number of inputs that need to
be considered when acquiring data with an EEG headset.

Figure 5: Structure of artificial neural network (ANN) for
classification of two EEG states.
R ESULTS
A large and realistic urban 3D virtual world has been
implemented, in which users can control a virtual brainactuated wheelchair to navigate this world. Within this
virtual environment, there are five incrementally more difficult game levels for game-based learning and practicing
of brain control of the wheelchair (see Figures 6–10 for
screenshots).
Three young and healthy male students in their twenties
using the virtual training environment were all able to
learn how to control the virtual wheelchair only with their
minds. In particular, the students were able to utilise all

Figure 6: Level 1.

Figure 11: Scatter plot of EEG amplitudes for channel 4
(A) and 7 (B) at 20Hz.

Figure 7: Level 2.

Figure 8: Level 3.

the different BCI commands to complete all game levels
succesfully with good scores in a self-paced asynchronous
mode; to toggle between asynchronous mode and autopilot
mode; and to tell the autopilot to plan and move the
wheelchair to one of five geographical locations in the
virtual world, all only by means of EEG brain waves.
Lower completion time of a game level is equivalent
to better control of the brain-actuated wheelchair. A timer
system was used that both displays the amount of time the
user have spent in a GUI as well as stores the completion
time in a high score system.
Two safety features were implemented, namely collision avoidance and rolling protection. Navigating the
wheelchair, if suddenly on collision course with an object
tagged as “collidable” (which is almost any object in the
virtual environment), the safety brakes will engage if the
wheelchair gets too close. In addition, brakes are also
engaged to avoid unwanted rolling if the wheelchair’s
angle relative to the horizontal plane reaches a certain
threshold and no control command is present.
The A* algorithm was used as basis both for path
planning in the autopilot mode, and for simulating people
walking around on streets.
Preliminary ANN experiment

Figure 9: Level 4.

Figure 10: Level 5.

To gain some insight into the acquired EEG data, a
scatter plot of the signal amplitude of EEG channels 4
and 7 at 20 Hz for either “meditation” samples or “push”
samples is depicted in Figure 11. The two cognitive states
are not aggregated in clusters, which means that observing
EEG at just one particular frequency generally is not
sufficient for classification of a BCI command. It it is for
this reason we had to use bands of frequency with mean
power spectral density instead. As a matter of fact, running
the ANN with a set of discrete frequencies proved to be
insufficient for classifying the two EEG states. Instead,
when running the ANN as described previously using the
mean power spectral densities for six bands of frequencies,

the ANN was able to consistently discern meditation
from the “push” command with an error rate of 0–4%,
depending on the initialisation values of the network, and
the distribution of samples in the training, validation, and
test sets.
D ISCUSSION
In this paper, we present a game-based learning framework for controlling brain-actuated wheelchairs. The
framework may be used to train paraplegic patients with
paralysis of the upper body in a safe virtual environment before introducing them to real wheelchairs. The
virtual wheelchair is controlled only using brain waves
through the low-cost COTS product Emotiv EPOC headset. Moreover, the virtual environment has been developed
using the Unity 3D game engine. Compared to existing
works, we use a game-based learning methodology to
motivate, enhance and speed up the learning process. The
framework is modular and flexible, with easy extensions
to more features such as game levels and skill training,
multiplayer options, interfacing to a real wheelchair, and
more.
Safety and Control
Safety is an extremely important issue before actual
real-world brain-actuated wheelchairs can be used. Here,
we have implemented two safety features, namely collision avoidance and rolling protection. Collision avoidance
would be much harder to do in a real-world, uncertain,
dynamic environment than in our controlled virtual world,
and would likely require the use of advanced artificial intelligence (AI) for computer vision, planning, and
decision-making. Using the rolling protection mechanism
for a real wheelchair would need a gyroscope but should
be fairly easy to implement for the physical wheelchair.
A safety feature that probably should be implemented
include some kind of AI emergency interruption, where
the AI overrides the BCI command provided by the user
if the execution of the command can be dangerous, such
as driving the wheelchair into a street with heavy traffic.
It may also be that the inclusion of a third semiautonomous, or cue-paced (synchronous), mode could
improve control and thereby safety. In such a mode, the
user is presented with cues such as a visual image, text,
sound, or similar for triggering particular BCI commands.
An AI decision-support system could infer what would be
a good command at a given moment, for example, turning
left at an intersection, and present the user with the cue
that corresponds to turning left.
Finally, one could take advantage of the steady state
visually evoked potential (SSVEP), much like we do in our
accompanying paper submitted concurrently (Verplaetse
et al., 2016). When providing a user with a computer
screen rapidly switching between two colours at a given
frequency, one can evoke a SSVEP for higher EEG activation and better mental control. According to a survey
by Zhu et al. (2010), BCI systems based on the SSVEP
provide a higher level of information throughput and

require shorter training than BCI systems using that are
not augmented with SSVEP. The SSVEP could be used
in a manner similar as the cue-paced mode above to aid
in generating certain EEG patterns and BCI commands at
discrete points in time.
Reverse-Engineering Emotiv Software
The preliminary ANN experiment is equivalent to a
first baby step towards reverse-engineering the proprietary
software developed by Emotiv for generating BCI commands based on EEG signals. We were able to successfully
implement a simple ANN able to classify two EEG states:
meditation and “push.“ The experiment has provided some
insight into how we can use ANNs for such brain wave
pattern recognition. However, we acknowledge that there
is still much work to do, and that the task we adopted
was simple. If we had chosen two BCI commands that
both require concentration and mental focus on a command (as opposed to meditation, which aims to reduce
concentration), it may have been more difficult for the
ANN to perform classification. Likewise, with more BCI
commands to classify, the problem also becomes harder.
Nevertheless, our experiment does seem to indicate that
ANNs are suitable for solving this problem.
Future Work
As future work, it would be interesting to consider
the possibility of adding some level of adaptability to
the proposed game-based methodology to improve the
learning experience. This could be achieved by developing
a specific learning algorithm that can adapt the level of
external assistance provided to the subject according to
the subject’s experience. A similar algorithm has been
presented by several researchers (Philips et al., 2007;
Millan et al., 2009). The underlying idea was to provide
the subject with an adaptive level of support, thereby
complementing the user’s capabilities at any moment,
whenever necessary. An inexperienced user will receive
more assistance than an experienced one. If, after some
time, the performance of the user has improved, the
assisting behaviours will be less activated. By introducing
this adaptability, the users remain in maximal control.
To make the game-based learning experience more
immersive and therefore even more engaging for the user,
the integration with an open-source low-cost framework
for a fully-immersive haptic, audio and visual experience
like the one proposed by Sanfilippo, Hatledal and Pettersen
(2015) may be considered. This framework allows for
establishing a kinesthetic link between a human operator
interacting with a computer-generated environment.
One more possible future work that we are considering
is the possibility of implementing a shared control system
between the a simulated and a real wheelchair. The system
can then serve the purpose as a platform for virtual
prototyping of the real wheelchair, where modelling, features, functionality and so forth can be simulated before
the real physical wheelchair is built. This approach may
also be very useful for minimising the difficulties for the

subjects to switch from a simulated system to a real system
when the training programme is terminated. In addition,
comparative studies can be performed concerning usability
and taking into account human factors.
The concept of EEG and BCI can probably be beneficial
for other human assistance technologies. One exciting application could be that of intelligent prostheses or exoskeletons that likely would require the use of machine learning
algorithms and evolutionary computation, with which we
have extensive experience at NTNU in Ålesund (e.g., see
Sanfilippo et al., 2013, 2014; Sanfilippo, Hatledal, Styve,
Zhang and Pettersen, 2015; Bye et al., 2015; Bye and
Schaathun, 2015; Alaliyat et al., 2014; Hatledal et al.,
2014, for work relating to genetic algorithms, particle
swarm optimisation, ANNs, and more).
Other possible work to be considered in the future may
include testing of different machine learning algorithms
and compare their corresponding performances. In order
to do this, a machine learning framework that provides a
selection of existing learning approaches and allows for
implementing new algorithms can be used as presented
in (Hatledal et al., 2014). This framework can be used
to develop a standard benchmark suite for testing and
measuring the effectiveness and accuracy of the compared
methods.
Finally, we would like to draw attention to an accompanying paper we submit concurrently, in which we use
a similar system as described here, designed to provide
partially monoplegic stroke patients with a rehabilitation
platform using EEG brain control of a virtual paretic hand
(Verplaetse et al., 2016).
R EFERENCES
Alaliyat, S., Yndestad, H. and Sanfilippo, F. (2014), Optimisation
of Boids Swarm Model Based on Genetic Algorithm and
Particle Swarm Optimisation Algorithm (Comparative Study),
Proceedings of the 28th European Conference on Modelling
and Simulation.
Bye, R. T., Osen, O. L. and Pedersen, B. S. (2015), A computerautomated design tool for intelligent virtual prototyping of
offshore cranes, Proceedings of the 29th European Conference
on Modelling and Simulation (ECMS’15), pp. 147–156.
Bye, R. T. and Schaathun, H. G. (2015), A simulation study of
evaluation heuristics for tug fleet optimisation algorithms, Operations Research and Enterprise Systems. In Communications
in Computer and Information Science, Springer, pp. 165–190.
Craig, D. A. and Nguyen, H. (2007), Adaptive EEG thought
pattern classifier for advanced wheelchair control, Proceedings
of the 29th IEEE Annual International Conference on Engineering in Medicine and Biology Society (EMBS), pp. 2544–
2547.
Duvinage, M., Castermans, T., Petieau, M., Hoellinger, T.,
Cheron, G. and Dutoit, T. (2013), Performance of the Emotiv
Epoc headset for P300-based applications, Biomedical Engineering Online 12(1), 56.
Emotiv (2014), Emotiv Software Development Kit.
http://emotiv.com/developer/SDK/UserManual.pdf
Hart, P. E., Nilsson, N. J. and Raphael, B. (1968), A formal basis
for the heuristic determination of minimum cost paths, IEEE
Transactions on Systems Science and Cybernetics 4(2), 100–
107.
Hatledal, L. I., Sanfilippo, F. and Zhang, H. (2014), JIOP: a
java intelligent optimisation and machine learning framework,

Proceedings of the 28th European Conference on Modelling
and Simulation (ECMS), Brescia, Italy, pp. 101–107.
Klimesch, W. (1999), EEG alpha and theta oscillations reflect
cognitive and memory performance: a review and analysis,
Brain Research Reviews 29(2), 169–195.
Leeb, R., Friedman, D., Müller-Putz, G. R., Scherer, R., Slater,
M. and Pfurtscheller, G. (2007), Self-paced (asynchronous)
BCI control of a wheelchair in virtual environments: a case
study with a tetraplegic, Computational Intelligence and Neuroscience 2007.
Mathworks, Inc. (2015), MATLAB Neural Network Toolbox, The
Mathworks, Inc., Natick, Massachusetts.
Millan, J. D. R., Galán, F., Vanhooydonck, D., Lew, E., Philips,
J. and Nuttin, M. (2009), Asynchronous non-invasive brainactuated control of an intelligent wheelchair, Proceedings of
the IEEE Annual International Conference on Engineering in
Medicine and Biology Society (EMBC), pp. 3361–3364.
Niedermeyer, E. and da Silva, F. L. (2005), Electroencephalography: basic principles, clinical applications, and related
fields, Lippincott Williams & Wilkins.
Philips, J., Millán, J. d. R., Vanacker, G., Lew, E., Galán, F.,
Ferrez, P. W., Brussel, H. V. and Nuttin, M. (2007), Adaptive shared control of a brain-actuated simulated wheelchair,
Proceedings of the IEEE 10th International Conference on
Rehabilitation Robotics (ICORR), pp. 408–414.
Sanfilippo, F., Hatledal, L. I. and Pettersen, K. Y. (2015),
A fully-immersive hapto-audio-visual framework for remote
touch, Proceedings of the 11th IEEE International Conference
on Innovations in Information Technology (IIT’15), Dubai,
United Arab Emirates.
Sanfilippo, F., Hatledal, L. I., Schaathun, H. G., Pettersen, K. Y.
and Zhang, H. (2013), A universal control architecture for
maritime cranes and robots using genetic algorithms as a
possible mapping approach, Proceedings of the IEEE International Conference on Robotics and Biomimetics (ROBIO),
Shenzhen, China, pp. 322–327.
Sanfilippo, F., Hatledal, L. I., Styve, A., Zhang, H. and Pettersen,
K. Y. (2015), Integrated flexible maritime crane architecture
for the offshore simulation centre AS (OSC): A flexible
framework for alternative maritime crane control algorithms,
IEEE Journal of Oceanic Engineering PP(99), 1–12.
Sanfilippo, F., Hatledal, L. I., Zhang, H. and Pettersen, K. Y.
(2014), A mapping approach for controlling different maritime
cranes and robots using ANN, Proceedings of the 2014 IEEE
International Conference on Mechatronics and Automation
(ICMA), Tianjin, China, pp. 594–599.
Susi, T., Johannesson, M. and Backlund, P. (2007), Serious
games: An overview.
Tanaka, K., Matsunaga, K. and Wang, H. O. (2005),
Electroencephalogram-based control of an electric wheelchair,
IEEE Transactions on Robotics 21(4), 762–766.
Tobias, S., Fletcher, J. D. and Wind, A. P. (2014), Game-based
learning, Handbook of research on educational communications and technology, Springer, pp. 485–503.
Vaque, T. (1999), The history of EEG Hans Berger: Psychophysiologist. A Historical Vignette., Journal of Neurotherapy: Investigations in Neuromodulation, Neurofeedback and
Applied Neuroscience 3(1), 1–9.
Verplaetse, T., Sanfilippo, F., Rutle, A., Osen, O. L. and Bye,
R. T. (2016), On Usage of EEG Brain Control for Rehabilitation of Stroke Patients, Proceedings of the 30th European
Conference on Modelling and Simulation (ECMS ’16) (submitted for publication), Regensburg, Germany. Submitted for
publication.
Yegnanarayana, B. (2009), Artificial neural networks, PHI Learning Pvt. Ltd.
Zhu, D., Bieger, J., Molina, G. G. and Aarts, R. M. (2010), A
survey of stimulation methods used in SSVEP-based BCIs,
Computational intelligence and neuroscience 2010, 1.

AUTHOR B IOGRAPHIES
ROLF -M AGNUS H JØRUNGDAL is currently a MSc
student in simulation and visualisation at NTNU in Ålesund (formerly Aalesund University College). He completed his BSc degree in automation engineering in 2015,
where his thesis provided the foundation for the project
described in this paper.
F ILIPPO S ANFILIPPO1 received the BSc degree in
computer engineering from the University of Catania,
Catania, Italy, in 2009 and the MSc degree in computer
engineering from the University of Siena, Siena, Italy, in
2011. In 2008, he was a Visiting Scholar at the School of
Computing and Intelligent Systems, University of Ulster,
Londonderry, United Kingdom and in 2010 a Visiting
Fellow at the Technical Aspects of Multimodal Systems
(TAMS) research group, Department of Mathematics, Informatics and Natural Sciences, University of Hamburg,
Hamburg, Germany. In 2015, he received a PhD degree
from the Department of Engineering Cybernetics, Norwegian University of Science and Technology (NTNU),
Trondheim, Norway. For his PhD studies, he was awarded
a research scholarship from the IEEE Oceanic Engineering
Society (OES) Scholarship program. He is currently working as a Post-Doctoral Researcher at the Department of
Engineering Cybernetics, NTNU in Trondheim, Norway.
His research interests include control methods, robotics,
artificial intelligence and modular robotic grasping.
OTTAR L. O SEN is MSc in Cybernetics from the
Norwegian Institute of Technology in 1991. He is the head
of R&D at ICD Software AS and an assistant professor at
NTNU in Ålesund.
A DRIAN RUTLE2 holds PhD and MSc degrees in
Computer Science from the University of Bergen, Norway.
Rutle is an associate professor at the Department of
Computing, Physics and Mathematics at the Bergen University College, Norway. Rutle’s main interest is applying
theoretical results from the field of model-driven software
engineering to practical domains and has expertise in
the development of formal modelling frameworks and
domain-specific modelling languages. He also conducts
research in the fields of modelling and simulation for
virtual prototyping purposes.
ROBIN T. B YE3 graduated from the University of New
South Wales, Sydney with a BE (Hons 1), MEngSc, and
a PhD, all in electrical engineering. Dr. Bye began working at NTNU in Ålesund (formerly Aalesund University
College) as a researcher in 2008 and has since 2010
been an associate professor in automation engineering.
His research interests belong to the fields of artificial
intelligence, cybernetics, and neuroengineering.

1 filipposanfilippo.inspitivity.com
2 www.rutle.no

3 www.robinbye.com

