EEG in the classroom: Synchronised neural recordings during video
presentation
Andreas Trier Poulsen1,+,* , Simon Kamronn1,+ , Jacek Dmochowski2,3 , Lucas C. Parra3 , and
Lars Kai Hansen1

arXiv:1604.03019v3 [q-bio.NC] 27 Dec 2016

1

Technical University of Denmark, DTU Compute, Kgs. Lyngby, Denmark
2
Stanford University, Department of Psychology, Palo Alto, USA
3
City College of New York, Department of Biomedical Engineering, New York, USA
*
atpo@dtu.dk
+
these authors contributed equally to this work
December 30, 2016

Abstract
We performed simultaneous recordings of electroencephalography (EEG) from multiple students in a classroom, and measured the inter-subject correlation (ISC) of activity evoked by a
common video stimulus. The neural reliability, as quantified by ISC, has been linked to engagement and attentional modulation in earlier studies that used high-grade equipment in laboratory
settings. Here we reproduce many of the results from these studies using portable low-cost equipment, focusing on the robustness of using ISC for subjects experiencing naturalistic stimuli. The
present data shows that stimulus-evoked neural responses, known to be modulated by attention,
can be tracked in for groups of students with synchronized EEG acquisition. This is a step towards
real-time inference of engagement in the classroom.

Introduction
Engagement and attention are important in situations of learning, but most methods for measuring of
attention or engagement are intrusive and unrealistic in everyday situations (Robinson, 1997; Cohen
et al., 1990; Radwan, 2005). Recently, inter-subject correlation (ISC) of electroencephalography
(EEG) has been proposed as a marker of attentional engagement (Dmochowski et al., 2012, 2014;
Ki et al., 2016) and we ask in this work whether it can be recorded robustly with commercial-grade
wireless EEG devices in a classroom setting. Furthermore, we address two other issues related to the
robustness of the signal: The potential neurophysiological origin of the measure and the robustness
of the detection scheme to inter-subject variability in spatial alignment.
User engagement has been defined as ‘... the emotional, cognitive and behavioural connection
that exists, at any point in time and possibly over time, between a user and a resource’ (Attfield
et al., 2011). Traditional approaches to measuring engagement are based on capturing user behaviour
via user interfaces, self-report, or manual annotation (O’Brien and Toms, 2013). However, tools from
cognitive neuroscience are increasingly being employed (Szafir and Mutlu, 2013). Recent efforts in
neuroscience aim to elucidate perceptual and cognitive processes in a more realistic setting and using
naturalistic stimuli (Dmochowski et al., 2012; Ringach et al., 2002; Hasson et al., 2004; Lahnakoski
et al., 2014; Lankinen et al., 2014; Chang et al., 2015). From an educational perspective such
quantitative measures may help identify mechanisms that make learning more efficient (Szafir and
Mutlu, 2013), align services better with students needs (Attfield et al., 2011), or monitor critical

Figure 1: Experimental setup for joint viewings. (Left): 9 subjects where placed on a line to induce a cinemalike experiences. (Right): Subjects seen from the back, watching films projected onto a screen. Tablets recording
EEG are resting on the tables behind the subjects. The signal is transmitted wirelessly from each subject.

task performance (Lin et al., 2013). The potential uses of engagement detection in the classroom are
numerous, e.g., real-time and summary feedback for the teacher, motivational strategies for increased
student engagement, and screening for impact of teaching materials. Before the findings of tracking
attentional responses with neural activity (Dmochowski et al., 2012, 2014; Ki et al., 2016) can be
employed in a real-time classroom scenario, several issues must be addressed first, including: 1) Is it
possible to reproduce the ISCs to naturalistic stimuli under the adverse conditions of a classroom? 2)
Are the ISCs robust to inter-student variability of the spatial information processing networks? And
3) can ISCs be recorded with equipment that is both comfortable and affordable enough to make it
a realistic technology for schools?
Here we investigate the feasibility of recording such neural responses from students who are
viewing videos. We use an approach developed by Dmochowski et al. (2012) that uses inter-subject
correlation (ISC) of EEG evoked responses. The basic premise is that subjects who are engaged
with the content exhibit reliable neural responses that are correlated across subjects and repetitions
within the same subject. In contrast, a lack of engagement manifests in generally unreliable neural
responses (Ki et al., 2016). ISC of neural activity while watching films have been shown to predict
the popularity and viewership of TV-series and commercials (Dmochowski et al., 2014), and shows
clinical promises as a measure of consciousness levels in non-responsive patients (Naci et al., 2015)
(fMRI study). We argue here that the neural reliability of students indeed may be quantified on a
second-by-second basis in groups and in a classroom setting, and we seek to investigate the robustness
of measuring it with electroencephalography (EEG) responses during exposure to media stimuli.
To enable correlations between multi-dimensional EEG, correlated component analysis (CorrCA)
was introduced (Dmochowski et al., 2012). CorrCA finds multiple spatial projections that are shared
amongst subjects, such that their components are maximally correlated across time. Here we are
interested in the reproducibility of using CorrCA as a measure of inter-subject correlation, and will
focus predominantly on the first component, which captures most of the neural responses shared
across students.
The main goal of the present work is to determine whether student neural reliability can be
quantified in a real-time manner based on recordings of brain activity in a classroom setting using a
low-cost, portable EEG system – the Smartphone Brain Scanner (Stopczynski et al., 2014a). With
regard to the robustness of the detection scheme, we report on both theoretical and experimental
investigations.
First, we show that ISC evoked by rich naturalistic stimuli is robust enough to
be reproduced with commercial-grade equipment, and to be recorded simultaneously from multiple
subjects in a classroom setting. This opens up for the possibility of real-time estimation of student
attentional engagement. Secondly, we show mathematically that the CorrCA algorithm is surprisingly
2

ISC

0.2

r = 0.61
p < 0.0002

p > 0.01, uncorrected
Present study
Dmochowski et. al

Joint

0.1
0

ISC

0.2

r = 0.64
p < 0.0002

Individual

0.1
0
1

2

3
Time (min.)

4

5

6

(a)

(b)

Figure 2: ISC of neural responses to naturalistic stimuli are robust across different groups of subjects and
reproducible in a classroom setting. (a) Comparison between the ISC obtained by Dmochowski et al. 2012
and the present study for the first CorrCA component and the first viewing of Bang! You’re Dead. The ISC
is calculated with a 1-second resolution (5 s windows, 80% overlap). The grey area indicates chance levels for
ISC (p > 0.01 estimated with time-shuffled surrogate data, uncorrected for multiple comparisons). (b) The
corresponding scalp projections of the first three components obtained from the correlated component analysis
(CorrCA) of each of the four subject groups watching Bang! You’re Dead the first time. For each component,
CorrCA finds one shared set of weights for all subjects in the group. Four distinct groups of subjects watched
videos in different scenarios: individually on a tablet computer (Individual ), individually with order of scenes
scrambled in time (Scrambled ), and jointly in a classroom as seen in Fig. 1 (Joint 1 and Joint 2 ). For each
projection, the polarity was normalized so the value at the Cz electrode is positive.

robust to variations in the spatial patterns of brain activity across subjects. Finally, we demonstrate
that the level of ISC is related to a very basic visual response that is modulated by narrative coherence
of the video stimulus.

Results
To monitor neural reliability we used video stimuli as they provide a balance between realism and
reproducibility (Hasson et al., 2004). We recorded EEG activity using the Smartphone Brain Scanner
while subjects watched short video clips of approximately 6 minutes duration, either individually or
in a group setting (Fig. 1). To measure reliability of EEG responses, we used correlated components
analysis (CorrCA, see Methods) to extract maximally correlated time series with shared spatial
projection across repeated views within the same subject (inter-viewing correlation, IVC), or between
subjects (inter-subject correlation, ISC).
One of our main points of interest is to investigate the robustness of ISC from EEG recorded in a
classroom through comparisons with results previously measured in a laboratory setting (Dmochowski
et al., 2012). We therefore employed similar methods of analysis and calculated ISCs and IVCs
in 5 second windows with 80 % overlap to investigate their temporal development in a 1-second
resolution. We chose to analyse the EEG with CorrCA in a broad frequency band (0.5 and 45 Hz),
instead of investigating specific frequency bands, to keep the analysis methods comparable with the
prior lab-based study. Moreover, CorrCA is a method used for robustly measuring ISC with low
computational costs; hence making it a good candidate for long term real-time analyses on small
devices in a classroom setting.
The subjects watched three video clips, which were presented twice in random order. The first
video was a suspenseful excerpt from the short film, Bang! You’re Dead, directed by Alfred Hitchcock.
It was selected because it is known to effectively synchronize brain responses across viewers (Hasson
3

Table 1: Correlation coefficients between the ISC time courses obtained in a laboratory setting (Dmochowski
et al., 2012) and those obtained in the present study (groups Individual, Joint 1 and Joint 2 ). Inter-subject
correlation (ISC) measures similarity of responses between subjects for the first and second viewings (v1,v2), and
the inter-viewing correlation (IVC) measures similarity within-subject between the two views. Coefficients are
calculated for the first CorrCA component recorded while watching Bang! You’re dead. **: p < 0.01.

Individual
Joint group 1
Joint group 2

ISC v1

ISC v2

IVC

0.64**
0.51**
0.61**

0.33**
0.15**
0.28**

0.49**
0.44**
0.54**

Table 2: Scenes described by the subjects as having the strongest impression on them. Based on the 30 subjects
which saw Bang! You’re Dead with uninterrupted narrative. In a post-experiment questionnaire, subjects were
asked to describe the scenes that made the strongest impression on them. Their answers were collected in the
eight groups. The subjects each mentioned 1.77 scenes on average (0.77 std.). 29 subjects (97 %) mentioned
either scenes where the boy points the gun at his mother or at other people.

Scene

Approx. times

The boy shoots (or points gun at) mother
The boy shoots (or points gun at) at people
The boy loads another bullet into gun
The uncle discovers his gun is gone
The boy finds and loads gun
The boy points at mirror or shoot towards camera
When the father did not run after the boy
The abrupt ending

2:25 and 3:00
2:10, 3:30 and 5:30
6:10
4:35
0:25 and 1:40
0:40, 1:50 and 5:25
3:00
6:14

No of times mentioned (%)
16 (53
15 (50
8 (27
4 (13
4 (13
4 (13
1 (3
1 (3

%)
%)
%)
%)
%)
%)
%)
%)

et al., 2008; Dmochowski et al., 2012). The second video was an excerpt from Sophie’s Choice,
directed by Alan J. Pakula (1982), and the third was an uneventful baseline video of people silently
descending an escalator. For both the joint and individual recording scenarios, the time course of
the ISC, based on the first CorrCA component from subjects watching the film, closely reproduces
results obtained previously in a laboratory setting (Fig. 2a and Table 1).
An indication of the stability of the technique is provided by the spatial patterns of the neural
activity that drives these reproducible responses. Similar to other component extraction techniques,
such as independent component analysis or common spatial patterns (Parra and Sajda, 2003; Koles
et al., 1990), CorrCA reduces the signal of multiple electrodes to a few components. The ISC is then
computed for the first few components, which capture most of the correlation between recordings.
The strongest three correlated components show a stable pattern of activity across the different
groups and recording conditions (Fig. 2b), all three obtaining significant spatial correlations between
groups (rcomp1 = 0.97, rcomp2 = 0.91, rcomp3 = 0.79, all with p < 0.002 for uncorrected permutation
test), for Bang! You’re Dead. The robustness to recording conditions is also apparent for the second
film clip from Sophie’s Choice (rcomp1 = 0.51, p < 0.002; rcomp2 = 0.48, p = 0.008; rcomp3 = 0.36,
p = 0.033), albeit with a lower average correlation, which for the first two components may be due
to noisy scalp maps for the Joint 1 group and Individual group, respectively (see supplementary
Fig. S1). For the baseline video, only the first component achieved significant average correlation
between groups (rcomp1 = 0.46, p = 0.014). The lower stability in the scalp maps obtained for
Sophie’s Choice and the baseline video could be explained by the lower ALD of these stimuli (see
below), since these films obtain lower average IVC compared to Bang! You’re Dead for all groups
(Fig. 3).
Previous research has indicated the potentials of ISC as a marker of engagement of conscious
processing (Dmochowski et al., 2012, 2014; Naci et al., 2015; Lahnakoski et al., 2014; Ki et al.,
2016). To further investigate this, we asked subjects post-experiment to describe the film segments
4

Figure 3: Distribution and mean of IVC calculated from the first CorrCA component for subject groups and
films. Violin plots show distributions of IVC estimated using a squared exponential (normal) kernel with bandwidth of 0.005 (Hoffmann, 2015). Horizontal black bars denote distribution means. For visualisation purposes,
the extreme 2.5% values at either end of the distributions were left out of the violin plots (but were kept for
estimating mean and p-values). A block permutation test (block size B = 25 s) was employed to estimate
statistical significant differences in the mean IVC between viewing conditions (uncorrected for multiple comparisons). For both films there were significant differences in mean IVC between groups with normal narrative and
the Scrambled group (Bang! You’re Dead : pIndividual = 0.006, pJoint 1 = 0.033, pJoint 2 = 0.004; Sophie’s Choice:
pIndividual = 0.059, pJoint 1 = 0.37, pJoint 2 = 0.012). However, there were no significant differences between groups
with the original, unscrambled narrative. Note that the Scrambled group did not watch the baseline video.

(or ”scenes”) that made the biggest impact on them. We quantified their answers by assigning each
answer to one of eight general scene descriptions. Table 2 shows that the scenes most frequently
mentioned are ”Boy pointing gun at mother” or ”Boy pointing gun at people”, and 29 out of 30
subjects mentioned one or both of the scenes as having had high impact on them. The most
frequently mentioned scene occurs around 2:25, where a peak in the ISC can be seen (Fig. 2a). The
high impact of this particular scene was confirmed by the suspense ratings presented in Naci et al.
(2015). See Dmochowski et al. (2012) for additional descriptions and examples of scenes eliciting
high ISC in Bang! You’re Dead.
To determine if the portable equipment, which uses only 14 channels, can detect varying levels
of neural reliability, a second group of subjects watched the same two film clips individually, but
now with scenes scrambled in time. This intervention is a widely used tool to create a baseline with
similar low-level stimuli, yet reduced engagement (Miller and Selfridge, 1950; Anderson et al., 2006;
Hasson et al., 2008; Dmochowski et al., 2012). See Methods for more information on the definition
and time scales of the scrambled scenes. Despite using consumer-grade EEG we find that IVC is
significantly above chance for a large fraction of the original engaging clip, but drops dramatically
when the scenes are scrambled in time (mean IVC, Fig. 3, p < 0.01, for Bang! You’re Dead ). Also
the baseline video, which subjects reported not to engage them at all, only obtained significant ISC
(p < 0.01, uncorrected) in 2.3 % of the 354 tested time windows, compared to the 54.1 % significant
windows obtained for Bang! You’re Dead.
For experiments conducted in less controlled, everyday settings as in this study, it is important
to assess across-session reproducibility. To test this, we recorded a second group of subjects in a
classroom setting who watched the material together (Joint 1 and 2 ). These two groups obtained
mean IVCs comparable to the individual recordings (Fig. 3, Bang! You’re Dead : p > 0.49, Sophie’s
Choice: p > 0.26), and also showed reproducibility between the groups of simultaneous recordings
(Fig. 3, Bang! You’re Dead : p > 0.49, Sophie’s Choice: p > 0.08).
Robustness to inter-subject variations in the spatial brain structure is a basic question when
5

0.075

0.1
0

1

2

3
Time (min.)

4

5

6

ALD

ISC

0.2

0.15

r = 0.71
p < 0.0002

0

Figure 4: The ISC of the first CorrCA component is temporally correlated with the average luminance differences
(ALD) of the film stimulus. ALD is calculated as the frame-to-frame difference in pixel intensity, smoothed to
match the 5 s window of ISC, and mainly reflects the frequency of changes in camera position. Data computed
from the neural responses of subjects watching Bang You’re Dead.
Table 3: Correlation coefficients between the ALD and the ISC for the two viewings (v1,v2) as well as the IVC
for the first correlated component. The correlation is presented for Bang You’re dead and Sophie’s Choice for
the Individual and Scrambled (Scr) groups. **: p < 0.01.

ISC v1

ISC v2

IVC

0.71**
0.50**
0.54**
0.42**

0.61**
0.24**
0.45**
0.01

0.56**
0.23**
0.35**
-0.22**

Bang You’re Dead
Sophie’s Choice
Bang You’re Dead (Scr)
Sophie’s Choice (Scr)

applying CorrCA to classroom data.
CorrCA is derived under the assumption that the spatial
networks of subjects are identical. This assumption could be challenged by inter-individual differences, however, it turns out to be surprisingly robust to such variability (Kamronn et al., 2015). To
demonstrate this, we briefly analyse a ’worst case’ scenario in which the true mixing weights of two
subjects form a pair of orthogonal vectors. The observations are assumed to consist of a single true
signal, z, mixed into D dimensions with additive Gaussian noise; X1 = a1 z| +  , X2 = a2 z| +  .
Given a large sample, the covariance matrices are given as R11 = P · a1 a|1 + σ 2 I , R12 = P · a1 a|2 ,
where P is the variance of z and σ 2 signifies the noise variance. For simplicity the weight vectors
are assumed to be unit length. The two matrices in Eq. (3) can then be written as

(R11 + R22 )

−1

1
=
P

#
!−1
a|1
2σ 2
[a1 a2 ] | +
I
;
P
a2
"

"

R12 + R21

#
a|2
= P · [a1 a2 ] | ,
a1

(1)

using block matrix notation. With a|1 a2 = 0, ka1 k2 = ka2 k2 = 1 and the Woodbury identity, the
product of the two matrices in Eq. (1) can be expressed as

(R11 + R22 )−1 (R12 + R21 ) =

P
(a1 a|2 + a2 a|1 ).
+P

2σ 2

(2)

An eigenvector of matrix (2) takes the form αa1 + βa2 , with α = ±β and ± 2σ2P+P as eigenvalues.
By applying this eigenvector to observations, X1 and X2 , we see that CorrCA still identifies the
relevant time series, z.
For the first CorrCA component, the channels weighted most heavily are the ones positioned over
the occipital lobe (see Fig. 2b). To estimate how much of the ISC was driven by basic low-level
visual processing, we analysed the relation between ISC and a measure of frame-to-frame luminance
fluctuations (average luminance difference, ALD; see methods). Note that to avoid synchronised eye
6

Bang! You're Dead

Sophie's Choice
0.15

0.2

slope: 8.183

slope: 1.872

0.1

0.15

ISC

slope: 1.329

0.1

slope: 5.589

0.05
0.05
0

Narrative
Scrambled

0
0.04

0.08

0.12

0.005

ALD

0.010

0.015

ALD

Figure 5: Relation between the ISC and the ALD for different conditions. Each point indicates a point in the
ISC time course as seen in Fig. 2a (5 s windows, 80% overlap) and the corresponding ALD calculated from the
visual stimulus. It is evident that time points with higher luminance fluctuations (hight ALD) result in higher
correlation of brain activity across subjects (high ISC). The indicated ”slope” is a least squares fit of the slope
of lines passing through (0,0). The slope indicates the strength of ISC for a given ALD value. For both films
there is a significant drop in the slope (p < 0.01: block permutation test with block size B = 25sec), thus the
original narrative (blue) elicits higher ISC than the less engaging scrambled version of the films (red). Note that
brightness of the scenes in Sophie’s Choice is much lower than in Bang! You’re dead, resulting in an ALD that
is lower by almost a factor 10.

artefacts and to ensure that only signals of neural origin contributed to the measured correlations,
we removed independent components related to eye artefacts from the EEG (see methods).
Figure 4 and Table 3 show that there is a significant correlation between the ISC and the ALD
for both Bang! You’re Dead and Sophie’s Choice for the first CorrCA component. This suggests
that this portion of the correlated activity may indeed be driven by low-level visual evoked responses.
However, the degree of engagement, here represented by narrative coherence, appear to modulate the
amplitude of the ISC time course, since even though the scrambled stimulus was driven by the visual
stimulus, it was so to a lesser extent. Previous research has shown that visual evoked potentials
(VEP) are modulated by spatial attention (Johannes et al., 1995) and that even feature-specific
attention enhances steady-state VEPs (Müller et al., 2006). We quantify the effect of scrambling
the narrative by comparing the sensitivity (slope) of ISC to ALD in both the normal and scrambled
conditions by fitting a simple linear model (Fig. 5). For both films we found significant reductions
of the ISC/ALD slope in the scrambled version (p < 0.01; block permutation test, with block size
B = 25 s).

Discussion
We have demonstrated that student neural reliability to media stimuli may be quantified using EEG
in a classroom setting. For educational technology cost and robustness are key features, hence, we
aimed at establishing a realistic scenario based on low-cost consumer grade equipment, the Smartphone Brain Scanner, focusing on several potential sources that could degrade robustness.
We have provided evidence that salient aspects of the neural reliability previously detected with
laboratory grade equipment can be reproduced in a realistic setting. We recorded fully-synchronized
EEG with nine subjects in a real classroom and found that the level of neural response reliability
matched prior laboratory results. The robustness of CorrCA and ISC is granted by the reproducibil7

ity between recording conditions, both of the ISC time-courses throughout the film clips and of the
spatial topographies of the first three CorrCA components. For the film clip from Bang! You’re
Dead we saw that seven subjects were enough to obtain stable topographies for all three components,
whereas for Sophie’s Choice and the baseline video the results were more noisy, suggesting that more
subjects are needed to obtain stable results. Previous research shows that ten subjects provided for
stable results in a case involving non-narrative baseline videos or films with lower ISC and IVC in a
laboratory setting (Dmochowski et al., 2012).
Mathematically, we have shown that our detection scheme, CorrCA, is robust to inter-subject
variability in spatial configurations of brain networks, or induced by cap misalignment. In the
calculations, we assumed two subjects in a worst case scenario where the subjects’ spatial projections
are orthogonal. This result conforms well with simulations that show that, even for multiple subjects
with randomly drawn spatial projections, CorrCA was able to find the relevant times series (Kamronn
et al., 2015). The simulations also showed that increasing the number of subjects decreased the signalto-noise ratio, presumably due to the estimated common projection not being able to fit with the
different projections of each subject.
We have presented results that further indicate a relationship between changes in ISC and
viewer engagement. Through a basic analysis of questionnaires on scenes of high impact, we found
that high ISC indeed is associated with high impact. We have also showed a relationship between
neural responses to luminance fluctuations and coherence of stimulus narrative. For both the films
presented, we saw a significant drop in the average IVC for subjects watching the film sequences in
which the narrative had been temporally scrambled. At the same time no significant difference was
found between the groups watching the film sequences that had not been scrambled, which further
underlines the robustness of the measure.
It may appear surprising that there exists a significant correlation between the raw EEG signals
of various students in the classroom. However, it is well-known that eye scan patterns in a film
audience follow a specific pattern after a scene change, activating the dorsal pathway (Unema et al.,
2005). A valid assumption could therefore be that the correlation is due to synchronised artefacts
from eye movements, but this has recently been shown not to affect attentional modulation of ISC
(Ki et al., 2016). Also, it is known that stimuli in the form of flashing images elicit VEPs, which are
modulated in amplitude by the luminance (Armington, 1968). When recorded with EEG, the spatial
distribution of the early VEP at 100ms (P100) is similar to the scalp maps of the first correlated
component (C1 in Fig. 2b) (Johannes et al., 1995; Sandmann et al., 2012).
We investigated whether low-level visual processes could be a driving force behind the measured
ISCs by correlating the ISC with changes in luminance in the video stimuli, as measured by the
ALD. We found that luminance fluctuations drive a significant portion of the ISC.
In all four groups of subjects Sophie’s Choice obtained lower IVC compared to Bang! You’re
Dead.
This difference could be explained by the fact that the film clip also had a much lower
ALD. Also, Fig. 4 indicates that the passage in Bang! You’re dead with the highest and most
sustained ISC (around 1:20 to 1:50) coincides with the interval with the most scene changes. This
relationship could, however, also be due to more complex processes, as fast-paced cutting is a known
cinematographic tool used by Hitchcock to induce suspense and thereby increase the attention of the
viewer (Bordwell, 2002).
The strong link between ISC and luminance fluctuations due to scene cuts have also recently
been presented in a fMRI study (Herbec et al., 2015). This is something that would be interesting to
take into account for future studies investigating the applicability of ISC. Baseline videos could be
created in ways to achieve similar ALD features as the target stimuli. The baseline video, created for
this study, consisted of one continuous scene of people entering and exiting an escalator in a relaxed
manner, which did not produce any significant correlation. Future studies might use a baseline video
containing scene cuts of faces and body parts, to also take the effect of editing into account.
To investigate the possibility of higher level processes also being at play, we analysed the linear
8

relationship between ISC and luminance fluctuations at a given time in the video stimulus. The
scrambling operation aimed to test for a change in attentional engagement while controlling for low
level features. The premise was that subjects would be less attentive to the stimulus, i.e. less ”engaged”, if they did not follow the narrative arch of the story. With that in mind, Fig. 4 and 5 suggest
that ISC is driven by stimulus-evoked responses that are modulated by attentional engagement with
the stimulus.
We have demonstrated the feasibility of tracking inter-subject correlation in a classroom setting; a
measure that has been related to attentional modulation (Ki et al., 2016). We have shown that ISC is
robust to recording equipment and conditions, and we have presented evidence that the amplification
of ISC in films that have a strong and coherent narrative is due to attentional modulation of visual
evoked responses. Thus ISC may be used as an indirect electrophysiological measure of engagement
through an attentional top-down modulation of low-level neural processes. Recent research has
shown that attentional modulation of neural responses takes place in speech perception (Mesgarani
and Chang, 2012; Mirkovic et al., 2015), which lends credibility to a similar process occurring in the
visual system. The evidence that such a basic and well defined mechanism could be at play further
adds to the robustness of the approach in real everyday scenarios.

Methods
Protocol. Four groups of subjects watched the video stimuli in different scenarios. The first group
(N = 12, Individual ) watched videos individually in an office environment on a tablet computer
(Google Nexus 7 tablet, with a 7” (17.8 cm) screen) with earphones. The second group (N = 12) saw
the videos in the same manner, but the scenes of the film stimulus were scrambled in time resulting
in the narrative being lost (Scrambled ). The objective of this condition was to demonstrate that the
similarity of responses across subjects is not simply the result of low-level stimulus features (which are
identical in the Individual and Scrambled conditions), but instead, is modulated narrative coherence,
which presumably engages viewers. Two additional groups (N = 9, N = 9) watched the original
videos on a screen in a classroom (Figure 1, Joint 1 and Joint 2 ), with sound projected through
loudspeakers. An attempt was made to create viewing conditions for the subjects in the joint groups,
that were similar to the viewing conditions for the individual group, i.e., lights were dampened and the
projected image produced approximately the same field-of-view (see supplementary materials). The
central question was whether the viewing condition (i.e., in a group versus individually) influences
the level of ISC across subjects.
Stimuli. The first video clip was a suspenseful excerpt from the short film Bang! You’re Dead
(1961) directed by Alfred Hitchcock. It was selected because it is known to elicit highly reliable brain
activity across subjects in fMRI (Hasson et al., 2004) as well as EEG (Dmochowski et al., 2012).
Our second stimulus was a clip from Sophie’s Choice, directed by Alan J. Pakula (1982), which has
been used earlier to study fMRI activity in the context of emotionally salient naturalistic stimuli
(Raz et al., 2012). A third non-narrative control video was recorded in a Danish metro station of
several people who were being transported quietly on an escalator. Each video clip had a length of
approximately six minutes and was shown twice to each subject. For each viewing the order of the
clips was randomized, while the same random order was used the second time the clips were shown.
A combined video was created for each of the six possible permutations of the order of the clips,
starting with a 10 second 43 Hz tone for use in post processing synchronization, and 20 seconds black
screen between each film clip. The total length of the video amounted to 39 minutes. An additional
control stimulus (Scrambled ) was created by scrambling the order of the scenes in Bang! You’re Dead
and Sophie’s Choice in accordance with previous research (Hasson et al., 2008; Dmochowski et al.,
2012). In these studies, scene segments were defined in varying temporal scales (36 s, 12 s, and 4 s)
that consisted of multiple camera positions, ”shots”. For this study we defined a scene as a single
shot (i.e. the segment between two scene cuts) with the added rule that a scene must not exceed 250
9

frames (∼ 10 s) to reduce subjects’ ability to infer the narrative from long scenes. This procedure
resulted in 73 scenes lasting between 0.5 and 10 seconds and corresponded to the intermediate to
short time-scales employed in previous studies (Hasson et al., 2008).
Subjects. A total of 42 female subjects (mean age: 22.4y, age range: 18-32y), who gave written
informed consent prior to the experiment, were recruited for this study. Non-invasive experiments
on healthy subjects are exempt from ethical committee processing by Danish law (Den Nationale
Videnskabsetiske Komité, 2014). Among the 42 recordings, nine were excluded due to unstable
wireless communication that precluded proper synchronization of the data across subjects (five from
the Individual group, one from the Scrambled group and three from the two Joint groups). The
difference in the number of recordings in the different groups could give unfair advantages with
respect to noise when using CorrCA or calculating ISC. We therefore decided to randomly choose
four subjects from the Scrambled group and one from Joint 2 group and excluded these from the
analyses. This was to ensure that each group had seven fully synchronized recordings.
Portable EEG – Smartphone Brain Scanner. Research grade EEG equipment is costly,
time-consuming to set up, and immobile. However, recently consumer grade EEG equipment that is
more affordable and has increased comfort has appeared. Here we use the modified 14 channel system,
’Emocap’, based on the EEG Emotiv EPOC headset. For details and validation, see (Stopczynski
et al., 2014a,b). In this study it was implemented on Asus Nexus 7 tablets. An electrical trigger and
associated sound was used to synchronize EEG and video signals in the individual viewing condition,
while a split audio signal (simultaneously feeding into microphone and EEG amplifiers) was used to
synchronize the nine subjects EEG recordings and the video in the joint viewing condition (see supplementary materials for further information on synchronisation). The resulting timing uncertainty
was measured to be less than 16 ms. The EEG was recorded at 128 Hz and subsequently bandpass
filtered digitally using a linear phase windowed sinc FIR filter between 0.5 and 45 Hz and shifted to
adjust for group delay. Eye artefacts were reduced with a conservative pre-processing procedure using
independent component analysis (ICA), removing up to 3 of the 14 available components (Corrmap
plug-in for EEGLAB (Delorme and Makeig, 2004; Viola et al., 2009)).
Correlated component analysis to measure ISC and IVC. CorrCA was presented in
Dmochowski et al. 2012, as a constrained version of Canonical Correlation Analysis (CCA). CorrCA
seeks to find sets of weights that maximises the correlation between the neural activity of subjects
experiencing the same stimuli. For each neural component, CorrCA finds one shared set of weights
for all subjects in the group.
Given two multivariate spatio-temporal time series (termed view in CorrCA), {X1 , X2 } ∈ RD×N ,
with D being the number of measured features (EEG channels) in the two views and N the number
of time samples, CCA estimates weights, {w1 , w2 }, which maximize the correlation between the
components, y1 = X|1 w1 and y2 = X|2 w2 . The weights are calculated using two eigenvalue equations,
with the constraint that the components belonging to each multivariate time series are uncorrelated
(Hardoon et al., 2004). CorrCA is relevant for the case where the views are homogeneous, e.g.,
using the same EEG channel positions, and imposes the additional constraint of shared weights
w = w1 = w2 . This assumption can potentially increase sensitivity involving fewer parameters. In
CorrCA the weights are thus estimated through a single eigenvalue problem;
(R11 + R22 )−1 (R12 + R21 ) w = ρw,

(3)

where, Rij = N1 Xi X|j , is the sample covariance matrix (Dmochowski et al., 2012). To illustrate the
spatial distribution of the underlying physiological activity of the components, we use the estimated
forward models (”patterns”) as discussed in (Parra et al., 2005; Haufe et al., 2014).
Average luminance difference (ALD). Video clips were converted to grey scale (0-255) by
averaging over the three colour channels. We then calculated the squared difference in pixel intensity
from one frame to the next and took the average across pixels. These signals were non-linearly
10

re-sampled at 1Hz by selecting the maximum ALD for each 1 s interval to emphasise the large
differences during changes in camera position (see figure S2 in supplementary materials for an
comparison between frame-to-frame and smoothed difference). These values were then smoothed
in time by convolving with a Gaussian kernel with a ”variance” parameter of 2.5 s2 . This down
sampling and smoothing was aimed at matching the temporal resolution of the ALD to that of the
time-resolved ISC computation (5 s sliding window with 1 s intervals).
Statistical testing. In order to evaluate the statistical relevance of the correlations, we employed
a simple permutation test (P = 5000 permutations) (Dmochowski et al., 2012).
To test the
robustness of the obtained weights for the spatial projections, we calculated the average correlation
of all possible pairings of the four conditions groups for a given component. Again, we employed a
permutation test (P = 5000 permutations) to evaluate statistical relevance by randomly permuting
the channel order for each group and recalculating the average correlation. When testing differences
in average IVC between conditions, we used a block permutation test (block size B = 25 s, P = 5000
permutations) to account for temporal dependencies.

References
Anderson, D. R., Fite, K. V., Petrovich, N., and Hirsch, J. (2006). Cortical activation while watching
video montage: An fmri study. Media Psychology, 8(1):7–24.
Armington, J. C. (1968). The electroretinogram, the visual evoked potential, and the area-luminance
relation. Vision research, 8:263–276.
Attfield, S., Kazai, G., Lalmas, M., and Piwowarski, B. (2011). Towards a science of user engagement.
In WSDM Workshop on User Modelling for Web Applications. ACM International Conference on
Web Search And Data Mining.
Bordwell, D. (2002). Intensified continuity: visual style in contemporary American film. Film
Quarterly, 55(3):16–28.
Chang, W.-T., Jääskeläinen, I. P., Belliveau, J. W., Huang, S., Hung, A.-Y., Rossi, S., and Ahveninen,
J. (2015). Combined MEG and EEG show reliable patterns of electromagnetic brain activity during
natural viewing. NeuroImage, 114:49–56.
Cohen, A., Ivry, R. B., and Keele, S. W. (1990). Attention and structure in sequence learning.
J.Exp.Psychol.[Learn.Mem.Cogn.], 16(1):17–30.
Delorme, A. and Makeig, S. (2004). EEGLAB: An open source toolbox for analysis of single-trial
EEG dynamics including independent component analysis. Journal of Neuroscience Methods,
134(1):9–21.
Den Nationale Videnskabsetiske Komité (2014). Vejledning om anmeldelse, indberetning mv. (sundhedsvidenskablige forskningsprojekter).
Dmochowski, J. P., Bezdek, M. a., Abelson, B. P., Johnson, J. S., Schumacher, E. H., and Parra,
L. C. (2014). Audience preferences are predicted by temporal reliability of neural processing.
Nature Communications, 5:1–9.
Dmochowski, J. P., Sajda, P., Dias, J., and Parra, L. C. (2012). Correlated components of ongoing
EEG point to emotionally laden attention - a possible marker of engagement? Frontiers in human
neuroscience, 6(May):112.
Hardoon, D. R., Szedmak, S., and Shawe-taylor, J. (2004). Canonical correlation analysis; An
overview with application to learning methods. Neural computation, 16(12):2639–2664.
11

Hasson, U., Landesman, O., Knappmeyer, B., Vallines, I., Rubin, N., and Heeger, D. J. (2008).
Neurocinematics: The neuroscience of film. Projections, 2(1):1–26.
Hasson, U., Nir, Y., Levy, I., Fuhrmann, G., and Malach, R. (2004). Intersubject synchronization of
cortical activity during natural vision. science, 303(5664):1634–1640.
Haufe, S., Meinecke, F., Görgen, K., Dähne, S., Haynes, J.-D., Blankertz, B., and Bießmann, F.
(2014). On the interpretation of weight vectors of linear models in multivariate neuroimaging.
NeuroImage, 87:96–110.
Herbec, A., Kauppi, J. P., Jola, C., Tohka, J., and Pollick, F. E. (2015). Differences in fMRI
intersubject correlation while viewing unedited and edited videos of dance performance. Cortex,
71:341–348.
Hoffmann, H. (2015). violin. m-Simple violin plot using matlab default kernel density estimation.
Johannes, S., Münte, T. F., Heinze, H. J., and Mangun, G. R. (1995). Luminance and spatial
attention effects on early visual processing. Cognitive Brain Research, 2:189–205.
Kamronn, S., Poulsen, A. T., and Hansen, L. K. (2015). Multiview Bayesian Correlated Component
Analysis. Neural Computation, 27(10):2207–2230.
Ki, J. J., Kelly, S. P., and Parra, L. C. (2016). Attention strongly modulates reliability of neural
responses to naturalistic narrative stimuli. The Journal of Neuroscience, 36(10):3092–3101.
Koles, Z. J., Lazar, M. S., and Zhou, S. Z. (1990). Spatial patterns underlying population differences
in the background eeg. Brain topography, 2(4):275–284.
Lahnakoski, J. M., Glerean, E., Jääskeläinen, I. P., Hyönä, J., Hari, R., Sams, M., and Nummenmaa,
L. (2014). Synchronous brain activity across individuals underlies shared psychological perspectives. NeuroImage, 100:316–24.
Lankinen, K., Saari, J., Hari, R., and Koskinen, M. (2014). Intersubject consistency of cortical MEG
signals during movie viewing. NeuroImage, 92:217–224.
Lin, C.-T., Huang, K.-C., Chuang, C.-H., Ko, L.-W., and Jung, T.-P. (2013). Can arousing feedback rectify lapses in driving? prediction from eeg power spectra. Journal of neural engineering,
10(5):056024.
Mesgarani, N. and Chang, E. F. (2012). Selective cortical representation of attended speaker in
multi-talker speech perception. Nature, 485(7397):233–236.
Miller, G. A. and Selfridge, J. A. (1950). Verbal context and the recall of meaningful material. The
American journal of psychology, 63(2):176–185.
Mirkovic, B., Debener, S., Jaeger, M., and De Vos, M. (2015). Decoding the attended speech
stream with multi-channel EEG: implications for online, daily-life applications. Journal of Neural
Engineering, 12(4):046007.
Müller, M. M., Andersen, S., Trujillo, N. J., Valdés-Sosa, P., Malinowski, P., and Hillyard, S. a.
(2006). Feature-selective attention enhances color signals in early visual areas of the human brain.
Proceedings of the National Academy of Sciences of the United States of America, 103(38):14250–4.
Naci, L., Sinai, L., and Owen, A. M. (2015). Detecting and interpreting conscious experiences in
behaviorally non-responsive patients. NeuroImage.
12

O’Brien, H. L. and Toms, E. G. (2013). Examining the generalizability of the user engagement scale
(ues) in exploratory search. Information Processing & Management, 49(5):1092–1107.
Parra, L. and Sajda, P. (2003). Blind source separation via generalized eigenvalue decomposition.
The Journal of Machine Learning Research, 4:1261–1269.
Parra, L. C., Spence, C. D., Gerson, A. D., and Sajda, P. (2005). Recipes for the linear analysis of
EEG. NeuroImage, 28(2):326–41.
Radwan, A. A. (2005). The effectiveness of explicit attention to form in language learning. System,
33(1):69–87.
Raz, G., Winetraub, Y., Jacob, Y., Kinreich, S., Maron-Katz, A., Shaham, G., Podlipsky, I., Gilam,
G., Soreq, E., and Hendler, T. (2012). Portraying emotions at their unfolding: a multilayered
approach for probing dynamics of neural networks. NeuroImage, 60(2):1448–61.
Ringach, D. L., Hawken, M. J., and Shapley, R. (2002). Receptive field structure of neurons in
monkey primary visual cortex revealed by stimulation with natural image sequences. Journal of
vision, 2(1):2–2.
Robinson, P. (1997). Individual differences and the fundamental similarity of implicit and explicit
adult second language learning. Language Learning, 47(1):45–99.
Sandmann, P., Dillier, N., Eichele, T., Meyer, M., Kegel, A., Pascual-Marqui, R. D., Marcar, V. L.,
Jäncke, L., and Debener, S. (2012). Visual activation of auditory cortex reflects maladaptive
plasticity in cochlear implant users. Brain, 135:555–568.
Stopczynski, A., Stahlhut, C., Larsen, J. E., Petersen, M. K., and Hansen, L. K. (2014a). The
Smartphone Brain Scanner: A Portable Real-Time Neuroimaging System. PloS one, 9(2):e86733.
Stopczynski, A., Stahlhut, C., Petersen, M. K., Larsen, J. E., Jensen, C. F., Ivanova, M. G., Andersen,
T. S., and Hansen, L. K. (2014b). Smartphones as pocketable labs: visions for mobile brain imaging
and neurofeedback. International journal of psychophysiology, 91(1):54–66.
Szafir, D. and Mutlu, B. (2013). Artful: adaptive review technology for flipped learning. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pages 1001–1010.
ACM.
Unema, P. J., Pannasch, S., Joos, M., and Velichkovsky, B. M. (2005). Time course of information
processing during scene perception: The relationship between saccade amplitude and fixation
duration. Visual Cognition, 12(3):473–494.
Viola, F. C., Thorne, J., Edmonds, B., Schneider, T., Eichele, T., and Debener, S. (2009). Semiautomatic identification of independent components representing EEG artifact. Clinical Neurophysiology, 120(5):868–877.

Acknowledgements
We thank Ivana Konvalinka, Arek Stopczynski and the DTU Smartphone Brain Scanner team for
their assistance and helpful discussions. This work was supported by the Lundbeck Foundation
through the Center for Integrated Molecular Brain Imaging and by Innovation Foundation Denmark
through Neurotechnology for 24/7 brain state monitoring.
13

Author contributions statement
ATP, SK, JD, LP and LKH designed research; SK, ATP and LKH performed research; ATP, SK,
JD, LP, and LKH contributed analytical tools; ATP, SK, LKH analysed data; ATP, SK, JD, LP, and
LKH wrote the paper.

Additional information
Competing financial interests The authors declare that the research was conducted in the absence
of any commercial or financial relationships that could be construed as a potential conflict of interest.

14

