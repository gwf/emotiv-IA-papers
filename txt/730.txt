Multimodal Fusion for Cognitive Load Measurement
in an Adaptive Virtual Reality Driving Task
for Autism Intervention
( )
Lian Zhang1 ✉ , Joshua Wade1, Dayi Bian1, Jing Fan1,
2
Amy Swanson , Amy Weitlauf2,3, Zachary Warren2,3, and Nilanjan Sarkar1,4

1

2

Electrical Engineering and Computer Science Department, Vanderbilt University,
Nashville, TN 37212, USA
lian.zhang@vanderbilt.edu
Treatment and Research in Autism Spectrum Disorder (TRIAD), Nashville, USA
3
Pediatrics and Psychiatry Department, Nashville, USA
4
Mechanical Engineering Department, Vanderbilt University,
Nashville, TN 37212, USA

Abstract. A virtual reality driving system was designed to improve driving skills
in individuals with autism spectrum disorder (ASD). An appropriate level of
cognitive load during training can help improve a participant’s long-term
performance. This paper studied cognitive load measurement with multimodal
information fusion techniques. Features were extracted from peripheral physio‐
logical signals, Electroencephalogram (EEG) signals, eye gaze information and
participants’ performance data. Multiple classiﬁcation methods and features from
diﬀerent modalities were used to evaluate participant’s cognitive load. We veri‐
ﬁed classiﬁcations’ result with perceived tasks’ diﬃculty level, which induced
diﬀerent cognitive load. We fused multimodal information in three levels: feature
level, decision level and hybrid level. The best accuracy for cognitive load meas‐
urement was 84.66 %, which was achieved with the hybrid level fusion.
Keywords: Autism · Virtual reality · Multimodal fusion · Cognitive load
measurement

1

Introduction

Autism spectrum disorder (ASD) is a common disorder that impacts 1 in 68 children in
the US [1]. Although at present there is no single accepted intervention, treatment, or
known cure for ASD, there is a growing consensus that appropriately targeted individ‐
ualized behavioral and educational intervention programs have the potential to posi‐
tively impact the lives of individuals with ASD and their families [2, 3]. However the
availability of trained autism clinicians is limited and the cost associated with traditional
therapies is enormous. As a result, the development of economical and eﬀective assistive
therapeutic tools for autism intervention is urgent.
© Springer International Publishing Switzerland 2015
M. Antona and C. Stephanidis (Eds.): UAHCI 2015, Part III, LNCS 9177, pp. 709–720, 2015.
DOI: 10.1007/978-3-319-20684-4_68

710

L. Zhang et al.

A growing number of studies have investigated the application of technology,
speciﬁcally computer and virtual reality (VR) systems, to autism intervention. There are
numerous reasons why incorporating VR technology into intervention may be particu‐
larly relevant for children and adolescents with ASD. The VR-based intervention plat‐
form is characterized by malleability, controllability, modiﬁable sensor stimulation,
individualized approach, safety, and the potential to reduce problematic aspects of
complex adaptive life skills [4]. These systems could not only help children with ASD
generalizing learned skills to the real world, but also provide more control over how the
basic skills are taught.
At present, most VR-based platforms have been designed to improve social skill
deﬁcits in ASD population [5, 6]. However, other activities of daily life that are important
for functional independence for individuals with ASD have not received similar atten‐
tion. In this work, we focus on VR-based driving since independent driving is often seen
as a proxy measure for functional independence and quality of life for adults across a
variety of disability and non-disability groups. It is noted that many individuals with
ASD fail to obtain driving independence [7]. In addition, an emerging literature suggests
that individuals with ASD display processing diﬀerences in driving environments that
may be linked to unsafe driving behaviors. Despite its importance, to our knowledge,
only two studies have investigated driving interventions for teenagers with ASD [8, 9].
Previous work has investigated the use of technological interventions for driving
skills in people with ASD, but no studies have developed a closed-loop individual‐
ized system to the best of our knowledge. Reimer and colleagues (2013) and Classen
and colleagues (2013) presented participants with a set of driving scenarios using a
driving simulator paradigm within a real vehicle that was converted into a simula‐
tion tool [8, 9]. Classen and colleagues found a higher error in driving performance
for teens with ASD or Attention Deficit-Hyperactivity Disorder (ADHD) compared
with typically developing (TD) group [9]. Reimer and colleagues found different
gaze patterns and physiological signals, such as heart rate and skin conductance
level (SCL), between the TD control group and individuals with autism [8]. They
also found the variation of heart rate in TD group under different cognitive condi‐
tion. These related research work highlighted the need for deeper research in indi‐
vidualized driving system for autism intervention.
We plan to develop an individualized intervention system that can maximize a
participant’s long-term performance by adapting the diﬃculty level of a driving task.
Task diﬃculty directly aﬀects a participant’s cognitive load [10]. A lot of studies modu‐
lated cognitive loads using diﬀerent task diﬃculties [11, 12]. An appropriate cognitive
load could maximize individual’s long-term performance [13]. An individualized
system, which can measure the user’s current cognitive load and modulate the cognitive
load to its optimal level by adjusting the task diﬃculty, has the ability to eﬀectively
improve the user’s performance.
This paper measured cognitive load from multimodal signals, including performance
data and three classes of psycho-physiological signals: peripheral physiological signals
(heart rate, SCL etc.), EEG signals, and eye gaze signals. Performance-based measure was
traditional way for cognitive load measurement [14]. Performance features, including
reaction time, accuracy and error rate, indicated a participant’s cognitive load [11].

Multimodal Fusion for Cognitive Load Measurement in an Adaptive Virtual Reality

711

Psycho-physiological measurement have been shown to provide real time information
about cognitive load [15–17], which in turn can be used to our individualized difficulty
level adjustment. For example, eye gaze offers rich physiological information, such as blink
rate and pupil diameter, to reflect a user’s cognitive state [18, 19]. EEG are sensitive and
reliable data for memory load measurement [20, 21]. Peripheral physiological signals, such
as electrocardiogram (ECG), photoplethysmogram (PPG), electromyogram (EMG), respi‐
ration (Resp.) and skin temperature (SKT), can reflect the variation of cognitive load [22,
23] as well as affective states [24, 25].
Integrating such psycho-physiological signals with performance data has the poten‐
tial to increase the robustness and accuracy for cognitive load measurement [26]. Son
and colleagues integrated performance with physiological data to estimate a driver’s
cognitive workload and got the best result with selected performance features and phys‐
iological features [27]. Koenig and colleagues have quantiﬁed the cognitive load of
stroke patients with both psycho-physiological and performance data and applied in a
closed-loop system [12]. Although other work has studied applications of cognitive load
in individualized intelligent systems with multimodal information fusion techniques,
this has not yet been done for individuals with ASD to the best of our knowledge.
The multimodal information fusion techniques could be presented in three levels:
feature level, decision level and hybrid level [28]. The feature level fusion was an easily
accomplished approach because it required only one learning phase on the combined
feature vector [29]. However, the synchronization from multimodal information was found
to be more challenging [30]. Decision level fusion combined the sub-decisions of each
modality to arrive at a more robust decision [31]. However, it was not good at reflecting the
correlation between features of different modalities [32]. Hybrid level fusion methods seek
to combine the advantages of feature level fusion and decision level fusion [28].
In our previous work [33, 34], we presented a novel VR driving environment with
the ultimate aim of developing an intervention platform capable of enhanced teaching
of driving skills to teenagers with ASD. In this paper, we present our current work in
fusing multimodal information to assess one’s cognitive load during driving. We eval‐
uated multiple classiﬁcation methods for multimodal fusion and compared three levels
of fusion in this paper. The long term goal is to close the loop in such a way that the
driving task can be autonomously adapted to one’s cognitive load to optimize perform‐
ance, which is beyond the scope of this current paper.

2

Methods

2.1 Experimental Setup
The virtual driving system was designed with three components: a driving simulator, a
data acquisition module and a therapist rating module, shown in Fig. 1. Participants used
the driving simulator to engage in driving tasks. The data acquisition module acquired
their psycho-physiological information and performance data in real time. One therapist
observed and rated participants’ emotional state and cognitive state from another room.
All the data collected were synchronized by time stamped events from the driving simu‐
lator via a local area network (LAN).

712

L. Zhang et al.

Participant

Data capture

Eye gaze

EEG

Physiological

Driving
simulator

Therapist rating

performance

Fig. 1. The framework of the VR-based driving system

The driving simulator was composed of a virtual environment and a Logitech G27
steering wheel controller, shown in Fig. 2. The models in the virtual environment (e.g.,
the city, the car, and pedestrians) were realized with the modeling tools ESRI CityEngine
(www.esri.com/cityengine) and Autodesk Maya (www.autodesk.com/maya). The game
engine Unity3D (www.unity3d.com) was used to manipulate the logic of the system.
The system was composed of six diﬀerent levels of diﬃculty to invoke diﬀerent cogni‐
tive loads. Each level included three driving assignments. Each assignment included a
series of eight driving tasks in order to train the speciﬁed driving behaviors, such as
stopping properly at a stop sign, yielding to pedestrians, merging lanes, and turning left.

Fig. 2. The driving simulator and environment

Participants controlled a vehicle in the virtual environment using the controller. Their
driving behaviors and task performance were logged within the system. In addition to
recording performance data, the data acquisition system recorded psycho-physiological
data with psycho-physiological sensors shown in Fig. 3. A Tobii X120 remote eye
tracker (www.tobii.com/) was used to track the participant’s eye gaze. Biopac MP150
(www.biopac.com) sensors recorded ECG, EMG, Resp., SKT, PPG, and galvanic skin
response (GSR)signals wirelessly [35, 36]. The GSR and PPG sensors were attached on

Multimodal Fusion for Cognitive Load Measurement in an Adaptive Virtual Reality

713

the participant’s toes instead of ﬁngers to reduce the motion artifact from driving [37].
An Emotiv EEG headset (www.emotiv.com) recorded 14 channels of EEG signals.

Fig. 3. The psycho-physiological sensors [37]

In the therapist rating module, a therapist observed and rated the participant’s aﬀec‐
tive state and the apparent diﬃculty of the assignment using a 0-9 Likert scale. The
rating categories included diﬃculty level, engagement, enjoyment, boredom, and frus‐
tration. The module electronically recorded the rating in two ways: (1) the observer
continuously rating aﬀect and diﬃculty level during assignments, and (2) the observer
providing an overall rating as a summary at the end of each assignment.
A total of 10 teenagers with ASD, with ages from 13 to 17 years, were involved in
the experiment. We recruited teenagers with ASD through an existing university clinical
research registry. The participants had a clinical diagnosis of ASD with scores at or
above clinical cutoﬀ on the Autism Diagnostic Observation Schedule [38]. Their cogni‐
tive functioning was measured using either the Diﬀerential Ability Scales [39] or the
Wechsler Intelligence Scale for Children [40].
Each participant completed six visits on diﬀerent days. The duration of each visit
was approximately one hour including device setup, baseline measurement, driving
practice, and the main task completion. As part of each visit, three researchers organized
the sensors and carried out eye tracker calibration. After a three-minute period used for
recording baseline physiological and EEG data, participants practiced driving for three
minutes in a free-form practice mode. Finally, participants completed three driving
assignments, which were unique except for the ﬁrst and the last visit.
2.2 Feature Extraction
Eye Gaze Features. Eye gaze data was tracked by the eye tracker with a frequency of
120 Hz. The eye tracker had an average accuracy of 1 cm for gaze position tracking
when the participants sat approximately 70 cm away from the monitor. In addition to
gaze position, the eye tracker also measured the pupil diameter and blink.
The eye gaze data was preprocessed by reducing the noise with the median value
method [41]. The blink rate and pupil diameter were calculated from the preprocessed
eye tracker data. For the blink rate, the closure duration used a range from 75 to 400 ms
[42]. The eye gaze features included mean and standard deviation of blink rate, pupil
diameter, and ﬁxation duration.

714

L. Zhang et al.

Physiological Features. The physiological signals were recorded with a 1000 Hz
sample rate. The physiological features were preprocessed as shown in Fig. 4. First, we
removed the outliers of physiological signals. Then, we removed signal noise with
diﬀerent high/low pass ﬁlters and notch ﬁlters [35, 36].

Remove outlier

Preprocess

Extract features

Fig. 4. Physiological signal analysis process

Sixty physiological features were calculated including sympathetic power, para‐
sympathetic power, very low-frequency power and ratio of powers of ECG, Mean Inter‐
beat Interval of ECG, mean and standard deviation of the amplitude and peak values of
PPG. The details of the physiological features can be found in [37].
EEG Features. The Emotiv EEG headset recorded signals from 14 channels from posi‐
tions AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, and AF4, deﬁned by the
10-20 system of electrode placement [43]. There were two additional reference elec‐
trodes at locations P3 and P4. The bandwidth of the EEG headset was from 0.2 Hz to
45 Hz and the recorded sampling rate was 128 Hz.
After removing outliers, EEG signals were ﬁrst ﬁltered between 0.2 Hz and 45 Hz.
We then removed eye blink, eye movement, and muscle movement artifacts by applying
EOG-EMG artifact correction algorithm provided by EEGLab [44]. After this prepro‐
cessing, spectral features - averaged power of each channel on alpha (8-13 Hz), beta
(13-30 Hz), and gamma (30-45 Hz) bands-were then extracted from the clean signals
[20]. A total of forty-two EEG features were extracted.
Performance Features. Performance features were extracted from the driving behavior
data and task performance data. Performance features included the number of failure
during one assignment, the score achieved during one assignment, the levels of accel‐
erating acceleration and braking, and the average speed.
2.3 Data Fusion Methods
In order to evaluate cognitive load, features from diﬀerent modalities were input into
classiﬁers The classiﬁers in Matlab (http://www.mathworks.com/) and Machine
Learning Toolbox (MLT) (http://mirlab.org/jang/matlab/toolbox/machineLearning/)
were used, including Support Vector Machine (SVM), Naïve Bayes (NB), Gaussian
Mixture Models (GMM), K-Nearest Neighbors with (KNN), Quadratic Classiﬁer
(QCL), Decision Tree (DT), and Linear Classiﬁer (LCL).
The therapist’s overall rating of diﬃculty level was used as the ground truth for
cognitive load classiﬁcation methods. The 0-9 Likert scaled diﬃculty level rating was
grouped and relabeled as low (diﬃculty level less than ﬁve) and high (diﬃculty level

Multimodal Fusion for Cognitive Load Measurement in an Adaptive Virtual Reality

715

larger than ﬁve) to reﬂect a binary-level cognitive load. In brief, we hypothesized based
on prior published research [12] that a participant in a high diﬃculty level task had high
cognitive load; while a participant in a low diﬃculty level task had low cognitive load.
Three level fusion approaches were implemented to fuse multimodal information:
feature fusion, decision fusion and hybrid fusion. Figure 5 (a) showed the feature level
fusion. All features were input into the preprocess module, which normalized features
and reduced their dimension with principal component analysis. The cognitive load was
evaluated with the preprocessed features.

Physiological features

EEG features
Preprocess

Classifier

Cognitive load

Performance features

Eye gaze features

(a)
Physiological features

Preprocess

Classifier 1

EEG features

Preprocess

Classifier 2
Fusion

Performance features

Preprocess

Classifier 3

Eye gaze features

Preprocess

Classifier 4

Cognitive load

(b)
EEG features
Feature fusion

Sub-Decision

Eye gaze features
Fusion
Physiological features

Preprocess

Classifier 1

Eye gaze features

Preprocess

Classifier 2

Cognitive load

(c)

Fig. 5. (a) Feature fusion framework; (b) decision fusion framework; and (c) hybrid fusion
framework.

716

L. Zhang et al.

Figure 5 (b) showed decision level fusion. We preprocessed the features from each
modality separately and then input them into diﬀerent classiﬁers. Each classiﬁer output
a cognitive load as a sub-decision. The fusion part summed all sub-decisions
) with weights for the ﬁnal cognitive load (Dﬁnal) as shown in Eq. (1).
(
(1)
Figure 5 (c) gave one example of hybrid level fusion. Hybrid level fusion combined
the feature level fusion and decision level fusion. Features from more than one modality
(EEG and eye gaze) were preprocessed to make one sub-decision; while other modalities
features (physiological and performance) were preprocessed separately to assess other
sub-decisions. The ﬁnal decision summed all sub-decisions with weights.

3

Results

Each assignment yielded one data sample. A total of 180 data samples were extracted
from ten participants. However, thirty-nine data samples were unusable due to factors
largely unrelated to the task, such as participants’ eye gaze moving out of eye tracker
detection range, some electrodes of the EEG sensor were displaced during their experi‐
ments, or one instance of the Biopac physiological sensors stopped working.
3.1 The Feature Level Fusion Results
The accuracies of diﬀerent classiﬁers with features from each modality as well as all the
modalities combined are shown in Fig. 6, with numerical values presented in Table 1.
The best results of three psycho-physiological modalities– eye gaze, EEG and physio‐
logical features - were from the KNN method. For the performance, the best result was
from the SVM method. KNN method also achieved the highest accuracy, 81.57 %, for
feature fusion. The feature fusion outperformed all individual modality classiﬁcation.

Fig. 6. The accuracies of diﬀerent classiﬁers

Multimodal Fusion for Cognitive Load Measurement in an Adaptive Virtual Reality

717

Table 1. Accuracies of each modality and all modalities with diﬀerent methods

Feature
fusion

Physiolog‐
ical

EEG

Eye

Performance

SVM

72.03 %

61.32 %

60.35 %

60.99 %

69.41 %

NB

69.90 %

70.86 %

63.97 %

56.62 %

67.71 %

GMM

69.51 %

72.77 %

67.44 %

57.60 %

68.38 %

KNN

81.57 %

79.29 %

80.58 %

71.92 %

60.49 %

QCL

68.92 %

76.73 %

71.07 %

64.28 %

63.48 %

DT

63.50 %

69.99 %

63.79 %

62.62 %

68.67 %

LCL

76.65 %

67.03 %

59.29 %

58.24 %

67.71 %

3.2 The Decision Level Fusion and Hybrid Level Fusion Result
For the decision level fusion, we tested all classifiers for every modality to get the subdecision. We then tested various combinations of weights for every sub-decision. The best
accuracy was achieved when using SVM for performance modality features and KNN for
all three psycho-physiological modalities features. The weight for a sub-decision of one
modality was proportional to the best accuracy of the modality. The results indicated that
for one modality, the best method for decision fusion was consistent with the best method
for its individual classification. The best decision fusion accuracy was 80.95 %, which was
similar to the best accuracy of the feature fusion.
Hybrid level fusion outperformed the feature level and decision level fusion with a
best accuracy of 84.66 %. The best accuracy was achieved when eye gaze and EEG
features were combined for one sub-decision with KNN method and weight w1, phys‐
iological features for one sub-decision with KNN method and weight w2, and SVM
method and weight w3 for performance features (0.5 > w1 > w2 > w3 and
w1 + w2 + w3 = 1).

4

Conclusions

This paper focused on multimodal fusion for cognitive load measurement during driving
intervention for individuals with ASD. The signals for the cognitive load measurement
were composed of physiological signals, EEG signal, eye gaze, and performance data.
Seven machine learning methods were explored to classify individual modality
features and multimodal fusion. The KNN method yielded the best results for all the
psycho-physiological related features, features from physiological signal, EEG data, and
eye gaze. The SVM method yielded the highest accuracy for performance features.
This paper compared three levels multimodal information fusion approaches,
feature level fusion, decision level fusion and hybrid level fusion, for cognitive load
measurement. The multimodal fusion approaches outperformed individual modalities

718

L. Zhang et al.

in cognitive load measurement. Hybrid fusion had the best result of 84.66 % compared
to other fusion methods.
The results will be used to choose an optimal game diﬃculty level for individuals
with ASD to provide a more challenging yet fruitful skill development opportunity in
the future.
Acknowledgment. This work was supported in part by the National Institute of Health Grant
1R01MH091102-01A1, National Science Foundation Grant 0967170 and the Hobbs Society
Grant from the Vanderbilt Kennedy Center.

References
1. Wingate, M., Kirby, R.S., Pettygrove, S., et al.: Prevalence of autism spectrum disorder
among children aged 8 years-autism and developmental disabilities monitoring network, 11
sites, United States, 2010. MMWR Surveillance Summaries, vol. 63, p. 2 (2014)
2. Rogers, S.J.: Empirically supported comprehensive treatments for young children with
autism. J. Clin. Child Psychol. 27(2), 168–179 (1998)
3. Cohen, H., Amerine-Dickens, M., Smith, T.: Early intensive behavioral treatment:
Replication of the UCLA model in a community setting. J. Dev. Behav. Pediatr. 27(2), S145–
S155 (2006)
4. Strickland, D.: Virtual reality for the treatment of autism. In: Studies in Health Technology
and Informatics, pp. 81–86 (1997)
5. Tartaro, A., Cassell, J.: Using virtual peer technology as an intervention for children with
autism. In: Towards Universal Usability: Designing Computer Interfaces for Diverse User
Populations, vol. 231, p. 62. John Wiley, Chichester (2007)
6. Lahiri, U., Bekele, E., Dohrmann, E., et al.: Design of a virtual reality based adaptive response
technology for children with autism. IEEE Trans. Neural Syst. Rehabil. Eng. 21(1), 55–64
(2013)
7. Cox, N.B., Reeve, R.E., Cox, S.M., et al.: Brief Report: Driving and young adults with ASD:
Parents’ experiences. J. Autism Dev. Disord. 42(10), 2257–2262 (2012)
8. Reimer, B., Fried, R., Mehler, B., et al.: Brief report: Examining driving behavior in young
adults with high functioning autism spectrum disorders: A pilot study using a driving
simulation paradigm. J. Autism Dev. Disord. 43(9), 2211–2217 (2013)
9. Classen, S., Monahan, M.: Evidence-based review on interventions and determinants of
driving performance in teens with attention deﬁcit hyperactivity disorder or autism spectrum
disorder. Traﬀ. Inj. Prev. 14(2), 188–193 (2013)
10. Galy, E., Cariou, M., Mélan, C.: What is the relationship between mental workload factors
and cognitive load types? Int. J. Psychophysiol. 83(3), 269–275 (2012)
11. Hussain, M.S., Calvo, R.A., Chen, F.: Automatic cognitive load detection from face,
physiology, task performance and fusion during aﬀective interference. Interacting with
computers, p. iwt032 (2013)
12. Koenig, A., Novak, D., Omlin, X., et al.: Real-time closed-loop control of cognitive load in
neurological patients during robot-assisted gait training. IEEE Trans. Neural Syst. Rehabil.
Eng. 19(4), 453–464 (2011)
13. Engström, J., Johansson, E., Östlund, J.: Eﬀects of visual and cognitive load in real and
simulated motorway driving. Transp. Res. Part F: Traﬃc Psychol. Behav. 8(2), 97–120 (2005)
14. Paas, F., Tuovinen, J.E., Tabbers, H., et al.: Cognitive load measurement as a means to
advance cognitive load theory. Educat. Psychol. 38(1), 63–71 (2003)

Multimodal Fusion for Cognitive Load Measurement in an Adaptive Virtual Reality

719

15. Taelman, J., Vandeput, S., Spaepen, A., et al.: Inﬂuence of mental stress on heart rate and
heart rate variability, pp. 1366–1369
16. Zhai, J., Barreto, A.: Stress Recognition Using Non-invasive Technology, pp. 395–401
17. Mehler, B., Reimer, B., Coughlin, J.F., et al.: Impact of incremental increases in cognitive
workload on physiological arousal and performance in young adult drivers. Transp. Res. Rec.:
J. Transp. Res. Board 2138(1), 6–12 (2009)
18. Palinko, O., Kun, A.L., Shyrokov, A., et al.: Estimating cognitive load using remote eye
tracking in a driving simulator, pp. 141–144
19. Pomplun, M., Sunkara, S.: Pupil dilation as an indicator of cognitive workload in humancomputer interaction
20. Zarjam, P., Epps, J., Lovell, N.H., et al.: Characterization of memory load in an arithmetic
task using non-linear analysis of EEG signals, pp. 3519–3522
21. Zarjam, P., Epps, J., Chen, F., et al.: Classiﬁcation of working memory load using wavelet
complexity features of EEG signals, pp. 692–699
22. Novak, D., Mihelj, M., Munih, M.: A survey of methods for data fusion and system adaptation
using autonomic nervous system responses in physiological computing. Inter. with Comput.
24(3), 154–172 (2012)
23. Wilson, G.F., Russell, C.A.: Performance enhancement in an uninhabited air vehicle task
using psychophysiologically determined adaptive aiding. Hum. Factors: J. Hum. factors
Ergon. Soc. 49(6), 1005–1018 (2007)
24. Sarkar, N.: Psychophysiological control architecture for human-robot coordination-concepts
and initial experiments, pp. 3719–3724
25. Rani, P., Sarkar, N., Smith, C.A., et al.: Aﬀective communication for implicit human-machine
interaction, pp. 4896–4903
26. Chen, F.: Robust Multimodal Cognitive Load Measurement, DTIC Document (2014)
27. Son, J., Park, M.: Estimating cognitive load complexity using performance and physiological
data in a driving simulator
28. Atrey, P.K., Hossain, M.A., El Saddik, A., et al.: Multimodal fusion for multimedia analysis:
a survey. Multimedia Syst. 16(6), 345–379 (2010)
29. Snoek, C.G., Worring, M., Smeulders, A.W.: Early versus late fusion in semantic video
analysis, pp. 399–402
30. Wu, Z., Cai, L., Meng, H.: Multi-level fusion of audio and visual features for speaker
identiﬁcation. In: Zhang, D., Jain, A.K. (eds.) ICB 2005. LNCS, vol. 3832, pp. 493–499.
Springer, Heidelberg (2005)
31. Koelstra, S., Muhl, C., Soleymani, M., et al.: Deap: A database for emotion analysis; using
physiological signals. IEEE Trans. Aﬀect. Comput. 3(1), 18–31 (2012)
32. Liu, E.S., Theodoropoulos, G.K.: Interest management for distributed virtual environments:
A survey. ACM Comput. Surv. (CSUR) 46(4), 51 (2014)
33. Wade, J., Bian, D., Zhang, L., Swanson, A., Sarkar, M., Warren, Z., Sarkar, N.: Design of a
virtual reality driving environment to assess performance of teenagers with ASD. In:
Stephanidis, C., Antona, M. (eds.) UAHCI 2014, Part II. LNCS, vol. 8514, pp. 466–474.
Springer, Heidelberg (2014)
34. Bian, D., Wade, J.W., Zhang, L., Bekele, E., Swanson, A., Crittendon, J.A., Sarkar, M.,
Warren, Z., Sarkar, N.: A novel virtual reality driving environment for autism intervention.
In: Stephanidis, C., Antona, M. (eds.) UAHCI 2013, Part II. LNCS, vol. 8010, pp. 474–483.
Springer, Heidelberg (2013)
35. Liu, C., Conn, K., Sarkar, N., et al.: Physiology-based aﬀect recognition for computer-assisted
intervention of children with Autism Spectrum Disorder. Int. J. Hum.-Comput. Stud. 66(9),
662–677 (2008)

720

L. Zhang et al.

36. Liu, C., Rani, P., Sarkar, N.: An empirical study of machine learning techniques for aﬀect
recognition in human-robot interaction, pp. 2662–2667
37. Bian, D., Wade, J., Swanson, A., et al.: Physiology-based aﬀect recognition during driving
in virtual environment for autism intervention. In: 2nd international conference on
physiological computing system (Accepted, 2015)
38. Lord, C., Risi, S., Lambrecht, L., et al.: The Autism Diagnostic Observation Schedule—
Generic: A standard measure of social and communication deﬁcits associated with the
spectrum of autism. J. Autism Dev. Disord. 30(3), 205–223 (2000)
39. Elliott, C.D.: Diﬀerential Ability Scales-ll. Pearson, San Antonio (2007)
40. Wechsler, D.: Wechsler intelligence scale for children (1949)
41. Komogortsev, O.V., Gobert, D.V., Jayarathna, S., et al.: Standardization of automated
analyses of oculomotor ﬁxation and saccadic behaviors. IEEE Trans. Biomed. Eng. 57(11),
2635–2645 (2010)
42. Benedetto, S., Pedrotti, M., Minin, L., et al.: Driver workload and eye blink duration. Trans.
Research Part F: Traﬃc Psychol. Behav. 14(3), 199–208 (2011)
43. Klem, G.H., Lüders, H.O., Jasper, H., et al.: The ten-twenty electrode system of the
International Federation,” Electroencephalogr. Clin. Neurophysiol. 52 (suppl.), 3 (1999)
44. Delorme, A., Makeig, S.: EEGLAB: an open source toolbox for analysis of single-trial EEG
dynamics including independent component analysis. J. Neurosci. Methods 134(1), 9–21
(2004)

