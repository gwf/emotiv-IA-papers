INFORMATICA, 2014, Vol. 25, No. 3, 425–437
 2014 Vilnius University
DOI: http://dx.doi.org/10.15388/Informatica.2014.22

425

Modeling Human Emotions as Reactions
to a Dynamical Virtual 3D Face
Vytautas KAMINSKAS, Egidijus VAŠKEVIČIUS,
Aušra VIDUGIRIENĖ ∗
Department of Systems Analysis, Vytautas Magnus University
Vileikos g. 8, 40444 Kaunas, Lithuania
e-mail: v.kaminskas@if.vdu.lt, e.vaskevicius@if.vdu.lt, a.vidugiriene@if.vdu.lt
Received: January 2014; accepted: September 2014
Abstract. This paper introduces a comparison of one linear and two nonlinear one-step-ahead
predictive models that were used to describe the relationship between human emotional signals
(excitement, frustration, and engagement/boredom) and virtual dynamic stimulus (virtual 3D face
with changing distance-between-eyes). An input–output model building method is proposed that
allows building a stable model with the smallest output prediction error. Validation was performed
using the recorded signals of four volunteers. Validation results of the models showed that all three
models predict emotional signals in relatively high prediction accuracy.
Key words: 3D face, human emotions, input–output model, parameter estimation, prediction, model
validation.

1. Introduction
Virtual environments are already a part of our daily life (work tasks, e-learning, online shopping, etc.). These environments aﬀect the users’ emotions in both positive and
negative ways. It is important to investigate and describe relationships between various virtual stimuli and human reactions to them. Human state observation is an important task for this purpose. Plenty of bio-signals are used for human state monitoring
(Zisook et al., 2013; Suprijanto et al., 2009; Raudonis et al., 2012). EEG-based signals are used because of their reliability and quick response (Sourina and Liu, 2011;
Hondrou and Caridakis, 2012). A number of systems and methods are used for emotion
recognition, but not so many for emotion control in virtual environment (Hoey et al., 2013;
Khushaba et al., 2012).
We have previously investigated linear input–output structure models for exploring human responses (excitement, meditation, frustration, and engagement/boredom) to virtual
3D face features (distance-between-eyes, nose width, and chin width) using EEG-based
bio-signals (Vaškevičius et al., 2014; Vidugirienė et al., 2013, 2014).
We continue to use virtual 3D face as stimulus for eliciting human emotions as the
majority of information when communicating is transferred by face features (Mehrabian,
1981) and a person is used to react to them in a very short time (Willis and Todorov, 2006).
* Corresponding author.

426

V. Kaminskas et al.

In this study we compare a modiﬁed linear and two types of nonlinear input–output
models to describe the dependencies between human emotions (excitement, frustration,
engagement/boredom) and virtual 3D face feature (distance-between-eyes).

2. Observations and Data
A virtual dynamic 3D face with changing distance-between-eyes was used for input as
stimulus (shown in a monitor) and EEG-based preprocessed excitement, frustration and
engagement/boredom signals of a volunteer was measured as output (Fig. 1). The output
signals were recorded using Emotiv Epoc device that records EEG inputs from 14 channels
(according to international 10–20 locations): AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8,
FC6, F4, F8, AF4 (Emotiv Epoc speciﬁcations).
A dynamic stimulus was formed from a changing woman face. One 3D face created
with Autodesc MAYA was used as a “neutral” one (Fig. 2, middle). Other 3D faces were
formed by changing distance-between-eyes in an extreme manner (Fig. 2, top, bottom).
The transitions between normal and extreme stages were programmed. Experiment plan
for input is shown in Fig. 2.

Fig. 1. Input–output scheme for the experiments.

Fig. 2. Input signal: experiment plan.

Modeling Human Emotions as Reactions to a Dynamical Virtual 3D Face

427

At ﬁrst “neutral” face was shown for 5 s, then the distance-between-eyes was increased
continuously and in 10 s the largest distance between eyes was reached, then 5 s of steady
face was shown and after that the face came back to “normal” in 10 s. Then “normal” face
was shown for 5 s, followed by 10 s of continuous change to the face with the smallest
distance between eyes, again 5 s of steady face was shown and in the next 10 s the face
came back to “normal”. The experiment was continued in the same way further using 3 s
time intervals for steady face and 5 s for continuous change. “Neutral” face has 0 value,
largest distance-between-eyes corresponds to value 3 and smallest distance-between-eyes
corresponds to value −3. Values of the output signals (excitement, frustration and engagement/boredom) vary from 0 to 1. If excitement, frustration or engagement is low, the value
is close to 0 and if it is high, the value is close to 1. The signals were recorded with the
sampling period of T0 = 0.5 s.
Four volunteers (three males – volunteers Nos. 1–3, and one female – volunteer No. 4)
were tested. Each volunteer was watching one animated scene of approximately 100 s, and
EEG-based signals were measured and recorded simultaneously.
3. Building of Mathematical Models
Dependencies between human emotion signals (excitement, frustration, and engagement/boredom) as reactions to virtual 3D face feature (distance-between-eyes) changes
are described by input–output structure dynamical model (Kaminskas, 1982):


(1)
A z−1 yt = θ0 + B z−1 f (xt ) + εt ,

where

m
 X
bj z−j ,
B z−1 =
j =0

n
X

ai z−1
A z−1 = 1 +

(2)

i=1

and
f (xt ) = xt ,

(3)

f (xt ) = |xt |,

(4)

f (xt ) = xt2 ,

(5)

where f (xt ) is a linear, absolute or square transform function of input, yt is an output
(excitement, frustration, or engagement/boredom), xt is an input (distance-between-eyes)
signal respectively expressed as
yt = y(tT0 ),

xt = x(tT0 )

(6)

with sampling period T0 , θ0 is a constant value (which indicate that the values of human
reaction signals are positive), εt corresponds to white-noise signal, and z−1 is the backward shift operator (z−1 xt = xt −1). A sign | | denotes absolute value.

428

V. Kaminskas et al.

This type of function f (xt ) was chosen to examine if a volunteer reacts to the changes
of a 3D face (increase or decrease of distance-between-eyes) directly (3) or the reaction
depends on the change of the amplitude and not on the direction of the change (4)–(5).
Parameters (coeﬃcients of the polynomials (2)), orders (degrees m and n of the polynomials (2)) and constant θ0 of the models (1)–(5) are unknown. They have to be estimated
according to the observations obtained during the experiments with the volunteers.
Equation (1) can be expressed in the following form:
yt = θ0 +

m
X

bj f (xt −j ) −

j =0

n
X

(7)

ai yt −i + εt .

i=1

It is not diﬃcult to see that (7) can be expressed as the linear regression equation:
yt = dTt c + εt

(8)

where


dTt = 1, f (xt ), f (xt −1), . . . , f (xt −m ), −yt −1, . . . , −yt −n ,

(9)

and
cT = [θ0 , b0 , b1 , . . . , bm , a1 , a2 , . . . , an ],

(10)

T is a vector transpose sign.
For the estimation of unknown parameter vector c we use a method of least squares
(Kaminskas, 1982):
ĉ = Q−1 q,

(11)

where Q and q are expressed as follows
Q=

M
X

dt dTt ,

q=

M
X

yt dt

(12)

t =1

t =1

and M is a number of observation values that are used to build a model. After calculating
the estimates of model parameters, stability condition of a model is veriﬁed (Kaminskas,
1982). It means that the roots
ziA :

ÂM (z) = 0,

i = 1, 2, . . . , n

(13)

of the following polynomial
n
X

âiM zn−i
ÂM (z) = zn ÂM z−1 = zn +
i=1

(14)

Modeling Human Emotions as Reactions to a Dynamical Virtual 3D Face

429

have to be in the stability domain, i.e. in the unit disk
ziA < 1.

(15)

If the condition (15) is not satisﬁed, coeﬃcients of polynomial ÂM (z−1 ) are projected
âi = γ âiM ,

0 < γ 6 1, i = 1, 2, . . . , n,

(16)

to the stability domain. It is easy to calculate γ constant when n 6 2, because the stability
domain is deﬁned by:
−1 < â1 < 1,

and

if n = 1,

(17)


 1 + â1 + â2 > 0,
1 − â1 + â2 > 0, if n = 2

−1 < â2 < 1,

(1)
γ = min 1, γ1 ,

(18)

if n = 1,

(19)

1
− γ0 ,
|â1M |

γ = min 1, γ1(2), γ2(2) , γ3(2) , if n = 2,

− M 1 M − γ0 , if â1M + â2M < 0,
(2)
â1 +â2
γ1 =
1,
in other cases,

1
− M M − γ0 , if â2M − â1M < 0,
â2 −â1
γ2(2) =
1,
in other cases,
(1)

γ1 =

γ3(2) =

1
− γ0
|â2M |

(20)
(21)
(22)
(23)
(24)

where positive constant γ0 ∈ [0.001, 0.01]. When n > 3, nonlinear inequalities appear in
the system of inequalities that deﬁne the stability domain and the calculation of γ becomes
more complex.
Estimates of the model orders – m̂ and n̂ – are deﬁned from the following conditions
(Kaminskas, 1982):
n̂:

σε [m, n + 1] − σε [m, n]
6 δ,
σε [m, n]

n = 1, 2, . . .

(25)

m̂:

σε [m + 1, n] − σε [m, n]
6 δ,
σε [m, n]

m = 1, 2, . . . , n

(26)

430

V. Kaminskas et al.

where
v
u
N
u1 X
σε [m, n] = t
ε̂t2 [m, n]
N

(27)

t =1

is one-step-ahead output prediction error standard deviation,
ε̂t [m, n] = yt − ŷt |t −1[m, n]

(28)

is one-step-ahead output prediction error,



ŷt |t −1 = θ̂0 + z 1 − Â z−1 yt −1 + B̂ z−1 f (xt )

(29)

is one-step-ahead output prediction (Kaminskas, 1982, 2007), z is the forward shift operator (zyt = yt +1) and δ > 0 is a chosen constant value. Usually in identiﬁcation practice
δ ∈ [0.01 ÷ 0.1] what corresponds to a relative variation of prediction error standard deviation from 1% to 10%.
This way stable input–output models are built that ensure the best one-step-ahead output signal prediction.
4. Validation of Models
Validation of the models (1)–(5) was performed for each of 4 volunteers (three males
and one female). Each model is selected from nine possible models (when n = 1, 2, 3,
m = 0, 1, 2, 3) using the rules (25)–(26). Figure 3 demonstrate prediction error standard
deviations for an input–output pairs for one male (volunteer No. 2) and one female (volunteer No. 4) using the linear model (1)–(3). The analysis of data showed that relations
between distance-between-eyes input and excitement output signal can be modelled when
model order is m̂ = 0, n̂ = 1, frustration output signal can be modelled when model order
is m̂ = 0, n̂ = 2, and engagement/boredom output signal can be modelled when model
order is m̂ = 0, n̂ = 3. According to data analysis results, one-step-ahead prediction of
output signals for every model can be performed using the following expressions
ŷt +1|t = θ̂0 − â1 yt + b̂0 f (xt +1)

(30)

in the case of excitement output signal,
ŷt +1|t = θ̂0 − â1 yt − â2 yt −1 + b̂0 f (xt +1) + b̂1 f (xt )

(31)

in the case of frustration output signal, and
ŷt +1|t = θ̂0 − â1 yt − â2 yt −1 − â3 yt −2 + b̂0 f (xt +1)
in engagement/boredom output signal case.

(32)

Modeling Human Emotions as Reactions to a Dynamical Virtual 3D Face

431

Fig. 3. Prediction error standard deviations for emotional signals with diﬀerent model orders using linear model
(1)–(3) for volunteer No. 2 (left), and volunteer No. 4 (right).

Prediction accuracies were evaluated using the following measures:
– prediction error standard deviation
v
u N−1
u1 X
(yt +1 − ŷt +1|t )2 ,
σε = t
N

(33)

t =0

– relative prediction error standard deviation
v
u N−1
u1 X
(yt +1 − ŷt +1|t )2 ,
σ̃ε = t
N

(34)

t =0

– and average absolute relative prediction error
|ε̄| =

N−1
1 X yt +1 − ŷt +1|t
∗ 100%.
N
yt +1
t =0

(35)

432

V. Kaminskas et al.

One-step-ahead predictions (30)–(32) were performed using the observation data that
were used to build a model (M = 124, in (12)) and the additional ones that were not used to
build a model (in total N = 200, in (33)–(35)). Prediction accuracy measures are provided
in Table 1.
Figures 4–7 show one-step-ahead prediction results when using all models (1)–(5) for
all four volunteers. Thick solid line denotes an observed signal, thick dotted line denotes
Table 1
Prediction accuracy measures.
Vol.
No.

Output

1

Excitement
Frustration
Engagement

2

f (xt ) = xt
σ̃ε
(%)

|ε̄|
(%)

σε

0.045
0.015
0.006

7.2
2.7
0.8

5.7
1.8
0.6

Excitement
Frustration
Boredom

0.025
0.015
0.004

9.9
3.5
0.7

3

Excitement
Frustration
Boredom

0.036
0.012
0.004

4

Excitement
Frustration
Boredom

0.017
0.016
0.006

σε

f (xt ) = xt2

f (xt ) = |xt |
σ̃ε
(%)

|ε̄|
(%)

σε

σ̃ε
(%)

|ε̄|
(%)

0.045
0.015
0.006

7.2
2.7
0.8

5.8
1.9
0.6

0.045
0.015
0.006

7.2
2.8
0.8

5.8
1.9
0.6

7.2
2.6
0.5

0.025
0.015
0.004

10.3
3.6
0.7

7.5
2.6
0.5

0.025
0.015
0.004

10.3
3.6
0.7

7.5
2.7
0.5

11.8
2.2
0.5

9.0
1.6
0.4

0.036
0.012
0.004

11.7
2.2
0.5

9.1
1.6
0.4

0.036
0.012
0.004

11.8
2.2
0.5

9.1
1.6
0.4

9.0
3.2
0.8

5.7
2.4
0.6

0.018
0.016
0.006

9.0
3.1
0.8

5.7
2.3
0.6

0.018
0.016
0.006

9.0
3.1
0.8

5.7
2.3
0.6

Fig. 4. One-step-ahead prediction results when using models (1)–(5) for volunteer No. 1.

Modeling Human Emotions as Reactions to a Dynamical Virtual 3D Face

Fig. 5. One-step-ahead prediction results when using models (1)–(5) for volunteer No. 2.

Fig. 6. One-step-ahead prediction results when using models (1)–(5) for volunteer No. 3.

433

434

V. Kaminskas et al.

Fig. 7. One-step-ahead prediction results when using models (1)–(5) for volunteer No. 4.

Table 2
Estimated parameters for excitement signal prediction using ﬁrst order model (30).
f (xt )

xt

|xt |

xt2

Volunteer
No. 2
(male)

b̂0
â1
θ̂0

0.0012
−0.9139
0.0189

−0.0042
−0.8898
0.0303

−0.0016
0.8868
0.0302

Volunteer
No. 4
(female)

b̂0
â1
θ̂0

0.0007
−0.9097
0.0085

−0.0006
−0.9099
0.0093

−0.0002
−0.9099
0.0092

Table 3
Estimated parameters for frustration signal prediction using second order model (31).
f (xt )

xt

|xt |

xt2

Volunteer
No. 3
(male)

b̂0
b̂1
â1
â2
θ̂0

−0.0049
0.0044
−1.7377
0.8113
0.0367

0.0121
−0.0142
−1.7277
0.8131
0.0454

0.0001
−0.0007
−1.7375
0.8162
0.0408

Volunteer
No. 4
(female)

b̂0
b̂1
â1
â2
θ̂0

−0.0088
0.0092
−1.5874
0.6131
0.0118

−0.0140
0.0135
−1.5825
0.6084
0.0126

−0.0035
0.0034
−1.5805
0.6065
0.0122

435

Modeling Human Emotions as Reactions to a Dynamical Virtual 3D Face
Table 4
Estimated parameters for engagement/boredom signal prediction using third order model (32)
f (xt )

xt

|xt |

xt2

Volunteer
No. 1
(male)

b̂0
â1
â2
â3
θ̂0

0.0001
−2.3133
1.9004
−0.5582
0.0232

0.0022
−2.2207
1.7634
−0.4881
0.0409

0.0007
−2.2331
1.8847
−0.5018
0.0379

Volunteer
No. 4
(female)

b̂0
â1
â2
â3
θ̂0

0.0007
−1.9525
1.1013
−0.1208
0.0197

0.0008
−1.9745
1.1392
−0.1364
0.0187

0.0003
−1.9730
1.1320
−0.1312
0.0187

predicted signal and thin solid line denotes prediction error at every time moment. Vertical
line denotes M position as model parameters were estimated in the interval from 0 to M
(equal to 124). As sampling period T0 = 0.5 s, M is 62 s.
Examples of parameter estimates of the models (30)–(32) are given in Tables 2–4.
The prediction results show that excitement can be predicted with less than 9%, frustration – with less than 3% and engagement/boredom – with less than 1% absolute relative
prediction error. According to the prediction accuracy, all three models (1)–(5) are similar.
These models are more accurate than previously investigated linear models (Vaškevičius
et al., 2014; Vidugirienė et al., 2013, 2014).
5. Conclusions
Three alternative models – one linear and two nonlinear – were proposed to describe
the dependencies between human emotional signals (excitement, frustration, and engagement/boredom) and 3D face feature (distance-between-eyes). The linear model describes
the reaction of a volunteer to the direct changes of a 3D face (increase or decrease of
distance-between-eyes). The second and the third models describe the reaction to the amplitude of a change of a 3D face and not to the direction of the change.
A method for building input–output models was proposed that allows building
stable models for one-step-ahead predictions of excitement, frustration, and engagement/boredom signals with the smallest prediction errors.
Validation of the models showed that each volunteer has an individual reaction to the
given stimuli, and the reactions can be described using ﬁrst order (n = 1, m = 0) model
for excitement signals, second order (n = 2, m = 1) model for frustration signals and third
order (n = 3, m = 0) model for engagement/boredom signals. These three models with
diﬀerent input transform function (linear, absolute, and quadratic) are similar in respect
to prediction accuracies.
Engagement/boredom signal was predicted with the least absolute relative prediction
error (less than 1%). Frustration signal was predicted with less than 3% absolute relative
prediction error. Excitement signal was predicted with the largest prediction error, but less
than 9%.

436

V. Kaminskas et al.

Acknowledgment. Postdoctoral fellowship of Ausra Vidugirienė is funded by European
Union Structural Funds project “Postdoctoral Fellowship Implementation in Lithuania”
within the framework of the Measure for Enhancing Mobility of Scholars and Other Researchers and the Promotion of Student Research (VP1-3.1-ŠMM-01) of the Program of
Human Resources Development Action Plan.

References
Emotiv Epoc speciﬁcations. Brain–computer interface technology. Available at:
http://www.emotiv.com/ upload/manual/sdk/EPOCSpecifications.pdf.
Hoey, J., Schroeder, T., Alhothali, A. (2013). Bayesian aﬀect control theory. In: IEEE 2013 Humaine Association
Conference on Affective Computing and Intelligent Interaction, Geneva, Switzerland, 2013, pp. 166–172.
Hondrou, C., Caridakis, G. (2012). Aﬀective, natural interaction using EEG: sensors, application and future
directions. In: Artificial Intelligence: Theories and Applications, Vol. 7297. Springer, Berlin, pp. 331–338.
Kaminskas, V. (1982). Dynamic Systems Identification via Discrete-Time Observations. Part 1 – 1982, Part 2 –
1985. Mokslas, Vilnius (in Russian).
Kaminskas, V. (2007) Predictor-based self tuning control with constraints. In: Model and Algorithms for Global
Optimization, Optimization and Its Applications, Vol. 4. Springer, Berlin, pp. 333–341.
Khushaba, R.N., Greenacre, L., Kodagoda, S., Louviere, J., Burke, S., Dissanayake, G. (2012). Choice modeling
and the brain: a study on the electroencephalogram (EEG) of preferences. Expert Systems with Applications,
39(16), 12378–12388.
Mehrabian, A. (1981). Silent Messages: Implicit Communication of Emotions and Attitudes. Wadsworth Publishing Company, Belmont, 1981.
Sourina, O., Liu, Y. (2011). A fractal-based algorithm of emotion recognition from EEG using arousal-valence
model. In: Proceedings of Biosignals, pp. 209–214.
Raudonis, V., Paulauskaitė-Tarasevičienė, A., Kižauskienė, L. (2012). The gaze tracking system with natural
head motion compensation. Informatica, 23(1), 105–124.
Suprijanto, L. Sari, V. Nadhira, I.G.N. Merthayasa, I.M. Farida (2009). Development system for emotion detection based on brain signals and facial images. World Academy of Science, Engineering and Technology, 26,
931–938.
Vaškevičius, E., Vidugirienė, A., Kaminskas, V. (2014). Identiﬁcation of human response to virtual 3D face
stimuli. Information Technologies and Control, 43(1), 47–56.
Vidugirienė, A., Vaškevičius, E., Kaminskas V. (2013). Modeling of aﬀective state response to a virtual 3D
face. In: UKSim-AMSS 7th European Modelling Symposium on Mathematical Modelling and Computer
Simulation (EMS 2013), Manchester, UK, pp. 167–172.
Vidugirienė, A., Vaškevičius, E., Kaminskas, V. (2014). Two predictive models for identiﬁcation of human responses to virtual 3D face stimuli. In: Proceedings of the 9th International Conference on Electrical and
Control Technologies, Kaunas, Lithuania, pp. 78–83.
Willis, J., Todorov, A. (2006). First impressions: making up your mind after a 100-ms exposure to a face. Psychological Science, 17(7), 592–598.
Zisook, M., Hernandez, J., Goodwin, M.S., Picard, R.W. (2013). Enabling visual exploration of long-term physiological data. In: Proceedings of the 2013 IEEE Conference on Visual Analytics Science and Technology,
Altanta, Georgia, USA, October, 2013.

Modeling Human Emotions as Reactions to a Dynamical Virtual 3D Face

437

V. Kaminskas is a head of System Analysis Department of Vytautas Magnus University.
He graduated from the Department of Automatics of the Kaunas Polytechnical Institute
in 1968. He has PhD (1972) and DrSc (1983) degrees in the ﬁeld of technical cybernetics
and information theory. In 1984 he was awarded the title of the Professor. From 1991 he is
a member of Lithuanian Academy of Sciences. His research interests are dynamic system
modeling, identiﬁcation and adaptive control. He is the author of 4 monographs and about
200 scientiﬁc papers on these topics.
E. Vaškevičius is a head of Multimedia Laboratory in Vytautas Magnus University since
1995 and a PhD student. His main ﬁelds of interest and research are computer graphics,
virtual arts and exhibitions, as well as impact of virtual reality on humans.
A. Vidugirienė is an associate professor and a post-doc researcher at the Department of
System Analysis, Faculty of Informatics, Vytautas Magnus University. She received a PhD
degree (Informatics) in 2010 from Vytautas Magnus University, Kaunas, Lithuania. Her
research interests concern emotion regulation modeling using bio-feedback, multimedia
systems control using feedback, signal processing and prediction.

Žmogaus emocijų, kaip reakcijų į virtualų kintantį trimatį veidą,
modeliavimas
Vytautas KAMINSKAS, Egidijus VAŠKEVIČIUS, Aušra VIDUGIRIENĖ
Šiame straipsnyje pateikiamas vieno tiesinio ir dviejų netiesinių modelių, kurie aprašo sąryšius tarp
žmogaus emocinių signalų (susijaudinimo, susierzinimo ir susidomėjimo/nuobodulio) ir virtualaus
dinaminio stimulo (virtualaus trimačio veido su besikeičiančiu atstumu tarp akių), palyginimas. Pasiūlytas „įėjimas–išėjimas“ tipo modelių, kurie yra stabilūs ir prognozuoja išėjimo signalą su mažiausia paklaida, sudarymo metodas. Modelių validavimas buvo atliktas naudojant keturių tiriamųjų
signalus. Validavimo rezultatai parodė, kad visi trys modeliai prognozuoja emocinius signalus santykinai aukštu tikslumu.

