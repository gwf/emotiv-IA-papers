brain
sciences
Article

Multimodal Affective State Assessment Using fNIRS
+ EEG and Spontaneous Facial Expression
Yanjia Sun 1, * , Hasan Ayaz 2,3,4,5
1
2
3
4
5

*

and Ali N. Akansu 1

Department of Electrical and Computer Engineering, New Jersey Institute of Technology,
Newark, NJ 07102, USA; akansu@njit.edu
School of Biomedical Engineering, Science and Health Systems, Drexel University,
Philadelphia, PA 19104, USA; hasan.ayaz@drexel.edu
Department of Psychology, College of Arts and Sciences, Drexel University, Philadelphia, PA 19104, USA
Department of Family and Community Health, University of Pennsylvania, Philadelphia, PA 19104, USA
Center for Injury Research and Prevention, Children’s Hospital of Philadelphia, Philadelphia, PA 19104, USA
Correspondence: yanjia.sun@njit.edu

Received: 5 October 2019; Accepted: 1 February 2020; Published: 6 February 2020




Abstract: Human facial expressions are regarded as a vital indicator of one’s emotion and intention,
and even reveal the state of health and wellbeing. Emotional states have been associated with
information processing within and between subcortical and cortical areas of the brain, including the
amygdala and prefrontal cortex. In this study, we evaluated the relationship between spontaneous
human facial affective expressions and multi-modal brain activity measured via non-invasive and
wearable sensors: functional near-infrared spectroscopy (fNIRS) and electroencephalography (EEG)
signals. The affective states of twelve male participants detected via fNIRS, EEG, and spontaneous
facial expressions were investigated in response to both image-content stimuli and video-content
stimuli. We propose a method to jointly evaluate fNIRS and EEG signals for affective state detection
(emotional valence as positive or negative). Experimental results reveal a strong correlation between
spontaneous facial affective expressions and the perceived emotional valence. Moreover, the affective
states were estimated by the fNIRS, EEG, and fNIRS + EEG brain activity measurements. We show
that the proposed EEG + fNIRS hybrid method outperforms fNIRS-only and EEG-only approaches.
Our findings indicate that the dynamic (video-content based) stimuli triggers a larger affective
response than the static (image-content based) stimuli. These findings also suggest joint utilization of
facial expression and wearable neuroimaging, fNIRS, and EEG, for improved emotional analysis and
affective brain–computer interface applications.
Keywords: functional near-infrared spectroscopy (fNIRS); electroencephalography (EEG); facial
emotion recognition; brain–computer interface (BCI)

1. Introduction
The face has long been considered as a window with a view to our emotions [1]. Facial
expressions are regarded as one of the most natural and efficient cues enabling people to interact
and communicate with others in a nonverbal manner [2]. With the systematic analysis of facial
expression [3], the link between facial expression and emotion has been demonstrated empirically
in psychology literature [1,4]. Decades of behavioral research revealed that facial expression carries
information for a wide-range of phenomena, from psychopathology to consumer preferences [5–7].
The recent advances in electronics and computational technologies allow recording facial expressions
at increasingly high resolutions and advanced the analysis performance. A better understanding of
facial expressions can contribute to human-computer interactions and emerging practical applications

Brain Sci. 2020, 10, 85; doi:10.3390/brainsci10020085

www.mdpi.com/journal/brainsci

Brain Sci. 2020, 10, 85

2 of 20

that employ facial expression recognition, such as in education, entertainment, interactive games,
clinical diagnostics, and many others.
When and how to capture spontaneous facial expressions, as well as the methods to interpret
associated mental states and the underlying neurological mechanisms are growing research areas [8–10].
In this study, we extended our previous work [11] to investigate the relationship between spontaneous
human facial emotion analysis and brain signals generated due to reactions to both static (image)
and dynamic (video) stimuli. We jointly analyze the affective states by using multimodal brain
activity measurements. The facial emotion recognition method utilizes image processing and pattern
recognition and classification to decode the universal emotion types [12]. Namely, these primitive
emotions are anger, disgust, fear, happiness, sadness, and surprise [13]. Facial expressions can be
coded by the facial action coding system (FACS) which describes an expression through the action units
(AU) of individual muscles [14]. Although facial expression descriptions may be precise, automatic
recognition of the emotions behind specific facial expressions from images remains a challenge without
the availability of context information [10]. Some existing image classification methods have achieved
high recognition rates for facial emotions based on benchmarked databases containing a variety of
posed facial emotions [15,16]. However, these datasets are built from images of subjects performing
exaggerated expressions that are quite different than spontaneous and natural presentations [17].
The neural mechanisms of emotion processing have been a fundamental research area in cognitive
neuroscience and psychiatry in part due to clinical applications relating to mood disorders [18,19].
Researchers have shown that neurophysiological changes are induced by non-consciously perceived
emotional stimuli [20]. In particular, prefrontal cortex (PFC) has been identified as an important
region that facilitates emotion regulation and, as a result, functional neuroimaging of PFC has been
used to investigate neural correlates of emotion processing [21–25]. Findings from these studies
have suggested that monitoring PFC activity using non-invasive neuroimaging approaches, including
functional near-infrared spectroscopy (fNIRS) [26] and electroencephalography (EEG) [27], presents an
opportunity for automatic emotion recognition. These tools enable measuring the brain activity in
natural everyday settings with minimal restrictions on participants during measurement. Hence, they
are ideal tools for the Neuroergonomics approach [28–30] that is focusing on studying the brain with
real/realistic settings as opposed to artificial lab settings. Findings from these tools can be used for
mapping the brain function as well as decoding mental states.
fNIRS is a non-invasive and portable neuroimaging method that can quantify the changes of
cerebral oxygenated and deoxygenated hemoglobin concentrations using near-infrared light attenuation.
fNIRS measures cortical hemodynamic response similarly to functional magnetic resonance imaging
(fMRI), but without limitations and restrictions on the subject such as staying in a supine position
within a confined space or exposure to loud noises [31]. As a portable and cost-effective functional
neuroimaging modality, fNIRS is uniquely suitable to study cognition and emotion processing-related
brain activities due to relatively high spatial resolution and a practical sensory setup [22,31–34].
EEG is a non-invasive, portable, and widely adopted neuroimaging technique used to detect brain
electrophysiological patterns. It measures electrical potentials through electrodes placed on the scalp.
Due to its high temporal resolution, EEG is an ideal candidate for monitoring event-related brain
dynamics. Furthermore, EEG has been widely used to investigate the brain signals implicated in
emotion processing [35,36]. It has been reported that asymmetric brain activity in frontal region is a
key biomarker observed for emotional stimuli using EEG, fNIRS and fMRI [37–40]. Davidson et al.
proposed that activity differences between the left and right PFC hemisphere as acquired by EEG were
associated with the processing of positive and negative affects [41]. According to this view of frontal
asymmetry, the left prefrontal cortex is thought to be associated with positive affect, and the right
prefrontal cortex activity is related to negative affect [42].
The measurement of neural correlates of cognitive and affective processes using concurrent
EEG and fNIRS, multimodal functional neuroimaging, has seen growing interest [43–46]. As fNIRS
and EEG measure complementary aspects of brain activity (hemodynamic and electrophysiological,

Brain Sci. 2020, 10, 85
Brain Sci. 2020, 10, x FOR PEER REVIEW

3 of 20
3 of 20

respectively), a hybrid brain data incorporates more information and enabling higher mental decoding
respectively),
a hybridearlier
brain findings
data incorporates
more information
and enabling
higher
mental
accuracy
[43] confirming
[47]. Specifically,
in [43] we showed
that body
physiological
decoding accuracy [43] confirming earlier findings [47]. Specifically, in [43] we showed that body
measures (heart rate and breathing) did not contribute any new information to fNIRS + EEG based
physiological measures (heart rate and breathing) did not contribute any new information to fNIRS
classification of cognitive workload. Another recent study reported in [48] utilized fNIRS and EEG as
+ EEG based classification of cognitive workload. Another recent study reported in [48] utilized
well as with autonomic nervous system measures, including skin conductance responses and heart rate,
fNIRS and EEG as well as with autonomic nervous system measures, including skin conductance
for emotion analysis. Authors reported strong effects observed in fNIRS and EEG when comparing
responses and heart rate, for emotion analysis. Authors reported strong effects observed in fNIRS
positive
and negative valence. And, they confirmed prefrontal lateralization for valence. Finally, heart
and EEG when comparing positive and negative valence. And, they confirmed prefrontal
ratelateralization
didn’t showfor
any
effect, Finally,
but skinheart
conductance
difference although
valence.
rate didn’tresponse
show anydemonstrated
effect, but skina conductance
responseno
comparison
was
done
if
this
adds
to
EEG
or
fNIRS.
In
a
more
recent
study,
authors
used
prefrontal
demonstrated a difference although no comparison was done if this adds to EEG or fNIRS. In
a more
cortex
based
fNIRS
signals
recording
during
emotional
video
clips
to
recognize
different
recent study, authors used prefrontal cortex based fNIRS signals recording during emotionalpositive
video
emotions
In thisdifferent
study, we
investigated
spontaneous
affective
expressions
and brain activity
clips to[49].
recognize
positive
emotions
[49]. In thisfacial
study,
we investigated
spontaneous
facial
simultaneously
recorded and
usingbrain
both fNIRS
EEG modalities
for affective
estimation.
TheEEG
block
affective expressions
activityand
simultaneously
recorded
usingstate
both
fNIRS and
diagram
of
the
system
is
displayed
in
Figure
1.
modalities for affective state estimation. The block diagram of the system is displayed in Figure 1.

Figure
1. Framework
assessingthe
thespontaneous
spontaneous affective status
brain
Figure
1. Framework
ofofassessing
status in
inbrain
brainthrough
throughcomparing
comparing
brain
activity
and
spontaneous
facial
emotions.
fNIRS:
functional
near
infrared
spectroscopy;
SVM:
activity and spontaneous facial emotions. fNIRS: functional near infrared spectroscopy; SVM: Support
Support
VectorEEG:
Machine;
EEG: electroencephalography.
Vector
Machine;
electroencephalography.

This
paper
highlightsthe
thebenefits
benefitsofofmultimodal
multimodal wearable
wearable neuroimaging
This
paper
highlights
neuroimagingusing
usingultra-portable
ultra-portable
battery-operated
andwireless
wirelesssensors
sensors that
untethered
measurement
of participants,
ad
battery-operated
and
thatallows
allowsfor
forthe
the
untethered
measurement
of participants,
potentially
can
be
used
in
everyday
settings.
The
major
contributions
of
the
paper
are
summarized
ad potentially can be used in everyday settings. The major contributions of the paper are summarized
as follows:
as follows:

a.

a.

To the best of our knowledge, this is the first attempt to explore the relationship between

To the best of our knowledge, this is the first attempt to explore the relationship between
spontaneous human facial affective states and relevant brain activity by simultaneously using
spontaneous human facial affective states and relevant brain activity by simultaneously using
fNIRS, EEG, and facial expressions registered in captured video.
fNIRS, EEG, and facial expressions registered in captured video.

Brain Sci. 2020, 10, 85
Brain Sci. 2020, 10, x FOR PEER REVIEW

b.

c.

4 of 20
4 of 20

The
facial
affective
expressions
recorded
by aby
video
camera
are demonstrated
to be
Thespontaneous
spontaneous
facial
affective
expressions
recorded
a video
camera
are demonstrated
into line
the affective
states
coded
by bybrain
be inwith
line with
the affective
states
coded
brainactivities.
activities. This
Thisisis consistent
consistent with
Neuroergonomics
Neuroergonomics[30]
[30]and
andmobile
mobilebrain/body
brain/bodyimaging
imagingapproaches
approaches[50].
[50].
The
Theexperimental
experimentalresults
resultsshow
showthat
thatthe
theproposed
proposed multimodal
multimodal technique outperforms methods
using
usinga asubset
subsetofofthese
thesesignal
signaltypes
typesfor
forthe
thesame
sametask.
task.

The remainder
remainder of
of the
the paper
paper is
is organized
organized as
as follows.
follows. Section
and methods
The
Section 22 details
details the
the approach
approach and
methods
as well
as
well as
as the
the experimental
experimental design
design used
used in
in the
the study.
study. Section
Section 33 reviews
reviews analytical
analytical details
details and
and presents
presents
the
results.
Then,
the
discussion
and
concluding
remarks
are
given
in
the
last
section
of
the
paper.
the results. Then, the discussion and concluding remarks are given in the last section of the paper.
2. Materials and Methods

2.1. Participants
Twelve male
male participants
participants (age: µ =
= 27.58, σ =
= 4.81) volunteered for the study.
study. Each participant
gave written informed consent prior to participation in this study.
study. We have opted to recruit only male
participants
in in
order
to eliminate
the confounding
factorfactor
of menstrual
cycle phases’
impact
participants in
inthis
thisstudy
study
order
to eliminate
the confounding
of menstrual
cycle phases’
on
emotion
processing
in
female
volunteers
[51–54].
Participants
all
self-identified
as
right-handed
impact on emotion processing in female volunteers [51–54]. Participants all self-identified as rightand
self-reported
to have no
history
mentalofillnesses
or drug abuse,
wereand
compensated
for their
handed
and self-reported
to have
noof
history
mental illnesses
or drugand
abuse,
were compensated
time.
Thetime.
study
was
conducted
in accordance
with thewith
Declaration
of Helsinki
and approved
by the
for their
The
study
was conducted
in accordance
the Declaration
of Helsinki
and approved
Institutional
ReviewReview
Board of
the New
of Technology.
by the Institutional
Board
of theJersey
New Institute
Jersey Institute
of Technology.
2.2.
Experimental Protocol
Protocol
2.2. Experimental
Each
was assigned
Each participant
participant was
assigned to
to complete
complete two
two tasks
tasks according
according to
to the
the experimental
experimental protocol
protocol
shown
In the
the first
first task,
was asked
to watch
shown in
in Figure
Figure 2.
2. In
task, each
each participant
participant was
asked to
watch twenty
twenty videos
videos with
with various
various
emotional
content.
Each
video
lasted
10–17
s
such
that
the
participant
can
recognize
the
of
emotional content. Each video lasted 10–17 s such that the participant can recognize the type oftype
affect.
affect.
After
watching
a
video,
the
participant
answered
two
simple
questions
(e.g.,
Were
you
able
After watching a video, the participant answered two simple questions (e.g., Were you able to watch
to
video carefully?
were youinseeing?)
orderhetounderstood
verify he understood
the video
thewatch
videothe
carefully?
What wereWhat
you seeing?)
order to in
verify
the video content.
In
content.
In
addition,
each
participant
was
asked
to
evaluate
the
type
of
affect
(positive
or
addition, each participant was asked to evaluate the type of affect (positive or negative)negative)
and the
and
theof
degree
of theusing
affectausing
a ten-point
scale (ranging
1 = extremely
negative
degree
the affect
ten-point
Likert Likert
scale (ranging
from 1from
= extremely
negative
to 10to=
10
=
extremely
positive)
in
response
to
a
set
of
affective
states.
It
is
worth
noting
that
the
participant
extremely positive) in response to a set of affective states. It is worth noting that the participant did
did
not know
the video
contents
in advance.
The advantage
of experimental
this experimental
procedure
that
not know
the video
contents
in advance.
The advantage
of this
procedure
is thatis each
each
participant
naturally
ratings
in the
absence
priorknowledge.
knowledge.InInthe
thesecond
second task,
task,
participant
naturally
givesgives
the the
finalfinal
ratings
in the
absence
ofof
prior
each
participant
was
asked
to
observe
twenty
emotional
images
from
Nencki
Affective
Picture
System
each participant was asked to observe twenty emotional images from Nencki Affective Picture
(NAPS)
Each[55].
image
wasimage
displayed
five seconds.
Analogously,
the participantthe
answered
two
System [55].
(NAPS)
Each
was for
displayed
for five
seconds. Analogously,
participant
simple
questions
aboutquestions
the image
content
observing
the image.
answered
two simple
about
the after
image
content after
observing the image.

Figure 2. Experimental protocol.

Each participant was instructed in the experimental procedure in detail before performing the
experiment. Participants were asked to sit on a comfortable chair facing a computer screen in a quiet
room. Both fNIRS and
and EEG
EEG sensors
sensors were
were placed
placedon
onthe
theparticipant’s
participant’sforehead
foreheadand
andscalp,
scalp,respectively.
respectively.
There was no contact between these two hardware pieces. During the experiment, participants facial
reactions
reactions to the stimuli were video recorded
recorded by a webcam. Each participant was required to minimize
his head movements during the experiment
experiment in order to avoid signal artifacts
artifacts from
from head
head movement.
movement.
The experimental
approved by the Institutional
Institutional
experimental environment
environmentis
is shown
shown in
in Figure
Figure 3.
3. The study has been approved
Review Broad of New Jersey Institute of Technology. Before the experiment, each participant was
asked to sign an agreement to participate in the study.

Brain Sci. 2020, 10, 85

5 of 20

Review Broad of New Jersey Institute of Technology. Before the experiment, each participant was
Brain Sci. 2020, 10, x FOR PEER REVIEW
5 of 20
asked
to2020,
sign10,anx FOR
agreement
to participate in the study.
Brain Sci.
PEER REVIEW
5 of 20

Figure 3. Experimental environment and participant wearing brain monitoring sensors.
Figure 3.
3. Experimental
wearing brain
brain monitoring
monitoring sensors.
sensors.
Figure
Experimental environment
environment and
and participant
participant wearing

2.3. Brain Data Acquisition
2.3.
Brain Data
Data Acquisition
Acquisition
2.3. Brain
The neuroimaging systems used in this study consisted of two commercial products: a wireless
The
neuroimaging
used
in
two
products:
The Model
neuroimaging
systems
used(www.fnirdevices.com)
in this
this study
study consisted
consisted of
ofand
two commercial
commercial
products:
wireless
fNIRS
1100Wsystems
system
an Emotiv
EPOC aa wireless
headset
fNIRS
Model
1100W
system
(www.fnirdevices.com)
and
an
Emotiv
EPOC
headset
(www.emotiv.com).
fNIRS
Model
1100W
system
(www.fnirdevices.com)
and
an
Emotiv
EPOC
headset
(www.emotiv.com). A compact and battery-operated wireless mini-fNIRS system was used to
A
compact
and
battery-operated
mini-fNIRS
system
used
monitor
themeasures
prefrontal
cortex
(www.emotiv.com).
A cortex
compact
andparticipant
battery-operated
wireless
mini-fNIRS
system
was used
to
monitor
the
prefrontal
ofwireless
the
as shown
inwas
Figure
4.toThe
system
cortical
of
the
participant
as
shown
in
Figure
4.
The
system
measures
cortical
oxygenation
changes
during
monitor
the prefrontal
cortexthe
of task
the participant
as shown
in Figure
4. The
system
measures
cortical
oxygenation
changes during
and is composed
of three
modules:
a sensor
pad
that holds
nearthe
task
and
is
composed
of
three
modules:
a
sensor
pad
that
holds
near-infrared
light
sources
and
oxygenation
changes
during
the
task
and
is
composed
of
three
modules:
a
sensor
pad
that
holds
nearinfrared light sources and detectors to enable a fast placement of 4 optodes (2 light wavelengths
detectors
to
enable
a
fast
placement
of
4
optodes
(2
light
wavelengths
channels
and
an
ambient
channel
infrared
sources
andchannel
detectors
enable acontrol
fast placement
of 4 optodes
(2 light
wavelengths
channelslight
and an
ambient
pertooptode),
box hardware
for sampling
all channels
at 4
per
control
box
for
sampling
all channels
at
4 Hz, and
computerall
that
runs COBI
channels
an ambient
channel
optode),
control
forasampling
channels
at
4
Hz,optode),
and aand
computer
that hardware
runs
COBIper
Studio
software
[56]box
thathardware
controls
data
collection
and
receives
the
Studio
software
[56]
that
controls
data
collection
and
receives
the
data
wirelessly
from
the
hardware.
Hz,
and
a
computer
that
runs
COBI
Studio
software
[56]
that
controls
data
collection
and
receives
the
data wirelessly from the hardware. More information about the device and data collection procedures
More
information
about
the device More
and data
collectionabout
procedures
wasand
reported
in [31]. procedures
data
wirelessly
the hardware.
information
the device
data collection
was reported
infrom
[31].
was reported in [31].

Figure
Figure 4.
4. Components of functional near-infrared spectroscopy (fNIRS) system: wireless transmitter,
wireless
containing
and
Figure
4.
Components
functional
near-infrared
spectroscopy (fNIRS) system: wireless transmitter,
wirelessbox
box
containingofbattery,
battery,
andsensor
sensorpad
pad[31].
[31].
wireless box containing battery, and sensor pad [31].

The
headset
shown
in Figure
5 acquired
128 Hz128
EEGHz
signals
measuring
electrical
The Emotiv
EmotivEPOC
EPOC
headset
shown
in Figure
5 acquired
EEG by
signals
by measuring
differences
on
the
scalp,
and
then
transmitted
the
signals
wirelessly
to
a
Windows
PC.
The
system
The Emotiv
EPOC
headset
in transmitted
Figure 5 acquired
128wirelessly
Hz EEG to
signals
by measuring
electrical
differences
on the
scalp,shown
and then
the signals
a Windows
PC.
The
measures
the
electrical
potentials
of
the
scalp
caused
by
neurons
firing.
The
cap
has
14
electrodes
electrical
differences
on
the
scalp,
and
then
transmitted
the
signals
wirelessly
to
a
Windows
PC.
The
system measures the electrical potentials of the scalp caused by neurons firing. The cap has 14
located
over
10–20the
system
positions
AF3,
F7,
FC5,
O1,
O2,
P8,
T8,O2,
FC6,
and
system
measures
electrical
potentials
of F3,
the
scalp
caused
by
neurons
firing.
The
cap
has
14
electrodes
located
over
10–20
system
positions
AF3,
F7,T7,
F3,P7,
FC5,
T7,
P7,
O1,
P8,F4,
T8,F8,
FC6,
F4,AF4
F8,
electrodes
located
over 10–20
system The
positions
AF3, F7, felt
F3, pad
FC5,isT7,
P7,toO1,
O2, P8,
T8, FC6,
F4, F8,
and AF4 using
2 reference
electrodes.
saline-soaked
used
reduce
electrical
resistance
and
AF4
using
2
reference
electrodes.
The
saline-soaked
felt
pad
is
used
to
reduce
electrical
resistance
between the skin and the electrodes. Low electrode impedances are achieved using saline solution as
between
and the electrodes. Low electrode impedances are achieved using saline solution as
indicatedthe
byskin
software.
indicated by software.

Brain Sci. 2020, 10, 85

6 of 20

using 2 reference electrodes. The saline-soaked felt pad is used to reduce electrical resistance between
the skin and the electrodes. Low electrode impedances are achieved using saline solution as indicated
by software.
Brain Sci. 2020, 10, x FOR PEER REVIEW

6 of 20

Figure 5.
5. Emotiv
Emotiv EPOC
headset: USB
[57].
Figure
EPOC headset:
USB Dongle,
Dongle, EPOC
EPOC headset,
headset, and
and electrodes
electrodes [57].

2.4. Automatic Facial Emotion Recognition System
The automatic facial emotion recognition system proposed in [58] is used to identify the
spontaneous facial
facialaffective
affectivestates.
states.TheThe
system
not only
outperforms
the state-of-the-art
based
system
not only
outperforms
the state-of-the-art
based on
the
on
the posed
expressions
butprovides
also provides
satisfactory
performance
on spontaneous
emotions.
posed
expressions
but also
satisfactory
performance
on spontaneous
facialfacial
emotions.
The
The
system
used
to read
Chief
Executive
Officers’(CEO)
(CEO)facial
facialexpressions
expressionsto
to forecast
forecast firm
system
has has
beenbeen
used
to read
Chief
Executive
Officers’
performance by only using
using recorded
recorded video
video signal
signal [59].
[59]. It utilizes
utilizes Regional
Regional Hidden
Hidden Markov Model
(RHMM) as its classifier
classifier to
to train
train the
the states
states of
of three
three face
face regions:
regions: the eyebrows,
eyebrows, eyes, and mouth, as
tabulated in Table
describes a facial expression is mainly
Table 1.
1. Since the biological information that describes
registered in the movement of these three regions as sensed and quantified in frames of a video
sequence, it is natural to classify the states of each facial region rather than modeling
modeling the
the entire
entire face.
face.
Note from the
the table
tablethat
thatthe
themouth
mouthregion
regionisisslightly
slightlydifferent
different
from
eyebrows
and
regions.
from
thethe
eyebrows
and
eyeeye
regions.
In
In
addition
mouthitself,
itself,the
thelips
lipscorners
cornersare
are also
also important
important features. Considering
Considering a practical
addition
to to
thethe
mouth
practical
application, this system can classify frames as they come into analysis. To describe the states of face
regions,
onon
each
frame
of of
video,
as displayed
in Figure
6. They
are
regions, 41
41facial
facialfeature
featurepoints
pointsare
areidentified
identified
each
frame
video,
as displayed
in Figure
6. They
comprised
of 10offeature
points
on the
region,
12 points
on the on
eyelids,
8 points8 on
the mouth,
are comprised
10 feature
points
oneyebrows
the eyebrows
region,
12 points
the eyelids,
points
on the
mouth,
on theofcorners
theone
lips,
and feature
one anchor
point The
on the
nose. The 2D
10
points10onpoints
the corners
the lips,of
and
anchor
pointfeature
on the nose.
2D coordinates
of
facial
feature of
points
in feature
various points
face regions
are extracted
to formare
corresponding
sequences
coordinates
facial
in various
face regions
extracted toobservation
form corresponding
observation
sequences
classification.
an example,
Figure 7 displays
the
recognition
for
for
classification.
As anfor
example,
Figure As
7 displays
the recognition
rates for
emotion
typesrates
in each
frame
as types
a function
of frame
(time) inofaframe
video index
sequence.
emotion
in each
frame index
as a function
(time) in a video sequence.
The system serves the needs of this study for three main reasons. First, the objective and
1. States
three
face regions.
measurable response to a person’sTable
emotions
by of
the
system
is perceived as more natural, persuasive,
and trustworthy. This allows us to plausibly analyze the measured data. Second, the system can
Face Regions
Observable States
recognize the various affective states of interest. Its last and most important advantage is that the
Eyebrows
raise, fall, neutral
system automatically analyzes and measures the live facial video data in a manner that is intuitive
Eyes It helps the user to take actions
open, close,
neutral
and useful for different applications.
based
on this analysis.
Mouth

Mouth
open, close, neutral
Table
1.
States
of
three
face
regions.
Lips corners
up, down, pull, pucker, neutral

Face Regions
Eyebrows
Eyes
Mouth
Mouth
Lips corners

Observable States
raise, fall, neutral
open, close, neutral
open, close, neutral
up, down, pull, pucker, neutral

Brain Sci. 2020, 10, 85

7 of 20

Brain Sci.
Sci. 2020,
2020, 10,
10, xx FOR
FOR PEER
PEER REVIEW
REVIEW
Brain

of 20
20
77 of

Figure 6.
6. Facial
Facial
feature
points
on
face image.
image.
Figure
Figure
6.
Facial feature
feature points
points on
on aaa face
face
image.

Recognition rates
rates of
of emotion
emotion
types
as aa function
function
of
frame
index
(time)
in
video
sequence.
Figure 7.
7. Recognition
emotiontypes
typesas
functionof
offrame
frameindex
index(time)
(time)in
inaaavideo
videosequence.
sequence.
Figure
(b) Disgust;
Disgust; (c)
(c) Fear;
Fear; (d)
(d) Happiness;
Happiness; (e)
(e) Sadness;
Sadness; (f)
(f) Surprise.
Surprise.
(a) Anger;
Anger; (b)
(a)

The system
serves the needs of this study for three main reasons. First, the objective and
2.5. Stimuli
Stimuli
Evaluation
2.5.
Evaluation
measurable response to a person’s emotions by the system is perceived as more natural, persuasive,
The twenty
twenty emotional
emotional images
images used
used in
in the
the trials
trials were
were obtained
obtained from
from the
the Nencki
Nencki Affective
Affective Picture
Picture
and The
trustworthy.
This allows us to
plausibly
analyze the
measured data.
Second, the system
can
System
(NAPS)
[55].
It
is
a
standardized,
wide-range,
high-quality,
realistic
picture
database
that is
is
System
(NAPS)
[55].
It
is
a
standardized,
wide-range,
high-quality,
realistic
picture
database
that
recognize the various affective states of interest. Its last and most important advantage is that the
widely used
used for
for brain
brain imaging
imaging studies.
studies. We
We selected
selected ten
ten positive-content
positive-content images
images and
and ten
ten negativenegativewidely
content
ones
according
to
the
given
valence
values
[11].
Each
image
is
classified
explicitly
with the
the
content ones according to the given valence values [11]. Each image is classified explicitly with

Brain Sci. 2020, 10, 85

8 of 20

system automatically analyzes and measures the live facial video data in a manner that is intuitive and
useful for different applications. It helps the user to take actions based on this analysis.
2.5. Stimuli Evaluation
The twenty emotional images used in the trials were obtained from the Nencki Affective Picture
System (NAPS) [55]. It is a standardized, wide-range, high-quality, realistic picture database that is
widely used for brain imaging studies. We selected ten positive-content images and ten negative-content
ones according to the given valence values [11]. Each image is classified explicitly with the attached
emotion type if over 50% of all participants express the same facial affect. Otherwise, it is classified as
an ambiguous image that is discarded [58]. Eventually, all images are classified explicitly. Therefore,
all of them are utilized for the experiments.
For the video-content part of the experiment, we selected twenty videos in English from
Youtube.com. They were evenly selected based on the contents (positive and negative) and length
(10–17 s). The positive content contains the funny or happy clips, e.g., dogs mimic human’s behaviors
like doing exercises, sitting and eating food, pushing the baby stroller, etc. The negative clips show
sadness, anger, or disgust, e.g., the people living in poverty, wars, memorial service, etc. The selected
videos were independently watched and evaluated by all twelve participants. To make sure the
video contents are consistent with participants’ spontaneous affective states, we did the similar initial
evaluation as an image assessment to classify all video clips as explicit or ambiguous video. Each
video is classified explicitly with the attached emotion type if over 50% of all participants express the
same facial affect. Otherwise, it is classified as an ambiguous video that is discarded. Eventually, all
selected video clips were classified explicitly and used in the following experiments.
2.6. Data Pre-Processing for Brain Activity and Facial Expression
EEG signals were passed through a low-pass filter with 30 Hz cutoff frequency. ICA analysis was
performed in EEGLAB, an open-source toolbox for analysis of single-trial EEG dynamics [60]. It was
used to detect and remove the artifacts in the raw EEG signals following the approach described in [61].
The average of the 5-s baseline brain signal before each trial was subtracted from the brain response
data for baseline adjustment. To capture the affective states (positive or negative) in different brain
regions, four frequency bands as Power Spectral Density (PSD) features were extracted from EEG
signals to identify brain patterns. The correlation between EEG spectral density in these frequency
bands and the spontaneous affective state were compared via 14 electrodes. Additional information
about correlations is included in the Appendix A.
A low-pass filter with 0.1 Hz cutoff frequency was used to achieve noise reduction in fNIRS
signals [26]. Motion artifacts were eliminated prior to extracting the features from fNIRS signals by
applying a fast Independent Component Analysis (ICA) [62]. The independent component was selected
through modeling the hemodynamic response. The modeled hemodynamic response represented the
expected hemodynamic response to the given stimulus calculated by convolving the stimulus function
and a canonical hemodynamic response function (HRF). The HRF [63] consists of a linear combination
of two Gamma functions as
!
tα1 −1 β1 α1 e−β1 t
tα2 −1 β2 α2 e−β2 t
−c
h(t) = A(
(1)
Γ ( α1 )
Γ ( α2 )
where A controls the amplitude, α and β control the shape and scale respectively, and c determines the
ratio of the response to undershoot. A t-test was used to select the independent component associated
with the hemodynamic response [64]. It is expected that the independent component with the highest
t-value is associated with the hemodynamic response to a given stimulus.
Inference of participants’ facial affective expressions is based on facial emotion recognition. Facial
affective expressions can be influenced by various factors including age, gender, race, time of day, and
the general health of the participant. To control for these factors, we calibrated our measurements

Brain Sci. 2020, 10, 85

9 of 20

over the first 5 s of facial expressions. During this 5 s calibration period, we computed the mean
measures of all emotion states. We then subtracted these mean measures from the remainder of facial
expressions for baseline adjustment [59]. The emotion type of a facial video clip v was recognized
by calculating the largest probability among Anger, Disgust, Fear, Happiness, Neutral, Sadness, and
Surprise expressed as
Pv Emotion = max(Pi ), v = 1, . . . , Nvideo , i = 1, . . . 7

(2)

where Nvideo is the total number of videos clips, each of which contains the participant’s facial response
to the stimulus. Pi is the probability of an emotion type that is obtained by summing the overall
probabilities
ofxthis
type in each frame of a video clip.
Brain
Sci. 2020, 10,
FORemotion
PEER REVIEW
9 of 20
Happiness is coded as a positive affect and Anger, Disgust, Fear, and Sadness are regarded as
Happiness
codedSurprise
as a positive
affect
and Anger,
Disgust,
Fear, and
Sadness are
regarded
a
a negative
affect.is Since
can be
revealed
as a result
of positive
or negative
affect,
it was as
not
negative
affect.
Since
Surprise
can
be
revealed
as
a
result
of
positive
or
negative
affect,
it
was
not
used
used in this experiment. Neutral emotion is neither positive or negative affect, so it was not used in
in
experiment.
Neutral
neither
negative
affect,affects
so it was
usedclip
in the
thethis
experiment,
either.
The emotion
ratings ofisboth
the positive
positive or
and
the negative
for anot
video
are
experiment,
either. The
ratings
of both
separately calculated
by the
system
as the positive and the negative affects for a video clip are
separately calculated by the system as


Pv A f f ect ==max
max( Ppos,, Pneg),,vv==1,1,
…,. . . , Nvideo
, ,

PHappiness

 Ppos =


(3)
 = PHappiness +max((PAnger ,, PDisgust , P, Fear , PSadness
)
,
)
subject to 
(3)

max
P
,
P
,
P
,
P
(
)
Fear
Anger
Disgust
Sadness
subject to 

,
,
)
 Pneg = PHappiness(+max(P, Anger , PDisgust
, PFear , PSadness )
=
(

,

,

,

)

A f f ect

where
isisthe
the
where Pv
therecognized
recognized affective
affectivestate
state of
ofthe
the corresponding
corresponding video
video clip.
clip. Figure
Figure 88 shows
shows the
spontaneous
facial
affective
states
of
a
participant
that
are
detected
by
system
when
he
is
watching
spontaneous facial affective states of a participant that are detected by system when he is watching
videos or
videos
or images
images during
during experiments.
experiments.

Figure 8. Recognition
Recognition rates
rates of
of emotion
emotion types
types as
as a function of frame index (time) in a video sequence.

2.7. Feature
Feature Extraction
Extraction
2.7.
The recorded
recorded data
data of
of raw
raw fNIRS
light intensity
intensity at
at two
two wavelengths
(730 nm
nm and
and 850
850 nm)
nm) is
is
The
fNIRS light
wavelengths (730
converted
to
the
relative
changes
in
hemodynamic
responses
in
terms
of
oxy-hemoglobin
(Hbo)
and
converted to the relative changes in hemodynamic responses in terms of oxy-hemoglobin (Hbo) and
deoxy-hemoglobin (Hbr)
using the
the modified
modified Beer-Lambert
Beer-Lambert Law
Law [56].
[56]. Total
Total hemoglobin
hemoglobin concentration
concentration
deoxy-hemoglobin
(Hbr) using
changes
(Hbt),
the
sum
of
Hbo
and
Hbr
and
an
estimate
of
the
total
blood
volume,
and
the
difference
of
changes (Hbt), the sum of Hbo and Hbr and an estimate of the total blood volume, and the
difference
Hbo
and
Hbr,
the
estimate
of
oxygenation
change,
were
also
calculated
for
each
optode.
We
calculated
of Hbo and Hbr, the estimate of oxygenation change, were also calculated for each optode. We
the mean, median,
standard
deviation,
and
the rangeand
of maximum
minimum
calculated
the mean,
median,
standardmaximum,
deviation,minimum,
maximum,
minimum,
the rangeand
of maximum
of
four
hemodynamic
response
signals
as
features,
4
×
4
×
6
=
96
fNIRS
features
for
each
trial.
and minimum of four hemodynamic response signals as features, 4 × 4 × 6 = 96 fNIRS features for
each trial.
The spectral power of EEG signals in different bands has been used for emotion analysis [65].
The logarithms of the power spectral density (PSD) for theta (4Hz < f ≤ 8 Hz), slow alpha (8Hz < f ≤
10 Hz), alpha (8 Hz < f ≤ 12 Hz), and beta (12 Hz < f ≤ 30 Hz) bands are extracted from all 14 electrodes
as features. In addition, the difference between the spectral power of all possible symmetrical pairs

Brain Sci. 2020, 10, 85

10 of 20

The spectral power of EEG signals in different bands has been used for emotion analysis [65]. The
logarithms of the power spectral density (PSD) for theta (4 Hz < f ≤ 8 Hz), slow alpha (8 Hz < f ≤ 10 Hz),
alpha (8 Hz < f ≤ 12 Hz), and beta (12 Hz < f ≤ 30 Hz) bands are extracted from all 14 electrodes as
features. In addition, the difference between the spectral power of all possible symmetrical pairs on
the right and left hemisphere is extracted to measure the possible asymmetry in the brain activity due
Brain Sci. 2020, 10, x FOR PEER REVIEW
10 of 20
to the valance of emotional stimuli [41]. The asymmetry features were extracted from four symmetric
pairs In
over
bands,
AF3–AF4, F7–F8,
F3–F4, and
FC5–FC6.
The truth
total number
of EEG
of a
thisfour
study,
the correlation
was calculated
using
the ground
for all video
andfeatures
image trials
trial
14 electrodes
is 14 ×
+ 4 × 4stimuli
= 72. and participants’ self-assessments on video stimuli) and
(the for
given
affective labels
on4 image
the facial affective states measured by the automatic facial emotion recognition system. The self3. Results
assessment has been widely used to measure mental states in the literature [7,58]. The result in Figure
9 shows
that allCorrelation
participants’
face affective states show a positive correlation with those reported by
3.1.
Preliminary
Analysis
participants (p < 0.01). It complies with our hypothesis that the facial affective expression may reflect
In this study, the correlation was calculated using the ground truth for all video and image trials
the affective state in the mind. However, it is likely that the self-assessment provided by the
(the
given
affective
labelsfrom
on image
stimuli and participants’
self-assessments
on video
stimuli)
and the
participant
is derived
the participants’
recall or from
second thoughts.
In this
section,
we
facial
affective
states measured
by the the
automatic
facial emotion
recognition
self-assessment
examine
the relationship
between
facial affective
expressions
andsystem.
mental The
states.
In order to
has
been the
widely
used to measure
statesfor
in further
the literature
[7,58].
The result
in Figure
9 shows that
support
hypothesis,
we buildmental
the model
analysis
as detailed
in this
section.
all participants’
affective
show a positive
correlation
reportedstates.
by participants
Moreover, face
we looked
at states
the correlation
of brain
activities with
with those
the affective
Please see
(p
<
0.01).
It
complies
with
our
hypothesis
that
the
facial
affective
expression
may
reflect
the are
affective
Appendix A Tables A1 and A2. The findings indicate that lower frequency band signals
more
state
in
the
mind.
However,
it
is
likely
that
the
self-assessment
provided
by
the
participant
is
derived
highly correlated with positive affective states delivered by participants who are triggered by both
from
or from
second
thoughts.
this section,
wethan
examine
relationship
videothe
andparticipants’
images. Therecall
correlation
from
the frontal
headIn
is slightly
higher
that inthe
posterior
of the
between
the
facial
affective
expressions
and
mental
states.
In
order
to
support
the
hypothesis,
we build
head. The findings are in line with the previous work [66].
the model for further analysis as detailed in this section.

Figure
Figure9.
9. Correlation
Correlation of
of the
the participants’
participants’ facial
facial affective
affectivestates
statesand
andthe
theground
groundtruth
truthover
overall
alltrials.
trials.The
The
average
is
0.5
marked
with
the
red
dashed
line.
average is 0.5 marked with the red dashed line.

Moreover, we looked at the correlation of brain activities with the affective states. Please see
3.2. Affective State Detection from Brain Activity
Appendix A Tables A1 and A2. The findings indicate that lower frequency band signals are more
There
were with
forty positive
trials (twenty
images
twenty
as mentioned
in the section
of
highly
correlated
affective
states and
delivered
by videos
participants
who are triggered
by both
Methods)
for each
Polynomial
Vector higher
Machine
was used
for
video
and images.
Theparticipant.
correlation from
the frontalSupport
head is slightly
than(SVM)
that in posterior
of the
classification.
fNIRS
EEG
features
were concatenated
head.
The findings
areand
in line
with
the previous
work [66]. to form a larger feature vector before
feeding them into the model. For comparison, we also applied the univariate modality of either fNIRS
3.2.
Affective
State Detection
from Brain
Activity
or EEG
features
for recognition
of the
affective state. We applied a leave-one-out approach for
training
and
testing.
the features
ofand
nineteen
trials
overasall
participants
were
extracted
to train
There
were
fortyThat
trialsis,
(twenty
images
twenty
videos
mentioned
in the
section
of Methods)
the
model.
The
remaining
one
trial
of
each
participant
was
used
for
testing.
The
performance
the
for each participant. Polynomial Support Vector Machine (SVM) was used for classification. fNIRSofand
experiment
was
validated
through
twenty-times
iterations.
EEG features were concatenated to form a larger feature vector before feeding them into the model. For
The method
jointly
using
and modality
EEG features
shows
0.75oraccuracy
of recognition
(imagecomparison,
we also
applied
thefNIRS
univariate
of either
fNIRS
EEG features
for recognition
of
content
stimuli),
which
outperforms
the
techniques
where
only
one
of
them
is
utilized
(0.63
for
EEG
the affective state. We applied a leave-one-out approach for training and testing. That is, the features
and 0.62 for fNIRS). The same finding is observed from the experiment using video-content stimuli
(0.8 for EEG + fNIRS, 0.72 for fNIRS, and 0.62 for EEG). The recognition performance for joint use of
fNIRS and EEG along with the cases where only one is used are displayed in Figures 10 and 11 for
image and video content type stimuli, respectively. This observation is consistent for almost all trials.
In some trials, univariate modality of EEG or fNIRS shows higher performance. We observed that

Brain Sci. 2020, 10, 85

11 of 20

of nineteen trials over all participants were extracted to train the model. The remaining one trial of
each participant was used for testing. The performance of the experiment was validated through
twenty-times iterations.
The method jointly using fNIRS and EEG features shows 0.75 accuracy of recognition
(image-content stimuli), which outperforms the techniques where only one of them is utilized
(0.63 for EEG and 0.62 for fNIRS). The same finding is observed from the experiment using video-content
stimuli (0.8 for EEG + fNIRS, 0.72 for fNIRS, and 0.62 for EEG). The recognition performance for
joint use of fNIRS and EEG along with the cases where only one is used are displayed in Figures 10
and 11 for image and video content type stimuli, respectively. This observation is consistent for
almost all trials. In some trials, univariate modality of EEG or fNIRS shows higher performance. We
observed that some participants reflected strongly to the stimuli while some participants showed
Brain Sci. 2020, 10, x FOR PEER REVIEW
11 of 20
higher affective tolerance to the same stimuli. This finding is consistent with our pilot study. The
proposed
multi-modalofmethod
performs
overare
10%
better than
the single-modality
for the
average
performances
these three
methods
compared
in Figures
12 and 13 formethods
image-content
same
stimuli. The stimuli,
averagerespectively.
performances
of standard
these three
methods
are compared
in Figures
12 and
13
and
video-content
The
deviation
error
bars show the
variability
of the
for
image-content
and
video-content
stimuli,
respectively.
The
standard
deviation
error
bars
show
performance for all trials. The area of receiver operating characteristic (ROC) curve for EEG + fNIRS
the variability
the performance
all 0.80
trials.
area of receiver
(ROC)
reaches
0.77 forofimage-content
trialsfor
and
forThe
video-content
trials.operating
The ROCcharacteristic
curves in Figure
14
curve
for
EEG
+
fNIRS
reaches
0.77
for
image-content
trials
and
0.80
for
video-content
trials.
The
ROC
and Figure 15 also show that the performance for joint use of fNIRS and EEG exceeds the approach
curvesonly
in Figures
and 15
show that
theand
performance
for joint
use of fNIRS
and EEG exceeds
using
one of14
them.
Thealso
standard
error
95% confidence
intervals
are calculated
in Tablesthe
2
approach
using
only
one
of
them.
The
standard
error
and
95%
confidence
intervals
are
calculated
in
and 3. In addition, we found that the proposed method recognizes the affective response to videoTables 2 stimuli
and 3. In
addition,
we found
thecaused
proposed
recognizes
the affective
response
to
content
more
accurately
thanthat
those
by method
image-content
stimuli.
It is likely
that the
video-content
stimuli
more
accurately
those caused
by image-content
stimuli.ones.
It is likely that the
dynamic
(video)
stimuli
provide
morethan
contextual
information
than static (image)
dynamic (video) stimuli provide more contextual information than static (image) ones.

Figure 10.
10. Comparison
Comparison of
of the
the proposed
proposed method
method (EEG
(EEG ++fNIRS)
fNIRS)and
andthe
theones
oneswhere
where only
only EEG
EEG or
or fNIRS
fNIRS
Figure
employed for
for all
all image-content
image-content trials.
trials.
employed
Table 2. Performance comparisons of the proposed method (EEG + fNIRS) and the ones where only
EEG or fNIRS employed for all image-content trials.
Model

Observation

ROC Area

Standard Error

95% Confidence Interval

EEG+fNIRS
fNIRS
EEG

240
240
240

0.77
0.63
0.62

0.02
0.03
0.02

0.74–0.80
0.57–0.69
0.58–0.66

Figure 10. Comparison of the proposed method (EEG + fNIRS) and the ones where only EEG or fNIRS
Brain Sci. 2020,
10, 85 for all image-content trials.
employed

Brain Sci. 2020, 10, x FOR PEER REVIEW

12 of 20

12 of 20

Figure
11.
Comparison
of the
proposed
fNIRS)and
andthe
the
ones
where
or 12
fNIRS
Figure
of the
proposedmethod
method(EEG
(EEG +
+ fNIRS)
ones
where
onlyonly
EEGEEG
or fNIRS
Brain Sci.
2020,11.
10,Comparison
x FOR PEER REVIEW
of 20
employed
for all
trials.
employed
forvideo-content
all video-content
trials.

Figure 12. Performances of the proposed fNIRS + EEG method along with EEG only and fNIRS only

Figure
12. Performances
of the proposed
+ EEG
method along
with
EEG only and fNIRS only
techniques
for all image-content
trials asfNIRS
displayed.
Whiskers
standard
deviation.
Figure
12. Performances
of the proposed
fNIRS + EEG
methodare
along
with EEG
only and fNIRS only
techniques for all image-content trials as displayed. Whiskers are standard deviation.
techniques for all image-content trials as displayed. Whiskers are standard deviation.

Figure 13. Performances of the proposed fNIRS + EEG method along with EEG only and fNIRS only
Figure 13. Performances of the proposed fNIRS + EEG method along with EEG only and fNIRS only
techniques
for all video-content trials as displayed. Whiskers are standard deviation.
techniques
for all video-content
trials as displayed.
Whiskers
standard
deviation.
Figure
13. Performances
of the proposed
fNIRS + EEG
methodare
along
with EEG
only and fNIRS only

Tabletechniques
3. Performance
comparisontrials
of the
method (EEG
+ fNIRS)
and the ones where only
for all video-content
as proposed
displayed. Whiskers
are standard
deviation.
EEG or fNIRS employed for all video-content trials.
Model

Observation

ROC Area

Standard Error

95% Confidence Interval

EEG+fNIRS
fNIRS
EEG

240
240
240

0.80
0.66
0.62

0.03
0.03
0.02

0.75–0.85
0.61–0.71
0.58–0.66

Figure
13. Performances of the proposed fNIRS + EEG method along with EEG only and fNIRS only
Brain Sci. 2020,
10, 85
techniques for all video-content trials as displayed. Whiskers are standard deviation.

13 of 20

Figure
14. ROC
curve
comparison
ofofthe
(EEG+ +fNIRS)
fNIRS)
and
where
Figure
14. ROC
curve
comparison
theproposed
proposed method
method (EEG
and
thethe
onesones
where
only only
Brain
Sci.
2020,
10,
x
FOR
PEER
REVIEW
13 of 20
EEG
or fNIRS
employed
image-contenttrials
trials
EEG or
fNIRS
employed
forfor
all all
image-content

Figure
15. ROC
curve
comparison
(EEG+ +
fNIRS)
where
Figure
15. ROC
curve
comparisonofofthe
theproposed
proposed method
method (EEG
fNIRS)
andand
the the
onesones
where
only only
EEG
or fNIRS
employed
video-contenttrials.
trials.
EEG or
fNIRS
employed
forfor
allall
video-content

3.3. Similarity
Facial Affective
andmethod
Affective
States
Coded
Brain
Activity
Tableof2. Spontaneous
Performance comparisons
of theStates
proposed
(EEG
+ fNIRS)
andby
thethe
ones
where
only
EEG or fNIRS employed for all image-content trials.

To assess the spontaneous affective states estimated through brain activity, first we evaluate the
ROC
Area
Standard
Interval
reliability of theModel
automaticObservation
facial emotion
recognition
systemError
that is 95%
usedConfidence
to recognize
the participants’
EEG+fNIRS
240
0.77
0.02
0.74–0.80
facial reaction to the given stimulus. In order to achieve this, we calculated the affect recognition
fNIRS
240
0.63
0.03
0.57–0.69
accuracy of the system
by comparing
the detected
results
on image stimuli and
EEG
240
0.62
0.02with the given labels
0.58–0.66
participants’ self-assessment on video stimuli. The affect recognition rate of the system for each trial are
3. Performance
comparison
of the
proposed method
(EEG
+ fNIRS) and the
onlyrepresents
tabulated Table
in Tables
4 and 5 for
image and
video-content
trials,
respectively.
Ti , ones
i = 1,where
. . . , 20
EEGThe
or fNIRS
employed
for allofvideo-content
the ith trial.
overall
accuracy
the systemtrials.
reaches 0.74 (σ = 0.10) for image-content stimuli and
0.80 (σ = 0.10) Model
for video. Observation
The results are
satisfactory
and indicate
thatConfidence
the system
performs well to
ROC
Area
Standard
Error
95%
Interval
EEG+fNIRS
0.80
0.03
0.75–0.85
detect a person’s
spontaneous240
facial expressions.
fNIRS
240
0.66
0.03
0.61–0.71
EEGTable 4. Affect
240 recognition
0.62
0.58–0.66
rate of system0.02
for each image-content
Trial.

3.3. Similarity
Facial
States
the Brain Activity
T1
T2 of Spontaneous
T3
T4Affective States
T5 and Affective
T6
T7Coded by T8
T9

T10

0.92 To assess
0.75 the spontaneous
0.67
0.75
0.83 estimated
0.75through0.67
0.83 first we0.83
0.83
affective states
brain activity,
evaluate the
reliability
of
the
automatic
facial
emotion
recognition
system
that
is
used
to
recognize
the
T11
T12
T13
T14
T15
T16
T17
T18
T19
T20
participants’ facial reaction to the given stimulus. In order to achieve this, we calculated the affect
0.67
0.75
0.67
0.5
0.75
0.75
0.92
0.75
0.67
0.58
recognition accuracy of the system by comparing the detected results with the given labels on image
stimuli and participants’ self-assessment on video stimuli. The affect recognition rate of the system
for each trial are tabulated in Tables 4 and 5 for image and video-content trials, respectively. Ti, i = 1,
…, 20 represents the ith trial. The overall accuracy of the system reaches 0.74 (σ = 0.10) for imagecontent stimuli and 0.80 (σ = 0.10) for video. The results are satisfactory and indicate that the system
performs well to detect a person’s spontaneous facial expressions.
Next, we estimated the degree of the similarity of spontaneous affective states expressed on the
face and those coded by brain activity (EEG + fNIRS). Similarity scores were calculated by the

Brain Sci. 2020, 10, 85

14 of 20

Table 5. Affect recognition rate of system for each video-content Trial.
T1

T2

T3

T4

T5

T6

T7

T8

T9

T10

0.83

0.67

0.83

0.59

0.92

0.83

0.67

0.83

0.92

1

T11

T12

T13

T14

T15

T16

T17

T18

T19

T20

0.75

0.83

0.83

0.83

0.83

0.83

0.67

0.92

0.67

0.83

Next, we estimated the degree of the similarity of spontaneous affective states expressed on
the face and those coded by brain activity (EEG + fNIRS). Similarity scores were calculated by the
correlation of spontaneous facial affect recognized by the system and the affective states translated by
participants’ brain signals across video-content and image-content trial per participant, respectively.
The results in Tables 6 and 7 show that the affective states expressed on face is correlated to that
delivered through brain triggered by both video and image stimuli, which is significant at p < 0.05.
That is, the spontaneous facial affective states can reflect the true brain affective responses to the stimuli.
Table 6. Correlation of spontaneous facial affect and affect state translated by participants’ brain signals
triggered by image stimuli.

phi coefficient
p value

phi coefficient
p value

Participant 1

Participant 2

Participant 3

Participant 4

Participant 5

Participant 6

0.58
0.008

0.45
0.045

0.50
0.023

0.54
0.013

0.49
0.027

0.47
0.036

Participant 7

Participant 8

Participant 9

Participant 10

Participant 11

Participant 12

0.81
0.000

0.64
0.002

0.61
0.004

0.74
0.000

0.68
0.001

0.68
0.001

Table 7. Correlation of spontaneous facial affect and affect state translated by participants’ brain signals
triggered by video stimuli.

phi coefficient
p value

phi coefficient
p value

Participant 1

Participant 2

Participant 3

Participant 4

Participant 5

Participant 6

0.47
0.036

0.70
0.001

0.59
0.006

0.51
0.020

0.49
0.027

0.52
0.018

Participant 7

Participant 8

Participant 9

Participant 10

Participant 11

Participant 12

0.81
0.000

0.58
0.008

0.52
0.018

0.55
0.011

0.50
0.023

0.68
0.001

4. Discussion
This study provides new insights for the exploration and analysis of spontaneous facial
affective expression associated with simultaneous multimodal brain activity in the form of two
wearable and portable neuroimaging techniques—fNIRS and EEG—that measure hemodynamic and
electrophysiological changes, respectively. We have demonstrated that affective states can be estimated
from human spontaneous facial expressions and brain activity via wearable sensors. The experimental
results are founded on the premise that the participant has no knowledge of stimuli prior to the
experiment. The spontaneous facial expressions of participants can be triggered by emotional stimuli.
Moreover, specific neural activity changes are found due to the perception of the emotional stimuli. In
addition, we found that video-content stimuli more readily induce the participants’ affective states than
image-content stimuli. This can be explained as dynamic (video) stimulus provides more contextual
information than a static (image) one. Compared to the static (image) stimuli, dynamic (video) ones
trigger enhanced emotion delivered by brain activity as also shown in [67].
In this study, the findings were derived from the combined analysis of cortical hemodynamic
and electrophysiological signals. The neural activities were measured by two non-invasive, wearable
and complementary neuroimaging techniques, fNIRS and EEG. The complementary nature of fNIRS
and EEG has been reported in the literature with multimodality studies [43,47,68–70]. Particularly,

Brain Sci. 2020, 10, 85

15 of 20

both of them have received considerable attention on emotion inference and emotional mapping on
brain activities [21,41,71]. The proposed hybrid method for affective state detection jointly using fNIRS
and EEG signals outperforms techniques that employ only EEG or only fNIRS. The same results are
observed using both video or image-content types of stimuli. The method jointly using fNIRS and
EEG features shows 0.8 accuracy (video-content stimuli) and 0.75 accuracy (image-content stimuli)
which outperforms the techniques where only one of them is utilized. The results here confirm earlier
multimodal fNIRS + EEG studies and highlight the complementary information content in both signal
streams [47].
The video stream to measure facial reactions to different stimuli offers prompt, objective,
and accurate recognition performance in continuous time. The regional facial features are highlighted
since they convey significant information relevant to expressions. It is natural to classify the states of
each facial region rather than considering the holistic features of the entire face for recognition [72].
The experimental results support our hypothesis by showing a high correlation between recognized
facial affective expressions and the ground truth for all trials (the given labels on image stimuli and
participant’s self-assessment on video stimuli).
The study described here provides important albeit preliminary information about wearable and
ultra-portable neuroimaging sensors. It is important to highlight the fact that EPOC EEG electrodes
are sensitive to external interference and non-brain signal sources such as muscle activity. Long, thick
hair of participants could prevent electrodes from touching the scalp properly in order to collect
“clean” brain signals. The challenging nature of measuring EEG signals may cause an adverse effect
on our analysis of the relationship of facial activities and the affective states translated by their brain
signals. Moreover, the fNIRS measures of the PFC hemodynamic response were used based on earlier
studies [21]; however, monitoring of other brain areas could increase the overall classification accuracy.
Finally, some prior work has shown that men and women differ in the neural mechanisms underlying
their expression of specific emotions [73]. It is noted that all subjects involved in this study were male.
However, future work may extend this study and its findings to all sexes.
The video sequences and images used in this study display short duration content, although all
participants stated that they were able to understand all stimuli. However, it is of interest to address
how the participants react to the content stimuli with longer durations in future studies. The findings
in this study indicate that the spontaneous facial affective expressions are interrelated to the measured
brain activity. It is likely that facial reactions to the longer duration-content stimuli might differ in
frequency. The audience’s physiological responses to two-hour long movies were measured in [74]
and revealed significant variations in affective states throughout the media. The extension of this work
might benefit the specific applications that require the feedback of longer-duration content such as
online education and entertainment. Also, the accuracy score per subject must be interpreted with
caution. In a two class and ten testing trials per class to fit with experimental constraints, classification
performance should be higher than 70% to be statistically significant (p < 0.05) [75,76]. Considering
both image-content and video-content, average performance of classifier with EEG + fNIRS passed
this limit. Further improvements with preprocessing methods and/or machine learning methodologies
could improve and optimize the classifier performance.
5. Conclusions
To the best of our knowledge, this is the first attempt to detect affective states by jointly using fNIRS,
EEG, and capture of facial expressions. The study reveals a strong correlation between spontaneous
facial affective expressions and the affective states delivered by brain activities. The experimental results
show that the proposed EEG + fNIRS multimodal method outperforms fNIRS-only and EEG-only
approaches. The experimental results confirm the feasibility of the proposed method. In addition, the
results highlight the reliability of spontaneous facial expression and use of wearable neuroimaging as
promising methodologies to serve for various practical applications in the future. As the sensors used

Brain Sci. 2020, 10, 85

16 of 20

in the study allow untethered and mobile measurements, the approach demonstrated can be readily
adapted in the future for measurements in real-world settings.
Author Contributions: Conceptualization, H.A., A.N.A., Y.S.; Methodology, Y.S., H.A., & A.N.A.; Formal Analysis,
Y.S., H.A., & A.N.A.; Resources, Y.S., H.A., & A.N.A.; Data Curation, Y.S., H.A., & A.N.A.; Writing—Original
Draft Preparation, Y.S.; Writing—Review and Editing, Y.S., H.A., & A.N.A.; Project Administration, A.N.A. & H.A.
All authors have read and agreed to the published version of the manuscript.
Acknowledgments: Authors would like to thank Adrian Curtin and Jesse Mark for their help with the paper.
Conflicts of Interest: fNIR Devices, LLC manufactures the optical brain imaging instrument and licensed IP and
know-how from Drexel University. H.A. was involved in the technology development and thus offered a minor
share in the startup firm fNIR Devices, LLC. The authors declare that the research was conducted in the absence of
any commercial or financial relationships that could be construed as a potential conflict of interest.

Appendix A
To illustrate the affective states (positive or negative) in different brain regions, four frequency
bands as Power Spectral Density (PSD) features shown in Tables A1 and A2 extracted from EEG signals
to identify brain patterns. The tables show the correlation between EEG spectral density in selected
frequency bands and the spontaneous affective state conveyed from participants via 14 electrodes. The
finding indicates that lower frequency band signals is more highly correlated with positive affective
state delivered by participants who are triggered by both video and image. The correlation from
frontal head is slightly higher than that in posterior of head. The findings are in line with the previous
work [66].
Table A1. Correlation between EEG PSD frequency bands and ground truth after triggered by image
contents where the significant correlation (p value ≤ 0.05) is indicated with *.
Anterior

Posterior

Anterior

AF3

F7

F3

FC5

T7

P7

O1

O2

P8

T8

FC6

F4

F8

AF4

Theta

0.72

0.62

0.61

0.57

0.81

0.48

0.69

0.59

0.53

0.48

0.56

0.68

0.61

0.46

Slow
Alpha

0.69

0.59

0.61

0.56

0.80

0.47

0.68

0.59

0.53

0.48

0.55

0.68

0.60

0.46

Alpha

0.11 *

0.10

0.55

0.48

0.67

0.38

0.51

0.56

0.51

0.42

0.53

0.66

0.58

0.45

Beta

−0.32

−0.24

−0.15

−0.32

−0.19

−0.21

−0.30

0.11 *

0.25

−0.13

0.10

0.24

0.20

0.25

* The correlation with p value >0.05.

Table A2. Correlation between EEG PSD frequency bands and ground truth after triggered by video
contents where the significant correlation (p value ≤ 0.05) is indicated with *.
Anterior

Posterior

Anterior

AF3

F7

F3

FC5

T7

P7

O1

O2

P8

T8

FC6

F4

F8

AF4

Theta

0.61

0.58

0.74

0.38

0.62

0.46

0.54

0.47

0.46

0.56

0.47

0.56

0.51

0.47

Slow
Alpha

0.60

0.56

0.73

0.37

0.62

0.46

0.53

0.46

0.46

0.55

0.46

0.55

0.51

0.46

Alpha

0.50

0.19

0.64

0.33

0.54

0.41

0.43

0.36

0.44

0.50

0.16 *

0.47

0.46

0.41

Beta

−0.21 *

−0.22

−0.41

−0.08 *

−0.30

−0.09

−0.29

−0.23

0.18

−0.05

−0.54

−0.19

−0.09

−0.15

* The correlation with p value >0.05.

References
1.
2.

Russell, J.A.; Dols, J.M.F. The Psychology of Facial Eexpression; Cambridge University Press: Cambridge,
UK, 1997.
Frith, C. Role of facial expressions in social interactions. Philos. Trans. R. Soc. B Biol. Sci. 2009, 364, 3453–3458.
[CrossRef]

Brain Sci. 2020, 10, 85

3.
4.
5.

6.
7.
8.
9.
10.
11.

12.
13.
14.
15.

16.
17.

18.

19.
20.
21.
22.

23.
24.

25.
26.

17 of 20

Ekman, P.; Friesen, W.V. Measuring facial movement. Environ. Psychol. Nonverbal Behav. 1976, 1, 56–75.
[CrossRef]
Ekman, P. Facial expression and emotion. Am. Psychol. 1993, 48, 384–392. [CrossRef] [PubMed]
Hall, J.; Philip, R.C.M.; Marwick, K.; Whalley, H.C.; Romaniuk, L.; McIntosh, A.M.; Santos, I.;
Sprengelmeyer, R.; Johnstone, E.C.; Stanfield, A.C.; et al. Social Cognition, the Male Brain and the
Autism Spectrum. PLoS ONE 2012, 7, e49033. [CrossRef] [PubMed]
McDuff, D.; El Kaliouby, R.; Cohn, J.F.; Picard, R.W. Predicting ad liking and purchase intent: Large-scale
analysis of facial responses to ads. IEEE Trans. Affect. Comput. 2014, 6, 223–235. [CrossRef]
North, M.S.; Todorov, A.; Osherson, D.N. Accuracy of inferring self- and other-preferences from spontaneous
facial expressions. J. Nonverbal Behav. 2012, 36, 227–233. [CrossRef]
Erickson, K.; Schulkin, J. Facial expressions of emotion: A cognitive neuroscience perspective. Brain Cogn.
2003, 52, 52–60. [CrossRef]
Morecraft, R.J.; Stilwell–Morecraft, K.S.; Rossing, W.R. The motor cortex and facial expression: New insights
from neuroscience. Neurologist 2004, 10, 235–249. [CrossRef]
Zeng, Z.; Pantic, M.; Roisman, G.I.; Huang, T.S. A Survey of Affect Recognition Methods: Audio, Visual, and
Spontaneous Expressions. IEEE Trans. Pattern Anal. Mach. Intell. 2009, 31, 39–58. [CrossRef]
Sun, Y.; Ayaz, H.; Akansu, A.N. Neural correlates of affective context in facial expression analysis:
A simultaneous EEG-fNIRS study. In Proceedings of the 2015 IEEE Global Conference on Signal and
Information Processing (GlobalSIP), Orlando, FL, USA, 14–16 December 2015; pp. 820–824.
Sariyanidi, E.; Gunes, H.; Cavallaro, A. Automatic analysis of facial affect: A survey of registration,
representation, and recognition. IEEE Trans. Pattern Anal. Mach. Intell. 2014, 37, 1113–1133. [CrossRef]
Ekman, P.; Friesen, W.V.; Ellsworth, P. Emotion in the Human Face: Guide-Lines for Research and an Integration of
Findings: Guidelines for Research and an Integration of Findings; Pergamon: New York, NY, USA, 1972.
Ekman, P.; Friesen, W.V. Facial Action Coding System; Consulting Psychologists Press: Palo Alto, CA, USA, 1978.
Liu, P.; Han, S.; Meng, Z.; Tong, Y. Facial Expression Recognition via a Boosted Deep Belief Network.
In Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition, Columbus, OH,
USA, 23–28 June 2014; pp. 1805–1812.
Li, Y.; Wang, S.; Zhao, Y.; Ji, Q. Simultaneous Facial Feature Tracking and Facial Expression Recognition.
IEEE Trans. Image Process. 2013, 22, 2559–2573.
Valstar, M.F.; Gunes, H.; Pantic, M. How to distinguish posed from spontaneous smiles using geometric
features. In Proceedings of the 9th international conference on Multimodal Interfaces, Nagoya, Aichi, Japan,
12–15 November 2007; pp. 38–45.
Damasio, A.R.; Grabowski, T.J.; Bechara, A.; Damasio, H.; Ponto, L.L.; Parvizi, J.; Hichwa, R.D. Subcortical
and cortical brain activity during the feeling of self-generated emotions. Nat. Neurosci. 2000, 3, 1049–1056.
[CrossRef] [PubMed]
Gray, J.R.; Braver, T.S.; Raichle, M.E. Integration of emotion and cognition in the lateral prefrontal cortex.
Proc. Natl. Acad. Sci. USA 2002, 99, 4115–4120. [CrossRef] [PubMed]
Tamietto, M.; De Gelder, B. Neural bases of the non-conscious perception of emotional signals. Nat. Rev.
Neurosci. 2010, 11, 697–709. [CrossRef]
Nishitani, S.; Shinohara, K. NIRS as a tool for assaying emotional function in the prefrontal cortex. Front.
Hum. Neurosci. 2013, 7, 770. [CrossRef]
Herrmann, M.J.; Glotzbach, E.; Mühlberger, A.; Gschwendtner, K.; Fallgatter, A.J.; Pauli, P. Prefrontal
Brain Activation During Emotional Processing: A Functional Near Infrared Spectroscopy Study (fNIRS).
Open Neuroimag J. 2011, 5, 33–39.
Tai, K.; Chau, T. Single-trial classification of NIRS signals during emotional induction tasks: Towards a
corporeal machine interface. J. Neuroeng. Rehabil. 2009, 6, 39. [CrossRef]
Dolcos, F.; LaBar, K.S.; Cabeza, R. Dissociable effects of arousal and valence on prefrontal activity indexing
emotional evaluation and subsequent memory: An event-related fMRI study. Neuroimage 2004, 23, 64–74.
[CrossRef]
Lucas, I.; Balada, F.; Blanco, E.; Aluja, A. Prefrontal cortex activity triggered by affective faces exposure and
its relationship with neuroticism. Neuropsychologia 2019, 132, 107146. [CrossRef]
Ayaz, H.; Shewokis, P.A.; Bunce, S.; Izzetoglu, K.; Willems, B.; Onaral, B. Optical brain monitoring for
operator training and mental workload assessment. Neuroimage 2012, 59, 36–47. [CrossRef]

Brain Sci. 2020, 10, 85

27.
28.
29.
30.
31.

32.

33.

34.

35.
36.
37.
38.
39.
40.
41.
42.
43.
44.
45.

46.
47.
48.

49.

18 of 20

De Vos, M.; Debener, S. Mobile EEG: Towards brain activity monitoring during natural action and cognition.
Int. J. Psychophysiol. Off. J. Int. Organ. Psychophysiol. 2014, 91, 1–2. [CrossRef] [PubMed]
Curtin, A.; Ayaz, H. The Age of Neuroergonomics: Towards Ubiquitous and Continuous Measurement of
Brain Function with fNIRS. Jpn. Psychol. Res. 2018, 60, 374–386. [CrossRef]
Gramann, K.; Fairclough, S.H.; Zander, T.O.; Ayaz, H. Trends in neuroergonomics. Front. Hum. Neurosci.
2017, 11, 165. [CrossRef] [PubMed]
Ayaz, H.; Dehais, F. Neuroergonomics: The Brain at Work and in Everyday Life; Academic Press: London,
UK, 2018.
Ayaz, H.; Onaral, B.; Izzetoglu, K.; Shewokis, P.A.; McKendrick, R.; Parasuraman, R. Continuous monitoring
of brain dynamics with functional near infrared spectroscopy as a tool for neuroergonomic research: Empirical
examples and a technological development. Front. Hum. Neurosci. 2013, 7, 871. [CrossRef] [PubMed]
Kreplin, U.; Fairclough, S.H. Effects of self-directed and other-directed introspection and emotional valence
on activation of the rostral prefrontal cortex during aesthetic experience. Neuropsychologia 2015, 71, 38–45.
[CrossRef] [PubMed]
Bendall, R.C.; Eachus, P.; Thompson, C. A brief review of research using near-infrared spectroscopy to
measure activation of the prefrontal cortex during emotional processing: The importance of experimental
design. Front. Hum. Neurosci. 2016, 10, 529. [CrossRef]
Rodrigo, A.H.; Ayaz, H.; Ruocco, A.C. Examining the neural correlates of incidental facial emotion encoding
within the prefrontal cortex using functional near-infrared spectroscopy. In Proceedings of the International
Conference on Augmented Cognition, Toronto, ON, Canada, 17–22 July 2016; pp. 102–112.
Huang, Y.; Yang, J.; Liu, S.; Pan, J. Combining Facial Expressions and Electroencephalography to Enhance
Emotion Recognition. Future Int. 2019, 11, 105. [CrossRef]
Kim, M.-K.; Kim, M.; Oh, E.; Kim, S.-P. A review on the computational methods for emotional state estimation
from the human EEG. Comput. Math. Methods Med. 2013, 2013, 573734. [CrossRef]
Canli, T.; Desmond, J.E.; Zhao, Z.; Glover, G.; Gabrieli, J.D. Hemispheric asymmetry for emotional stimuli
detected with fMRI. Neuroreport 1998, 9, 3233–3239. [CrossRef]
Davidson, R.J. What does the prefrontal cortex “do” in affect: Perspectives on frontal EEG asymmetry
research. Biol. Psychol. 2004, 67, 219–233. [CrossRef]
Harmon-Jones, E.; Gable, P.A.; Peterson, C.K. The role of asymmetric frontal cortical activity in emotion-related
phenomena: A review and update. Biol. Psychol. 2010, 84, 451–462. [CrossRef] [PubMed]
Wheeler, R.E.; Davidson, R.J.; Tomarken, A.J. Frontal brain asymmetry and emotional reactivity: A biological
substrate of affective style. Psychophysiology 1993, 30, 82–89. [CrossRef] [PubMed]
Davidson, R.J.; Jackson, D.C.; Kalin, N.H. Emotion, plasticity, context, and regulation: Perspectives from
affective neuroscience. Psychol. Bull. 2000, 126, 890–909. [CrossRef]
Davidson, R.J.; Fox, N.A. Asymmetrical brain activity discriminates between positive and negative affective
stimuli in human infants. Science 1982, 218, 1235–1237. [CrossRef] [PubMed]
Liu, Y.; Ayaz, H.; Shewokis, P.A. Multisubject “learning” for mental workload classification using concurrent
EEG, fNIRS, and physiological measures. Front. Hum. Neurosci. 2017, 11, 389. [CrossRef] [PubMed]
Khan, M.J.; Hong, K.-S. Hybrid EEG–fNIRS-based eight-command decoding for BCI: Application to
quadcopter control. Front. Neurorob. 2017, 11, 6. [CrossRef] [PubMed]
Von Lühmann, A.; Wabnitz, H.; Sander, T.; Müller, K.-R. M3BA: A mobile, modular, multimodal biosignal
acquisition architecture for miniaturized EEG-NIRS-based hybrid BCI and monitoring. IEEE Trans. Biomed.
Eng. 2016, 64, 1199–1210. [CrossRef]
Balconi, M.; Vanutelli, M.E. Hemodynamic (fNIRS) and EEG (N200) correlates of emotional inter-species
interactions modulated by visual and auditory stimulation. Sci. Rep. 2016, 6, 23083. [CrossRef]
Fazli, S.; Mehnert, J.; Steinbrink, J.; Curio, G.; Villringer, A.; Müller, K.-R.; Blankertz, B. Enhanced performance
by a hybrid NIRS-EEG brain computer interface. NeuroImage 2012, 59, 519–529. [CrossRef]
Balconi, M.; Grippa, E.; Vanutelli, M.E. What hemodynamic (fNIRS), electrophysiological (EEG) and
autonomic integrated measures can tell us about emotional processing. Brain Cogn. 2015, 95, 67–76.
[CrossRef]
Hu, X.; Zhuang, C.; Wang, F.; Liu, Y.-J.; Im, C.-H.; Zhang, D. fNIRS Evidence for Recognizably Different
Positive Emotions. Front. Hum. Neurosci. 2019, 13. [CrossRef]

Brain Sci. 2020, 10, 85

50.
51.
52.

53.

54.
55.

56.
57.
58.

59.
60.
61.

62.
63.
64.
65.
66.

67.

68.
69.

70.

19 of 20

Gramann, K.; Gwin, J.T.; Ferris, D.P.; Oie, K.; Jung, T.-P.; Lin, C.-T.; Liao, L.-D.; Makeig, S. Cognition in action:
Imaging brain/body dynamics in mobile humans. Rev. Neurosci. 2011, 22, 593–608. [CrossRef]
Sundström Poromaa, I.; Gingnell, M. Menstrual cycle influence on cognitive function and emotion
processing—From a reproductive perspective. Front. Neurosci. 2014, 8, 380. [CrossRef] [PubMed]
Derntl, B.; Windischberger, C.; Robinson, S.; Lamplmayr, E.; Kryspin-Exner, I.; Gur, R.C.; Moser, E.;
Habel, U. Facial emotion recognition and amygdala activation are associated with menstrual cycle phase.
Psychoneuroendocrinology 2008, 33, 1031–1040. [CrossRef] [PubMed]
Guapo, V.G.; Graeff, F.G.; Zani, A.C.T.; Labate, C.M.; dos Reis, R.M.; Del-Ben, C.M. Effects of sex hormonal
levels and phases of the menstrual cycle in the processing of emotional faces. Psychoneuroendocrinology 2009,
34, 1087–1094. [CrossRef] [PubMed]
Farage, M.A.; Osborn, T.W.; MacLean, A.B. Cognitive, sensory, and emotional changes associated with the
menstrual cycle: A review. Arch. Gynecol. Obstet. 2008, 278, 299–307. [CrossRef] [PubMed]
Marchewka, A.; Żurawski, Ł.; Jednoróg, K.; Grabowska, A. The Nencki Affective Picture System (NAPS):
Introduction to a novel, standardized, wide-range, high-quality, realistic picture database. Behav. Res.
Methods 2014, 46, 596–610. [CrossRef]
Ayaz, H.; Shewokis, P.A.; Curtin, A.; Izzetoglu, M.; Izzetoglu, K.; Onaral, B. Using MazeSuite and functional
near infrared spectroscopy to study learning in spatial navigation. JoVE J. Vis. Exp. 2011, 3443. [CrossRef]
Badcock, N.A.; Mousikou, P.; Mahajan, Y.; de Lissa, P.; Thie, J.; McArthur, G. Validation of the Emotiv EPOC®
EEG gaming system for measuring research quality auditory ERPs. PeerJ 2013, 1, e38. [CrossRef]
Sun, Y.; Akansu, A.N. Automatic inference of mental states from spontaneous facial expressions.
In Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), Florence, Italy, 4–9 May 2014; pp. 719–723.
Akansu, A.; Cicon, J.; Ferris, S.P.; Sun, Y. Firm Performance in the Face of Fear: How CEO Moods Affect Firm
Performance. J. Behav. Finance 2017, 18, 373–389. [CrossRef]
Delorme, A.; Makeig, S. EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including
independent component analysis. J. Neurosci. Methods 2004, 134, 9–21. [CrossRef] [PubMed]
Jung, T.-P.; Makeig, S.; Humphries, C.; Lee, T.-W.; Mckeown, M.J.; Iragui, V.; Sejnowski, T.J. Removing
electroencephalographic artifacts by blind source separation. Psychophysiology 2000, 37, 163–178. [CrossRef]
[PubMed]
Hyvarinen, A. Fast and Robust Fixed-point Algorithms for Independent Component Analysis. Trans. Neural
Netw. 1999, 10, 626–634. [CrossRef] [PubMed]
Lindquist, M.A.; Loh, J.M.; Atlas, L.Y.; Wager, T.D. Modeling the hemodynamic response function in fMRI:
Efficiency, bias and mis-modeling. Neuroimage 2009, 45, S187–S198. [CrossRef] [PubMed]
Santosa, H.; Hong, M.J.; Kim, S.-P.; Hong, K.-S. Noise reduction in functional near-infrared spectroscopy
signals by independent component analysis. Rev. Sci. Instrum. 2013, 84, 073106. [CrossRef] [PubMed]
Koelstra, S.; Patras, I. Fusion of facial expressions and EEG for implicit affective tagging. Image Vis. Comput.
2013, 31, 164–174. [CrossRef]
Pan, J.; Xie, Q.; Huang, H.; He, Y.; Sun, Y.; Yu, R.; Li, Y. Emotion-Related Consciousness Detection in Patients
With Disorders of Consciousness Through an EEG-Based BCI System. Front. Hum. Neurosci. 2018, 12, 198.
[CrossRef]
Trautmann, S.A.; Fehr, T.; Herrmann, M. Emotions in motion: Dynamic compared to static facial expressions
of disgust and happiness reveal more widespread emotion-specific activations. Brain Res. 2009, 1284, 100–115.
[CrossRef]
Sutton, S.K.; Davidson, R.J. Prefrontal brain asymmetry: A biological substrate of the behavioral approach
and inhibition systems. Psychol. Sci. 1997, 8, 204–210. [CrossRef]
Liu, Y.; Ayaz, H.; Curtin, A.; Onaral, B.; Shewokis, P.A. Towards a hybrid P300-based BCI using simultaneous
fNIR and EEG. In Proceedings of the International Conference on Augmented Cognition, Las Vegas, NV,
USA, 21–26 July 2013; pp. 335–344.
Leamy, D.J.; Collins, R.; Ward, T.E. Combining fNIRS and EEG to Improve Motor Cortex Activity Classification
during an Imagined Movement-Based Task. In Proceedings of the Foundations of Augmented Cognition.
Directing the Future of Adaptive Systems; Schmorrow, D.D., Fidopiastis, C.M., Eds.; Springer: Heidelberg,
Germany, 2011; pp. 177–185.

Brain Sci. 2020, 10, 85

71.

72.
73.
74.

75.
76.

20 of 20

Leon-Carrion, J.; Damas, J.; Izzetoglu, K.; Pourrezai, K.; Martín-Rodríguez, J.F.; Barroso y Martin, J.M.;
Dominguez-Morales, M.R. Differential time course and intensity of PFC activation for men and women in
response to emotional stimuli: A functional near-infrared spectroscopy (fNIRS) study. Neurosci. Lett. 2006,
403, 90–95. [CrossRef]
White, M. Parts and Wholes in Expression Recognition. Cogn. Emot. 2000, 14, 39–60. [CrossRef]
Whittle, S.; Yücel, M.; Yap, M.B.H.; Allen, N.B. Sex differences in the neural correlates of emotion: Evidence
from neuroimaging. Biol. Psychol. 2011, 87, 319–333. [CrossRef] [PubMed]
Fleureau, J.; Guillotel, P.; Orlac, I. Affective Benchmarking of Movies Based on the Physiological Responses
of a Real Audience. In Proceedings of the 2013 Humaine Association Conference on Affective Computing
and Intelligent Interaction, Washington, DC, USA, 2–5 September 2013; pp. 73–78.
Müller-Putz, G.; Scherer, R.; Brunner, C.; Leeb, R.; Pfurtscheller, G. Better than random: A closer look on BCI
results. Int. J. Bioelectromagn. 2008, 10, 52–55.
Combrisson, E.; Jerbi, K. Exceeding chance level by chance: The caveat of theoretical chance levels in brain
signal classification and statistical assessment of decoding accuracy. J. Neurosci. Methods 2015, 250, 126–136.
[CrossRef] [PubMed]
© 2020 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).

