Usability and Cost-effectiveness in Brain-Computer
Interaction: Is it User Throughput or Technology Related?
Athanasios Vourvopoulos
Madeira-ITI,
Universidade da Madeira (UMa)
Funchal, Portugal
athanasios.vourvopoulos@m-iti.org
ABSTRACT

In recent years, Brain-Computer Interfaces (BCIs) have been
steadily gaining ground in the market, used either as an
implicit or explicit input method in computers for
accessibility, entertainment or rehabilitation. Past research in
BCI has heavily neglected the human aspect in the loop,
focusing mostly in the machine layer. Further, due to the high
cost of current BCI systems, many studies rely on low-cost
and low-quality equipment with difficulties to provide
significant advancements in physiological computing. OpenSource projects are offered as alternatives to expensive
medical equipment. Nevertheless, the effectiveness of such
systems over their cost is still unclear, and whether they can
deliver the same level of experience as their more expensive
counterparts. In this paper, we demonstrate that effective
BCI interaction in a Motor-Imagery BCI paradigm can be
accomplished without requiring high-end/high-cost devices,
by analyzing and comparing EEG systems ranging from
open source devices to medically certified systems.
Author Keywords

Brain-Computer Interaction; EEG; Motor-Imagery; Costeffectiveness; Usability
ACM Classification Keywords

Human-centered computing---Human computer interaction
(HCI)---HCI design and evaluation methods---Usability
testing
INTRODUCTION

Brain-computer interfaces (BCIs) are communication
systems that aim at providing users with an alternative
control input to computers. BCIs detect changes in brain
signal modulation and translate them into control commands
[34]. Currently, electroencephalography (EEG) is utilized as
the main acquisition technology for BCI [29]. The acquired
EEG signal represents a macroscopic measurement of neural
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than the author(s) must be
honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from Permissions@acm.org.
AH 2016, February 25 - 27, 2016, Geneva, Switzerland
Copyright is held by the owner/author(s). Publication rights licensed to
ACM.
ACM 978-1-4503-3680-2/16/02…$15.00
DOI: http://dx.doi.org/10.1145/2875194.2875244

Sergi Bermudez i Badia
Madeira-ITI,
Universidade da Madeira (UMa)
Funchal, Portugal
sergi.bermudez@m-iti.org
activity of the brain, mediated by a layer of bone, tissue and
fluid. For this reason, EEG has low spatial resolution and the
signal acquisition by surface electrodes make EEG
susceptible to interference from a variety of sources such as
physical movement, power line noise, and other electronic
equipment.
The three main types of EEG based BCI paradigms include:
(a) Steady State Visual Evoked Potentials (SSVEP), (b) P300
BCI and (c) Motor-Imagery (MI) BCI. SSVEP is caused by
visual stimulation of flashing lights and are measured from
the primary visual cortex of the brain [33]. P300 BCI is
generated by measuring the brain evoked response 300ms
after stimulus onset, of positive and negative deflections in
the EEG signal [10]. Finally, ERS/ERD stands for event
related synchronization/desynchronization through Motor
imagery (MI) of limb movements, localized at the motor and
somatosensory cortex of the brain [20].
Despite the increased attention that BCI technology had with
the launch of low-cost commercial EEG devices in the last
few years, BCIs are hardly used outside the laboratory
environment [16]. This is mainly due to the fact that current
BCI systems lack reliability and good performance in
comparison to other types of interfaces [15]. For instance,
MI-BCI requires long training trials per session and settings
are subject specific. As consequence, these long and
repetitive training sessions can result in user fatigue and
declining performance over time. Moreover, prolonged
training is problematic in generating EEG oscillatory
rhythms modulated during MI, such as mu (μ) and beta (β)
rhythms [28]. To solve these issues, past research in BCIs
has investigated various methodologies to create new
experimental designs [7], specialized algorithms [27] and
signal processing techniques [19] that minimize current
limitations of EEG-based BCIs.
In Human-Computer Interaction (HCI) research, there is an
increased interest in working towards novel approaches for
improving the communication bandwidth and quality of the
loop using various BCI technologies [1,30]. Unfortunately,
due to the cost and complexity, these brain sensing
technologies have a set of disadvantages for HCI research
[13]. Consequently, HCI is limited of such equipment
compared with alternative user interfaces. In the past,
researchers have explored brain-computer interfacing
technology with low-cost alternatives [4,12], with reduced

capabilities and black box algorithms. Due to the big cost of
current equipment, a large portion of previous research has
been done in the medical domain with high-end devices
costing between 20,000-250,000 USD [e.g. Biosemi
(Biosemi B.V., Amsterdam, Netherlands), g.tec (Guger
Technologies, Graz, Austria) or EGI (Electrical Geodesics,
Inc., Oregon, USA)]. Because of these limitations,
advancements in HCI research for Brain-Computer
Interaction have been hampered.
BACKGROUND

In the last few years, low-cost commercial EEG devices and
Open Source projects [36,37], are offered as alternatives to
expensive medical equipment. However, results are mixed
and it is not clear if they can deliver comparable user
experiences as compared to medical grade EEG systems.
In a recent study, it was investigated the difference in
comfort between the Emotiv EPOC headset and silver
chloride scalp discs in a P300 paradigm [14]. It was found
that the Emotiv EPOC was more uncomfortable than the
attached disks and discomfort increased over time. Another
comparative study between the Emotiv EPOC and a medical
grade headset in a P300 paradigm reported that the Emotiv
was a better in terms of price, setup process and
intrusiveness. On the other hand, the ANT system was
reported to be more comfortable, cheaper to maintain and
more durable [6]. A usability comparison between four
commercially oriented EEG systems: B-Alert, Emotiv
EPOC, Biosemi’s ActiveTwo and QUASAR’s Dry Sensor
Interface, revealed that overall in (i) the adaptability for
different head sizes, (ii) comfort and preference, (iii)
variance for the recording scalp locations for the recording
electrodes , (iv) the stability of the electrical connection and
(v) the integration between the EEG system and stimulus
presentation, participants preferred the B-Alert system [5].
In MI, a new comparative study between the Emotiv EPOC
and the Biosemi ActiveTwo system, showed that
performance is comparable between the same number of
sensors and sensor positions for a three class MI [17]. Many
studies have investigated the usability of BCI applications as
a whole. Nijboer et al, investigated the acquisition
component and compared the usability of three different
EEG headsets (Biosemi, Emotiv EPOC and g.Sahara) in a
P300-paradigm including also classification score
information [21].
Overall, most of the comparative studies have used the P300
paradigm but similar information between different headsets
in SSVEP or MI is limited. MI-BCI training is based on
visuo-motor imagination and together with other mental task
imagination (e.g. mental subtraction, word association)[8] is
the only paradigm of endogenous nature that does not require
external stimulation but only the user’s imaginative action.
In addition, MI is considered the most important type of BCI
paradigm for motor function restoration. Results from
previous studies have proven mental practice of action to be
useful in MI-BCI [25], and have shown beneficial effects of

motor imagery practice during stroke recovery [24].
Unfortunately, an estimated 15-30% of people cannot use a
BCI system, resulting in a big amount of BCI illiteracy in the
user base [31].
In this paper, our main focus is on the MI-BCI paradigm
because it is self-paced, and also because of its utilization in
the health domain. Our target is to demonstrate that braincomputer interaction throughput in non-expert users is not
technology related but user related and it can be
accomplished without requiring such high-end and high-cost
devices.
To this end we performed a (1) usability assessment
following the same protocol as a previous study using the
P300 paradigm [21] in order to have comparable results, and
(2) by performing a cost-effectiveness analysis of all tested
EEG systems from both BCI studies, in two different
paradigms (P300 and MI). For that purpose, a pilot study
with 8 non-expert participants using 3 different EEG
systems, ranging from an open-source project, commercial
system for gaming, to medical certified systems, and a total
of 24 BCI training sessions, was conducted.
METHODOLOGY
Participants

8 users (mean age of 29 ± 4.9 years old, all male) were
recruited as a voluntary sample, based on their motivation to
participate in the study. All participants were right handed
with no previous known neurological disorder, nor previous
experience in BCIs. All participants were University students
and academic staff. Finally, all participants provided their
written informed consent before participating in the userstudy.
Experimental Design

The experiment followed a within-subject design, with each
participant taking part in overall three BCI training sessions,
one per day, by using a different headset on each session in
a randomized order. Before the first session, informed
consent was obtained and demographical information was
collected. At the beginning of each session, a BCI headset
was applied by the experimenters, who logged the time (in
minutes) it took from the conductive gel application to the
moment that good EEG signals were achieved. Participants
then were asked about their perceived setup time (in minutes)
and to answer a set of usability questions before starting the
experiment.
Experimental Setup

The experimental setup was composed by a desktop
computer (OS: Windows 8.1, CPU: Intel® Core™ i5-4440
at 3.3 GHz, RAM: 8GB DDR3 1600MHZ, Graphics: Nvidia
GT 630 1GB GDDR3), running the BCI training task. In
addition the Vuzix iWear VR920 (Vuzix, NY, USA) head
mounted display (HMD) was used by the participants in
order to focus their attention on the training and prevent any
external visual stimulation from the environment. The HMD

is made of two 640x480 twin LCD displays, 32-degree field
of view (FOV), 3/4" eye relief and 5/16" eye box.
The BCI set up comprised of 3 EEG systems. The spatial
distribution of the electrodes followed the 10-20 system
configuration [35] with the following electrodes over the
somatosensory and motor areas: Frontal-Central (FC5, FC6),
Central (C1, C2, C3, C4), and Central-Parietal (CP5, CP6) as
illustrated in Figure 1. All three headsets connected via
bluetooth to the desktop computer for the EEG signal
acquisition. Data filtering and classification was performed
through the OpenVibe platform [26]. The RehabNetCP [32]
software was used to mediate between the openBCI system
and OpenVibe via the Lab Streaming Layer protocol (LSL).
For all EEG data, a Common Spatial Patterns (CSP) filter
was used, and the classification of motor-imagery actions
from the extracted EEG features were determined through a
Linear Discriminant Analysis (LDA).
Open Source System

The Open-Source BCI system (see Figure 2a) is based on the
ADS1299 Analog-to-Digital Converter (ADC) developed by
Texas Instruments (TI, Dallas, Texas, United States) [38].
This system provides 8 EEG channels operating at sample
rates between 250 and 16000 Hz, with a resolution of 24 bits
per channel. The current prototype operated at 250 Hz. An
ATmega328 Arduino UNO board was used to sample the
ADC board, and for data transmission based on the first
OpenBCI V1 data format. The cost for all components and
electrodes for the complete system is calculated at 211 euro
including VAT.
Enobio 8

Enobio (Neuroelectrics, Barcelona, Spain) is a wearable,
wireless EEG sensor with 8 EEG channels and a triaxial
accelerometer, for the recording and visualization of 24 bit
EEG data at 500 Hz (see Figure 2 b). Enobio is a CE
medically certified product and it is currently classified as an
investigational device under US federal law [39]. The cost of
the system including VAT is calculated at 6150 euro.
g.MOBIlab+

The g.MOBIlab+ biosignal amplifier (g.tec, Graz, Austria) is
a wireless EEG system, composed of 8 active EEG
electrodes (see Figure 2 c) equipped with a low-noise bio-

Figure 1. Electrode configuration used for the experiment
based on the 10-20 system. Electrodes are placed over the
motor and somatosensory cortices and reference electrode
at the left ear lobe.

signals amplifier and a 16-bit A/D converter at 256 Hz [9].
The cost including VAT is estimated at 9696 euro.
BCI Training

The BCI training was based on the Graz-BCI paradigm [23]
with directional arrows feedback. When an arrow appears on
the screen, the user has to perform a mental rehearsal of a
motor task such as grasping, throwing or waving with the
corresponding hand. The action selected for mental imagery
needs to be constant during the whole duration of the training
session in order to train a linear classifier to distinguish
successfully left from right hand imagery. Each participant
went through 3 complete training sessions followed by 3
online sessions (1 set per day for each headset) within one
week. On each session, the participant had to perform 20
repetitions per class (left or right) of a 30 seconds baseline
measurement followed by cue based motor-imagery training.
The cue duration (using a unidirectional arrow) lasted for 4
seconds and was followed by 1.5 second pause. After the
completion of the training session, a 5 minute rest was
followed by an online MI-BCI session with the trained
classifier. The classification performance of the offline
session quantifies the ability of the classifier to distinguish
the two classes (left and right hand imaginary) with crossvalidation -based error estimation. In the online session, the

Figure 2. From left to right, the openBCI system (a.i) with snap-on type of electrodes using a neoprene cap (a,ii), the Enobio
system (b.i) attached in the back of a neoprene cap (b.ii), and the gMOBIlab+ system (c.i) with active electrodes (c.ii)

Figure 3. LDA classification performance. (a) Classification score between the two classes from the training data, (b)
classification score of the two classes from a new dataset during the online session.

classifier needs to identify the two classes from a new stream
of data that is acquired online by the user when trying to
perform mental imagery within a specific time window.
Finally, for all 3 sessions, from 8 participants, 24 EEG
datasets were gathered and analyzed.
Questionnaires

Prior to the BCI training session, demographic data of the
participants were collected together with a handedness
assessment through the Edinburgh inventory [22]. After each
setup, participants completed a usability questionnaire (used
in a similar usability study [21]) for comparison. On this
questionnaire, participants were asked to estimate the
number of minutes it took for the headset to be setup (from
the moment of electrode placement until the decision of the
experimenter that signals were good). Then, they proceeded
to rate on a 7-point Likert scale the ‘speed of setup’ (1 = very
fast, 7 = very slow), level of ‘comfort’ of the headset (1 =
very comfortable, 7 = very uncomfortable), and ‘ease of
setup’ (1 = very easy, 7 = very difficult). Finally, the NASA
Task Load Index (TLX) questionnaire [11] was used after
each session in order to assess the perceived workload to use
each EEG headset in terms of Mental Demand, Temporal
Demand, Physical Demand, Performance, Effort and
Frustration in a likert scale with 21 points (1 = very low, 21
= very high).
RESULTS

What is the Effectiveness of High-End EEG-based BCI
devices?
In this study, effectiveness was measured in terms of
performance - as objectively assessed by the classification
accuracy during motor imagery task - and subjectively
through the reported workload and the usability reports.

Performance

Classification Performance was computed as the success rate
of the correct recognized classes of the training data and also
the classifier performance during online task with the use of
new data. Mean classification accuracy across participants
and conditions was used for statistical analysis through a
repeated measure ANOVA since the data was normally
distributed as indicated by the Shapiro-Wilk Test.
Training

A statistically significant difference was found between the
different headsets from the training data (F(1.370, 9.590) =
21.112, p < 0.005). Post hoc tests using the Bonferroni
correction revealed that openBCI (M = 56.2, SD = 2.3)
performed significantly worse (p<0.05) than Enobio (M =
67.8, SD = 3.4) and g.tec (M = 65.6, SD = 4.8) (Figure 3 a).
Enobio and g.tec had no significant differences.
Task

We observed that the classifier performance with the new
data acquired during the online task dropped for all headsets.
We found no statistically significant main effect of BCI
headset in performance (F(1.997, 13.980) = 16.695, p =
0.563). The highest mean performance was achieved by the
g.MOBIlab+ system (M = 51.9 %, SD = 4.2%), followed by
the openBCI system (M = 50.5%, SD = 4%), and finally the
Enobio system (M = 49%, with the highest data variability
SD = 6.6%) (Figure 3 b).
Workload

To assess how different headset technology may affect the
perceived task workload required to perform the MI task we
used the reports from the TLX questionnaire. We found
again no significant main effect between the three conditions
(F(1.679, 11.756) = 0.694, p = 0.495), nor in overall
workload score as derived from the weighted sum of the TLX

domains (Mental Demand, Temporal Demand, Physical
Demand,
Performance,
Effort
and
Frustration).
Nevertheless, the openBCI system had the highest score in
Temporal Demand (M = 8.8, SD = 4.7), Performance (M =
11, SD = 3.2) and Frustration (M = 8.4, SD = 5). Enobio
scored the highest in Mental (M = 12.7, SD = 3.7) and
Physical Demand (M = 6.7, SD = 2.9). Finally, g.MOBIlab+
scored the highest in Effort (M = 12.5, SD = 2.8) (Figure 4).
Usability

Friedman’s analysis showed no significant effect of type of
headset in any of the usability questions (see Table 1). The
scores obtained were the following: for speed of setup (1-7)
the mean value was M = 5, for all headsets; for ease of setup
(1-7) Enobio and g.tec scored higher (M = 6) than openBCI
(M = 5); for comfort (1-7), g.MOBIlab+ was the highest (M
= 6) over the other two (M = 5). Finally, on appearance (110), g. MOBIlab+ scored the lowest (M = 3) and openBCI
and Enobio had a higher score (M = 6) (see Table 1 a).
Cost-Effectiveness Analysis

The concept of cost-effectiveness is used in medical decision
making and can be illustrated graphically on the costeffectiveness (CE) plane [3]. The CE plane provides a
geometrical interpretation of relative cost-effectiveness in
terms of their assessed performance (see Figure 5).
Typically, one or more new strategies are compared against
an existing standard. Since there is no standard available for
EEG systems, a within system comparison was performed
with available data from the literature and the current study.
One can visualize the results of such comparisons in CE
plane (see Figure 5) in which both the MI and P300
effectiveness over cost is represented. For the sake of
comparison, we only considered the offline classification

Figure 4. Sub-components of the NASA TLX questionnaire
for obtaining task workload.

score from our study to match the available data from the
previously mentioned P300 study [21]. Additionally, we
estimated the cost of the devices reported in that study
through online search. From the calculation of the costeffectiveness ratios (CER) we found that the openBCI
system was ranked first with the lowest CER (CER = 3.76),
followed by Emotiv (CER = 6.48), Enobio (CER = 90.65),
g.MOBIlab (CER = 147.83). The g.Sahara (CER = 159.49)
and the Biosemi system (CER = 237.29) score the highest,
CER ratio.
A repeated measures ANOVA with a Greenhouse-Geisser
correction determined that mean CER differed statistically
significantly between different BCI systems in both training
(F(1.209, 8.462) = 742.410, p < 0.001) and online (F(1.779,
12.456) = 339.260, p < 0.001). Post hoc tests using the
Bonferroni correction revealed that the openBCI system was
statistically significantly better from Enobio and
gMOBIlab+ systems as well as Enobio from gMOBIlab+.
DISCUSSION

Figure 5. CE plane for cost (0-21000 euro) and effectiveness
(1-100) for the offline classification on both studies. Systems
that locate themselves further or closer from the origin (0,0)
if they are more or less effective, and above or below the
origin if they are more or less costly.

From the technology side, the effect of intrinsic variability,
low signal-to-noise ratio and non-stationarities of EEG
signals [14] may explain the low classification accuracies
obtained during task performance. The introduction of new
EEG data combined with artifact contamination in the
signals may have caused low performance of the LDA
classifier as given by the hyperplane distance. From a user
perspective, one of the biggest challenges in BCI research is
to understand and solve the problem of “BCI Illiteracy” that
is affecting an estimated 15 to 30% of the users [31]. Current
limitations are based on the inability of many users to
voluntarily modulate the amplitude of the sensory-motor
rhythm in order to control the feedback application.
Unfortunately, comparisons across different studies have
been problematic since different groups use different
performance thresholds [2]. To date, and to the best of our
knowledge, there are no similar studies that investigate the

(a)

(b)

Biosemi (32
channel)

Emotiv (14
channel)

g.Sahara (8
channel)

openBCI (8
channel)

Enobio (8
channel)

g.MOBIlab (8
channel)

Classification
accuracy

88.5% (± 18.3)

61.7% (± 25.7)

62.7% (± 37.7)

56.2 (± 2.3)

67.8 (± 3.4)

65.6 (± 4.8)

Real time to
setup

20.3 min (± 6.7)

6.9 min (± 3.3)

12.1 min (± 2.9)

6.7 min (± 2.4)

6.1 min (± 1)

4.8 min (± 0.6)

Participants’
estimation of
time to setup

14.4 min (± 5.2)

6.4 min (± 4.7)

9.7 min (± 2.9)

6.4 min (± 2.3)

5 min (± 1.5)

5.6 min (± 1.9)

Speed of setup

4

2

4

5

5

5

Ease of setup

6

3

3

5

6

6

Comfort

3

4

3

5

5

6

Appearance

6

6

4

6

6

3

Table 1. Overview of the usability scores and classification accuracy during training from this study (a) with openBCI, Enobio
and g.MOBIlab, compared to a similar P300 study [21] with different headsets and number of electrodes and placement (b)
using the Biosemi, Emotiv and g.Sahara systems.

ratio of cost effectiveness, a subjective measure through user
experience, and an objective measure which is the
classification score. From our current data, we can
distinguish a trend in different dimensions concerning the
classification performance, perceived workload, usability
and cost-efficiency. From the P300 classification (Table 1 a),
we can distinguish greater standard deviations compared
with MI and also lower scores in usability.
Unfortunately, the small sample from both studies result in a
low statistical power that may prevent capturing some
effects. Nevertheless, the fusion of two studies, involving
two BCI paradigms is a significant step towards
understanding the technology transfer and acceptance of
BCIs from non-expert users.
CONCLUSION

So far, we found no significant differences in the online
performance among the 3 EEG headsets from the set of data
derived from both subjective sources - through the
questionnaires - as well as objective data - derived from the
online performance. Given the current findings, devices
seem to have similar effectiveness and we can conclude that
there is no perceived difference in terms of comfort,
appearance, speed/ease of setup and overall workload in the
actual system performance. Hence, the low-cost openBCI
open source system is the more cost-effective BCI solution
as compared with its commercial medical grade counterparts.
The comparison in the P300 study [21] considered different
electrode configurations across systems, and a different
interaction paradigm (P300 vs MI). Although we cannot
directly compare classification scores, we observed that

regardless of the BCI paradigm, usability and CER analysis
indicate that medical grade and more expensive systems do
not necessarily add value on the experience level of the users.
Therefore, we can conclude that brain-computer interaction
performance/throughput, at least for the particular case of
non-expert users, is not technology related and it can be
accomplished without requiring high-end and high-cost
devices. Current results provide useful pointers towards
leveraging research of Brain-Computer Interaction for nonexpert users and minimizing BCI illiteracy.
FUTURE WORK

In future work, it is important to consider the factors
influencing not only user performance but also user
experience for the advancement of physiological computing
research. Future studies need to be based on similar protocols
and experimental designs in order to be comparable,
including system cost. Finally, based on our current findings,
open-source EEG systems rather than commercial low-cost
systems need to be further investigated because of their
potential as they offer high CER and feasible and effective
alternatives for BCI studies.
ACKNOWLEDGMENTS

This work is supported by the European Commission
through the RehabNet project - Neuroscience Based
Interactive Systems for Motor Rehabilitation - EC (303891
RehabNet FP7-PEOPLE-2011-CIG), by the Fundação para a
Ciência e Tecnologia (Portuguese Foundation for Science
and Technology) through SFRH/BD/97117/2013, and
LARSyS (Laboratório de Robótica e Sistemas em
Engenharia e Ciência) through UID/EEA/50009/2013.

REFERENCES

1.

Daniel Afergan, Evan M. Peck, Erin T. Solovey, et al.
2014. Dynamic Difficulty Using Brain Metrics of
Workload. Proceedings of the 32Nd Annual ACM
Conference on Human Factors in Computing Systems,
ACM,
3797–3806.
http://doi.org/10.1145/2556288.2557230
2. Brendan Z. Allison and Christa Neuper. 2010. Could
Anyone Use a BCI? In Brain-Computer Interfaces,
Desney S. Tan and Anton Nijholt (eds.). Springer
London, 35–54. Retrieved November 20, 2015 from
http://link.springer.com/chapter/10.1007/978-1-84996272-8_3
3. William C. Black. 1990. The CE Plane A Graphic
Representation of Cost-Effectiveness. Medical
Decision
Making
10,
3:
212–214.
http://doi.org/10.1177/0272989X9001000308
4. Daniel Chen and Roel Vertegaal. 2004. Using Mental
Load for Managing Interruptions in Physiologically
Attentive User Interfaces. CHI ’04 Extended Abstracts
on Human Factors in Computing Systems, ACM, 1513–
1516. http://doi.org/10.1145/985921.986103
5. W. David Hairston, Keith W. Whitaker, Anthony J.
Ries, et al. 2014. Usability of four commerciallyoriented EEG systems. Journal of Neural Engineering
11,
4:
046018.
http://doi.org/10.1088/17412560/11/4/046018
6. Matthieu Duvinage, Thierry Castermans, Mathieu
Petieau, Thomas Hoellinger, Guy Cheron, and Thierry
Dutoit. 2013. Performance of the Emotiv Epoc headset
for P300-based applications. Biomedical Engineering
Online 12: 56. http://doi.org/10.1186/1475-925X-12-56
7. André Ferreira, Athanasios Vourvopoulos, and Sergi
Bermudez I Badia. 2015. Optimizing Performance of
Non-Expert Users in Brain-Computer Interaction by
Means of an Adaptive Performance Engine. Lecture
Notes in Computer Science/Artificial Intelligence
(LNCS/LNAI), Springer.
8. Elisabeth V. C. Friedrich, Reinhold Scherer, and Christa
Neuper. 2012. The effect of distinct mental strategies on
classification performance for brain–computer
interfaces. International Journal of Psychophysiology
84,
1:
86–94.
http://doi.org/10.1016/j.ijpsycho.2012.01.014
9. g.tec. g.tec medical engineering. Retrieved September
14, 2015 from http://www.gtec.at/Products/Hardwareand-Accessories/g.MOBIlab-Specs-Features
10. Christoph Guger, Shahab Daban, Eric Sellers, et al.
2009. How many people are able to control a P300based brain–computer interface (BCI)? Neuroscience
Letters
462,
1:
94–98.
http://doi.org/10.1016/j.neulet.2009.06.045
11. Sandra G. Hart. 2006. Nasa-Task Load Index (NASATLX); 20 Years Later. Proceedings of the Human
Factors and Ergonomics Society Annual Meeting 50, 9:
904–908. http://doi.org/10.1177/154193120605000909

12. Yoshifumi Kitamura, Yoshihisa Yamaguchi, Imamizu
Hiroshi, Fumio Kishino, and Mitsuo Kawato. 2003.
Things Happening in the Brain While Humans Learn to
Use New Tools. Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, ACM, 417–
424. http://doi.org/10.1145/642611.642684
13. Johnny Chung Lee and Desney S. Tan. 2006. Using a
Low-cost
Electroencephalograph
for
Task
Classification in HCI Research. Proceedings of the 19th
Annual ACM Symposium on User Interface Software
and
Technology,
ACM,
81–90.
http://doi.org/10.1145/1166253.1166268
14. F. H. Lopes Da Silva. 1978. Analysis of EEG nonstationarities. Electroencephalography and Clinical
Neurophysiology. Supplement, 34: 163–179.
15. Fabien Lotte. 2012. On the need for alternative feedback
training approaches for BCI. Retrieved October 24,
2014 from https://hal.inria.fr/hal-00834391
16. Fabien Lotte, Florian Larrue, and Christian Mühl. 2013.
Flaws in current human training protocols for
spontaneous Brain-Computer Interfaces: lessons
learned from instructional design. Frontiers in Human
Neuroscience
7:
568.
http://doi.org/10.3389/fnhum.2013.00568
17. Juan-Antonio Martinez-Leon, Jose-Manuel CanoIzquierdo, and Julio Ibarrola. Are low cost Brain
Computer Interface headsets ready for motor imagery
applications? Expert Systems with Applications.
http://doi.org/10.1016/j.eswa.2015.11.015
18. L. Mayaud, M. Congedo, A. Van Laghenhove, et al.
2013. A comparison of recording modalities of P300
event-related potentials (ERP) for brain-computer
interface (BCI) paradigm. Neurophysiologie Clinique =
Clinical
Neurophysiology
43,
4:
217–227.
http://doi.org/10.1016/j.neucli.2013.06.002
19. Dennis J. McFarland, William A. Sarnacki, and
Jonathan R. Wolpaw. 2011. Should the parameters of a
BCI translation algorithm be continually adapted?
Journal of Neuroscience Methods 199, 1: 103–107.
http://doi.org/10.1016/j.jneumeth.2011.04.037
20. Christa Neuper, Michael Wörtz, and Gert Pfurtscheller.
2006. ERD/ERS patterns reflecting sensorimotor
activation and deactivation. Progress in Brain Research
159:
211–222.
http://doi.org/10.1016/S00796123(06)59014-4
21. Femke Nijboer, Bram van de Laar, Steven Gerritsen,
Anton Nijholt, and Mannes Poel. 2015. Usability of
Three Electroencephalogram Headsets for Brain–
Computer Interfaces: A Within Subject Comparison.
Interacting
with
Computers:
iwv023.
http://doi.org/10.1093/iwc/iwv023
22. R. C. Oldfield. 1971. The assessment and analysis of
handedness:
The
Edinburgh
inventory.
Neuropsychologia
9,
1:
97–113.
http://doi.org/10.1016/0028-3932(71)90067-4
23. G. Pfurtscheller, C. Neuper, G. R. Müller, et al. 2003.
Graz-BCI: state of the art and clinical applications.

24.

25.

26.

27.

28.

29.

30.

IEEE transactions on neural systems and rehabilitation
engineering: a publication of the IEEE Engineering in
Medicine and Biology Society 11, 2: 177–180.
http://doi.org/10.1109/TNSRE.2003.814454
Floriana Pichiorri, Giovanni Morone, Manuela Petti, et
al. 2015. Brain-computer interface boosts motor
imagery practice during stroke recovery. Annals of
Neurology. http://doi.org/10.1002/ana.24390
Girijesh Prasad, Pawel Herman, Damien Coyle,
Suzanne McDonough, and Jacqueline Crosbie. 2010.
Applying a brain-computer interface to support motor
imagery practice in people with stroke for upper limb
recovery:
a
feasibility
study.
Journal
of
NeuroEngineering and Rehabilitation 7, 1: 60.
http://doi.org/10.1186/1743-0003-7-60
Yann Renard, Fabien Lotte, Guillaume Gibert, et al.
2010. OpenViBE: an open-source software platform to
design, test, and use brain-computer interfaces in real
and virtual environments. Presence: teleoperators and
virtual environments 19, 1: 35–53.
B. Rivet, A. Souloumiac, V. Attina, and G. Gibert.
2009. xDAWN Algorithm to Enhance Evoked
Potentials: Application to Brain #x2013;Computer
Interface. IEEE Transactions on Biomedical
Engineering
56,
8:
2035–2043.
http://doi.org/10.1109/TBME.2009.2012869
Donald L. Schomer and F. H. Lopes da Silva. 2011.
Niedermeyer’s
Electroencephalography:
Basic
Principles, Clinical Applications, and Related Fields.
Lippincott Williams & Wilkins.
Donald L. Schomer and F. H. Lopes da Silva. 2011.
Niedermeyer’s
Electroencephalography:
Basic
Principles, Clinical Applications, and Related Fields.
Lippincott Williams & Wilkins.
Daniel Sjölie, Kenneth Bodin, Eva Elgh, Johan
Eriksson, Lars-Erik Janlert, and Lars Nyberg. 2010.
Effects of Interactivity and 3D-motion on Mental
Rotation Brain Activity in an Immersive Virtual
Environment. Proceedings of the SIGCHI Conference
on Human Factors in Computing Systems, ACM, 869–
878. http://doi.org/10.1145/1753326.1753454

31. Carmen Vidaurre and Benjamin Blankertz. 2010.
Towards a cure for BCI illiteracy. Brain Topography
23, 2: 194–198. http://doi.org/10.1007/s10548-0090121-6
32. Athanasios Vourvopoulos, Ana Lucia Faria, Monica S
Cameirao, and Sergi Bermudez i Badia. 2013.
RehabNet: A distributed architecture for motor and
cognitive neuro-rehabilitation. 2013 IEEE 15th
International Conference on e-Health Networking,
Applications
Services
(Healthcom),
454–459.
http://doi.org/10.1109/HealthCom.2013.6720719
33. Yijun Wang, Xiaorong Gao, Bo Hong, Chuan Jia, and
Shangkai Gao. 2008. Brain-Computer Interfaces Based
on Visual Evoked Potentials. IEEE Engineering in
Medicine and Biology Magazine 27, 5: 64–71.
http://doi.org/10.1109/MEMB.2008.923958
34. Jonathan R Wolpaw, Niels Birbaumer, Dennis J
McFarland, Gert Pfurtscheller, and Theresa M
Vaughan. 2002. Brain-computer interfaces for
communication and control. Clinical neurophysiology:
official journal of the International Federation of
Clinical Neurophysiology 113, 6: 767–791.
35. 1958. Report of the committee on methods of clinical
examination in electroencephalography: 1957.
Electroencephalography and Clinical Neurophysiology
10,
2:
370–375.
http://doi.org/10.1016/00134694(58)90053-1
36. OpenBCI. Retrieved September 16, 2015 from
http://www.openbci.com/
37. Welcome to the OpenEEG project. Retrieved
September
16,
2015
from
http://openeeg.sourceforge.net/doc/
38. ADS1299 | Medical Analog Front End | Analog Front
End (AFE) | Description & parametrics. Retrieved
September
14,
2015
from
http://www.ti.com/product/ADS1299/description
39. Products / ENOBIO / ENOBIO 8. Neuroelectrics.
Retrieved
September
14,
2015
from
http://www.neuroelectrics.com/products/enobio/enobio
-8/

