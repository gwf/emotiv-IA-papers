Insight2OSC: using the brain and the body as a musical
instrument with the Emotiv Insight
Constanza Levicán, Andrés Aparicio, Vernon Belaunde, Rodrigo F. Cádiz
Center for Research in Audio Technologies
Music Institute
Pontificia Universidad Católica de Chile
Santiago, Chile
{cglevican,aaparicio,vabelaunde,rcadiz}@uc.cl

ABSTRACT
Brain computer interfaces are being widely adopted for music creation and interpretation, and they are becoming a truly new category of musical instruments. Indeed, Miranda has coined the term
Brain-computer Musical Interface (BCMI) to refer to this category.
There are no "plug-n-play" solutions for a BCMI, these kinds of
tools usually require the setup and implementation of particular
software configurations, customized for each EEG device. The
Emotiv Insight is a low-cost EEG apparatus that outputs several
kinds of data, such as EEG rhythms or facial expressions, from the
user’s brain activity. We have developed a BCMI, in the form of
a freely available middle-ware, using the Emotiv Insight for EEG
input and signal processing. The obtained data, via blue-tooth is
broad-casted over the network formatted for the OSC protocol. Using this software, we tested the device’s adequacy as a BCMI by
using the provided data in order to control different sound synthesis algorithms in MaxMSP. We conclude that the Emotiv Insight is
an interesting choice for a BCMI due to its low-cost and ease of
use, but we also question its reliability and robustness.
—

Author Keywords
BCI, BCMI, EEG, Emotiv Insight

ACM Classification
H.5.5 [Information Interfaces and Presentation] Sound and Music
Computing.

1.

INTRODUCTION

A brain-computer interface (BCI) is a physiological computing
system, specialized and designed to operate based on brain activity [7]. Hans Berger was the first person who measured waves from
the brain in 1924, using a EEG device [11]. Neural activity often
possesses a repetitive rhythmic quality, and these rhythms are classified according to their fundamental frequencies and are denoted
as alpha, beta, theta or delta waves [15].
The fact that a machine can read signals from the brain has boosted
the imaginations of musicians, engineers, scientists, artists and
other enthusiasts, and EEG has made its way into applications
in several realms, including music [11]. Many musicians and researchers dream with a day when musical ideas could be transmitted by simply making musical thought audible, an ideal performance without any physical limitations, where the performer plays

Licensed under a Creative Commons Attribution
4.0 International License (CC BY 4.0). Copyright
remains with the author(s).
NIME’17, May 15-19, 2017, Aalborg University Copenhagen, Denmark.

with the expressiveness imagined in his mind [8]. In other words,
musicians want to transform a BCI system into a musical instrument. In 1949, Raymond Scott wrote: “Perhaps within the next
hundred years, science will perfect a process of thought transference from composer to listener. The composer will sit alone on the
concert stage and merely think his idealized conception of his music. Instead of recordings of actual music sound, recordings will
carry the brainwaves of the composer directly to the mind of the
listener” [6]. Now, nearly a century after Berger’s discovery, these
dreams are becoming a reality [16].

1.1

BCMI

Figure 1: The schematics of a BCMI system. Adapted from
[12, p.225].
A BCMI system is a BCI which returns a musical output. Figure
1 represents the steps of a BCMI system as described by Miranda
[12, p. 227]. This systems contains the following stages:
1. EEG Input: EEG data recordings from the scalp.
2. Signal processing: amplification, analysis and classification
of the data.
3. Transformation Algorithm: transforms the non-musical information into musical parameters.
4. Musical Engine: the system where musical parameters are
turn into music.
5. Audio/Visual Stimuli (optional): it provides neuro-feedback
to the user.
Miranda clearly identifies three separate ways in which such systems can be used for musical purposes: “When looking back on
research into music and brainwaves, we can separate systems into
three categories: ones for EEG sonification, ones for EEG musification and ones for BCI control. EEG sonification is the translation of EEG information into sound, for non-musical and predominantly medical purposes. EEG musification is the mapping of EEG
information to musical parameters; however, the EEG data are arbitrary and when possible can offer only loose forms of control.
BCI control is inherent in systems where direct cognitive real-time
control of music is achievable.” [12, p. 227]
Most of the attempts to use BCI for musical purposes reported by
the literature sonify EEG signals directly [15] [17] [4] [10], but
others have developed other approaches, such as the rendering of
musical chords with organic nuances [8], real-time notation [3] [2],
networked musical performances [1], and the study of the differ-

287

ences between music imagery an music perception [13].

2.

EMOTIV INSIGHT

The Emotiv Insight is a portable EEG which provides a wireless
connectivity with computers and smartphones. It has a free software tool called Emotiv Xavier Control Panel compatible with
Windows, OSC, Linux, Android and iOS. This program offers
a brain-computer interface which detects emotional and mental
states and also facial expressions in real-time. However, there is
no access to the raw data collected by the device through this tool.
Nonetheless, Emotiv provides a SDK with several examples that
can be used to obtain information from the equipment for different
purposes.
The headset has seven electrodes: two electrodes for the ground
and noise reduction and five for scalp data measuring. According
to the 10-20 International system, the names of the electrodes that
the Insight has are: AF3, AF4, T7, T8 and Pz, as they are placed in
the frontal lobe (frontal cortex), temporal lobe (parietal-temporal
lobe) and parietal lobe (parietal-occipital cortex).

Figure 2: Emotiv Insight: stealth black model. (Image obtained from http://www.emotiv.com) One of the most notable
features if this headset is the usage of dry electrodes.
The Emotiv Insight has several features which make it an interesting candidate for a BCMI. Among them, we can list the following:
1. It has five channels of EEG data, more than others commecial devices.
2. It include two reference’s electrodes for noise cancellation.
3. Each channel has a data transmission rate of 128 Hz.
4. It has dry electrodes that do not need further preparation of
be wet with saline solutions.
5. It has a blue-tooth connection which allows receiving of the
data directly on a computer free of cables
6. It uses an internal lithium battery with minimum 4 hours of
duration.
7. It includes three inertial sensors: accelerometer, gyroscope
and magnetometer.
8. It costs around USD 300 and it is relatively easy to acquire,
directly from Emotiv’s website 1 .
9. Emotiv provides a free SDK on github with several examples for developers.
10. The SDK allows access for EEG rhythms, facial expressions
and inertial sensors.
Despite these nice features, we have found that the device has some
problems that can impair a BCMI system and cause a degraded
performance when compared to other similar tools. These disadvantages are:
1. The channel connections are unstable and every time that a
disconnection happens the device sends wrong data from the
electrodes.
2. The headset has a standard size and it does not work well
with all people. Every user has a different head size and
the apparatus did not fit correctly for all the members of our
team.
3. The appliance presents "pop-up" artifacts. These are errors
that occur when the electrode misses the contact with the
scalp and suddenly sends incorrect measurements [14].
All these problems manifested in all of the three headsets we own.
1

www.emotiv.com

3.

INSIGHT2OSC

We developed Insight2OSC, a free middle-ware 2 that receives data
from the Emotiv Insight device in real-time using his blue-tooth
connection and sends it through the network using the OSC protocol. Insight2OSC is based on some of the examples that are provided by Emotiv’s SDK. Not all of these examples are written in
the same programming language, and they send just one type of
data (as discussed in section 3.1) in a sequential way. Besides the
examples that we discuss in section 4, our software includes several advantages:
1. Modularity: all routines are contained in the same file, but
they called separately.
2. Parallelism and speed: one of the main features of our tool is
the efficiency and speed of data streaming. We used threads,
with a high level of parallelism, allowing us to send all the
data available in the Emotive Insight device concurrently.
3. Usability: our software runs on Windows or MacOS in standalone mode.
4. User experience: our middle-ware is user-friendly and very
simple to use. Additionally, the software displays the connection’s status of all electrodes using the color scheme detailed in table 1.
5. Anti "pop-up": in order to prevent considerable discontinuities in the data due to "pop-up" artifacts, the average
band power of the electrodes is taken into consideration only
when they have a good quality of connection with the skin.
When the software starts, it searches for an Emotiv Insight via
blue-tooth or waits for a device to connect. Once connected, it
will automatically start sending data to the IP and Port that it has
by default, as figure 3 displays. The IP and port can be changed
directly from the main window as shown in figures 4 and 5.

Figure 3: The flow of data in Insight2OSC. Data obtained via
blue-tooth is sent through the network using the OSC protocol,
to an audio software such as MaxMSP.
The Insight2OSC has two versions: Windows (shown in figure 4)
and OSX (shown in figure 5).
Color

Quality of connection
None
Bad
Poor
Good

Table 1: Color scheme for the connection quality of the device
electrodes.

2

Available from https://github.com/rcadiz/Insight2OSC.git

288

Figure 4: Insight2OSC main window for Windows. IP address
and port can be changed and electrode status are displayed
using a color scheme.

Figure 6: Emotiv Insight data and its OSC syntax.

4.

APPLICATIONS

We also developed four musical applications using the widely used
sound processing environment MaxMSP3 . All the described applications create sound based on the data described in section 3.1.
These applications can be seen in the accompanying video abstract.

4.1
Figure 5: Insight2OSC main window for OSX. IP address and
port can be changed and electrode status are displayed using a
color scheme.

3.1

Data

Insight2OSC sends all available data from the Emotiv Insight, as
follows:
1. Inertial sensors: the Emotiv Insight includes three inertial
3D sensors: gyroscope, accelerometer and magnetometer,
therefore, it sends a list of 3 decimal numbers for each sensor
(X, Y, Z)
2. Headset Status: a whole number for each headset measurement, including the percentage of battery (1-100), the signal
level of the wireless connection (1-4), the time of connection
(>1), the connection’s status of all electrodes (1-5).
3. Average band power: a decimal number corresponding to
the power of frequency bands: alpha, beta low, beta high
and gamma, for each electrode.
4. Mental states: a decimal number that represents the percentage of stress, relaxation, engagement, interest and focus of
the person.
5. Facial expressions: a 0 or 1 if the person had recently blinked
or winked, a decimal number representing the percentage of
surprise, furrow, smile and clench of the face expressions.
Figure 6 shows the labeling scheme we use for all the data that
is obtained from the Insight2OSC device according to the OSC
protocol syntax.

Direct mapping of EEG rhythms

The electrical signals of the brain present several spontaneous oscillatory signals [9]. For this reason, brain activity could be classified depending on different frequency bands called EEG rhythms
or brainwaves. Delta rhythm (0-4 Hz) is not present in normal and
waking adult, theta rhythm (4-7 Hz) is low in waking adults [14].
The most used rhythms on BCI system are alpha (8-13 Hz) and
beta (14-20 Hz) [9]. Alpha is higher in a relaxed situation [14]. In
particular, the alpha rhythm is higher when a person is in a waking,
relaxed or eye-closed state [14] [5] and it is lower when the subject
is in an alert state or receiving visual stimuli [5]. beta rhythm is
present when a person moves because is part of the sensorimotor
rhythms as well as the gamma rhythm (30 and more Hz) [16] [9].
It is very hard to visualize the raw signal due to the oscillations
between 0 Hz and 100 Hz that it contains. One of the ways to
study the signal is with a frequency domain analysis. Therefore, in
any BCI it is crucial to have all the frequency bands power.
The sonification of EEG rhythms is a useful method to analyze the
electrodes raw signal due to the fact that it can create very easily
distinguishable sounds from each other.
Additionally, it appears that the musification of EEG rhythms is an
interesting approach for live performances. Perhaps the most famous one is the 1976 performance by Alvin Lucier, who mapped
directly the power of his alpha rhythm in a live performance called
Music for Solo Performer [12]. After that, several artists have
mapped their EEG activity in real-time on their performances.
In the attached video abstract, we show the usage of the power of
the alpha rhythm when mapped to control the overall dynamic of a
sound. It is possible to appreciate a decreasing of the total volume
when going into a more alerted state.

4.2

Body sensors

The Emotiv Insight provides the measurements of three inertial
sensors: gyroscope, accelerometer and magnetometer. These sen3

http://www.cycling74.com

289

sors give information about all types of head movements. The interesting part is that sensor measurements gives really accurate values, with a very small margin of error. It allow us to notice the difference between fast or slow movements, or even detect a turning
head movement or an straight one. These values are completely
manipulable by the user and have many musical applications.
In the video abstract, we propose a musification of the 3-axis gyroscope, used to control the pitch, timbre and dynamics of a sound.
As gyroscope were used, there is no sound when the user is standing still.

4.3

Neuroscience-based input

A user-orientated BCMIs is an interface created to deduce the user’s
intentions [12]. They are based on advanced neuroscience knowledge and digital signal processing. In this case, we propose a userorientated BCMI system using the power of its beta rhythm.
Sensorimotor rhythms (SVMs) are oscillations of the electric field
which could be recorded by the EEG, and include mu (8-12 Hz),
beta (18-20 Hz) and gamma (30-200 Hz) [16] [9]. The SVM are
related to real and imaginary movements [16]. When a person
moves some SVMs increase and others decrease. The first process is called event-related synchronization (ERS) and the second
is called event-related desynchronization (ERD) [16].
For example, when the user moves there is an increase in the power
of the beta rhythm as a result of ERS. In consequence, a peak
occurs one second after the onset of the movement. In the video
abstract, we identify the peak of a beta rhythm in order to control
the pitch and timbre of different sounds.

4.4

Facial expressions

Facial expressions produce artifacts in the EEG signals [14]. In
consequence, BCI systems should be implemented with complex
digital signal processing algorithms in order to eliminate them.
Nevertheless, it is not always necessary to eliminate them. We
think that using these artifacts might be something interesting for
musical reasons. The Emotiv Insight provides facial expressions,
which are estimated using internal signal processing algorithms.
The provided facial expressions are: blink, wink left, wink right,
surprise, furrow, smile and clench.
In the video abstract, we propose the musification of facial expression in order to alter some aspects of the sound. In particular, we
use smiles to increase the tempo of a steady rhythm.

5.

CONCLUSIONS

We have developed a simple tool that allows accessing all available
data provided by the Emotiv Insight device, and routing it to any
OSC-enabled audio software such as MaxMSP. This data includes
power of EEG rhythms, facial expressions, mental states and inertial sensors. Our software has two versions: Windows and OSX.
We want to conceive the Emotiv Insight appliance as a musical
instrument. Our motivation is to allow musicians to transform this
data into musical parameters in order to use it for musification and
build their BCMI. In addition, our BCMI allows for any user to
close the neuro-feedback loop (figure 1) by training himself using
our musification.
As the Emotiv Insight provides a considerable amount of data with
a great sample-rate, this middle-ware provides a great number of
possibilities for audio-visual performances in real-time. We believe that Emotiv Insight device is an interesting choice for BCMI
but we also question its reliability and robustness.
To strengthen our appreciation, we present four musical applications based on several types of data: the information from the inertial sensors, facial expressions, direct mapping of EEG rhythms
and the detection of the user’s body movement via peaks in the
power of the beta rhythm.

6.

ACKNOWLEDGEMENTS

This research was partially funded by Fondecyt Grant #1161328,
and Dirección de Artes y Cultura, Vicerrectoría de Investigación,
Pontificia Universidad Católica de Chile.

7.

REFERENCES

[1] A. D. Brouse. The Interharmonium: an investigation into
networked musical applications and brainwaves. PhD
thesis, McGill University, 2004.
[2] R. F. Cádiz and P. de la Cuadra. Kara: a BCI approach to
composition. In Proceedings of the International Computer
Music Conference and Sound Music Computing
(ICMC|SMC), pages 350–354, Athens, Greece, 2014.
[3] J. Eaton and E. Miranda. Real-time notation using brainwave
control. In Sound and Music Computing Conference, 2013.
[4] M. Elgendi, B. Rebsamen, A. Cichocki, F. Vialatte, and
J. Dauwels. Real-time wireless sonification of brain signals.
In Advances in Cognitive Neurodynamics (III), pages
175–181. Springer, 2013.
[5] R. I. Goldman, J. M. Stern, J. Engel, and M. S. Cohen.
Simultaneous EEG and fMRI of the alpha rhythm.
Neuroreport, physiology,Parietal Lobe,Parietal Lobe:
physiology,Thalamus,Thalamus: physiology,
13(18):2487–92, 2002.
[6] M. Grierson. Composing with brainwaves: minimal trial
p300b recognition as an indication of subjective preference
for the control of a musical instrument. In Proceedings of
International Computer Music Conference (ICMC ’08),
2008.
[7] H. Gurkok and A. Nijholt. Affective brain-computer
interfaces for arts. In Affective Computing and Intelligent
Interaction (ACII), 2013 Humaine Association Conference
on, pages 827–831. IEEE, 2013.
[8] T. Hamano, T. M. Rutkowski, H. Terasawa, K. Okanoya,
and K. Furukawa. Generating an integrated musical
expression with a brain–computer interface. In Proceedings
of the New Interfaces for Musical Expression Conference
(NIME 2013), pages 49–54, 5 2013.
[9] B. He, S. Gao, H. Yuan, and J. R. Wolpaw. Brain Computer
Interfaces. In B. He, editor, Neural Engineering, chapter 2,
pages 87–152. Springer US, 2 edition, 2013.
[10] T. Hinterberger. Orchestral sonification of brain signals and
its application to brain-computer interfaces and performing
arts. In Proceedings of the 2nd International Workshop on
Interactive Sonification (ISon 2007). York, UK. Online,
2007.
[11] E. R. Miranda. Brain-computer music interface for
composition and performance. International Journal on
Disability and Human Development, 5(2):119–126, 2006.
[12] E. R. Miranda. Guide to Brain-Computer Music Interfacing.
Springer, 2014.
[13] F. Olthuis. EEG differences between music imagery and
music perception. In 15th Twente Student Conference,
Enschede, The Netherlands, 6 2011.
[14] A. J. Rowan and E. Tolunsky. Primer of EEG. Butterworth
Heinemann, 2003.
[15] A. Stella. Auditory Display of Brain Oscillatory Activity
with Electroencephalography. PhD thesis, Universidad
Pompeu Fabra, Barcelona, 2012.
[16] J. R. Wolpaw and E. W. Wolpaw. Brain–computer
interfaces: something new under the sun, pages 3–12.
Oxford University Press, New York, 2012.
[17] D. Wu, Y. Liu, and D.-Z. . Z. Yao. Listen to the brain in real
time - the chengdu brainwave music. Journal of Electronic
Science and Technology of China, 7(1), 3 2009.

290

