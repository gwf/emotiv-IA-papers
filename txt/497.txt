bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

1
2
3

Brain, music and emotion:

4

An EEG proof-of-concept study on musically continuous, non-personalized emotional

5

responses

6
7
8
9

Efthymios Papatzikis1¶* and Anri Herbst2&

10
11
12

1

Faculty of Communication, Arts and Sciences, Canadian University Dubai, Dubai, U.A.E.

13

2

South African College of Music, University of Cape Town, Cape Town, South Africa

14
15
16

* Corresponding author:

17

E-mail: efp331@mail.harvard.edu

18
19
20

¶

21

&

E.P. designed the protocol, collected and analysed the data, prepared the manuscript.
A.H. designed the protocol, collected the data, and contributed to parts of the manuscript.

1

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

22
23

Abstract

24

It has been repeatedly reported that motivation for listening to music is majorly driven by

25

the latter’s emotional effect. There is a relative opposition to this approach, however,

26

suggesting that music does not elicit true emotions. Counteracting this notion, contemporary

27

research studies indicate that listeners do respond affectively to music providing a scientific

28

basis in differentially approaching and registering affective responses to music as of their

29

behavioral or biological states. Nevertheless, no studies exist that combine the behavioral

30

and neuroscientific research domains, offering a cross-referenced neuropsychological

31

outcome, based on a non-personalized approach specifically using a continuous response

32

methodology with ecologically valid musical stimuli for both research domains. Our study,

33

trying to fill this void for the first time, discusses a relevant proof-of-concept protocol, and

34

presents the technical outline on how to multimodally measure elicited responses on evoked

35

emotional responses when listening to music. Specifically, we showcase how we measure

36

the structural music elements as they vary from the beginning to the end within two different

37

compositions, suggesting how and why to analyze and compare standardized, non-

38

personalized behavioral to electroencephalographic data. Reporting our preliminary

39

findings based on this protocol, we focus on the electroencephalographic data collected from

40

n=13 participants in two separate studies (i.e., different equipment and sample background),

41

cross-referencing and cross-validating the biological side of the protocol’s structure. Our

42

findings suggest (a) that all participants – irrespectively of the study – reacted consistently

43

in terms of hemispheric lateralization for each stimulus (i.e., uniform intra-subjective

44

emotional reaction; non-statistically significant differentiation in individual variability) and

45

(b) that diverse patterns of biological representations emerge for each stimulus between the
2

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

46

subjects in the two studies (variable inter-subjective emotional reaction; statistically

47

significant differentiation in group variability) pointing towards exogenous to the

48

measurements process factors. We conclude discussing further steps and implications of our

49

protocol approach.

50

Introduction

51

Research studies indicate that listeners respond affectively to music, often

52

associating basic or primary emotions such as happiness, sadness, fear and anger with

53

musical stimuli [for example, 1-20]. Konečni [21-23] adds the sublime as a further category

54

of evoked responses, while it has also been found that listeners clearly react to music’s

55

specific structural elements (according to the level of their musical expertise [24] to form a

56

final emotionally charged evoked response [25, 26]. It must be noted that any emotional

57

association with or evoked response to music starts early in life. Even three-year-old

58

children are capable of associating musical excerpts with emotions [27, 28].

59

Although a scientific basis for approaching and registering affective responses to

60

musici mostly exists with specific focus on either their behavioral or biological states

61

separately (for example the MRI study by Koelsch et al. [29]; or Küssner & Koelsch [30]),

62

it is rare – if existing at all – to find studies that have combined the behavioral and

63

neuroscientific research domains, providing a cross-referenced neuropsychological and

64

non-personalised outcome that successfully connects standardized semantic emotion labels

65

with personal perception to biology, specifically using a continuous response methodology

66

with ecologically valid musical stimuli of the same cohort for both research domains. It is

67

even rarer for studies to follow the aforementioned path using specifically the EEG modality

68

in their experimental investigations [10, 11, 31-36].

3

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

69

Background

70

In providing a larger context for this study, Altenmüller et al. [11] used cortical

71

direct current electroencephalography (dc-EEG; focusing on bands between 0.01-1Hz) to

72

measure 160 short complex sound sequences comprised of 15sec excerpts; 120 musical and

73

40 environmental ones. At the behavioral level, they measured the short sound sequences

74

using a Likert scale which ranged from (1) "like very much" to a (5) "do not like at all,"

75

matching personalized semantic labels to the related evoked responses. Although the

76

experiment was entirely successful in its parts, it did not use extended and continuous –

77

hence ecologically valid – musical stimuli. Ιt also did not manage to fill the non-

78

personalised semantic code-personal perception gap as it did not measure emotion but

79

rather preference.

80

Sammler et al. [31] on the other hand, intentionally manufactured short consonant

81

(happy) and dissonant (sad) organized non-musical sound sequences (1 min long) yet to

82

avoid the personalization issue mentioned above. In their study, they managed to measure

83

and report objectively on the Alpha band activation correlation to the valence distinguished

84

evoked responses (happy-sad) in an eyes-closed condition. However, as they did not follow

85

a musicologically elaborate example (musical composition), an ecologically valid musical

86

stimulus was not fully evident in this case too, projecting a limited framework of

87

generalization in music-specific neuropsychologically defined evoked emotions.

88

On the contrary, the EEG approach that took place in the Mikutta et al. studies [33,

89

34] investigated possible correlates of experience-driven changes of emotional arousal,

90

presenting a continuous and ecologically valid stimulus (Beethoven’s 5th symphony first

91

movement; 7.4 mins) to both amateur and professional musicians. The researchers examined

92

the tonic and phasic sides of the electrophysiological data they collected, finding that

93

experienced musicians rate their physiological arousal in a more consistent way than
4

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

94

amateur musicians, which is on par with their elevated brain activation. However, the

95

researchers did not use a standardized platform of registered emotional ratings to the music

96

stimulus, providing in the end only subjective arousal measurements. These measurements

97

brought to the fore an intrapersonal dimension of evoked responses to the ecologically valid

98

musical stimulus, however as a rather subjective perceptual construct of a musical

99

experience, that led to a personalized emotional reaction as opposed to an objective reaction.

100

One could therefore speculate if it was the level of the musical experience (i.e., attention to

101

specific details or lack thereof; knowledge or lack of certain musical information) that

102

influenced musicians to rate their emotional responses in a consistent or inconsistent way,

103

or if it was the actual content of the musical piece being known to them (i.e., anticipation

104

and knowledge of expectancy) that led to the final result of consistency. It furthermore raises

105

the question of whether unfamiliarity or incapacity to understand the piece would lead to a

106

different result? Would the emotional result – hence, brain activation – and rating be

107

similarly consistent?

108

Sourina, Liu, and Nguyen [32] worked on more novel research designs, slightly

109

filling the gap of standardized comparative measurements between behavior semantics and

110

biology. As in the previous case, however, the major part of their work focused on the

111

personalized dimension of explaining musically evoked emotions rather the study of an

112

actual non-personalised correlation between the behavioral semantics of emotions and their

113

parallel brain activation. Using a real-time EEG-based human emotion recognition

114

algorithm, the research team incorporated fractal dimension values extracted from the

115

available EEG-Sound envelops cross-registration, pinpointing indeed a detailed path on the

116

EEG analysis using a mixture of a standardised pool of affective sound stimuli – the

117

International Affective Digital Sounds (IADS) [37] as well as 1 min long non-standardised

118

musical excerpts (i.e., the standardized material was not ecologically valid, while the
5

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

119

ecologically valid material was not standardized). The behavioral measurements, although

120

following a non-personalised standardization path, were not cross-referenced to the

121

neuroimaging data, failing to address the inter-modularly assigned semantic labels. In

122

addition, only six basic emotion labels were used, excluding a broad spectrum of available

123

emotional labels, therefore most probably ‘disguising’ potentially stated evoked responses

124

and perceived emotional states.

125

Finally, the Hsu et al. [35] recent study followed a similar approach as Sourina et al.

126

[32] with adding an extra layer of data and therefore better understanding towards the

127

‘semantics-perception-biology’ system discussed here. More specifically, the researchers

128

focused on devising a loop protocol, bidirectionally feeding the studied dimensions of

129

information (behavior and EEG), while managing to present a ‘transformational matrix,'

130

based on an algorithm which created a personalized ‘semantics-perception-biology’

131

framework. Although the researchers bidirectionally fed and successfully aligned the two

132

investigated dimensions, it is of our opinion that in this case too, the EEG cross-correlation

133

to the behavioral semantics was not very strong, as (a) the team did not fully employ an

134

ecologically valid musical stimulus (they used the IADS as in the aforementioned study),

135

hence limiting the emotional perception and measured response, while (b) they based the

136

whole experiment on the subjects’ real-time self-feeding and tempering of the model’s data

137

– through a well-developed UI – that most probably interacted with the overall projected

138

emotional content and its registration without offering any control for this component of the

139

data.

140

In summary, the above studies show a growing attempt to refine combining

141

standardized measurements of all three neuropsychology and musicology domains

142

(behavior-biology-musical structure). However, it is also evident that an all-inclusive

143

protocol, where neuropsychological and musicological methods are employed equally, has
6

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

144

not been presented so far. In this sense, all aforementioned research protocols may have left

145

aside valuable information that musicology with specific reference to musical structure has

146

to offer in understanding emotionally-evoked responses in the brain and behavior, and

147

whether the musical structure (as a whole – e.g., measurement and time series of a musical

148

piece – or in parts – e.g., a cadenza, appoggiaturas or chord progression) may interact with

149

and alter the neuropsychological system in a standardized way, regardless of the level of the

150

musical experience. This, at least, has already been found to be only the case for the

151

behavioral side of things [38, 39].

152

Aims and Hypothesis

153

Considering all the above, the aim of our research project is to measure for the first

154

time in a multimodal way emotional responses to structural elements as they vary from the

155

beginning to the end in a composition (variation measured through continuous response

156

throughout the musical composition) within different compositions, analyzing and

157

comparing standardized behavioral to electroencephalographic data. Our general research

158

hypothesis is that emotion, projecting a particular valence or intensity to a specific (or a set

159

of) structural element(s) of a musical stimulus, provides a joint behavioral and

160

electroencephalographic synchronization imprint to all participants when registered in an

161

ecologically valid testing environment.

162

Following the complexity of the aim and our hypothesis, as well as to be more

163

transparent and concise in the way we explain our findings and Modus Operandi, we

164

decided to present the essential components of our protocol in separate parts – as proof of

165

concept written constructs – making known in detail the process and practical implications

166

that such an endeavor entails. Therefore, in this article, we do not aim to present the final

167

joint findings and analysis of both the behavioral and neuroimaging data, rather provide an

7

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

168

in-depth discussion of the conceptual framework of our EEG protocol. A brief presentation

169

of the behavioral part is, however, included.

170

Research Method

171

For the behavioral part, in brief, we followed a grounded theory approach [40] by

172

looking for emergent themes when analysing a combination of various tasks (drawings

173

[41,42], self-report narratives and choosing of adjectives from the updated standardized

174

Hevner list (see [43]). For the electrophysiological part – the main focus of this article – we

175

based our work on the Schmidt and Trainor [10] model (see further below for more

176

information), focusing on the EEG data collection and analysis stage for a defined sample

177

(n=13), aiming to replicate the electrophysiological process that Schmidt and Trainor

178

followed in their experiment. Compared to all other related and previously discussed

179

experiments, our study’s novelty was that we, first of all, run two mini-studies (Study 1 and

180

Study 2) with different equipment aiming towards a cross-verification of results.

181

Furthermore, we created and used for the musical stimuli a non-personalized (standardized)

182

measurements setting, applied to both the electrophysiological and behavioral approaches.

183

A brief outline of the combined neuropsychological research protocol follows (see

184

also Fig 1):

185

1.

Recording of participants’ neurophysiological baseline (ABR-DPR);

186

2.

Presentation of two different musical stimuli and neurophysiological

187

recording of emotional responses. The two musical stimuli were segregated by white noise;

188

3.

Repetition of Step 1;

189

4.

Presentation of the two previous musical stimuli and behavioral registration

190

of emotional responses through drawings, written narratives and choices of emotional

191

responses from a provided adjective list;

192

5.

Individual interviews to clarify, decode, and describe responses in Step 4.
8

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

193
194

Fig 1. The EEG and Behavioural Protocols Combined

195
196

As far as our data analysis method is concerned, our statistical computations followed

197

a similar binary analysis approach for both measured modalities (behavior and brain data),

198

calculating repeated measures (RM) ANOVAs and independent ANOVAs for the within-

199

and the between-subjects conditions respectively, according to a predetermined emotion-

200

musical-structure related framework (musicological aspect). For the EEG part, more

201

specifically, we focused on the hemispheric lateralization of activity (dependent variable)

202

and followed the alpha synchronization paradigm. This paradigm has been shown to

203

denominate qualitative states of emotional valence [10, 11, 44, 45].

204

Materials and procedures for the EEG

205

The Music Stimuli

206
207

The following two solo violin compositions were accordingly prepared in WAVlossless formats and used as musical stimuli to collect our data:

208
2091. (Composition A) Nikos Skalkottas (1904–1949), Sonate for Solo Violin, fourth movement:
210

Adagio – Allegro molto moderato (1925). Duration: 5”40’. Performed by Georgios

211

Demertzis.

2122. (Composition B) Hendrik Hofmeyr (b. 1957), Luamerava for Solo Violin, Commissioned
213

in 2000 by the South African Music Rights Organisation. Duration: 5’39”. Performed by

214

Ian Smit.

215
216

We chose these two solo violin works because they are similar in genre, timbre

217

(instrument) and duration, as well as being unfamiliar to most, if not all participants. They
9

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

218

furthermore met the criteria of not containing lyrics and being composed in two distinct

219

countries, one North of the equator and one South.

220

The South African composition by Hendrik Hofmeyr was composed post-apartheid

221

and the Greek Skalkottas composition after the First World War. Both composers lived

222

outside of their home countries in Europe for extended periods. Hofmeyr spent ten years in

223

Italy and Skalkottas spent 12 years in Germany. Hofmeyr is outspoken about his strong links

224

with tonality, and his composition styles could be described as representing neo-Classical

225

and neo-Romantic sound ideals [46-48]. On the contrary, Skalkottas was influenced by

226

Schönberg’s serialism, and he held high regard for Bartók [49-50].

227

The Hofmeyr work has a philosophical basis on the sonification of the music tonality

228

of the Shona Mbira dza vadzimu with its associated overtones that the composer combines

229

with his compositional ideas. Luamerava is furthermore loosely based on Mutwa Credo’s

230

work "Indaba my Children" and his reference to an African goddess [51]. Although not

231

strictly programmatic in nature, this composition is undergirded by extra-musical ideas,

232

which might be reflected in the participants’ responses. In contrast, the Skalkottas

233

composition is idiomatic at every turn, perhaps even more so than the later Sonata for Solo

234

Violin by Bela Bartók. The Solo Sonata by Skalkottas refers to Baroque models, combining

235

the Greek meta-war folk musical structure, and an atonal system that the composer often

236

follows in his works. This work provides an extensive background of musical emotions in

237

linear and complex forms, thus showcasing a depth of content to study and discuss. The

238

choice of these two musical stimuli holds much potential for the study of affect, as it could

239

render very different participant responses.

240

Each of the two musical compositions was approached as a continuous sound

241

stimulus, hence a hypothetical emotional macro-response. However, since knowing that

242

specific structural elements (e.g., tempi, chord progressions, modulations, pitch ranges,
10

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

243

cadenzas, instrument ranges, playing techniques) can also evoke emotional micro-responses

244

in a musical composition [25, 26] – all resulting in the relevant macro-response – we also

245

decided to identify the micro-responses in our study. To achieve this, we asked a systematic

246

musicologistii to perform a music-theoretical analysis. This analysis was cross-referenced

247

and verified by two other professional musicologists, based on aura-based and score-based

248

analyses as well as a combination of two.

249

The music-theoretical analysis identified and converted to time-specific tag points

250

the micro-responses of potential emotional change and produced noted memos in the

251

musical scores presented in Western staff notation. Most importantly, this binary micro- and

252

macro-emotional response approach enabled us to pinpoint and capture inter-subjective as

253

well as intra-subjective differentiations or similarities of neuronal oscillations for each

254

musical composition in the specified timeframes, hence to study emotional responses both

255

at a micro- as well as a macro-neurophysiological time-related-to-emotion frame.

256

The analysis of the Skalkotas composition rendered nine specified timepoints that

257

we refer to as tag points. The Hofmeyr rendered 14. These tag points for each composition

258

are shown in Table 1, presenting the corresponding time duration in minutes and seconds

259

since the beginning of each musical piece. Based on these tag points, we identified the

260

independent and dependent variables of the experiment. The independent variables were

261

our participants and the two musical compositions they listened to, whereas the dependent

262

variables were the hemispheric lateralization of the alpha band activation, as well as the

263

lateralized brain activation effect for and between each tag point.

264
265

Table 1. Tag points distribution

Tag points (in secs) in the two musical compositions
Number of
tag points

1

2

3

4

5

6

7

8

9

10

11

12

13

14

Hofmeyr
(14 tags; H
Tags)

3

11

24

41

73

152

160

188

198

208

232

281

292

299

11

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

Tag points (in secs) in the two musical compositions
Skalkotas
(9 tags; S
Tags)

266

2

36

65

122

146

186

279

297

312

Participants

267

Study 1. Three participants (music experts) (n=3) were randomly recruited from the

268

Music Department, the South African College of Music of the University of Cape Town

269

(UCT). They were attending the university for three or more years as music students, and

270

took one of the two following courses amongst playing their instruments and taking other

271

music courses: Music Theory and Analysis III/IV and Music History of Western music

272

III/IV. All three subjects played a musical instrument on a third-year level. Testing on the

273

subjects took place after Ethics Clearance following the Helsinki declaration was obtained

274

from the Human Research Ethics committee at the Faculty of Health Science, UCT.

275

Participants took part in the project once they have given full written informed consent, and

276

only after they received detailed information during a relevant presentation pertaining to

277

what is required of them and how the experiment works. They were informed about their

278

right to withdraw at any time they see fit. All three right-handed participants had no mental

279

health or drug usage history, while no physical hearing injuries or abnormalities were shown

280

during the experiment. Their ages were 23, 24, and 30 (Mean=25.7). At the time of the

281

experiment, they were enrolled for BMus degrees at UCT and were in their final year of

282

study. All three continued with postgraduate studies: Honours in Orchestral Conducting,

283

Masters in Music Therapy, and Masters in Musicology. As it has been shown that perception

284

and expression of emotions are differently imprinted in the brain for right-handed (RH)

285

participants versus the left-handed (LH) participants [52, 53], we recruited right-handed

286

subjects to avoid possible data analysis complications and distortions.

287

Study 2. For the second study, thirteen participants (n=13) were randomly recruited

288

from the Metropolitan College, Thessaloniki, Greece. All of them were related to music
12

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

289

through formal music education (instrumental and theoretical backgrounds), having variable

290

studies range extending between 2 and 10+ years. Although the majority had some music

291

tuition in their youth, not all studied music at tertiary level. Some were professional

292

musicians or students at the College at various stages in their studies. This is a significant

293

difference between Studies 1 and 2, but not detrimental to the experiment as it is important

294

to measure if the number for years to formal music instruction would play a significant role

295

in the findings. No mental health or drug usage history existed before the experiment, while

296

no physical hearing injuries or abnormalities were shown during the experiment. Testing

297

took place only after the Ethics process was concluded according to the steps we followed

298

in Study 1, including the Metropolitan College research administration's ethical approval in

299

this case as was further required. For Study 2, the mean age of the final 10 participants we

300

successfully processed, was 26.3 (Mean), while out of the 13 initial participants, the

301

remainder were mostly right-handed (7RH, 3LH). Regarding the three excluded

302

participants, one had to voluntarily withdraw from the EEG recording phase (personal

303

reasons), while the other two could not be finally processed due to the very noisy raw data

304

obtained from them (i.e., artifacts mostly generated because of excessive movement).

305

The Schmidt and Trainor Study

306

The Schmidt and Trainor study, a pioneer in its field, aimed at two distinctive goals.

307

The first was “to examine whether different musical excerpts induce different affective

308

states that can be indexed by measuring brain activity” [10], while the second was “to

309

examine whether measures of regional EEG activity can be used to differentiate emotions

310

along the two dimensions of valence and intensity simultaneously” [10]. The whole study

311

was critically based on three specific regional brain activation and emotion models: firstly,

312

primarily combining emotion to brain activity [44]; secondly, concerning the role of

13

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

313

absolute frontal activation in the intensity of emotion [54, 55], and thirdly considering

314

hemispherically lateralised brain activity in relation to emotional valence and intensity [45].

315

In summary, the Schmidt and Trainor study investigated the relationship between

316

regional brain activity evoked by music listening and induced emotional responses. The

317

researchers attempted to directly relate valence and intensity of emotional experience to

318

electrical activation in the brain through EEG while presenting to a group of undergraduates

319

(n=59) orchestral musical excerpts designed to induce joy, happiness, fear, and sadness. The

320

four musical excerpts were pre-rated by a different group of undergraduate students to

321

represent “intense-unpleasant (fear), intense-pleasant (joy), calm-pleasant (happy) and

322

calm-unpleasant (sad)” [10].

323

The investigators used the four orchestral excerpts (60secs/each) to record the

324

participants’ EEG activity, placing four electrodes on the left and right mid-frontal (F3, F4)

325

and parietal (P3, P4) scalp regions, according to the 10/20 Electrode Placement System [56].

326

All electrodes were referenced to the Cz region, while the collected raw data were bandpass

327

filtered between 1Hz and 100Hz, and sampled at 512Hz. Electrooculography (EOG) was

328

also performed to facilitate further deartifacting. The raw data were visually inspected to

329

exclude eye blinks, eye movements, and other motor movements, using specialized software

330

by James Long Company (EEG Analysis Program, Caroga Lake, NY). Finally, the artifact-

331

free data were analyzed using a Discrete Fourier Transform, while plotting results were

332

outputted in the alpha band between 8Hz and 13Hz.

333

Among the combined 2x2 ANOVA computations on gender, valence (pleasant-

334

unpleasant), intensity (intense, calm) and hemispheric lateralization (right, left) that were

335

performed in the study by Schmidt and Trainor, the valence by hemisphere analysis

336

presented significant effects, providing a significant interaction between valence and

337

hemisphere. Clear hemispheric lateralization was observed for the four musical excerpts in
14

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

338

relation to their nominated emotion, projecting higher activity on the left hemisphere for

339

positive emotions, and in reverse, higher activity on the right hemisphere for negative

340

emotions.

341

The EEG Data Collection

342

An electroencephalogram detects and registers the brain waves or electrical activity

343

of the brain. During the registration procedure, the EEG electrodes detect tiny electrical

344

voltages that result from the activity of the brain cells. The voltages are then amplified

345

through specific equipment (the amplifier, a computer, and a software program) and appear

346

as a graph on the screen.

347

Study 1. EEG data were collected using a 128-channel EGI Geodesic Sensor Net

348

(GSN) system, found in the Division of Biomedical Engineering, University of Cape Town.

349

EGI-GSN does not require any scalp preparation or abrasion, making it a very convenient

350

choice due to its ease of application on many different subjects in a short period. The

351

system’s combined temporal-topographic data collection abilities furthermore allow for a

352

more detailed topographic signal analysis due to the close intersensor distance.

353

The electrodes were placed following the EGI GSN128 electrode placement system,

354

while referenced to the central vertex (Cz). Data were collected through the following five

355

steps:

356

1.

Eyes Closed (EC) 1-minute session: This session took place in order to

357

calculate the Posterior Dominant Rhythm (PDR) of all subjects. It has been found that the

358

presence of a normal PDR indicates a well-functioning brain. A normal PDR is framed

359

between 8 Hz and 12 Hz and is usually found at the posterior parts of the brain (e.g., O1-O2

360

electrode regions) during an eyes-closed condition. A proper PDR may rule out all sorts of

361

pathology. If found to be less than 8.5 Hz or greater than 11.5 Hz, this should be considered

362

abnormal in adolescents and adults [57, 58].
15

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

363

2.

Auditory Brainstem Response (ABR) 1-minute session: An Auditory

364

Brainstem Response measurement took place in order to estimate hearing sensitivity

365

objectively in the participants. The ABR can be perceived as similar in the manufacturing

366

process to the Auditory Steady-State Response (ASSR) [59] although the former’s electrical

367

oscillations, hence the measurement reference, occur as a reaction much earlier in the time-

368

brain region domain (i.e., brainstem versus neocortex). The ABR can be thought of as an

369

electrophysiological response of the brainstem to rapid auditory stimuli. Its basic stimulus

370

structure is usually comprised of clicks or tone bursts. ABR relies on statistical calculations

371

to determine if and when a particular threshold is present. In our study, we used the

372

following stimuli configurations to measure the ABR: two conditions of broadband spectral

373

square wave clicks of 500Hz and 4000Hz lasting 30s each. One thousand sweeps of clicks

374

were included in each one of the two 30 secs segments (500Hz and 4000Hz), each having a

375

duration of 100 μsecs, and an inter-stimulus interval (ISI) rate of 33.3Hz. The two conditions

376

were manufactured at an air conduction binaural acoustical level in the range of -14.1 and -

377

14.3 dBFS, as shown in the following Fig 2. A 51 msecs ISI separated the two conditions.

378
379

Fig 2. The ABR stimuli.

380

The ABR stimuli were created using the PureData and ProTools software platforms. The signal

381

polarity was following a full spectral wave circle (positive to negative alternatively) showcasing a 0 to positive

382

initial deflection. To ascertain the RMS (Root Mean Square) of the ABR sections, about 4–5 seconds of each

383

condition was initially selected, and an audio statistical gathering plugin was then applied in Audacity, using

384

Nyquist to get the above information.

385
386

3.

Presentation of Compositions A and B segregated by White Noise (A-WN-

387

B). Both stimuli were acquired and used in a WAV-Lossless format, while delivered through

388

the Shure SE315 in-ear sound module. The Shure SE315 uses a vented balanced armature

389

driver. These in-earphones have a frequency range of 22Hz–18Hz and their sound isolating
16

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

390

design blocks out most ambient noise. Before using the musical stimuli, they were both

391

normalized and equalized to achieve the best possible sound quality. Following the protocol

392

of Eerola et al. [60], the music stimuli were equalized in terms of loudness, using peak RMS

393

(root mean square) value normalization and careful subjective evaluation. The process of

394

normalization can be defined as the boosting of “the highest sample of a digital sound file

395

to the maximum amplitude the system is capable of encoding, short of CLIPPING (0dBFS),

396

and then raising all other samples by the same proportion, in effect, raising the volume of

397

the sound file, so it is as loud as possible without going into overload. This process

398

maximizes resolution and minimizes certain types of noise” [61]. The entire A-WN-B set

399

was played through, following the same room conditions (light, temperature, air ventilation)

400

for all participants. EEG was continuously recorded following an eyes-open registration

401

approach. Previous EEG studies [62-64] investigating the difference between the eyes-open

402

versus the eyes-closed recording condition have shown that there is a difference in the power

403

level of the final raw data registration, and that the eyes-open approach should be preferred,

404

as the opposite presents a high-risk raw data reading result in the whole scalp EEG

405

registration process [63]. Additionally, the Schmidt and Trainor study [10] had been

406

conducted following an eyes-open registration process, too.

407
408

4.

The final two sessions repeated the first and second steps for purposes of

cross-referencing and evaluation of intrapersonal data.

409
410

Study 2. EEG data were collected using the Emotiv EPOC+ 14 high-resolution

411

channel system. The Emotiv has saline-soaked felt pads and does not require any scalp

412

preparation or abrasion. It is a very convenient unit to use, yet not as sensitive as the EGI

413

Geodesic Sensor Net (GSN) we used in Study 1. For the Emotiv system, the electrodes

414

(AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4) were placed in accordance
17

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

415

to the International 10-20 electrode placement system, while they were referenced to the

416

CMS/DRL electrodes placed on the mastoids. Data were collected through the same four

417

steps as presented above for Study 1.

418

Why EEG and not MRI

419

It has been found that in humans, emotional recognition related to music occurs just

420

after the first 500ms of the onset stimulus [65]. This fact implies that the process of

421

emotional alteration can potentially be extremely fast and abrupt, while the effect of the

422

musical stimulus onset and offset may greatly vary across narrow time frames [66],

423

involving multiple cortical activations and de-activations, as well as the interaction of

424

multiple neural networks. Based on this, identifying a suitable brain-imaging tool to

425

measure emotional reactions to music can be challenging. The two standard tools usually

426

employed in most of the relevant research protocols (EEG and MRI) each carry different

427

strengths and weaknesses [67,68].

428

As indicated before, our experiment aims to measure multimodal ways in which a

429

group of musicians responds to structural elements within two different musical

430

compositions. We, therefore, had to focus on the temporal rather than the spatial domain of

431

collecting neuroscientific data, as the structural elements inducing different emotions within

432

the different music compositions may greatly vary across narrow timeframes and raw data

433

sound epochs. Facing this dilemma, EEG proved more suitable than fMRI for the following

434

reasons:

435

Firstly: EEG is more precise in temporal data measurements than the fMRI. The

436

MRI technique is not known to measure direct neuronal activation, but rather an indirect

437

effect of this activation as represented in the oxygen ratio of the existing hemoglobin in the

438

blood flow at a particular brain region (oxyhemoglobin and deoxyhemoglobin ratio). This

18

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

439

effect is called the Blood Oxygen Level Depended (BOLD), with which researchers gain a

440

window on the temporal dynamics of the brain hemodynamic response.

441

Nevertheless, it is also known that “the BOLD response [even for a very short

442

stimulation period; approximately 1 second in duration] is typically delayed by 1 to 2

443

seconds and reaches its maximum after approximately 8s (typically between 5 and 10 s)”

444

[68]. Therefore, there is always a relevant delay in the recording of data related to the

445

temporal domain during the fMRI scanning process, which makes it very difficult to collect

446

and analyze condensed information or rapidly changing evoked responses. On the contrary,

447

the EEG module, by measuring the electrical activity of the brain directly, without delay,

448

can achieve a detailed recording of changes in brain activation, targeting even the first

449

milliseconds of a specific stimulus onset or offset [67]. In our case, using EEG assisted us

450

in investigating all relevant emotional changes of selected structural elements in the chosen

451

musical stimuli.

452

Secondly: As aforementioned, emotional responses to music may involve multiple

453

cortical activations and de-activations, as well as the interaction of multiple neural networks.

454

One result of these complex interactions is the presence of differential responses across

455

brain regions using the fMRI approach, may lead to incomplete modeling of the relevant

456

evoked bio-emotional elements, as we will not be able to follow a straightforward

457

application of standard general linear model (GLM) approaches that previous fMRI/emotion

458

research projects have used (for a brief overview, please see, for example [16]). Therefore

459

we will not be able to perform a detailed comparison of the tomographic data that our

460

research project will show with already existing ones, because of a non-uniform starting

461

point of emotional/temporal reference. Previous fMRI research projects have only referred

462

to relevantly more significant and ‘thinner' in quality (happiness; sadness) blocks of evoked

463

emotional responses through music, while our project endeavors to investigate thick, both
19

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

464

in time and quality, alterations of emotions, connecting abstract emotional responses (i.e.,

465

drawings, narrative expressions) to biological representations in the brain. We believe that

466

by using initially the EEG module in our project, and later on the sLORETA software [69],

467

we will be able to study the electromagnetic tomography in relation to the evoked emotional

468

responses and therefore more efficiently cross-reference it with essential studies in the field

469

– the Schmidt and Trainor study in our case – in order to validate it.

470

Thirdly: Using the EEG module in our study instead of the MRI module, avoids the

471

statistical interaction effect between the MRI scanner noise and the affective processes. It

472

has been strongly suggested (Skouras, Critchley & Koelsch, 2013) that the MRI scanner

473

noise can influence and distort affective brain processes like the ones we would like to

474

investigate.

475

EEG data processing and analysis

476

EEG processing

477

In order to process and analyze the EEG raw data for both studies, we used the

478

MatLab (Mathworks co.) EEG Lab version 11 software platform. The data were first

479

montaged and segregated into epochs, following first an ‘as-per-condition’ block

480

segregation and then an ‘as-per-tag-points’ segregation. A graphical illustration has been

481

already presented in our protocol design earlier on (Fig 1). The following EEG epochs were

482

created: ‘Eyes Closed,' ‘ABR,' ‘First musical stimulus' (with pinned tag points), ‘White

483

Noise,' ‘Second musical stimulus' (with pinned tag points), ‘ABR,' ‘Eyes Closed.' These

484

epochs were deartifacted using both a manual (i.e., thresholding, by eye trimming) as well

485

as a logarithmic signal decomposition approach (i.e., Independent Component

486

Analysis/ICA).

487
488

In more detail, the following configurations/steps were applied in creating and
processing these epochs:
20

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

489

1)

Whole EEG data baseline removed – initially sampled in 250Hz;

490

2)

Average montage for all electrodes;

491

3)

Data were filtered using a 1Hz low-pass and a 50Hz high-pass filter;

492

4)

False channels/data interpolated when needed;

493

5)

ICA through the ADJUST algorithm performed;

494

6)

Detailed artifact rejection performed;

495

7)

‘Epoching’ at tag-point level;

496

8)

Hemispheric lateralization categorization (Left-Right dominant hemispheric

497

activation; only for Study 1, please see explanation below);

498

9)

Major dipoles calculation (only for Study 1, please see explanation below);

499

10)

Computation of alpha-band asymmetry ratios on the closest to the four

500

‘Schmidt and Trainor’ electrodes. For Study 1, these were electrode numbers 25, 124, 53,

501

and 87 [71]; for Study 2, these were electrodes F3, F4, P7, P8.

502

Specifically, for the epoching process at the tag-point level (step 7 above; similar

503

process for Studies 1 & 2), the expectant points of emotional alteration (i.e. tag points) were

504

noted on the EEG timeline axis according to the available musicological analysis producing

505

specific EEG data envelopes (1sec pre-trigger stimulus up to 2secs post-trigger stimulus;

506

see Fig 3). These data envelopes were fully extracted from the whole EEG data set for a

507

separate tomographical and statistical analysis as described below.

508
509
510
511

Fig 3. Example of extracted tag epoch; Subject 3 Tag S8 from Study 1.

Analysis Design - Hemispheric Lateralization

512

In order to study the hemispheric lateralization dependent variable, we focused on

513

the created epochs (step 7 mentioned above). On these epochs, we followed three steps of

514

analysis for Study 1 (steps 8, 9, and 10) while only one step (step 10) for Study 2. The reason
21

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

515

for not following the same detailed path for both studies was that statistical insignificance

516

was found on the mean difference between the two sets of measurements (whole set of

517

electrodes vs. four electrodes; steps 8 and 10) in Study 1, deciding thereafter to exclude

518

steps 8 and 9 from Study 2 (presentation of relevant calculations in Results below).

519

Referring to Study 1, we controlled for the hemispheric lateralization after

520

measuring and analyzing the whole scalp alpha-band activation as projected on all available

521

electrodes and 100% of the EEG data collected for each composition and subject (Step 8).

522

Additionally, looking to validate the lateralized alpha-band activation further, a major

523

existing dipoles calculation was also performed using the BESA model (MEGIS Software

524

GmbH, Munich) (Step 9). This step revealed the most active dipoles (100% of the EEG

525

data) for each musical composition and subject, providing an extra level of evaluation of the

526

hemispheric lateralization of continuous-response activation. At a second level, we focused

527

only on the closest to the four main electrodes, the Smidt and Trainor (2001) study used

528

before.

529

After statistically validating our dependent variables, our calculations further

530

followed a two-level factor analysis. These factors were presented as ‘between-subjects' and

531

‘within-subjects' codes. For the ‘between-subjects' code, we compared the continuous

532

biological evoked response of emotion between subjects, nominating it to be the macro-

533

level response. For the ‘within-subject' code, we attempted to showcase the inherent

534

trajectory of the continuous biological evoked response of emotion for each participant and

535

individual musical composition, respectively. This time, this code was nominated to be the

536

micro-level response.

537
538

Based on this two-level factor analysis, we used a Mixed Design ANOVA model
(Fig 4) on the evoked and measured alpha brainwaves on each tag point, computing

22

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

539

a)

the statistical significance of the dominant hemispheric activation for each

540

tag and composition and for each participant separately (within-subject analysis; repeated

541

measures ANOVA) in order to study the cohesion of emotional biological responses

542

(biological quality of micro-responses); and

543

b)

the statistical significance of the overall dominant hemispheric activation for

544

each composition and for each participant in cross-comparison (between-subject analysis;

545

independent ANOVA) to study the cohesion of emotional biological responses for each

546

musical composition at a combined sample level (biological quality of macro-responses of

547

emotion).

548
549

Fig 4. Overview of the statistical analysis

550
551

For the aforementioned first case, we followed the approach of the repeated

552

measures ANOVA in order to see how much of the variability shown is a result of the

553

experimental manipulation (tag points in musical stimuli), relative to other random factors

554

(residual) existing in our data collection process (for example, exogenous sound stimulation,

555

prior emotional dynamics etc.) while in the second case, we followed the approach of the

556

independent ANOVA in order to establish if variance differences between participants are

557

isolated or not. In this latter case, the resulting F-test for structuring a sufficient baseline for

558

the overall emotional, biological response for each musical composition can be more potent

559

at a combined sample level.

560

Results

561

In order to categorize our sample in advance on possible differentiations and

562

variances due to brain structure qualities, we ran an audiometric threshold test using the

563

Auditory Brainstem Response (ABR) technique, as well as a Posterior Dominant Rhythm
23

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

564

(PDR) analysis. As far as the ABR analysis is concerned, all subjects had normal wave V

565

click-evoked latencies (<6.8 ms in response to the parameters set on structuring the ABR),

566

both before and after the presentation of the protocol stimuli for both Studies. This test

567

verified that no neurological acoustical disorder existed in any of our participants. The PDR

568

data analysis for all participants also showed no significant variations.

569

All electrodes hemispheric lateralization

570

Study 1. The immediately following results (Table 2) refer to the 128 electrodes

571

100% full scalp data, focusing on the alpha band hemispheric lateralization for all three

572

subjects recorded in Study 1. R (right) and L (left) present the corresponding dominating

573

hemispheric alpha band activation on each tag, for each different musical composition for

574

each participant. These calculations were based on the alpha band frequencies spectrum

575

analysis (8Hz; 10Hz; 12Hz) provided through calculated spectoplots for each tag and

576

participant separately (for example, Figs 5 and 6).

577
578
579
580
581
582
583
584
585
586
587
588
589
590
24

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

591
592
593
594
595
596
597
598

Table 2. Dominating hemispheric alpha band activation on each tag, 128 electrodes. L = Left; R = Right;
ERR in EEG = could not be calculated.

Subjects

599
600
601
602
603

S1

S2

S3

Composers
Skalkottas

Tags
S1
S2
S3
S4
S5
S6
S7
S8
S9

Seconds
2
36
65
122
146
186
279
297
312

EEG
R
R
R
R
R
L
R
R
R

EEG
L
L
L
R
R
L
R
L
L

EEG
R
R
R
R
R
R
L
R
R

Hofmeyr

H1
H2
H3
H4
H5
H6
H7
H8
H9
H10
H11
H12
H13
H14

3
16
24
41
73
152
160
188
198
208
232
281
292
299

R
R
R
L
ERR
L
R
L
R
R
R
L
L
R

R
L
L
L
R
R
L
R
L
R
R
R
R
L

R
R
R
R
R
R
R
R
R
R
R
R
R
R

Fig 5. Example of Subject 3, S8 Tag full scalp spectoplot alpha band; 100% of data for S8 tag epoch; 1 sec
baseline-2 secs active response.

25

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

604
605
606
607

Fig 6. Example for Subject 3, S8 Tag, dominant component lateralization on the electrode with maximum
power in the specific epoch.

608

The data in Table 2 were also verified following a major dipoles analysis on 100%

609

of the data for each one of the tags of the two musical compositions. Using the BESA model

610

of analysis software (MEGIS Software GmbH, Munich) as embedded in the EEGLab, we

611

computed the existing major dipoles concentration for each participant and each musical

612

composition individually. Again, we focused on the same band of activation (for example,

613

Fig 7).

614
615
616

Fig 7. Subject 3, Tag S8 Major Dipoles concentration in 100% of epoched data; Averaged alpha power

617

Four electrodes hemispheric lateralization

618

Study 1. We investigated the respective alpha band asymmetric activation between

619

hemispheres for each subject and musical composition separately. We first computed the

620

alpha power for the F3, F4, P3 and P4 electrodes as corresponding to the 128 EGI electrodes

621

system system (electrode numbers 25, 124, 53 and 87) [71] continuing with a computation

622

of the activation asymmetry ratios between the pairs of these four electrodes according to

623

the hemispheric index formula (var(x) – var (y)) / var (x) + var (y)) that has been reliably

624

used in previous similar investigations (for example [72, 73]). In this case (x) and (y) equals

625

to channel variance index of the filtered output on the left side and the right side,

626

respectively.

627

On the tag point number S8 for Subject 3, for example, calculations on the two pairs

628

of electrodes (25:124 and 53:87) rendered the following index numbers: for the 25:124 pair

629

of electrodes, anteriorly lateralized asymmetry computes at 0.4703 while for the 53:87 pair,

630

posteriorly lateralized asymmetry computes at -0.5882. These numbers mean that we got an
26

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

631

overall positive left side anteriorly localized domination of the alpha band for this set of

632

electrodes. Equally, for all the available pairs of electrodes and data, we computed

633

asymmetry reaching the result shown in Table 3 below. Statistically comparing the results

634

of Table 3 to Table 2 above (paired t-tests), we calculated the mean difference between the

635

two sets of measurements (whole set of electrodes vs. four electrodes). We found no

636

statistically significant differences between the two pairs of measurements (The p-values

637

for each pair was: [S1 Skalkotas p=1, S1 Hofmeyr p=0.605; S2 Skalkotas p=0.5, S2

638

Hofmeyr p=-0.043; S3 Skalkotas p=0.661, S3 Hofmeyr p=1).

639
640
641

Table 3. Dominating hemispheric alpha band activation on each tag, 4 electrodes, in Study 1. L =
Left; R = Right, ERR in EEG = could not be calculated

Subjects
Composers Tags
Skalkottas
1
2
3
4
5
6
7
8
9

Hofmeyr

1
2
3
4
5
6
7
8
9
10
11

S1

S2

S3

Seconds
2
36
65
122
146
186
279
297
312

EEG
R
R
R
R
R
R
R
R
R

EEG
L
L
L
L
R
L
R
L
R

EEG
R
R
R
R
R
R
L
L
R

3
16
24
41
73
152
160
188
198
208
232

R
R
R
R
ERR
R
R
R
R
R
R

L
R
R
L
R
L
L
R
R
L
R

L
R
R
R
R
L
L
L
L
L
L
27

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

12
13
14

281
292
299

R
R
R

R
R
R

R
R
R

642
643

Study 2. We investigated the respective alpha band asymmetric activation between

644

hemispheres for each subject and musical composition separately. We first computed the

645

alpha power for the F3, F4, P7 and P8 electrodes as corresponding to the 14 Emotiv+ EPOC

646

unit electrodes system, continuing with a computation of the activation asymmetry ratios

647

between the pairs of these four electrodes according to the hemispheric index formula

648

(var(x) – var (y)) / var (x) + var (y)) as employed in Study 1 before. For all the available

649

pairs of electrodes and data, we computed the following asymmetry results (Table 4).

650

Factor analysis

651

With these two studies, we aimed to investigate the micro- and macro-emotional

652

responses following an intra-subjective as well as inter-subjective data analysis. Therefore,

653

two specific hypotheses supported the statistical analyses of the neurophysiological data.

654

The intra-subjective statistical analysis was based on the null hypothesis (H01), suggesting

655

that all tags in an emotionally organized longitudinal sound stimulus evoke a solid emotional

656

soundscape. With this in mind, we followed for the two main conditions (the two musical

657

compositions) a repeated-measures one-way ANOVA (RM-ANOVA) on the tags for each

658

musical composition and each subject. On the inter-subjective statistical analysis side, the

659

null hypothesis (H02) we followed, suggested that all subjects present similar or close to

660

similar a biological reaction (hemispheric lateralization) in relation to the tags inserted in

661

the musical composition. In order to test this hypothesis, we followed for the two main

662

conditions (the two musical compositions) an independent ANOVA computation on the tags

663

for each musical composition, yet including the subjects as different variables in the same

664

equation this time, knowing from our previous calculations that indeed, each of these
28

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

665

variables perform and stand as uniform (i.e., as continuous and not as fluctuating evoked

666

responses intra-subjectively).

667

Study 1. For the H01, following a significance level of α=0.05 in our calculations

668

and a Bonferroni correction process, the Mauchly’s test indicated first that the assumption

669

of sphericity was not violated in both stimuli measurements. Then, the RM-ANOVA results

670

showed that there was no significant effect on the whole compositions’ evoked emotional

671

responses through any unique representations of the tags’ emotional power as presented to

672

the biological reactions [S Stimulus: F(8) = 0.684, p = .07] [H Stimulus: F(13) = 0.451, p =

673

0.933]. These findings suggest that each subject reacted at an average level uniformly in

674

terms of hemispheric activation for each different composition. In other words, the

675

emotional carriage, as shown in terms of brain activation, was almost uniform, suggesting

676

that the subjects indeed perceived each musical composition as having an average, holistic

677

emotional load.

678

For the H02, contrary to the previous results of the intra-subjective approach, the

679

inter-subjective calculations showed that there is a significant effect of the emotional power

680

carried from the different tag points on the biological reactions (hemispheric lateralisation)

681

[S Stimulus: F(1) = 84.64, p = .012] [H Stimulus: F(1) = 117.23, p = 0.008]. These findings

682

suggest that the subjects indeed reacted differently in terms of hemispheric lateralization for

683

each of the different composition’s tag lines. In other words, the emotional progression if

684

shown in biological terms for each musical composition, follows a completely different

685

synthesis for each individual.

686

Study 2. For the H01, following a significance level of α=0.05 again in our

687

calculations and a Bonferroni correction process, the Mauchly’s test indicated that this time,

688

the assumption of sphericity was violated in both stimuli measurements. Therefore, we

689

applied the RM-ANOVA calculations with a Greenhouse-Geisser correction. Through this
29

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

690

correction, we determined that the mean brain lateralization did not differ statistically

691

significantly between either the 14 noted tags for the H Stimulus [F(3.944, 35.495)=0.866,

692

p=0.493] or the nine noted tags for the S Stimulus [F(3.339, 30.055)=2.279, p=0.094]. Post-

693

hoc tests for both stimuli, using the Bonferroni correction, revealed that the continuous

694

stimuli of the Hofmeyr and the Skalkotas musical pieces elicited no actual differentiation in

695

terms of brain activity lateralization (p=1). In other words, the emotional carriage, as shown

696

in terms of brain activation for each tag was almost uniform, suggesting that the subjects

697

indeed perceived each musical composition as having an average, holistic emotional load.

698

This result is on par with the results of Study 1.

699

For the H02, contrary to the HO2 of the Study 1, the inter-subjective calculations

700

showed that there is no significant effect of the emotional power carried from the different

701

tag points on the biological reactions (hemispheric literalization) [S Stimulus: F(8, 81) =

702

1.251, p = 0.281] [H Stimulus: F(13, 126) = 0.263, p = 0.995]. These findings suggest that

703

the subjects reacted similarly in terms of hemispheric lateralization for each of the different

704

composition's tag lines. In other words, the emotional progression shown in biological terms

705

for each musical composition follows a similar progression line for each individual.

30

706

Table 4. Dominating hemispheric alpha band activation on each tag, 4 electrodes, Study 2. L = Left; R = Right.

Subjects
Composers Tags
Skalkottas
1
2
3
4
5
6
7
8
9
Hofmeyr

1
2
3
4
5
6
7
8
9
10
11
12
13
14

S1

S3

S4

S5

S8

S9

S10

S11

S12

S13

Seconds
2
36
65
122
146
186
279
297
312

EEG
R
R
R
R
R
R
R
R
R

EEG
R
R
R
R
R
R
R
R
R

EEG
R
R
R
R
L
R
R
R
R

EEG
L
L
R
L
R
L
L
L
L

EEG
R
R
R
L
L
R
R
R
R

EEG
R
L
R
R
R
L
R
R
R

EEG
R
R
R
R
R
R
R
R
R

EEG
R
L
R
R
L
L
L
R
L

EEG
R
L
R
R
L
L
L
R
L

EEG
L
L
L
L
L
L
L
R
L

3
16
24
41
73
152
160
188
198
208
232
281
292
299

L
R
R
R
R
R
L
R
R
R
R
L
R
R

R
L
L
R
R
R
R
R
R
R
R
L
R
R

R
L
R
R
R
R
R
L
R
L
R
R
R
R

R
L
L
R
L
L
L
L
L
L
L
L
L
L

R
R
R
R
R
R
R
R
R
R
R
R
R
R

R
R
R
R
L
L
R
R
R
R
R
R
R
R

R
R
R
R
R
R
R
R
R
R
R
R
R
R

L
L
L
L
L
L
L
L
L
L
L
L
L
L

L
L
L
L
L
L
L
L
L
L
L
L
L
L

L
L
L
L
L
L
L
L
L
L
L
L
L
L

31

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

708
709

Conclusion

710

The above results present the first evidence of our study for the emotionally induced

711

hemispheric lateralized activity as evoked through a continuous listening process of two

712

musical compositions diverse in cultural and musical technical elements. With our protocol,

713

we proposed an inter-subjective as well as an intra-subjective pathway of data analysis, and

714

we respectively studied the assigned musical compositions at a macro- and a micro-

715

emotional level of response considering a non-personalised response framework. After

716

recruiting and investigating two unrelated in terms of musical expertise, cultural and

717

educational background cohorts (i.e. EEG Study 1 and EEG Study 2), we found that there

718

is quite a similar approach for both of them on how they finally shape their emotional

719

reaction at the macro-response level (Study 1 = Study 2). Therefore, we conclude at this

720

initial stage that a musical composition – when seen as a continuous and holistic emotional

721

response – may indeed potentially induce an aligned biological response of emotion, no

722

matter what the differences are in terms of the equipment used to measure these responses

723

or the demographics of the recruited sample.

724

At the micro-response level, however, our study provided an entirely different

725

picture. In this case, our two studies presented a quite detached profile of statistical

726

differentiation (Study 1  Study 2) suggesting that any specific structural parts of music

727

evoking the holistic emotional reaction through a continuum, may not biologically be

728

perceived in the same way by individuals encompassing different demographics. It seems

729

that the quality of the biological steps a person follows in order to conclude to a particular

730

emotion on a specific musical composition could be variable, depending on either to the

731

cultural or the educational elements portraying their personal background. This fact proves,

732

on the one hand, our emotions’ complexity as human beings, as well as on the other the

32

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

733

intricacy of devising and running relevant to music protocols in the particular field of

734

behavioral neuroscience. It seems that structural elements of music are indeed able to fore-

735

bring a complex footprint of emotional reaction, different for individuals with detached

736

backgrounds and contexts, even if undergoing the same listening experience, whatsoever.

737

If a fully-understood quality of an emotional response is the ultimate goal, then, the

738

above datum-point begs for a multi-layered investigation approach. This multi-layered

739

approach can be achieved, in our point of view, by a grounded emotional variability

740

framework that could serve as a functional basis of organization and growth in context (i.e.,

741

investigation of similarities and differences in context over time). For this reason, we intend

742

to further introduce in our study the micro developmental variation dynamics theorem [74].

743

According to the theorem, when functional characteristics of human beings studied in

744

reference to emotional development, three units of analysis should be there to conclude the

745

study successfully. These three units are “the intra-individual variability [relative rapid and

746

reversible changes or differentiations], [the] intra-individual changes [relative stable

747

changes or differentiations], and [the] inter-individual differences (highly stable changes

748

over a long-time period or among individuals]” [74]. In our study, it seems that we have

749

already incorporated the ‘intra-individual variability’ and the ‘inter-individual differences’

750

units when measuring the intra-subjective and inter-subjective factors. What remains is to

751

integrate the ‘intra-individual changes' unit further as required, measuring emotional

752

reactions to a particular musical piece more than once over different timeframes (for more

753

details on this possible integration, please read [75]).

754

Concluding, the next steps will indeed follow a more extended framework of

755

analysis through the micro developmental variation dynamics theorem, as well as further

756

steps to recruit a more extended pool of participants and stimuli to investigate and

757

understand at a biological level. Finally, we will conclude our intended biopsychological
33

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

758

analysis and comparison, when both the neurophysiological as well as the behavioral data

759

will be statistically and qualitatively combined. Bio-psychologically measured emotional

760

reactions to whole musical compositions is definitely an under-researched subject in the

761

field of behavioral neuroscience, and therefore worthing of the most detailed and robust

762

possible basic research; especially when someone considers their full potential in real-life

763

applications which certainly extends to more than the clinical rehabilitation settings (e.g.,

764

Alzheimer's disease) or the socioemotional regulation applications through music (e.g.

765

depression).

766

767

Acknowledgments

768

We would like to acknowledge the help and advice given by Theo Herbst, Barry Ross, Miles

769

Warrington, and Vasiliki Pliogou in actualizing and completing this study successfully.

770

34

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

771

References

772
773

[1] Hevner, K. Experimental studies of the elements of expression in music. American
Journal of Psychology, 1936;48:246–268.

774
775

[2] Farnsworth, P. R. A study of the Hevner adjective list. Journal of Aesthetics and Art
Criticism, 1954:97–103.

776
777

[3] Farnsworth, P. R. The Social Psychology of Music. Ames: Iowa State University
Press; 1969

778
779
780

[4] Nørdenstreng, K. A comparison between the semantic differential and similarity
analysis in the measurement of musical experience. Scandinavian Journal of
Psychology, 1968;9:89–96.

781
782

[5] Wedin, L. A. A multidimensional study of perceptual-emotional qualities in music.
Scandinavian Journal of Psychology, 1972;13:241–257.

783
784

[6] Cupchick, C. C., Rickert, M., & Mendelson, J. Similarity and preference judgements
of musical stimuli. Scandinavian Journal of Psychology, 1982;23:273–282.

785
786
787

[7] Witvliet, C. V. & Vrana, S. R. The emotional impact of instrumental music on affect
ratings, facial EMG autonomic measures, and the startle reflex: Effects of valence and
arousal. Psychophysiology Supplement, 1996;91.

788
789

[8] LeBlanc, A. and Farnsworth, P.: Pioneer scholar of music listening preference.
Bulletin of the Council for Research in Music Education, 2001;149:3–12.

790
791
792

[9] Scherer, K. R. & Zentner, M. R. Emotional effects of music: production rules. In P.
N. Juslin & J. A. Sloboda (eds), Music and Emotion: Theory and Research. Oxford:
Oxford University Press; 2001: 361–387.

793
794

[10] Schmidt, L. A. & Trainor, L. J. Frontal brain electrical activity (EEG) distinguishes
valence and intensity of musical emotions. Cognition and Emotion, 2001;15(4):487–0.

795
796
797

[11] Altenmüller Eckart, Kristian Schürmann, Vanessa K. Lim, & Dietrich Parlitz, et
al. Hits to the left, flops to the right: Different emotions during listening to music are
reflected in cortical lateralisation patterns. Neuropsychologia. 2002;40(13):2242–2256.

798
799
800

[13] Zentner, M., Grandjean, D. & Scherer, K. R. Emotions evoked by the sound of
music: Characterization, classification, and measurement. Emotion, 2008;8(4):494–
521.

801
802

[14] Lundqvist, L., Carlsson, F., Hlimerson P. & Juslin, N. Emotional responses to
music: Experience, expression, and physiology. Psychology of Music, 2009;37:61–90.

803
804

[15] Juslin P. N. & Sloboda, J. editors. Handbook of Music and Emotion: Theory and
Research. Oxford: Oxford University Press; 2010

805
806

[16] Trost, W., Ethofer, T., Zentner, M & Vuilleumier, P. Mapping aesthetic musical
emotions in the brain. Cerebral Cortex, 2012 ;22:2769–2783.

807
808
809
810

[17] Vuoskoski, J. K., & Eerola, T. Can sad music really make you sad? Indirect
measures of affective states induced by music and autobiographical memories.
Psychology of Aesthetics, Creativity, and the Arts, 2012;6(3):204–13.
doi:10.1037/a0026937.

35

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

811
812
813

[18] Paquette S, Peretz I, Belin P. The “Musical Emotional Bursts”: a validated set of
musical affect bursts to investigate auditory affective processing. Frontiers in
psychology. 2013 Aug 13;4:509.

814
815

[19] Koelsch, S. Brain correlates of music-evoked emotions. Nature Reviews
Neuroscience, 2014;15(3):170–80. doi:10.1038/nrn3666.

816
817
818

[20] Eerola, T., & Peltola H-R. Memorable experiences with sad music: reasons,
reactions, and mechanisms of three types of experiences. PloS One, 2016;11(6):
e0157444.

819
820

[21] Konečni, V. J. The aesthetic trinity: Awe, being moved, thrills. Bulletin of
Psychology and the Arts, 2005;5(2):27–44.

821
822

[22] Konečni, V. J. Does music induce emotion? A theoretical and methodological
analysis. Psychology of Aesthetics, Creativity, and the Arts, 2008;2(2):115–129.

823
824

[23] Konečni, V. J., Brown, A., & Wanic, R. A. Comparative effects of music and
recalled life-events on emotional state. Psychology of Music, 2008;36(3):289–308.

825
826
827

[24] James CE, Britz J, Vuilleimier P, Hauert CA, Michel CM. Early neuronal responses
in right limbic structures mediate harmony incongruity processing in musical experts.
Neuroimage 2008;42:1597–1608.

828
829
830

[25] Gabrielsson, A. & Lindström E. The role of structure in the expression of emotions.
In P. N. Juslin & J. Sloboda J. (eds), Handbook of Music and Emotion: Theory and
Research. Oxford: Oxford University Press; 2010: 367–400.

831
832

[26] Gabrielsson, A. Strong Experiences with Music: Music is Much More than just
Music. Oxford University Press; 2011.

833
834

[27] Trainor, L. J. & Trehub, S. E. The development of referential meaning in music.
Music Perception, 1992;9:455–470.

835
836
837
838

[28] Bergman, D. & Gabrielsson, A. Children's judgments of structural patterns and
emotional expression in music. In A. Gabrielsson (ed.), Proceedings of The Third
Triennial ESCOM Conference. Uppsala: Uppsala University, Department of
Psychology; 1997 p. 430–434.

839
840
841

[29] Koelsch, S., Fritz, T., V Cramon D. Y., Müller K. & Friederici A. D, et al.
Investigating emotion with music: an fMRI study. Human Brain Mapping,
2006;27(3):239–50.

842
843
844
845

[30] Küssner, M. B. & Koelsch, S.. Music-induced emotions: An fMRI-study
investigating fear and happiness networks. Proceedings of the 3rd International
Conference of Students of Systematic Musicology; 2010 Sep 13-15; Cambridge, UK;
2010. p. 56-59.

846
847
848

[31] Sammler, D., Grigutsch, M., Fritz, T. & Koelsch, S. Music and emotion:
Electrophysiological correlates of the processing of pleasant and unpleasant music.
Psychophysiology 2007;44(2): p. 293–304. doi:10.1111/j.1469-8986.2007.00497.x.

849
850

[32] Sourina, O., Liu, Y., & Nguyen, M. K. Real-time EEG-based emotion recognition
for music therapy. Journal on Multimodal User Interfaces, 2012;5(1):27-35.

851
852
853

[33] Mikutta, C., Altorfer, A., Strik, W., & Koenig, T, et al. Emotions, arousal, and frontal
alpha
rhythm
asymmetry
during
Beethoven’s
5th
symphony. Brain
Topography, 2012;25(4):423-430.
36

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

854
855

[34] Mikutta, C. A., Maissen, G., Altorfer, A., Strik, W., & König, T. (2014). Professional
musicians listen differently to music. Neuroscience, 2014;268:102–111.

856
857

[35] Hsu, J. L., Zhen, Y. L., Lin, T. C., & Chiu, Y. S. Affective content analysis of music
emotion through EEG. Multimedia Systems, 2017 p.1–16.

858
859
860

[36] Putkinen, V., Makkonen, T. & Eerola, T. Music-Induced Positive Mood Broadens the
Scope of Auditory Attention. Social Cognitive and Affective Neuroscience [Internet]. 2017.
doi:10.1093/scan/nsx038.

861
862

[37] Bradley, M. M., & Lang, P. J. The International Affective Digitized Sounds (IADS-2):
Affective ratings of sounds and instruction manual. University of Florida: Gainesville; 2010

863
864

[38] Schubert, E. Measurement and time series analysis of emotion in music (Vol. 1). Sydney,
Australia: University of New South Wales; 1999

865
866

[39] Sloboda, J. A. Music structure and emotional response: Some empirical findings.
Psychology of music, 1991;19(2):110-120.

867
868

[40] Charmaz, K. Constructing Grounded Theory: A Practical Guide through Qualitative
Analysis. London; Thousand Oaks, California: Sage; 2014

869
870

[41] Tan, S.L., & Kelly, M. E. Graphic representations of short musical compositions.
Psychology of Music, 2004;32(2):191–212.

871
872
873

[42] LeBlanc, A., Jin, Y. C., Simpson, C. S. & Stamou, L., McCrary, J, et al. Pictorial versus
verbal rating scales in music preference measurement. Journal of Research in Music Education,
1998;46(3):425–35.

874
875

[43] Schubert, E. Update of the Hevner adjective checklist. Perceptual and Motor Skills,
2003;96(3c):1117–1122.

876
877
878

[44] Davidson, R. J., Schwartz, G. E., Saron, C., Bennett, J., & Goleman, D. J. Frontal versus
parietal EEG asymmetry during positive and negative affect. Psychophysiology,
1979;16(2):202–203.

879
880

[45] Heller, W. Neuropsychological mechanisms of individual differences in emotion,
personality, and arousal. Neuropsychology, 1993;7:476–489.

881
882

[46] Bezuidenhout, M. An interview with Hendrik Hofmeyr: Chronicles. Musicus.
2007;35(2):19–21.

883
884

[47] Franke, V. M. Structure and context in the orchestral compositions of Hendrik Hofmeyr.
Musicus, 2007;35(2):57–71.

885
886

[48] May, J. Hendrik Hofmeyr at fifty: a short biography with a worklist and discography:
Chronicles. Musicus, 2007;35(2):7–18.

887
888

[49] Romanou, K. Skalkottas, N. In K. Romanou editor. Serbian and Greek Art Music a Patch
to Western Music History. Bristol, UK; Chicago: Intellect Books; 2009 p. 163–185.

889
890

[50] Mantzourani, E. The Life and Twelve-Note Music of Nikos Skalkottas. Farnham, Surrey,
England; Burlington, VT: Ashgate; 2011

891
892

[51] Hofmeyr, H. (2007). Luamerava for solo violin. Journal of Musical Arts in Africa,
4(1):47–54.

893
894

[52] Heller, W., & Levy, J. Perception and expression of emotion in right-handers and lefthanders. Neuropsychologia, 1982;19:263–272.
37

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

895
896

[53] Brookshire, G., & Casasanto, D. Motivation and motor control: hemispheric specialization
for approach motivation reverses with handedness. 2012;PLoS One, 7(4), e36036.

897
898
899
900

[54] Dawson, G. Frontal electroencephalographic correlates of individual differences in
emotional expression in infants. In N.A. Fox editor. The development of emotion regulation:
Behavioural and biological considerations. Monographs of the Society for Research in Child
Development, 59 (2–3, Serial No. 240, pp. 135–151); 1994.

901
902
903
904

[55] Schmidt, L. A., & Fox, N. A. Patterns of cortical electrophysiology and autonomic activity
in adults’ shyness and sociability. Biological Psychology, L.A. Schmidt; 1994 38, 183–198.
Frontal brain electrical activity in shyness and sociability. Psychological Science,
1999;10:316–320.

905
906

[56] Jasper, H. H. The ten-twenty electrode system of the International Federation.
Electroencephalography and Clinical Neurophysiology, 1958;10:371–375.

907
908
909

[57] Marcuse, L. V., Schneider, M., Mortati, K. A., Donnelly, K. M., Arnedo, V., & Grant, A.
C, et al. Quantitative analysis of the EEG posterior-dominant rhythm in healthy adolescents.
Clinical Neurophysiology, 2008;119(8):1778–1781.

910
911

[58] Lodder, S. S. & van Putten, M. J. Automated EEG analysis: Characterizing the posterior
dominant rhythm. Journal of Neuroscience Methods, 2011;200(1):86–93.

912
913

[59] Korczak P., Smart J., Delgado R., Strobel T. M & Bradford, C, et al. Auditory steady-state
responses. Journal of the American Academy of Audiology, 2012;23(3):146–170.

914
915
916

[60] Eerola, T., Ferrer, R., & Alluri, V. Timbre and affect dimensions: evidence from affect
and similarity ratings and acoustic correlates of isolated instrument sounds. Music Perception:
An Interdisciplinary Journal, 2012;30(1):49–70.

917
918

[61] White, G. D. & Louie, D. J. The Audio Dictionary, third edition. Seattle; London:
University of Washington Press; 2005.

919
920

[62] Glass, A., & Kwiatkowski, A. W. Power spectral density changes in the EEG during
mental arithmetic and eye-opening. Psychological Research, 1970;33(2):85–99.

921
922
923

[63] Barry, R. J., Clarke, A. R., Johnstone, S. J., Magee, C. A. & Rushby, J. A, et al. EEG
differences between eyes-closed and eyes-open resting conditions. Clinical Neurophysiology.
2007;118(12): 2765–2773.

924
925
926

[64] Barry, R. J., Clarke, A. R., Johnstone, S. J., & Brown, C. R, et al. EEG differences in
children between eyes-closed and eyes-open resting conditions. Clinical Neurophysiology.
2009;120(10): 1806-1811.

927
928

[65] Peretz, I., Gagnon, L. & Bouchard, B. Music and emotion: perceptual determinants,
immediacy, and isolation after brain damage. Cognition, 1998;68(2):111–141.

929
930
931

[66] Nieminen, S., Istok, E., Brattico, E., Tervaniemi, M., Huotilainen, M, et al. The
development of aesthetic responses to music and their underlying neural and psychological
mechanisms. Cortex, 2011;47:1138–1146.

932
933

[67] Luck, S. J. An Introduction to the Event-Related Potential Technique. Cambridge, MA:
MIT Press; 2005

934
935
936

[68] Uludag, K., Dubowitz, D. J., & Buxton, R. B. Basic principles of functional MRI. In R.
R. Edelman, J. R. Hesselink, M. B. Zlatkin & J.V. Cures III editors. Clinical magnetic
resonance imaging. Vol. 1, 3rd ed. Philadelphia: Saunders Elsevier; 2005 p. 249–287.

38

bioRxiv preprint doi: https://doi.org/10.1101/790972. The copyright holder for this preprint (which was not peer-reviewed) is the
author/funder. It is made available under a CC-BY 4.0 International license.

937
938
939

[69] Pascual-Marqui R. D., Michel C. M & Lehmann, D. Low-resolution electromagnetic
tomography: a new method for localizing electrical activity in the brain. International Journal
of Psychophysiology, 1994;18:49–65.

940
941

[70] Skouras, S., Gray, M., Critchley, H. & Koelsch, S. fMRI scanner noise interaction with
affective neural processes, 2013. PloS one, 8(11):e80564.

942
943

[71] Luu, P. & Ferree, T. Determination of the HydroCel Geodesic Sensor Nets’ Average
Electrode Positions and Their 10–10 International Equivalents. Inc, Technical Note; 2005

944
945
946

[72] Yuvaraj, R., Murugappan, M., Ibrahim, N. M., Omar, M. I., Sundaraj, K., Mohamad, K.,
et al. On the analysis of EEG power, frequency, and asymmetry in Parkinson’s disease during
emotion processing. Behavioral and Brain Functions, 2014;10(1):12.

947
948
949

[73] Palaniappan, R. Utilizing gamma-band to improve mental task-based brain-computer
interface design. IEEE Transactions on Neural Systems and Rehabilitation
Engineering, 2006;14(3):299-303.

950
951
952

[74] Yan, Z., & Fischer, K. W. Pattern emergence and pattern transition in microdevelopmental
variation: Evidence of complex dynamics of developmental processes. Journal of
Developmental Processes, 2007;2(2):39–62.

953
954
955

[75] Fischer, K. W., & Bidell, T. R. Dynamic development of action, thought, and emotion. In
W. Damon & R. M. Lerner, editors. Theoretical Models of Human Development. Handbook
of Child Psychology. New York: Wiley; 2006 p. 313–399.

956
957

[76] Agawu, V. K. The challenge of semiotics. In: N. Cook & M. Everist editors. Rethinking
Music. Oxford: Oxford University Press; 1999 p. 138–160.

958
959
960

[77] Mugglestone, E. & Adler, G. Guido Adler’s “The scope, method, and aim of
musicology” (1885): An English translation with an historico-analytical commentary.
Yearbook for Traditional Music. 1981;13(1):1–21.

961
962
963

[78] Parncutt, R. Systematic musicology and the history and future of western musical
scholarship. Journal of Interdisciplinary Music Studies. 2007;1(1):1–32.

i Music can be defined as ‘sounds that are subject to some form of human organization (either in production or reception or both) and
communication that is not the same as spoken or written language’, with ‘internal affective processing’ as one of the contributing factors to the
universality of music (please see the views of Agawu [76] on the myth that music is a universal language. He makes it very clear that although
music shares some properties of language, it cannot be considered as a language in a linguistic sense.

ii The field of Musicology has been traditionally subdivided into History Musicology and Systematic Musicology (1885) [78]. In this
division Historic Musicology deals with research related to historic issues, and Systematic Musicology with music-theoretical, structural and stylerelated formal issues. Since Adler’s two-tiered division, the field of Musicology has expanded into numerous subdisciplines, of which
Neuromusicology is but one of a multitude of fields that are researched [79]. Despite the addition and rearrangement of subdisciplines in the
overarching umbrella term, Systematic Musicology still refers to the study of music theoretic principles. The music-theoretical analyses methods
have been standardised. As such there is no need for music-theoretical analyses of the two compositions by a panel of experts. The musictheoretical tools that are used in the analyses are universal, at least for Western Classical Music, as represented in the two different musical
compositions.

39

