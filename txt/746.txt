Real-Time Wireless Sonification of Brain Signals

Real-Time Wireless Sonification of Brain Signals
Mohamed Elgendi1,2, Brice Rebsamen3, Andrzej Cichocki4, Francois Vialatte4,5, Justin
Dauwels2
1Institute for Media Innovation, Nanyang Technological University, Singapore, 2School of
Electrical and Electronic Engineering, Nanyang Technological University, Singapore,
3Temasek Laboratories, National University of Singapore, Singapore, 4Lab. ABSP, RIKEN
Brain Science Institute, Wako-Shi, Japan, 5ESPCI ParisTech, Paris, France
e-mail: elgendi@ntu.edu.sg, brice.rebsamen@gmail.com, cia@brain.riken.jp,
fvialatte@brain.riken.jp, jdauwels@ntu.edu.sg
Summary
In this paper, an alternative representation of EEG is investigated, in particular, translation of EEG into sound;
patterns in the EEG then correspond to sequences of notes. The aim is to provide an alternative tool for
analysing and exploring brain signals, e.g., for diagnosis of neurological diseases. Specifically, a system is
proposed that transforms EEG signals, recorded by a wireless headset, into sounds in real-time. In order to assess
the resulting representation of EEG as sounds, the proposed sonification system is applied to EEG signals of
Alzheimer’s (AD) patients and healthy age-matched control subjects (recorded by a high-quality wired EEG
system). Fifteen volunteers were asked to classify the sounds generated from the EEG of 5 AD patients and 5
healthy subjects; the volunteers labeled most sounds correctly, in particular, an overall sensitivity and specificity
of 93.3% and 97.3% respectively was obtained, suggesting that the sound sequences generated by the
sonification system contain relevant information about EEG signals and underlying brain activity.

1. Introduction
One of the interesting multidisciplinary
applications of EEG is sonification, i.e., converting
the brain waves into music.
As far as we know, sonification was for the first
time attempted in 1965 by Alvin Lucier (composer)
and Edmond Dewan (physicist); in their composition,
called Music for Solo Performer [3], human brain
waves control percussion instruments. Although
several researchers and musicians tried to generate
sound from EEG signals, there are still many open
questions and challenges, and plenty of opportunities.
For example, the recent advent of convenient
wireless EEG headsets [4-8] may further stimulate
and advance the area of EEG sonification.
In this study we design and implement a system
that in real-time translates EEG signals, recorded
from a wireless EEG headset, into sounds. We assess
the sound representations in an offline fashion, by
applying our sonification system to EEG collected
from Alzheimer’s disease (AD) patients and from
healthy subjects. The sounds generated from AD
EEG should be distinct from sounds extracted from

the EEG of healthy subjects. We investigate whether
our EEG sonification system improves diagnosis of
AD, following an approach proposed earlier by
Vialatte et al. [9].
The paper is structured as follows. In the next
section we explain our methodology. In Section 3 we
evaluate our system offline by means of an EEG
dataset of AD patients and control subjects. In
Section 4 we discuss our results and offer concluding
remarks.

2. Methods
The proposed sonification system has two operating
modes: offline and real-time sonification. In the
offline mode, the system extracts sounds from EEG
signals that have been recorded earlier. In Section 3,
we will apply our system to an EEG dataset from
Alzheimer’s patients and control subjects, recorded
by a wired high-performance EEG system. In
real-time mode, EEG signals are acquired and
immediately transformed into sounds. In the
following, we will elaborate on the EEG signal
acquisition. Next we will explain how we extract
sounds from EEG.

Real-Time Wireless Sonification of Brain Signals

2.1. Data Acquisition
The real-time EEG signals have been collected using
a wireless EEG headset, specifically the Emotiv
EPOC wireless headset [4] with a sampling
frequency 128Hz. The headset has fourteen data
collecting electrodes and two reference electrodes.
The electrodes are placed approximately at the 10-20
locations AF3/4, F3/4, FC5/6, F7/8, T7/8, P7/8, and
O1/2. We used the software package BCI2000 [10] to
interface with the Emotiv EPOC wireless headset.
The headset transmits encrypted data wirelessly to a
laptop computer.
The Emotiv headset is mostly intended for
entertainment (e.g., video games) rather than research
or medical applications [4]. However, it is
inexpensive and user-friendly, and with suitable
signal processing, it may become suitable for
research and clinical purposes. In particular, the
device seems to be prone to various artefacts (such as
eye blinking, ECG, EMG, body movements, power
sources, etc). In our ongoing work, we are developing
real-time algorithms for removing artefacts, which is
a crucial step towards reliable real-time EEG
sonification.

2.2. Sonification
The system computes the relative power in three
non-overlapping frequency bands (4-10Hz, 10-20Hz,
and 20-30Hz) and generates notes from the computed
values. The EEG spectrum is known to depend on the
mental state (e.g., relaxation, sleep); moreover,
abnormal EEG spectra seem to be associated with
neurological disorders, e.g., Alzheimer’s disease
(AD) [11,12]. We characterize the EEG spectrum by
computing relative power in three different EEG
frequency bands. Relative power is a simple measure
that can readily be computed in real-time. In future
work, we will experiment with other spectral
measures as well.
We now provide more details on the sonification
algorithm. The power spectrum P is calculated for
each EEG channel; next relative power features f1, f2,
and f3 are calculated:
P(4  10Hz)
P(10  20Hz)
and
f1 
f2 
P(4  30Hz)
P(4  30Hz)
P(20  30Hz) .
f3 
P(4  30Hz)
Those features are averaged across all channels.
The averaged features are then mapped to music

notes. To keep the generated sounds as simple and
transparent as possible, we considered only notes
from one octave (MIDI Octave -1) with pentatonic
scale (five notes per octave); we limited ourselves to
only one instrument (acoustic bass). Obviously, one
could incorporate more music instruments and
multiple octaves. However, the extracted sound
easily becomes cacophonic and difficult to parse. In
the future, we will explore alternative schemes to
generate music from EEG relative power. We
consider the following three notes and corresponding
MIDI note number: (C,48), (E,52), and (A,57). Those
3 notes will be played according to the three values
of relative power (f1, f2, f3): If feature fi is above a
certain threshold THi, note i is played. More
precisely, the notes are generated as follows:
IF
IF
IF

f1  TH1
f 2  TH 2
f 3 TH3

THEN play bass note 48
THEN play bass note 52
THEN play bass note 57.

The EEG is divided in consecutive segments of 1s. In
each segment the features (f1, f2, f3) are computed,
and notes are generated according to the above rule.
Note that at most three notes can be generated for
each EEG segment; that occurs when all three
features are above threshold. Typically, however, one
or two notes are played during each segment, which
leads to simple sequences of notes. In future work,
we hope to extract more melodic and harmonic
compositions, perhaps by mapping features to
multiple notes, music samples, natural sounds, etc.
We implemented our sonification system in Python
(specifically, pyPortMidi [13] and Numpy [14]).
The generated MIDI sequences are synthesized by
SyFonOne [15] in conjunction with MIDI-YOKE
[16]. The sound sequences are saved into MP3 files
for further offline analysis.

3. Evaluation
Our sonification system translates EEG signals into
sounds. It is important to verify whether the sounds
are representative of EEG. To this end, we conducted
a test: We asked several volunteers to use our EEG
sonification system for diagnosing Alzheimer’s
disease. The procedure is as follows. By means of our
sonification system, we extract sounds from EEG
signals of Alzheimer’s patients (AD) and
age-matched control subjects. We ask the volunteers
to label the generated sounds (AD vs. healthy). If the

Real-Time Wireless Sonification of Brain Signals

sounds reliably represent the EEG signals, it should
be possible to distinguish sounds generated from AD
EEG from sounds extracted from healthy EEG.
Interestingly, the volunteers were indeed able to
reliably classify the sounds. In the following, we
describe our EEG data set; next we discuss the test
procedure, and present our results.

3.1. EEG Dataset
We consider EEG data of mild-AD patients and
age-matched control subjects. The EEG data set has
been analyzed in previous studies [17-19]; the data
was obtained using a strict protocol from Derriford
Hospital, Plymouth, U.K., and had been collected
using normal hospital practices [18]. This EEG
dataset is composed of 24 healthy Ctrl subjects (age:
69.4±11.5 years old; 10 males) and 17 patients with
mild AD (age: 77.6±10.0 years old; 9 males). The
EEG time series were recorded using 21 electrodes
positioned according to Maudsley system, similar to
the 10-20 international system, at a sampling
frequency of 128 Hz. EEGs were band-pass filtered
with digital third-order Butterworth filter (forward
and reverse filtering) between 0.5 and 30 Hz. For
each patient, an EEG expert selected by visual
inspection one segment of 20s artifact free EEG,
blinded from the results of the present study. From
each subject, one artifact-free EEG segment of 20s
was extracted and analysed.

3.2. Classification Procedure
A critical issue in our sonification system is the
choice of thresholds THi. Depending on the
application, we can determine the thresholds through
various statistical principles. In the application at
hand, we determine the thresholds THi with the aim
of detecting AD EEG. We noticed that relative EEG
power has substantially different values in AD
patients than in healthy subjects. By appropriately
choosing the thresholds, the generated sounds will
differ as well. Following this reasoning, we have
determined the thresholds as follows:
(μ (f )  σ A(f1 )) + (μH (f1 ) + σ H (f1 )
TH1 = A 1
,
2
(μ (f )  σ H (f2 )) + (μ A(f2 ) + σ A(f2 )
TH2 = H 2
,
2

(μH (f3 )  σ H (f3 )) + (μ A(f3 ) + σ A(f3 )
.
2
where μ A and σ A is the mean and standard deviation
TH3 =

respectively of the features for AD EEG, and
likewise μH and σ H for healthy (control) EEG. Those
choices of thresholds can be understood as follows.
For example, relative power in the 4-10Hz band is
clearly larger in AD patients. Therefore, we choose
the corresponding threshold TH1 below the mean
value (of relative power in the 4-10Hz band) for AD
EEG and above the mean value for control EEG. As a
result, for AD EEG the threshold TH1 will be reached
more often, which will lead to more frequent
low-pitch notes (bass note 48). Similarly, AD EEG
will yield fewer high-pitch notes (E,52) and (A,57).
Now we explain our survey in more detail. We asked
15 volunteers to listen to the generated sounds, and to
guess whether they stem from AD patients or healthy
subjects. Particularly, we asked each volunteer to
classify sound sequences from 10 different subjects
(one sequence from each subject). Each volunteer
was asked to score the sound sequences from 0 to 10
(0: certainly healthy, 5: unsure, and 10: certainly
Alzheimer’s). We did not provide any further details
about the sound files.
Prior to this test, each volunteer was trained with
sound sequences from 4 subjects (2 AD patients and
2 healthy subjects), so that they can learn to
appreciate how the sounds generally different in both
subjects groups; we also briefly explained how the
sounds were generated, and emphasized that, in our
sonification scheme, AD EEG tends to generate more
low-pitch notes.

3.3. Results
Overall, the volunteers were able to reliably label
the sound sequences; they correctly classified 95% of
the subjects, with sensitivity of 93.3% and specificity
of 97.3%. Note that we tested just 10 subjects out of
41, and classification on the entire database might be
worse. Nevertheless, this experiment demonstrates
that the proposed sonification system translates EEG
into meaningful sounds, which can for example be
used for detecting EEG abnormalities (as in, e.g., AD
EEG).
As a benchmark, we conducted linear

Real-Time Wireless Sonification of Brain Signals

discriminant analysis (LDA) with the same features
(f1, f2, f3) for the same 10 subjects; we average those
features over the entire EEG segment of 20s. In other
words, we do not consider here individual EEG
segments of 1s. We compute classification rates
through leave-one-out crossvalidation. It is
noteworthy that through this approach, at most 90%
of the subjects are correctly classified. In contrast,
our sonification system yielded classification rates of
95%.

4. Discussion and Conclusion
In this study we have developed a system that
translates EEG signals (acquired by a wireless
headset) to sounds in real-time. The proposed
sonification system has been validated offline by
means of a small EEG data set, collected with high
quality wired EEG headset.
Interestingly, the results show that the presented
sonification algorithm can be used to differentiate
offline, by listening to their sonified EEG, the subject
with the mild Alzheimer’s disease from control
subjects with 95% accuracy (see samples on internet
[20]), and therefore, it seems the real-time system
can be used as a reliable AD diagnostic tool.
Acknowledgments Mohamed Elgendi and Justin
Dauwels would like to thank the Institute for Media
Innovation (IMI) at Nanyang Technological
University (NTU) for partially supporting this project
(Grant M58B40020).

References
[1] Berger, H.: Über Das Elektrenkephalogramm Des
Menschen.
Archiv
für
Psychiatrie
und
Nervenkrankheiten 87 (1929) 527-570
[2] Berger, H.: On the Electroencephalogram of Man.
Electroencephalography and Clinical Neurophysiology
(1969) 28:133
[3] Lucier, A.: Statement on: music for solo performer.
Biofeedback and the Arts: Results of Early
Experiments (Vancouver, Canada: Aesthetic Research
Centre of Canada) (1967)
[4] EmotivSystems. Emotiv - brain computer interface
technology. http://emotiv.com.
[5] Imec:
http://www2.imec.be/be_en/press/imec-news/imecEE

GMDMWest.html.
[6] NeuroFocus: http://www.neurofocus.com/.
[7] MKS:
http://www.mks.ru/eng/Products/EEG/Neurobelt/.
[8] Biopac:
http://www.biopac.com/researchApplications.asp?Aid
=23&AF=437&Level=3.
[9] Vialatte, F., Musha, T., Cichocki, A.: Sparse Bump
Sonification: a New Tool for Multichannel EEG
Diagnosis of Brain Disorders. Artificial Intelligence in
Medicine (2010)
[10] BCI2000 - General-Purpose System for Brain
Computer
Interface
http://www.bci2000.org/BCI2000/Home.html.
[11] Dauwels, J., Srinivasan, K., Reddy, R., Musha, T.,
Vialatte, F., Latchoumane, C., Jeong, J., Cichocki, A.:
Slowing and loss of complexity in Alzheimer’s EEG:
Two sides of the same coin? International Journal of
Alzheimer's Disease((in press)) (2011)
[12] Vialatte, F., Cichocki, A., Dreyfus, G., Musha, T.,
Rutkowski, T.M., Gervais, R.: Blind Source
Separation and Sparse Bump Modelling of Time
Frequency Representation of Eeg Signals: New Tools
for Early Detection of Alzheimer's Disease. Paper
presented at the IEEE Workshop on Machine Learning
for Signal Processing, 28-28 Sept. 2005
[13] http://alumni.media.mit.edu/~harrison/code.html.
[14] http://new.scipy.org/download.html.
[15] http://www.synthfont.com/.
[16] http://www.midiox.com/.
[17]Goh, C., Ifeachor, E., Henderson, G., Latchoumane, C.,
Jeong, J., Bigan, C., Besleaga, M., Hudson, N.,
Capotosto, P., Wimalaratna, S.: Characterisation of
EEG at different stages of Alzheimer’s disease (AD).
Clinical Neurophysiology 117 (2006) 138-139
[18] Henderson, G., Ifeachor, E., Hudson, N., Goh, C.,
Outram, N., Wimalaratna, S., Del Percio, C., Vecchio,
F.: Development and assessment of methods for
detecting
dementia
using
the
human
electroencephalogram.
IEEE
Transaction
on
Biomedical Engineering 53 (2006) 1557-1568
[19] Dauwels, J., Vialatte, F., Latchoumane, C., Jeong, J.,
Cichocki, A.: EEG synchrony analysis for early
diagnosis of alzheimer’s disease: A study with several
synchrony measures and EEG data sets. Paper
presented at the 31st Annual International Conference
of the IEEE EMBS, Minneapolis, Minnesota, USA,
[20] http://sonification.webs.com/audio.htm.

