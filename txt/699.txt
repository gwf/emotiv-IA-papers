Proceedings of the 50th Hawaii International Conference on System Sciences | 2017

Excuse Me, Do I Know You From Somewhere? Unaware Facial Recognition
Using Brain-Computer Interfaces
Christopher Bellman
University of Ontario Institute
of Technology
Christopher.Bellman@uoit.net

Miguel Vargas Martin
University of Ontario Institute
of Technology
Miguel.VargasMartin@uoit.ca

Shane MacDonald
University of Ontario Institute
of Technology
Shane.MacDonald2@uoit.net

Ruba Alomari
University of Ontario Institute of Technology
Ruba.Alomari@uoit.ca

Ramiro Liscano
University of Ontario Institute of Technology
rliscano@ieee.org

Abstract

in/under (invasive) the skin, skull, or even the brain to
record the brain’s electrical activity [1]. While
traditional EEG setups are most commonly used and
associated with the medical and neuroscience research
community, Brain-Computer Interfaces (BCIs) have
been used in a more general and non-medical manner
to record these EEG signals without having to resort to
a medical procedure or lab setup. In recent years, noninvasive consumer-grade BCI headsets have been
made available to the market at reasonable prices to
allow for cheaper, broader access to EEG technology.
While modern consumer-grade headsets do not have
the features or quality that the medical-grade EEG
caps have, they are a useful tool for consumers and
researchers to be able to work with, play with, and
study the human brain. Some examples of consumer
use for BCI devices include EMOTIV’s wide variety
of brain-based games, which include such activities as
controlling an RC helicopter [2] or playing a game of
Tetris [3], all with your brain. Other examples include
NeuroSky’s online store where you can find games
such as bowling [4], or even a BCI version of the
widely popular Flappy Bird game [5]. With rapid
advancement in modern EEG technologies and the
introduction of consumer-grade non-invasive BCI
headsets, it seems to suggest that EEG signals and BCI
devices may start to become more and more involved
in our daily lives, perhaps someday being able to
accurately control your music based on your mood, or
drive your car by thinking about it. With these
potential practical application on the horizon, we, in
this work, show how we can utilize the human brain to
be able to recognize faces that we are unaware that we
have seen previously. We define an “unaware
recognition” as a participant “recognizing a face
without being aware that any facial recognition took
place in their mind”.

While a great deal of research has been done on
the human brain’s reaction to seeing faces and
reaction to recognition of these faces, the unaware
recognition of faces is an area where further research
can be conducted and contributed to. We performed a
preliminary experiment where participants viewed
images of faces of individuals while we recorded their
EEG signals using a consumer-grade BCI headset.
Pre-selection of the images used in each of the three
phases in the experiment allowed us to tag each image
based on what state of recognition we expect the image
to take – No Recognition, a Possible Unaware
Recognition, and a Possible Aware Recognition. We
find, after filtering, artifact removal, and analysis of
the participants’ EEG signals recorded from a
consumer-grade BCI headset, obvious differences
between the three classes of recognition (as defined
above) and, more specifically, unaware recognitions,
can be easily identified.

1. Introduction
Facial recognition research is not a newly studied
field and it has been the focus of many studies over the
past number of decades (see Section 2 – Related
Work). While conscious facial recognition is
becoming more and more understood and studied,
facial recognitions at an unaware level is an area that
has potential to be investigated, especially with
advancements in modern technologies that allow us to
approach the research from different angles.
Electroencephalography (EEG) is a technology
that uses electrodes placed on (non-invasive) or

URI: http://hdl.handle.net/10125/41298
ISBN: 978-0-9981331-0-2
CC-BY-NC-ND

1217

One of the brain’s features we can use to analyze
the brain’s response to stimuli are Event-Related
Potentials (ERPs). These ERPs are the brain’s reaction
to stimuli and appear as positive and negative spikes
or changes in voltage in the brain’s EEG signal, and
are simply ranges of time after a stimuli that we expect
these spikes to occur [6]. There are a number of ERPs
that are commonly used when analyzing EEG signals,
and we are focusing on a few specific ones most
commonly related to facial recognitions (see Section 2
– Related Work), as well as a more general look at the
signal to determine any apparent differences between
our three classifications of recognition; No
Recognition (NR), Possible Unaware Recognition
(PUR), and Possible Aware Recognition (PAR).
We designed an experiment where participants
wearing BCI headsets are shown a series of images
containing faces of individuals, and are told to view
the images and consider them. We break the
experiment up into three phases – simply “Phase One”,
“Phase Two”, and “Phase Three” – where the
participants are shown these faces, but with specific
images being shown in different orders and at varying
times. As the participants view these images, we
record their EEG signals which are later analyzed to
determine if unaware facial recognitions can be
accurately recorded and classified using BCI devices
and modern machine learning and analysis techniques.

2. Related Work
EEG analysis using BCIs to gather data has
become a very useful tool for gaining insight into the
workings of the human brain and how it can be used
to interact with computer systems. This relatively
cheap and safe technology works as an alternative for
those without the means to take part in more
expensive, elaborate, and invasive brain-reading.
From computer security [7] to visual design [8], BCIs
are being used in a number of fields to enhance and
improve current techniques, methodologies, and to
add new understanding in the way our brains work and
react.
In the next few sections, we discuss previous
works regarding such topics as unaware ERP
elicitations and application, facial recognitions, and
EEG signals and their processing and analysis.

2.1. Subconscious ERP Elicitations
While work regarding subconscious recognitions
using EEGs and BCIs are few, a number of works have
been done regarding the subconscious and the brain’s
subconscious reaction to events or activities. A

number of authors use “subconscious” in their work
but we prefer to use the term “unaware” for describing
events that take place in the brain without awareness.
Vargas Martin et al. [9] performed an experiment
where images of faces of famous individuals were
shown to participants in the hopes that roughly 20% of
images shown would be conscious recognized by the
participants and 80% would not. Being that famous
people are more likely to be seen often in passing
without conscious recognition, it was assumed that the
participants’ subconscious would recognize a subset
of the faces in the 80% group. They used different
ERPs for the testing and training of a support vector
machine (SVM) and found that they were able to
determine, with about 65% accuracy across all
explored ERPs, which faces in the set of famous
individuals were subconsciously recognized. Our
work presented here is similar to Vargas Martin et al.’s
experiment as faces were used as targets for aware and
unaware recognitions, but famous individuals were
used and the assumption was based on participants
recognizing some, but not all of the individuals
presented to them in their paper. We use faces that are
assumed to not be recognized at all and use a twophase approach to add an unaware recognition of
certain faces which are learned in the first phase. This
helps to remove any assumptions on participants’
levels of recognition of famous individuals. We hope
that with our analysis we can provide further depth
into the brain’s reaction to different classes of
recognition.
Shalgi and Deouell [10] ran an experiment
studying error processing in the human brain with
regards to conscious or unconscious errors where they
had participants bet money (real) on whether or not
they could answer questions in the experiment session
correctly. The level of wager that the participants
placed on an answer was used to define a level of
confidence in the answer. They found that an ErrorRelated Negativity index (ERN, an index for error
processing in the brain) remained the same for both
conscious and unconscious scenarios, but when only
looking at high-confidence trials (where participants
bet a larger sum of money), the ERN was only noticed
for errors that the participant was conscious of (i.e.,
they were aware of the error). This led them to
conclude that ERNs are related to awareness of an
error and that the amplitude of the elicited signal is
related to confidence.
The Implicit Association Test (IAT) [11] is a test
to determine if a participant has any associations (both
conscious and unconscious) between characteristics
such as human ones (e.g. gender, physical
characteristics, religion). Upon taking the IAT, a
participant may find biases or associations that they

1218

were unaware of along with associations that they had
(e.g. dark vs. bright symbolizing good vs. evil, being a
more obvious and common association). This
understanding that the brain can unconsciously think
and associate without the conscious mind being aware
of it is interesting and allows us to delve further into
the workings of the brain with respect to facial
processing and recognition.
To our knowledge, little other work has been
published regarding the subconscious recognition of
things or faces using EEG signals and consumer-grade
BCI devices.

2.2. Facial Recognition
Facial recognition is a process that human beings
do from dozens to potentially thousands of times per
day, and plays a huge role in our person-to-person
communication. From judging the structure of the
face, to the analysis of facial emotions or features, the
brain’s perception of faces is crucial in our recognition
process.
According to Seeck et al. [12], it was believed
human visual analysis did not take place before the
first 100 ms after visual stimulus, but they challenged
these assumptions. They had participants view faces
and recorded their EEG signals and found two Visual
Evoked Potentials (VEPs) – one occurring at the 5090 ms interval, and then again at the 190-600 ms, thus
challenging the previous notion that visual analysis
does not take place earlier than 100 ms [12]. George et
al. [13] confirmed the findings of Seeck et al. by
finding VEPs at the 50 ms range for facial recognition.
To challenge these findings by Seeck and George,
Debruille et al. [14] made the claim that it is facial
repetition – not recognition – that triggers VEPs before
the 100 ms mark, and that facial recognition takes
place in the post-100 ms time period. To back up these
claims, Eimer [15] ran a study regarding facial
recognition and found that facial identification
happens in the 130-200 ms range (the “N170” ERP),
which takes place separately from, and does not affect,
the facial recognition and familiarity process
(confirming Bentin and Deouell’s study [16]). Along
with this finding of facial identification, it was found
that the facial recognition and familiarity processes
take place in the 300-500 ms range (“N400f” ERP) and
the 500-700 ms range (“P600f” ERP) [15]. While
Eimer’s experiment [15] asked the participants to
judge their level of familiarity with a face shown to
them (chosen via results of a pilot study), our study
makes use of faces that are assumed to be completely
unfamiliar to the participants and faces are implicitly
learned during the study. Participants of our study are
not asked to do any action to indicate their level of

recognition during the later phases of the experiment,
thus minimizing artifacts or bias from any movement
or activity. With this knowledge of recognition,
varying levels of recognition were judged. In a study
by Bentin et al. [16], participants were shown faces of
famous individuals and non-famous, unfamiliar
individuals, and found that all faces shown triggered
an N170 ERP (confirming Eimer’s findings [15]), but
faces of individuals that were recognized (famous
individuals, in this case) elicited greater amplitude
during the N400 ERP whereas unfamiliar faces
elicited lesser amplitudes [16]. To supplement these
results, Caharel et al. [17] found that in a facial
recognition study of famous faces, non-famous
(unfamiliar) faces, and faces of the participant, less
familiar faces exhibited larger positive amplitudes
whereas self-images of the participant exhibited
smaller amplitudes.
The Sternberg task [18] tests and measures a
participant’s response time when presented with a
stimulus (in the case of Sternberg’s experiment,
symbols rather than images of faces), and it was found
that a positive recognition had a more rapid response
time than a negative recognition by an average of 50
ms. These findings may assist in our work as aware
facial recognitions may result in EEG signals with
lesser amplitudes than those that are unaware
recognitions, or even faces that are not recognized at
all, along with a knowledge that a recognized face may
show signals earlier than an unknown face to a subject.
Research by Mnatsakanian et al. [19] has
suggested that certain ERPs exist specifically for facial
recognition and processing (e.g. N170 [15] [16], VEPs
[12] [13]), which differs from non-facial recognition
that may be associated with different ERPs. This
insight offers us a suggestion of which specific ERPs
to focus on when considering facial recognition tasks,
and assists in future work on the subject.

2.3. EEG and Processing
A number of studies have taken place using BCIs
and EEG data to further understand the brain’s
function when assigned a variety of jobs or actions. In
a study by Solovey et al. [20], participants were given
a variety of multi-tasking activities. They used
Functional Near-Infrared Spectroscopy (fNIRS) [21]
to analyze the participants’ brains during the tasks and
were able to determine the mental processes involved
in multi-tasking [20]. Peck et al. [8] also made use of
fNIRS in analyzing the brain’s perception of visual
designs. fNIRS data was recorded from participants
viewing designs and choices within them and they
found they were able to determine how the brain reacts
to visual design [8].

1219

There are examples of research done using
machine learning techniques to classify various
features from EEG signals such as Lee et al. [22], who
used a Bayesian Network classifier on EEG data from
an experiment which tasked participants with a variety
of cognitive and non-cognitive activities. They found
that they could classify and identify the various tasks
performed by the participants with high average
accuracies of ~93% [22]. Our work does not make use
of machine learning techniques as the signals are
easily differentiated using a basic threshold, but in
future works with greater number of participants,
machine learning may be required to classify the
different levels of recognition within the signals.
The actual processing of EEG signals has been the
subject of study for a long time, and still seems to be
under debate as to which methods provide the best
results, especially in the BCI community. There are
various toolsets and libraries dedicated to assisting in
the capture, processing, manipulation, and analysis of
EEG signals from BCIs. For our work here, we choose
to use EEGLAB [23], a MATLAB toolbox, for
manipulation and pre-processing of raw EEG signals,
then we provide our own analysis and post-processing.

3. Experiment
In order to determine the feasibility of classifying
unaware facial recognitions using BCIs, an experiment
was conducted with a total of three phases. In the first
phase, a number of images depicting unknown (to the
participant) faces were shown to participants. In the
second phase, the same number of images were shown
to participants, but a number of images from the first
phase were included in this second phase to serve as
possible unaware recognitions. The rest of the images
in this phase were unknown faces, much like all of
phase one. In the final phase – phase three - the same
number of images were shown again to the
participants, but this time the participants were given
a single face from the list of unknown faces pulled
from phase one and added to phase two, and were
asked to memorize the face and continue when they
felt comfortable that they had the face memorized.
Additional images were also taken from phase one and
added in to phase three to improve the number of
possible unknown recognitions that each participant
had. In this preliminary study, the three phases were
conducted separately across two days (phase one on
day one, phases two and three on day two) to prevent
possible conscious recognition of images intended to
be subconsciously recognized.
At all times during the experiment while
participants are viewing images (or the gaps in

between), their brains’ electrical output is being
measured (EEG) and recorded using the EPOC
headset, which is then used as the input for our
analysis. No manipulation or analysis is done during
the experiment – all data is saved until after phase
three for processing and analysis.

3.1. Participants
For this preliminary study, the participants only
included two of the five authors of this paper. Since
there is no deception involved in the experiment and
the images chosen for each phase of the experiment are
unknown via random selection based on a pre-defined
seed, the research team can take part in the experiment
with minimal bias, however, this does not mean that
there is no impact on the study. The research team is
aware of the structure of the experiment which may
alter the way the individuals’ brains react to seeing the
images of different classifications (No Recognition,
Possible Unaware Recognition, and Possible Aware
Recognition). This is a limitation that we hope to
address in future work exploring the results of this
study where we will use a broader general participant
pool with minimal knowledge of the experimental
setup and design.

3.2. Experimental Setup and Stimuli
All phases of the experiment used the Emotiv
EPOC headset [24] for data capture, which
records/samples data from the brain at 128 Hz,
producing 14 channels (each sensor) of readings taken
approximately every 7.8125 ms. Each of these
channels can be reconstructed to form graphs of the
signals similar to those depicted in Figures 1, 2, 3, and
4.
The EPOC BCI device is a consumer-grade EEG
headset. We are interested in using this headset due to
its potential ubiquity among consumers who may own
BCI headsets for entertainment or accessibility
purposes, rather than lab technicians and medical
research personnel. This ubiquity may allow for more
rapid realization of practical applications in a more
meaningful and consumer-friendly way than a large,
expensive, and complicated BCI headset. While we
would not consider the EPOC as a toy that someone
would normally pick up at a toy store, we do consider
it a toy in general seeing that many games and
interactive applications have been developed for
entertainment purposes [3]. The EPOC headset has 14
electrodes, all located according to the 10-20 system
for EEG electrode positioning [25]. The 14 electrodes
used on the headset are: AF3, F7, F3, FC5, T7, P7, O1,
O2, P8, T8, FC6, F4, F8, and AF4. They are placed

1220

externally on the scalp using pre-dampened felt pads
to conduct the signal through the skin. Due to this, the
EPOC is considered a non-invasive type of BCI [1].
For this experiment, all images of faces shown to
participants come from the FERET database [26] [27].
These images are of individuals’ faces positioned in
the center of the image, converted from colour to grayscale, and only images that are frontal shots of faces in
the pictures are used. The assumption is that
participants will not initially recognize any of the faces
shown to them from the database. In total, 366 images
were shown to participants across three phases (see
Table 1 for a breakdown of phases and recognition
classes within them). In the first phase, 162 images
were shown which included 60 images that were
repeated to be implicitly learned for the following
phases, plus the 102 NR images. 102 images were
shown in both phase two and phase three. 10 of the
images in phase two were considered PUR images
(learned in phase one) and the other 92 images were
again NR images. In the third phase, another 102
images were shown with 10 PUR images as well as an
additional 20 images for PAR, and the rest were NR
images. Three phases were chosen to be able to
separate the different recognition classes of images
and to allow participants to implicitly learn faces in the
first phase (day one of the experiment) that would be
carried into the second and third phases (day two) as
PUR images.
Table 1. Recorded Data Summary
Phase Data Recorded
1
 “No Recognition” (NR)
2
 “No Recognition” (NR)
 “Possible Unaware Recognition
(PUR)
3
 “No Recognition” (NR)
 “Possible Unaware Recognition
(PUR)
 “Possible Aware Recognition”
(PAR)
The experiment was conducted within a program
written in Python by the research team. This software
was designed to construct the subsets of faces that
were shown to participants, organize the different sets
into the three phases, display the images to the
participants, and to send signals via emulated serial
port to the EPOC headset to provide markers in the raw
output describing when images were shown and what
recognition class they belonged to. It was designed to
be simple for the participants to use during the
experiment and to minimize the amount of participant
movement by removing any need to use the mouse or

keyboard during the three phases as this may add
artifacts to the signal output.
3.2.1. Phase One. In the first phase of the experiment,
a number of images were shown to the participants of
the study. They were tasked with simply considering
the images shown to them. The goal of this phase was
that the participants would not recognize any of the
faces shown to them so that we could use the data
gathered here as baseline “No Recognition” (NR) data
for comparisons later on during the data analysis. In
this phase the images that are to be shown as unaware
images in later phases are shown to the participant a
total of four times each to reinforce the implicit
learning of the images.
3.2.2. Phase Two. In the second phase of the
experiment, the same number of images were shown
to the participant as in phase one, but a number of
images were inserted into the list from phase one. The
goal of this phase was to generate more NR data, but
to also hopefully gather “Possible Unaware
Recognition” (PUR) data when the participants view
the images that were taken from the first phase.
Ideally, the participants would view and implicitly
learn the faces shown to them in phase one [28] [29]
[30], and then elicit an unaware recognition of these
images in phase two.
3.2.3. Phase Three. In the third and final phase of the
experiment, participants were shown one of the
images that was tagged as PUR from the second phase
and asked to memorize the face. Once the participant
felt comfortable enough with the face that they could
remember it, they moved on and were again shown a
number of images. The images in this phase consisted
of a majority of NR images which they had not seen
before, but also, spaced evenly throughout the images,
there was the image of the face that they were asked to
study at the beginning of the phase. Assuming the
participant had memorized the face well enough to be
able to recognize it, upon viewing that face, the
participant would elicit a “Possible Aware
Recognition” (PAR). This final classification of data
allows us to now compare the EEG signals between
the three classes (No Recognition, Possible Aware
Recognition, and Possible Unaware Recognition).
Since only a limited number of images could be taken
from phase one and placed into phase two as PUR data
images, more images were taken from phase one and
placed into phase three to supplement the PUR data
count. Each image shown to the participants in all
three phases is shown for one second, with a one
second blank screen in between before moving on to
the next image.

1221

3.3. Data Pre-Processing
Prior to analysis, data was pre-processed to
remove or fix any artifacts that were found in the data.
EEGLAB was used for data analysis and preprocessing. This was chosen due to its large, mature
development community and the libraries and
functionality included. The first step taken to remove
any artifacts caused by blinking or other disturbances
was to run the data through a band-pass filter, filtering
0.1-15 Hz (as recommended by Bougrain et al [31]).
Next, the filtered data was run through one of
EEGLAB’s ICA filters, which helps eliminate any
remaining artifacts and smooth the waveform [32].
After filtering and early artifact reduction/removal via
ICA filter, any sections of image data that exhibited
amplitudes of +/- 100 microvolts (usually caused by
movements of the participants such as coughing, head
movements, blinking, etc.) were removed from
consideration as the data for that image may corrupt
any further analysis.
As a result of this data pre-processing, a number
of images were removed from analysis per participant
included in the study. An average of 16 and 18 images
for Participant 1 and 2 respectively were removed per
sensor. In future work this may cause issues if images
such as the PUR images in phase two or PAR images
in phase three become corrupt, thus leading to less data
for meaningful analysis.

human-visible difference between the three
classifications of recognition exists. The results of this
signal averaging can be seen in the following four
graphs (Figures 1, 2, 3 and 4), containing the averaged
brain activity of the two subjects immediately
following exposure to the presented images.
In figures 1 and 2, the average signals collected
during seven unique ERPs are displayed for each of
the three recognition types. ERPs were averaged for
each participant individually to allow us to get a
general sense of where each image type falls in terms
of voltage for each participant and to determine if the
difference between classifications of image can be
determined by a human without the need for machine
learning tools. For this experiment we segmented the
data into the following ERPs: VEPS1, VEPS2, N170,
P2, N200, P3N400, and N400F. These ERPs are just
segments of time in which we expect the brain to react
in a specific way, so each sensor’s data can be
segmented (14 sensors × 7 ERPs). In each of the
graphs, certain averaged signals cut off at certain
points due to the length of the ERP they are associated
with (some ERPs are shorter than others). In figures 3
and 4, the signals for each recognition class is
presented for the full length that an image was shown.
The signals represented in these two figures are
averaged among all samples of each of their respective
classes. In all signal figures, green signals correspond
to NR images, blue signals correspond to PUR images,
and orange signals correspond to PAR images. All
data represented in these graphs are left-aligned rather
than being aligned to their actual ERP-defined offset
(time after stimulus). This is done to prevent missing
signal lines due to overlapping signals as certain ERPs
exist within other ERPs, thus producing the same
shape as another at certain times.
20

Voltage (μV)

To ensure that data collected from viewing
images belongs to the correct possible classification,
we pre-defined images that were shown to the
participant in the various stages based on what we are
looking for an image to be associated with. For
example, one of the images that exists in Phase One
that is picked to be shown again in Phase Two as a
PUR would be labelled as a PUR image on the backend. This allowed us to easily compare any
classification with another by looking at the tag that
was associated with it. This also helped us to form a
more obvious baseline for each of the classifications
as they may have traits such as average voltage level
or peaks that occur at specific times across all images
of the same class.

18
16
14
12
10

4. Analysis and Results

0

5

10 15 20 25 30 35 40 45

Time (7.8125 ms each sample)
Analysis of the data collected for this preliminary
study was limited to simple analysis of the voltage of
collected signals. First, the absolute value of the data
for every image in a class for each participant was
averaged at each time interval to produce an “average
signal”. This allowed us to view and determine if a

NR

PUR

PAR

Figure 1 – Participant 1’s averaged ERP
signals

1222

Voltage μV)

20
18
16
14
12
10
0

5

10 15 20 25 30 35 40 45

Time (7.8125 ms each sample)
NR

PUR

PAR

Figure 2 – Participant 2’s averaged ERP
signals

Voltage (μV)

20
18
16
14
12
10
0

5

10 15 20 25 30 35 40 45

Time (7.8125 ms each sample)
NR

PUR

PAR

Figure 3 – Participant 1’s averaged full-image
signals

headsets can be used to identify possible unaware
facial recognition. Unfortunately, being such a small
sample size, we cannot make a statistical analysis on
the results or develop more concrete conclusions of the
results in this study. Studies are in the process of being
conducted using more participants.
The differences between the three classes of
recognition can be easily identified by the human eye
without using any machine learning tools. The NR
(green) images are generally of higher voltage levels
and the PUR (blue) images appear slightly higher than
the PAR (orange) images. The faces that are not
recognized tend to be softer signals with less variance
in voltage levels while the two classes that have
recognition associated with them (PUR, PAR) tend to
be more active in voltage changes. Considering that
the aware recognitions (the faces that the participants
were asked to specifically identify in phase three)
share similar activity in the voltage levels as the
unaware recognitions (phase two and three), this helps
to reinforce that the brain is able to recognize these
faces in a similar fashion to the aware recognitions, but
at an unaware level. If reproducible in future work,
these results may assist in applications which look to
determine if an individual is recognized by another for
example.
There are specific ERPs that lend themselves
more to visual facial recognition than others, so future
work will inspect these closer, but we have chosen to
include the results of other well-known ERPs to see
what insight we could gather on their function in
relation to the ERPs associated with facial recognition
and identification.

5. Future Work

Voltage (μV)

20
18
16
14
12
10
0

5

10 15 20 25 30 35 40 45

Time (7.8125 ms each sample)
NR

PUR

PAR

Figure 4 – Participant 2’s averaged full-image
signals
In the two trials conducted thus far, there is a clear
distinction between NR, PUR, and PAR images,
which supports the theory that consumer-grade BCI

For the purposes of this paper, the study outlined
here is only a preliminary study to test the feasibility
of the theory and processing behind the idea of
classifying unaware recognitions. There is far more
research to be done on the subject in both the medical
field and in the Human-Computer Interaction field,
leaving room for great advancements. One of the
major areas that future work will continue with is the
number of participants. For this experiment,
participant numbers was kept low and involved only
the research team surrounding this project, but future
work would include a more general population, to
allow for a more diverse data collection.
Another area to be investigated in future work is
the left/right-handedness of the participants. The
EPOC headset that was used for this experiment has
14 sensors (seven on each side of the head – left/right),
so we can measure the certain areas of the brain that

1223

may be more responsive to viewing faces and reacting
appropriately. While this preliminary experiment
focuses purely on voltage levels, future work will
differentiate voltage levels across the two halves of the
brain to determine if a participant’s dominant hand and
areas of the brain play a role in classifying unaware
facial recognitions.
While not performed on these initial results, we
intend to use several additional methods of analysis in
future trials. In addition to what was performed here,
a variety of tools from the Scikit-Learn library for
Python [33] will be used for preprocessing and
classification. Initially, the feature set for all images
that are analyzed consists of the voltage reading at
each time interval (128 Hz, one sample per sensor
every 7.8125 ms).
This study was designed to take advantage of
modern consumer-grade BCI headsets rather than the
more expensive, elaborate, or even invasive lab/medical-grade devices. With the cost of such
consumer-grade devices being appropriate for home or
office use, additional applications are becoming
available to be explored by a wider audience, which is
what this study is directed towards. An area that this
work could be taken in the future with regards to
results is into a lab or medical study to delve further
into the differences between aware and unaware
recognitions.

6. Conclusions
While more participants are needed before any
concrete conclusions can be made, preliminary results
support the theory that consumer-grade BCI headsets
can be used to identify unaware facial recognition. We
explored the voltage differences that occur when the
human brain is subjected to faces of varying states of
recognition. From these limited results, we have found
that the three states of recognition that we measured in
the experiment (NR, PUR, and PAR) can be easily
differentiated by taking the absolute values of signals
after pre-processing. These results were obtained
using a consumer-grade toy BCI headset rather than
more expensive lab or medical grade headsets, thus
lending credence to the idea that cheaper and more
pervasive headsets found on the consumer marker can
be used for recognition-based tasks. Finding such
obvious differences in the voltages of these signals, we
hope that scaling the experiment up to a larger sample
size will produce a more statistically-sound result,
along with a more in-depth analysis.
If future trials produce similar results, this
knowledge could have several potential uses,
including aiding workers in finding missing persons,

other such recognition-based tasks, and the further
inspiration for development and use of low-cost,
consumer-grade toy BCIs for research purposes.

7. Acknowledgements
Portions of the research in this paper use the
FERET database of facial images collected under the
FERET program, sponsored by the DOD Counterdrug
Technology Development Program Office. This work
was supported in part by the Natural Sciences and
Engineering Research Council of Canada.

8. References
[1] T. Shashibala and B. W. Gawali, "Brain Computer
Interface Applications and Classification
Techniques," International Journal of Engineering
and Computer Science, vol. 5, no. 7, pp. 1726017267, 2016.
[2] EMOTIV Inc., "MindDrone," [Online]. Available:
http://emotiv.com/product/minddrone/. [Accessed 01
06 2016].
[3] EMOTIV Inc., "Cortex Arcade," [Online]. Available:
http://emotiv.com/product/cortex-arcade/. [Accessed
01 06 2016].
[4] NeuroSky, "MyndPlay Sports: Bowling," [Online].
Available:
http://store.neurosky.com/products/myndplay-sportsbowling. [Accessed 01 06 2016].
[5] NeuroSky, "FlappyMind - iOS," [Online]. Available:
http://store.neurosky.com/products/flappymind.
[Accessed 01 06 2016].
[6] B. Cahn and J. Polich, "Meditation States and Traits:
EEG, ERP, and Neuroimaging Studies,"
Psychological Bulletin, vol. 132, no. 2, pp. 180-211,
2006.
[7] J. Chuang, H. Nguyen and C. Wang, "I Think,
Therefore I Am: Usability and Security of
Authentication Using Brainwaves," Lecture Notes in
Computer Science, vol. 7862, pp. 1-16, 2013.
[8] E. Peck, B. Yuksel and A. Ottley, "Using fNIRS
Brain Sensing to Evaluate Information Visualization
Interfaces," in Proceedings of the SIGCHI
Conference on Human Factors in Computing
Systems - CHI '13, Paris, 2013.
[9] M. Vargas Martin, V. Cho and G. Aversano,
"Detection of Subconscious Face Recognition Using
Consumer-Grade Brain-Computer Interfaces," ACM
Transactions on Applied Perception (TAP), vol. 14,
no. 1, p. Article 7, 2016.
[10] S. Shalgi and L. Y. Deouell, "Is Any Awareness
Necessary for an Ne?," Frontiers in Human
Neuroscience, vol. 6, no. May, pp. 1-15, 2012.

1224

[11] A. Greenwald, T. Poehlman and E. Uhlmann,
"Understanding and Using the Implicit Association
Test," Journal of Personality and Social Psychology,
vol. 97, no. 1, pp. 17-41, 2009.
[12] M. Seeck, C. Michel, N. Mainwaring, R. Cosgrove,
H. Blume, J. Ives, T. Landis and D. Schomer,
"Evidence for Rapid Face Recognition from Human
Scalp and Intracranial Electrodes," Neuroreport, vol.
8, no. 12, pp. 2749-2754, 1997.
[13] N. George, B. Jemel, N. Fiori and B. Renault, "Face
and Shape Repetition Effects in Humans: A SpatioTemporal ERP Study," Neuroreport, vol. 8, no. 6, pp.
1417-1423, 1997.
[14] B. J. Debruille, F. Guillem and R. Bernard, "ERPs
and chronometry of face recognition: Following-up
Seeck et al. and George et al.," Neuroreport, vol. 9,
no. 15, pp. 3349-3353, 1998.
[15] M. Eimer, "Event-Related Brain Potentials
Distinguish Processing Stages Involved in Face
Perception and Recognition," Clinical
Neurophysiology, vol. 111, no. 4, pp. 694-705, 2000.
[16] S. Bentin and L. Y. Deouell, "Structural Encoding
and Identification in Face processing: ERP Evidence
for Separate Mechanisms," Cognitive
Neuropsychology, vol. 17, no. 1-3, pp. 35-55, 2000.
[17] S. Caharel, S. Poiroux, C. Bernard, F. Thibaut, R.
Lalonde and M. Rebai, "ERPs Associated With
Familiarity and Degree of Familiarity During Face
Recognition," International Journal of Neuroscience,
vol. 112, no. 12, p. 1499, 2002.
[18] S. Sternberg, "High-Speed Scanning in Human
Memory," Science, vol. 153, no. 3736, pp. 652-654,
1966.
[19] E. V. Mnatsakanian and I. M. Tarkka, "Matching of
Familiar Faces and Abstract Patterns: Behavioral and
High-Resolution ERP Study," International Journal
of Psychophysiology, vol. 47, no. 3, pp. 217-227,
2003.
[20] E. T. Solovey, F. Lalooses, A. Girouard, R. J. Jacob,
K. Chauncey, D. Weaver, M. Parasi, M. Scheutz, A.
Sassaroli, S. Fantini and P. Schermerhorn, "Sensing
Cognitive Multitasking for a Brain-Based Adaptive
User Interface," in Proceedings of the 2011 Annual
Conference on Human Factors in Computing
Systems - CHI '11, Vancouver, 2011.
[21] A. R. Harrivel, D. H. Weissman, D. C. Noll and S. J.
Peltier, "Monitoring Attentional State with fNIRS,"
Frontiers in Human Neuroscience, vol. 7, no.
December, p. 861, 2013.
[22] J. Lee and D. Tan, "Using a Low-Cost
Electroencephalograph for Task Classification in
HCI Research," in Proceedings of the 19th ACM
Symposium on User Interface Software and
Technology, Montreux, 2006.

[23] C. Brunner, A. Delorme and S. Makeig, "EEGLAB An Open Source MATLAB Toolbox for
Electrophysiological Research," Biomedizinische
Technik, vol. 58, no. 1, 2013.
[24] EMOTIV Inc., "Epoc+," [Online]. Available:
http://emotiv.com/epoc/. [Accessed 01 06 2016].
[25] F. W. Sharbrough, G. E. Chatrian, R. Lesser, T. W.
Picton, h. Luders and M. Nuwer, "American
Electroencephalographic Society Guidelines for
Standard Electrode Position Nomenclature," Journal
of Clinical Neurophysiology, vol. 8, no. 2, pp. 200202, 1991.
[26] P. J. Phillips, H. Wechsler, J. Huang and P. Rauss,
"The FERET Database and Evaluation Procedure for
Face Recognition Algorithms," Image and Vision
Computing J, vol. 16, no. 5, pp. 295-306, 1998.
[27] P. J. Phillips, H. Moon, S. A. Rizvi and P. J. Rauss,
"The FERET Evaluation Methodology for Face
Recognition Algorithms," IEEE Trans. Pattern
Analysis and Machine Intelligence, vol. 22, pp. 10901104, 2000.
[28] Y.-C. Tseng and C.-S. Ray Li, "Oculomotor
Correlates of Context-Guided Learning in Visual
Search," Perception & Psychophysics, vol. 66, no. 8,
pp. 1363-1378, 2004.
[29] M. M. Chun and Y. Jiang, "Contextual Cueing:
Implicit Learning and Memory of Visual Context
Guides Spatial Attention," Cognitive Psychology,
vol. 36, no. 1, pp. 28-71, 1998.
[30] A. Goujon, A. Didierjean and S. Poulet, "The
Emergence of Explicit Knowledge from Implicit
Learning," Memory & Cognition, vol. 42, no. 2, pp.
225-236, 2014.
[31] L. Bougrain, C. Saavedra and R. Ranta, "Finally,
What is the Best Filter for P300 Detection?," TOBI
Workshop III - Tools for Brain-Computer Interaction
- 2012, vol. 2012, pp. 2-3, 2012.
[32] J. C. Liao and W. C. Fang, "An ICA-Based
Automatic Eye Blink Artifact Eliminator for RealTime Multi-Channel EEG Applications," Digest of
Technical Papers - IEEE International Conference
on Consumer Electronics, vol. 1, no. 2, pp. 532-535,
2013.
[33] F. Pedregosa, G. Varoquaux, A. Gramfort, V.
Michel, B. Thirion, O. Grisel, M. Blondel, P.
Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas,
A. Passos, D. Cournapeau, M. Brucher, M. Perrot
and E. Duchesnay, "Scikit-learn: Machine Learning
in Python," Journal of Machine Learning Research,
vol. 12, pp. 2825-2830, 2012.

1225

