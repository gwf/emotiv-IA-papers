SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

3

A Survey of Interactive Systems based on BrainComputer Interfaces
Alessandro L. Stamatto Ferreira, Leonardo Cunha de Miranda, Erica E. Cunha de Miranda, Sarah Gomes Sakamoto
Department of Informatics and Applied Mathematics
Federal University of Rio Grande do Norte (UFRN)
Natal, RN, Brazil
alexmatto@ppgsc.ufrn.br, leonardo@dimap.ufrn.br, erica@dimap.ufrn.br, sarahsakamoto@ppgsc.ufrn.br

Abstract—Brain-Computer Interface (BCI) enables users to
interact with a computer only through their brain biological
signals, without the need to use muscles. BCI is an emerging
research area but it is still relatively immature. However, it is
important to reflect on the different aspects of the HumanComputer Interaction (HCI) area related to BCIs, considering
that BCIs will be part of interactive systems in the near future.
BCIs most attend not only to handicapped users, but also healthy
ones, improving interaction for end-users. Virtual Reality (VR) is
also an important part of interactive systems, and combined with
BCI could greatly enhance user interactions, improving the user
experience by using brain signals as input with immersive
environments as output. This paper addresses only noninvasive
BCIs, since this kind of capture is the only one to not present risk
to human health. As contributions of this work we highlight the
survey of interactive systems based on BCIs focusing on HCI and
VR applications, and a discussion on challenges and future of this
subject matter.

lost its function along with human evolution – to control a
television. However, it is possible to go further and use a part
of human body’s central axis, already presented in all forms of
human interaction: the brain.

Keywords—Brain-Computer Interface; BCI; Headset; EEG;
Human-Computer Interaction; HCI.

However, interactive aspects of BCIs remain poorly
explored by researchers, probably due to the intrinsic
complexity of areas involved in this research topic. New
studies are coming out guided by some computing areas such
as Human-Computer Interaction (HCI) and Virtual Reality
(VR). A prime example is a work by Solovey et al. [30], which
describes the use of brain-computer interaction in a multimodal interface. Also, Friedman et al. [49] present a braincomputer interface with virtual reality. There is therefore a
strong need for a detailed study to identify clearly and
objectively current limitation of brain-computer interface from
an interactive perspective.

I. INTRODUCTION
The evolution of technology provides significant changes
in the way users use interactive systems. With the everincreasing usage of tablets and smartphones, it can be observed
that interaction between users and applications will take place
through smaller displays and touchscreens. Whereas modern
controls such as WiiMote and Kinect highlight the need for
interaction adjustment considering user physical movements in
their context of use to support appropriate utilization of
systems. Therefore, design and development of interactive
systems should follow new trends of technologies in order to
provide better user experience, increasing productivity and
offering intuitive actions for execution of different tasks.
With technological advancements different kinds of
interaction which use our bodies have emerged, enabling the
use of various body parts other than our hands. For example,
Harrison et al. [10] demonstrated the possibility of using
human skin as a touch interface, Nam et al. [24] presented a
wheelchair controlled by tongue movements, and Liu et al. [17]
proposed an eye-tracking system as well as several examples of
eye-tracking for human-computer interaction. Also, Vernon
and Joshi [31] propose using a muscle above the ear – which

ISSN: 2236-3297

Brain-computer interface is a research field been studied
since middle of 70s in diverse areas of knowledge such as
neuroscience, biomedicine, automation and control engineering
and computer science. Meanwhile only recently cost and
accuracy required for civilian use have been achieved. People
with severe motor impairments are main beneficiaries of braincomputer interface researches, as persons with locked-in
syndrome, i.e. a rare condition characterized by paralysis of
voluntary muscles except for the eyes. Nevertheless, we realize
that people without any disability are also potential users of
solutions which promote interaction between humans and
computers through cerebral signals, in the most possible
natural way.

Millán et al. [22] present a brain-computer interface review
focused on motor substitution with neuroprostheses and
recovery through neurorehabilitation. The authors discuss
brain-computer interface applications in an assistive
technology context, such as using sensors in a wheelchair for
better brain-computer interface control. Lotte et al. [50] review
and explore brain-computer interface works that use VR,
focusing on the design of brain-computer interface based on
VR applications. In this paper, we discuss brain-computer
interface solutions in an interactive perspective, such as
evaluation of user’s cognitive workload and the lack of
freedom regarding visual attention. Furthermore, our work
focus on HCI and VR aspects, while taking in consideration

4

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

both healthy and impaired users. Afterwards, we identify and
discuss several challenges in this context.
This paper is organized as follows: Section II
contextualizes brain-computer interface area, introducing basic
concepts and technologies; Section III presents a survey of
brain-computer interface; Section IV presents several
challenges related to brain-computer interfaces; Section V
discusses this research topic; and Section VI concludes the
paper.
II.

Fig. 1 shows different equipment to gather cerebral
information in a noninvasive way with the above mentioned
techniques, i.e. EEG electrodes capturing electrical signals
(Fig. 1a), scanner fMRI with magnetic resonance imaging (Fig.
1b), and spectroscopic sensors with near-infrared radiation
(Fig. 1c). Nowadays, only EEG and fNIRS enable to gather
cerebral information in a real usage scenario due its relative
low-cost and portability. Moreover, EEG has the best temporal
resolution, which means that it captures signals faster than
others, and hence this method is the most used in BCIs.

BRAIN-COMPUTER INTERFACE

Brain-Computer Interface (BCI) is a mode of interaction
between human beings and computers which does not use any
muscle, since system is controlled through user’s mental
activity captured with specific equipment. According to
Wolpaw et al. [34], BCI is a communication system with two
adaptive components that mutually complement each other.
For these authors, at the current technology stage, users should
fit into BCI to control the system since it should adapt itself to
user’s mental signals. Hence, user must understand the system
which must adjust itself to user, both required for BCI to
succeed.
BCI requires reception of brain signals captured directly
from human brain. There are three different ways to capture
these signals, i.e. (i) invasive, (ii) partially invasive, and (iii)
noninvasive. Invasive capture is characterized by introduction
of implants into user’s encephalic mass, directly into the gray
matter, providing high quality signal reading; however it
causes great inconvenience and risks to human health. In
partially invasive capture, implants are placed beneath the
skull without drilling the brain. Despite its lower quality
signals, this signal capture form presents lower risks to health
as compared with invasive approach. Lastly, noninvasive
capture enables gathering information without any implant
since sensors are placed on the scalp, fully external to the body.
Noninvasive BCIs are more convenient and easy to use, and
due to technological advancements of current solutions,
provide good quality signal capture. It is also the only one to
not present risk to users’ health. For this reason, this paper
focuses only on noninvasive BCIs.
There are three most common techniques to obtain
cerebral information, i.e. (i) electroencephalogram (EEG),
(ii) functional magnetic resonance imaging (fMRI), and (iii)
functional near-infrared spectroscopy (fNIRS). With EEG,
brain activity is captured through sensors called electrodes. It is
possible because neurons communicate with each other via
electrical signals, which eventually reach brain surface and
then are captured by electrodes. The fMRI technique measures
brain activity through blood oxygenation and flow, which
increase in the specific area involved in mental process. This
capture technique requires usage of equipment with
considerable dimensions and a scanner with a large magnetic
field. And fNIRS method also measure brain activity through
blood oxygenation and flow, but it is based on identifying
variation of optical properties in brain images. Near-infrared
light is sent into the user’s forehead and, through light
detectors, the reflected rays are picked up and correlated to
specific concentration of oxygen.

Fig. 1. Nonivasive equipments used to capture cerebral information (a) EEG
electrodes (b) fMRI scanner (c) spectroscopic sensors. Sources: [1],1,2.

There are different capture devices, which vary greatly in
shape and may be a cap, tiara, headband, helmet, or even loose
electrodes. In this paper we unify all these terms in a single
one: headset. Hence, we consider as headset a set of sensors
placed on user’s head. For marketing purpose, companies have
been developed more portable headsets with attractive designs
at lower costs. These devices aims to provide greater comfort
as compared with equipment showed in Fig. 1. In 2009,
NeuroSky3 has launched MindSet, a wireless headset with a
single EEG electrode and capable of measuring user
concentration. This company has others headsets available
such as MindWave, launched in 2011. In 2009, other company
named Emotiv4, launched EPOC: a wireless headset in a tiara
format. EPOC has 14 EEG electrodes and a gyroscope, which
measures head movements. BCI researches with fNIRS in real
usage scenarios usually use sensors covered with a headband in
order to maximize comfort. Fig. 2 shows some BCI headsets,
i.e. NeuroSky MindSet (Fig. 2a), Emotiv EPOC (Fig. 2b), and
fNIRS sensors covered with a headband (Fig. 2c).

Fig. 2. BCI headsets (a) NeuroSky MindSet (b) Emotiv EPOC (c) fNIRS
snsors covered with a headband. Sources: [7],[1],[29].

1

http://blogs.oem.indiana.edu/scholarships/index.php/2009/10/26/neurons-an
d-electrodes/fmri_groot/.
2
http://www.spiegel.de/fotostrecke/fotostrecke-13782-3.html.
3
http://www.neurosky.com.
4
http://www.emotiv.com.

ISSN: 2236-3297

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

A. Thoughts Recognition
BCIs require recognizing a thought or mental activity in
order to activate an action. An ideal scenario should be to think
about turning a lamp on and then, BCI system recognizes this
thought and turns a lamp on automatically. Currently,
recognizing specific thought such as “turn lamp on” is still
very difficult. However, there are three mental activities
recognized with certain precision, which are commonly used
on BCIs applications, i.e. (i) concentration, in which Alpha
and Beta waves are used to estimate user’s attention and
relaxation/meditation, (ii) stimulus response, in which brain
responses are detected when user focus on certain flashing
graphic elements (visual stimulus) and/or special sound
patterns (sound stimulus), and (iii) imagined movement, in
which is possible to detect kinetic thoughts, such as imagining
your right hand opening and closing, due to the
synchronization and desynchronization of Mu rhythm.
The detection for a stimulus response (ii) is subdivided
into two types, i.e. oscillating stimulus and transient stimulus.
In oscillating stimulus, elements are differentiated by
frequency, such as LEDs where each one flashes – oscillates –
in a different frequency, inducing a natural response from brain
and generating electrical activity in the same or multiple
frequency of stimulus. Other example of oscillating stimulus is
in the case of two sounds from different frequencies, which
generates a specific response to the focus in each one. The
response for a visual oscillating stimulus is called Steady State
Visually-Evoked Potential (SSVEP) and a response for a
sound oscillating stimulus is called Steady State Auditory
Evoked Potential (SSAEP). Transient stimulus are
differentiated by response to a transition from a visual/sound
state to another. When an individual waits for a certain
stimulus among other similar stimulus, a wave called P300 is
generated as a response. An example would be five squares, off
most of the time, which each one turns on for a short time and
turns off again. User concentrates in one square and when this
square turns on, a P300 wave is generated due to the small
“surprise” caused by the transition from off to on. Likewise, it
is possible to identify a user’s response when listening a sound
repeatedly and then suddenly, a different sound occurs.
One of the first visual stimulus-based applications
commonly used in tests is typing, called speller. In this test,
screen contains letters from ‘A’ to ‘Z’ arranged in a grid/matrix
and user must concentrate in a specific letter. BCI recognizes
which letter and presents it to user. Then, user can focus and
concentrates in other letter and therefore, letter by letter to
form words. In SSVEP-based BCIs, each letter flashes
intermittently in different frequencies, whereas in P300-based
BCIs, one line/column flashes at time in a random pattern.
When the line corresponding to the user’s chosen letter flashes,
P300 wave is recognized, indicating that user is focusing on
that line. Likewise, when the column flashes this wave is
identified. Thus, with both the line and column recognized, it is
possible to identify the letter chosen by user. Fig. 3 shows two
different spellers which enables word entry like as a keyboard.
In Fig. 3a the speller is used on P300-based solutions and Fig.
3b illustrates a speller for SSVEP-based solutions.

ISSN: 2236-3297

5

Fig. 3. Spellers based on (a) P300 (b) SSVEP. Sources: [26],[13].

In BCIs, processing can be performed online and offline.
Online processing occurs in real time while the user utilizes a
BCI and offline processing is performed after user experiment
with a post-processing approach in order to obtain the
maximum precision. There is also a BCI classification
regarding rhythm, i.e. synchronous and asynchronous. In
synchronous BCIs, commands are interpreted at a constant
time rate. Therefore, after every certain amount of time, a
command is recognized regardless of user’s intent. Whereas
asynchronous BCIs – also called self-paced – give control to
the user to recognize a command only when wanted.
We consider pertinent presenting some fundamental
concepts of the BCI area in order to provide a refined
understanding about the literature works presented in this
survey. The objective of this background is to present a
theoretical overview, not an introduction tutorial about BCIs.
The following section presents a survey that comprises a
relevant part of the literature about this research topic.
III. SURVEY
The survey presented in this paper describes works which
address new BCIs. These interfaces are related to daily task
accomplishment, now possible through cerebral waves.
Therefore works presented demonstrate the potential of new
interaction forms with interactive systems through BCIs.
Moreover, to a greater identification of proposes and
challenges of this research area we grouped works according
detection approach it utilizes, i.e. (a) visual stimulus response,
(b) sound stimulus response, (c) concentration, (d) imagined
movement, and (e) neurofeedback.
The search strategy consisted of automatic and manual
searches in scientific libraries and bibliographic databases.
Automatic search was conducted in IEEE Xplore, ACM DL,
Springer, Elsevier, Scielo, Scopus, ISI Web of Knowledge, and
also in Google Scholar; manual search was made in PLoS
Biology and Frontiers in Neuroscience Journals. In the search
process we used a combination of the following keywords in
English (presented in this paper in alphabetical order): BCI,
brain, computer, HCI, human, interaction, interface, reality,
virtual, and VR. For this study we select only recent works, i.e.
paper published in the last five years.
A. Visual Stimulus Response
Mauro et al. [21] exhibit the use of a BCI to control the
cursor of a desktop operating system. They implemented two
BCIs based on P300, one exogenous and other endogenous; in
an exogenous interface the user focus is external, while on an
endogenous interface the user focus should be at the center
(internal). Both interfaces allow four movement directions with

6

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

four squares – one in each side – representing position
objectives for testing purposes. On the exogenous interface,
those squares flash, and the user has to focus on the square in
desired direction. The squares do not flash in the exogenous
interface. Instead, one letter is showed on the center of screen,
alternating between the initials of Italian words for directions,
i.e. alto (up), destra (right), basso (down), and sinistra (left).
The user has to count the occurrences of the letter representing
the desired direction. Eight patients, half healthy and the other
half in advanced state of paralysis, participated in the
experiment. Results demonstrated that there was no difference
on the precision of the healthy group versus the paralyzed one,
which indicates that the interfaces devised do not depend on
motor abilities.
Hood et al. [42] developed a BCI control for a car driving
virtual simulation using CARRS-Q. CARRS-Q simulator
consists of a platform with 180 degrees frontal projection and
three simulated mirrors. The BCI uses three LEDs as SSVEP
stimulus, each one offering a configurable command. One
possible configuration would be with three LEDs, i.e. one to
right steer the wheel, another one for left steering it, and the
last for straight steering. It reached good precision rates but
still cannot be applied in real situations, because it is not safe
enough. In the authors opinion, a virtual ambient will be
essential for improvements on the interface and safety
guarantees of future car-driving BCIs. Fig. 4 showcases the
system being used.

Fig. 4. BCI for car driving in virtual simulator CARRS-Q. Source: [42].

A different BCI application is a web browser created by
Mugler et al. [23]. The BCI is based on the P300 and follows
Mankoff et al. [20] web usability needs. For Mankoff et al.
[20] all web browser systems must offer the following features:
web navigation, page navigation with fewest commands
possible, history browsing, bookmarks, and text input. In
addition to those ones, the authors add functionalities like an
URL bar. Speed is considered an interesting factor but not a
need. Another BCI web browser is the one created by Xu et al.
[35], which uses the SSVEP approach. The application allows
searching in Google and inputting text. The authors mounted a
hardware, in which a small LED board with six LEDs is
attached on a conventional notebook. Each LED represents an
option and these LEDs are used to input characters, to navigate
a 6x6 menu, and to select web browsing commands like “next
URL” and “HOME”. The interface achieves great precision
(92%) but it is very slow (four and a half minutes for a simple
Google search). Although the described application is
interesting, Liu et al. [18] argue that SSVEP BCIs have a
fatigue factor due constant and intermittent flashes. For this
reason, these authors created an alternative BCI. Like the usual
SSVEP, the interface uses a visual stimulus differentiated by
frequency and also a movement frequency instead of the usual
light frequency. According to the authors, this new SSVEP

interface offers an increased comfort with a good precision
(83%).
Ceccoti [4] developed an asynchronous BCI speller based
on SSVEP. The speller objective was to achieve an intuitive
system where even inexperienced users could successfully use
it, while causing the least possible discomfort. Therefore, the
letters are divided into groups and what flashes is the contour
of those groups. The system automatically configures the BCI
and the asynchronous nature of the interface leaves the user
more relaxed. Another interesting work is the one presented by
Campbell et al. [2] that proposes the NeuroPhone, a iPhone
application in which phone calls are made through a BCI using
the EPOC. A contact photo grid is showed to the user and one
photo flashes at time in a P300 fashion. Then, the user focuses
on the contact to call. Fig. 5 shows the process of calling a
contact.

Fig. 5. Contacts flash, one by one, detecting the user visual focus. A phone
call is made to the one choosed. Source: [2].

A remote robot is controlled in the SSVEP BCI made by
Gergondet et al. [6]. In this system, a real-time video displays
the robot “vision”, i.e. a camera coupled on the robot. On the
controlling machine – notebook – the interface mixes the robot
vision with four red squares at the four sides – top, bottom, left,
and right – that act as the visual stimulus. Focusing on one of
these directions, the robot increases speed in the same
direction. For testing purposes the authors use a robot
benchmark known in the robotic area as SLALOM, the robot
BCI successfully passes in the benchmark test. Yuksel et al.
[37] employ a P300 BCI for object selection. These objects are
arranged in a multi-touch table display and a computer vision
algorithm computes the approximated shape for each object.
This shape is expanded and flashed under the object following
the same process in P300 spellers with an object grid instead of
a letter grid. Fig. 6 exhibits the BCI table in action.

Fig. 6. Multitouch display interface, where objects are enlightened one by
one, triggering a P300 wave. Source: [37].

Grierson e Kiefer [7] test MindSet – conventionally used to
detect concentration – for a BCI based on P300. The efficacy
was benchmarked through an experiment, in which two
squares – one blue and one red – take turns flashing and the

ISSN: 2236-3297

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

BCI has to detect which one the user is focusing. With squares
of same size the precision is good (78.5%), while with squares
of different sizes the precision is close to 100%. The results
indicate that it is possible to use a commercial headset for BCI
applications based on visual stimulus. Wang et al. [33]
developed a smartphone BCI application based on SSVEP for
calling contacts by number typing. The interface contains
numbers 0 to 9, a confirmation option (Enter), and a correction
option (Backspace). Frequencies between 9 and 11Hz are used
as visual stimulus. Normally those frequencies cannot be
achieved in a smartphone due to screen refresh rate but the
authors employ a special technique of black/white alternating
patterns to circumvent this restriction. A good precision was
achieved (close to 100%).

7

working correctly). Those tests indicate that Friend still needs
improvement for real world usage but it has great potential.

Fig. 7. A user selects the “prepare meal” in the FRIEND BCI system.
Source: [46].

Escolano et al. [45] developed a BCI telepresence system,
wherein a robot is remotely controlled from a different
geographic location. A camera on top of the robot shows his
current “vision”. The BCI is based on the P300 and the
graphical interface imposes options as augmented reality on the
robot vision with commands like “turn left” appearing as icons.
Besides the camera, the robot is also equipped with a laser
sensor, wheelchair and a location tracker based on
measurements of the rotations of the wheel. The BCI
developed has two operating modes: robot navigation, where a
point grid is used as Visual Stimulus for choosing a destination
and camera exploration, in which the point grid is used to
indicate where the camera should look at. Five users
participated in an experiment, having to navigate the robot
through close space situations, all users managed to control the
robot successfully to the final position.

Kaufmann et al. [15] developed the Optimized
Communication System, a P300 BCI speller where a single
button automatically configures and adjusts the system. The
user presses the button one time to start EEG signal capture,
and a second time to stop the capture and use the data collected
to configure and calibrate the system. It also improves on other
spellers in information transmission speed through word auto
completion – the word prevision was written in Python –, the
complete words appear together with the letters in the grid.
Following the idea of less configuration as possible, the
application creates the word base automatically - navigating
the web and computing word frequencies for a determined
idiom. Poli et al. [41] take BCI domain to space interaction,
creating an spaceship navigation BCI controller. BCI usage on
space applications have great potential since they allow
piloting, collaboration and machine control without the, in
several situations restrict, movement of hands. The developed
BCI uses P300 approach with an innovative graphical
disposition. Eight gray circles form a greater circle and one by
one is flashed to red or green – randomly selected – color.
Three volunteers participated in a simulation, whose goal was
to make a path with the ship, passing as close as possible to the
sun. The authors also devised a cooperative mode, in which the
control is made simultaneous by more than one person. On the
experiments the cooperative mode had better precision than the
single-person mode. Still, the BCI developed has to improve to
achieve high-enough precision for real world usage.
Nevertheless, it is an excellent result and offers an optimistic
vision for future BCI space applications.

An assistive BCI application is the one by Grigorescu et al.
[46], where the interface controls a robotic assistant for helping
people with motor deficiencies. The system is named by the
authors as FRIEND (Functional Robot with dexterous arm and
user-frIENdly interface for Disabled people). The BCI was an
addition to a new generation of the robot system to supply
quadriplegic needs. FRIEND combines several modules, i.e.
wheelchair, robotic arm with gripper, EEG headset, monitor
and camera (used for machine vision). The BCI uses the
SSVEP approach, where five LEDs act as visual stimulus, each
one representing a menu option. FRIEND is a semiautonomous robot and has two modes, i.e. (i) complete
autonomous mode, wherein objects are automatically
recognized and manipulated, and (ii) shared control mode,
where the user assists the system with ambient information,
like approximate object positions. System performance is
measured in four scenarios: prepare and serve a drink, prepare
and serve a meal (showed in Fig. 7), tasks at a library service
desk, and tasks for keyboard maintenance (checking if they're

Edlinger et al. [43][44] developed a domotics BCI
application based on an hybrid approach, using SSVEP for
turning on and off the application, while using P300 for
command selection and activation. They define four
requirements for domotics BCIs, i.e. (i) signal amplifiers most
work even on noisy environments, (ii) EEG capture has to be
made with a portable device, to avoid collisions and user
irritations, (iii) for real time experiments it's necessary to
connect the BCI to a virtual reality simulation, and (iv) the
communication interface between BCI and VR needs to offer a
satisfactory degree of freedom. In the VR the user is equipped
with 3D glasses and a head-position tracker. EEG signals are
captured through g.MOBIllab+ amplifier, those are send to a
PC which controls the virtual ambient using XVR (eXtreme
VR). Video output is projected into a high resolution surface
(powerwall). The virtual ambient is composed of three rooms,
each one with controllable devices like television, music
player, telephone, lights and door. Commands are divided into
seven categories: lights, music, telephone, temperature,

Hakvoort et al. [9] implemented a SSVEP BCI game,
whose goal is to lead sheep into a sheep pen by controlling
shepherds dogs. Kapeller et al. [14] remark the importance of
analyzing BCIs in a distracting context. They did a benchmark
where a user has to focus on SSVEP visual stimulus, overlaid
on a movie. The experiment showed that some precision – 6%
on average – is lost in this distraction context. One of the
participants had a very large loss of precision (40%) and
according to the authors this indicates that some users are more
sensible to visual distractions.

ISSN: 2236-3297

8

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

television, move and “go to”. Each of those categories act as an
interface “mask”, one screen that has only the relevant
commands for that category. Fig. 8 shows the BCI and
controlled virtual ambient.

Fig. 8. A (a) P300-based BCI for controlling (b) a virtual domotics
environment. Source: [44].

B. Sound Stimulus Response
Lotte et al. [19] developed a sound stimulus based BCI
using a P300 approach. Their objective is to present a BCI
efficient in a scenario of great mobility, where concentration in
a visual stimulus is a hard task and user movement can cause
interference on EEG capture, since movement is responsible
for a great part of brain electrical activity. The BCI developed
uses two sounds as stimulus, one rarely appears – a ring bell
sound (“Ding Dong”), while the other sound frequently plays –
a buzzer sound. The user has to focus on the ring bell sound,
counting the number of occurrences, eliciting a P300 wave
each time the sound plays. They conducted an analysis to
identify movement interference on P300 detection and three
movement states were tested, i.e. sitting, standing and walking.
Experiment results were promising: no significant precision
was lost due to movement interference and a good EEG
capture was possible in all states tested.
On the work of Kim et al. [16] a BCI based on steady state
evoked potential was conceived. However, sound frequency
(SSAEP) was used instead of the usual visual flashes frequency
(SSVEP). Two sounds of different frequencies are used, each
one in different sides of the user – left/right – to strengthen
contrast between them. A good precision was achieved, 71%
online and 86% offline. One disadvantage of such sound based
BCIs is the binary choice, i.e. the user can only choose
between two options (sounds). Hill and Schölkopf [12]
research ways to improve sound based BCIs. They use the
same approach of SSAEP – sound frequency – combined with
spatial location – one on the left of the user, the other on the
right – in a way that resembles the “surprise” associated to
P300 approaches. Their BCI achieves higher performance,
obtaining 85% precision online.
C. Concentration
Coulton et al. [5] created a smartphone game named Brain
Maze, which uses BCI as one of the controllers using the
MindSet. The game objective is to move a ball from a start
point to a finishing destination. Moving the ball is done
through accelerometer, but some obstacles need to be
overcome using a BCI. Some paths are blocked by closed
gates. There are two types of gates, i.e. attention and
meditation gates. The user has to increase concentration to

open attention gates, while he needs to relax to open meditation
gates.
Marchesi [40] presents a BCI prototype for interactive
cinema, the Neu system. Neu system measures the user degree
of concentration/relaxedness using a MindWave and those
measurements affect course of events in the history of the
interactive movie. Neu is an evolution of MOBIE System,
developed by the same author. Mobie monitors and records the
degree of concentration/relaxedness of the user while he
watches a movie. From this feedback the user engagement in
each scene is obtained. Neu takes this concept one step beyond
offering, in the authors’ viewpoint, a BCI interactive,
immersive, and personal experience.
D. Imagined Movement
Poor et al. [27] assess EPOC capacity in a BCI based on the
imagination of kinetic actions. On the experiments the
objective was to rotate a cube after an initial brain signal
recording and calibration. The precision was low (59%) but the
authors attribute the cause to training lack and immaturity of
Stimulus-Less BCI systems and techniques. Friedman et al.
[49] conduct research on navigation in a cave virtual
environment based on imagined movement. The VR consists
of a street with people spread out and stores on the sides - those
can be projected in stereo view shutter glasses for increased
realism and experience. To walk, the user has to imagine the
user feet moving, while head rotation – tracked through an
accelerometer – is used for changing walking direction and
imagined hands movement – to pass the impression of “touch”
– is used for interacting with other persons on the street. The
virtual people remain at still until the user interacts with them.
In that instant, they start to walk to indicate interaction success.
One of the first authors’ experiments [48] was conducted
with a quadriplegic patient, who successfully moved across the
virtual environment as displayed in Fig. 9. Another experiment
[47] with 10 participants compared BCI precision in two
different scenarios: a controlled choice scenario and a free
choice scenario. In the first one, user receives a sound cue for
the action he must realize and in the other one, user freely
chooses which action to perform. The precision on the
controlled choice scenario was higher (82.1%) than the free
choice scenario (75%), which presents a challenge to be
overcome, since freedom of choice is essential in interactive
systems.

Fig. 9. User walks in a virtual street imagining feet movement. Source: [48].

Leeb et al. [39] leverage a conventional game – i.e., which
was not originally designed to work with a BCI controller –
named PlanetPenguin Racer. In the original game a penguin is

ISSN: 2236-3297

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

controlled to descend a snow mountain, collecting fishes on the
way. The game was modified to float all the fish, suspending
them mid air. Jumping is the only way to catch the fish in this
new version and to jump, user has to imagine feet movement.
For increased immersion the game happens on a cave virtual
reality, where the user is surrounded by walls with projectors
directed to each one of them. As such, the game uses a
multimodal interface: the penguin’s direction is controlled by a
joystick and the jumps by BCI. An experiment with 14 users
demonstrated that concomitant use of joystick with BCI did not
decreased BCI precision. Furthermore, leg positioned sensors
proved that jumps – BCI control – did not use any muscles. A
pure joystick control achieved highest precision on catching
the fish, as expected by the authors. However, the majority of
users preferred the BCI controls, remarking the fun of jumping
only through mental power. According to authors, the game
needs only a short time of training, being entertaining without
leaving the player bored with long sessions of training.
E. Neurofeedback
Vi and Subramanian [32] were able to detect an electrical
potential called Error-Related Negativity, caused by user
frustration when an interaction does not occur as planned. An
example is when the user tries to select an option among others
but misses, by user or system error, and chooses one he did not
want, getting the user frustrated. A BCI detecting this
frustration – through Error-Related Negativity potential – a
system could try to auto-correct the interaction error, choosing
the closest option of the one miss-selected. An experiment was
made to measure the precision of successfully detecting this
potential. The authors choose to analyze the precision through
an interaction test known as Superflick, where a user has to
“throw”, with a drag-and-drop movement, a small ball into a
big target ball. If the user misses the target the system must
auto-correct the interaction, trying to achieve a most
satisfactory state for the user. The BCI application conceived
achieved 70% precision, and proved that is possible to detect
interaction errors, and use this information to provide a better
experience for the user, compensating the error with an action
rollback, giving a small advantage to a player, or selecting
close objects. Fig. 10 shows a user participating on the
Superflick test, “throwing” a ball while the headset captures
the user frustration.

9

current task is interrupted, and replaced, by another of
increased priority, (ii) delay task, where a user receives another
task but chooses to ignore it, implicitly indicating that few
resources must be allocated to that lower priority task, and (iii)
dual task, where the user works on two tasks of same priority,
and constantly switch between them. The BCI system
conceived by the authors adapts to allocate more resource to
tasks of higher priority. For testing this BCI an experiment was
made with 11 users, they needed to remotely control two
robots, one blue and one red. They could only control one
robot at time, switching between them. Both robots expanded
resources to move, and had to reach a specific destination and
send a signal. A priority was assigned to each robot, and
performance was compared between a Brainput interface doing
the robot control switch, and an interface where the robot
where more autonomous. The results showed the Brainput
interface as the better one.
F. Summary
Table I summarizes 29 interactive systems based on BCI
presented above and grouped in this table by year of
publication. As previously described, BCIs go beyond of
computer control, comprehending domains such as domotics,
assistive interfaces, robot control and electronic games. It is
important to highlight that most of stimulus-based BCIs uses a
visual form due to its higher precision. Table I shows approach
used by BCI classifying works in (V)isual stimulus, (S)ound
stimulus, (C)oncentration, (I)magined movement, and
(N)eurofeedback.
TABLE I.
Work
Year

Ref.

2007

[48]
[35]
[19]
[23]
[18]
[4]
[2]
[37]
[33]
[49]
[21]
[6]
[7]
[9]
[43]
[16]
[5]
[27]
[45]
[46]
[15]
[44]
[42]
[12]
[40]
[32]

2009

2010

2011

2012
Fig. 10. Superflick interaction test using a BCI for auto-correction. Source:
[32].

Solovey et al. [30] created a BCI application – Brainput –
that avails the priority of tasks being done by the user. This
priority is estimated through detection, using fNIRS sensors, of
three mental states of concurrency, i.e. (i) branching, when the

ISSN: 2236-3297

[30]
2013

[41]
[39]

LITERATURE WORKS PRESENTED IN THIS SURVEY.
Detection
Brief description
approach
I
Avatar control/walking in a virtual street
V
Web browsing and speller
S
BCI control while walking
V
Web browsing
V
Web browsing and speller
V
Asynchronous speller
V
Call a contact through smartphone
V
Object selection in a real ambient
V
Phone dialing
I
Avatar control/walking in a virtual street
V
Mouse control
V
Robot control with real time vision-camera
V
Headset benchmark
V
Sheep game
V
Domotic control in a virtual house
S
SSAEP benchmark
C
Smartphone Maze Game
I
Commercial headset benchmark
V
Telepresence robot with vision-camera
V
Auxiliary robot for disabled people
V
No configuration speller with word prediction
V
Domotic control in a virtual house
V
Car control on virtual CAVE simulation
S
SSAEP + P300 benchmark
C
Interactive movies
N
Superflick interaction test
Resource control through brain concurrency
N
detection
V
Spaceship control in a virtual simulation
I
CAVE VR game

10

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

IV. CHALLENGES
Through reflection on the BCI literature review, we
identified several challenges with implications for user
interaction. These challenges must be faced in order to BCIs
been used in a more effective manner with interactive systems.
To a better understanding, we grouped challenges in topics.
Most of existing BCIs causes a high level of fatigue,
demanding high concentration or attention to quick and
intermittent stimulus. In addition to fatigue inconvenience, BCI
may not work since user cannot reach enough level of
concentration. In [11], Hasan and Gan try to assure BCI’s
operation even when user is tired. The BCI implemented by
these authors properly monitors user performance and when it
declines, system activates an adaptation which reduces the
concentration limit necessary to interact with system. The use
of VR in BCI applications may assist in this process, providing
a high immersive environment. It motivates the user while
interacting with the system, consequently increasing users’
attention and concentration levels.
Concentration required to stimulus also causes a mixture
between input and output since mental activity is being
constantly monitored and user’s focal point changes the input.
Instead of relax, user must concentrate on a point as input and
look to the output. For example, a user watching a movie; the
user has to look at a specific point on screen instead of a part of
scene that the user wants to see. At this stage, interaction has a
forced aspect, instead of natural aspect presented in the case of
user may decide which region of visual output the user wants
to focus. A similar challenge occurs with traditional
interactions since interaction flux often depends on user
perceiving certain feedbacks, mainly the ones issued by
computational system. In VR environments this issue is
exacerbated since the lack of visual freedom may disrupt the
immersion. This challenge is also applied in other 3D
environments such as augmented reality, which may bring
other problems such as safety since user has to focus on a
certain point and may not pay attention in what comes ahead.
According to Wolpaw et al. [34], with current BCI
technologies, users must fit themselves into the system in order
to control it; speed and satisfaction of this adjustment depends
on system’s intuitiveness. During tasks accomplishments on
applications, the study of user’s visual focus and intuitiveness
of graphical interface are conducted by HCI researchers. These
researchers must apply HCI techniques in BCI context in order
to develop a visual interface with as fewer nuisances as
possible regarding constant transitions of user’s visual focus
and also easy-to-use, providing a fast user’s adjustment to the
system.
Using a BCI system is often a complex task. It is necessary
to verify electrodes’ position on user’s head and configure
different parameters before using the system. Furthermore,
users must know which technology is best suited to their needs,
including purpose and profile. Randolph [28] evidences that
factors such as gender, caffeine and experience on videogames
or musical instruments, affect mental states and waves, which
are tracked by capture technologies. Hence, different people
may have different needs regarding BCIs, which makes the use
of this kind of interaction in a practical way even more

difficult. 3D environments technologies are also impacted by
this challenge since technology which provides the best user
experience may vary for each user. So, when mixing both 3D
and BCI technologies, user’s needs must be considered
carefully.
In most cases BCIs do not provide mobility to users.
Users must obligatorily remain at still and quiet, preferably
sitting down, during test application. However, in a real using
situation user may need to use BCI while the user walks on the
street in order to control a smartphone, for example. In
addition, BCIs must also provide comfort to user. An EEG
headset must be easy to carry on and simple to use on daily
routine, as well as a person uses a headphone to listen to music.
An EEG headset must be lightweight, not only to provide
mobility but also to enhance use experience; its weight must
not be uncomfortable to user. Other significant inconvenience
is caused by a gel, which is applied on electrodes in most of
EEG headsets in order to enable signal capture, even though
Guger et al. [8] indicate that dry electrodes are priority to user.
To meet this requirement, authors present a non-commercial
EEG headset with dry electrodes and high precision.
Another similar challenge is the possible conflicts between
different interface devices, i.e. using a Head-MountedDisplay (HMD) together with an EEG headset can prove to be
difficult given that the EEG sensors must stay on position.
Choosing an EEG headset becomes a complex task if we
consider mobility and comfort requirements. Equipment
presented in Fig. 1 and Fig. 2 must be redesigned to be used in
real situations. Some of equipment use wires or cables, and
most of them require application of gel or saline substance to
capture mental signals with high precision. The contribution of
HCI in this BCI issue is exactly proposing the redesign of such
equipment considering, for example, accessibility, usability,
and ergonomic aspects. An ideal BCI headset must not use
wires or cables, which hamper mobility; neither use gel or
saline solutions, which are one more component to be carried
on and make headset’s use more difficult. We understand that
this device must be lightweight and without additional parts,
for example, batteries.
In BCIs, system needs to constantly adapt itself to user’s
signals. This adjustment must be fast and with precision.
Current BCIs present a very low information transmission
speed rate, being necessary, for example, almost two minutes
to “digitalize” a simple word. Nowadays, this challenge is
minimized with the use of word completers, accelerating the
speed, as described in [15]. The BCI precision does not
always reach a satisfactory value, mainly in BCI based on
visual stimulus. Sometimes, actions repetition or undo are
required, causing discomfort or even discontent in the usage of
interactive systems with this kind of interface. The sum of
these factors may generate frustrations to user and,
consequently, resistance to the BCI usage. Furthermore, the
performance of low-cost BCI commercial devices must be
investigated, such as EPOC which presents less electrodes than
other EEG headsets with medical purposes. In a comparison
performed by Al-Zubi et al. [1], this headset with 14 electrodes
presented only 5% fewer precision than a professional EEG
headset with 128 electrodes.

ISSN: 2236-3297

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

BCI tests and experiments are often conducted in
controlled environments, in laboratory, that does not
correspond to the real context of use of desktop computers,
where users usually perform different tasks in parallel and
work simultaneously, breaking their concentration constantly,
either to answer the phone or to fetch a glass of water. This fact
seems to demonstrate that currently asynchronous BCIs have
more advantages in real situations, since they provide greater
facility to the user when performing tasks in parallel, without
prejudicing interaction with computer.
BCIs with wireless headsets are more practical and
comfortable. Many manufacturers state that EEG signals are
encrypted before transferred to device. However, it is
necessary to note that, in a future with massive use of BCIs,
cryptography breaking enables attackers to capture cerebral
waves, which transmit not only commands but also mental
states and feelings. As BCI area progresses, this challenge has
more severe consequences since the higher the precision and
greater the amount of information, the greater is the risk of
privacy loss. Looking forward, when BCI technologies reach
advanced stage, information espionage will no longer use
phone tapping and network sniffers. Instead, it will use mind
tapping and cerebral signals sniffers. Thus, with research
advancements, new challenges arise and the presented survey
is taken as starting point to development and evolution of
BCIs.
V. DISCUSSION
The brain is in constant activity and humans think all the
time, even while they are asleep or dreaming. When a person
uses a computer, an information wave is lost, e.g. concentration
level, frustrations, cognitive workload and user’s tension. This
information could be used in BCIs, enabling interactive
systems to adjust to their users, for example, changing the
amount of text and figures [25], changing desktop screen sizes
to control resources [30] and to offer more space to important
applications, identifying and correcting interaction mistakes
[32], choosing a more suitable video to user [36], or even
presenting more relevant information based on the user
cerebral activity.
Invasive BCIs based on implanted chips using
biocompatible materials are mature enough to enable monkeys
to control a mechanical arm only with thought, as for example
presented by Carmena et al. [3]. However, it requires a long
period of training. Also, the invasion levels, costs and health
risks make it an unviable procedure to healthy humans.
Whereas noninvasive BCIs are not mature enough to be used
as a single input, although it may already be adopted on
multimodal interfaces efficiently in real usage scenarios. In
addition, BCIs without stimulus, in which a simple thought
would control an entire system, are the most desired ones.
However, with current technologies, without stimulus, it is
only possible to recognize mental states with certain precision,
such as concentration.
Thus, based on the BCI literature review, we identified that
most of noninvasive BCIs depends on visual stimulus to reach
satisfactory precision and speed (see Table I). Such a care with
design becomes even more important in these interfaces, since

ISSN: 2236-3297

11

input/output depends on system’s graphical interface. BCIs
based on sound stimulus also face a similar difficult, because
these BCIs require a special concern regarding warning sounds
in order to sounds do not disturb the feedback to sound
stimulus.
There are no ready-to-use models in which we can model
interaction between users and computers via brain waves. It is
henceforth necessary to develop methods, techniques,
approaches and technologies to greater support works in this
area. The interaction documentation of a specific user based on
information captured from user’s brain may be used to evaluate
interfaces and indicate, as a result, the system’s intuitiveness.
Moreover, user’s behavior pattern directly captured from brain
activity may contribute to enhance the interaction quality of
adaptive interfaces since system may learn about user’s
behavior and automatically provide a molded interface.
For widespread use of this interaction form, it is necessary
to face current limitations and overcome its challenges.
Whereas precision involves partially HCI area, concentration,
speed, comfort, environment, difficulty in its usage, and
privacy are constant concerns of interactive systems. The fact
that most of works using noninvasive BCIs needs visual
stimulus increases the need of a greater concern from HCI and
VR communities in BCI research. Even in BCIs without visual
stimulus, these areas may contribute since it is important to
consider the feedback, making the command more intuitive
with HCI and more immersive with VR. BCI technologies are
a fundamental step to more transparent ubiquous interactions,
in which we’ll control different devices simply by our “will”.
Through capture and identification our thoughts, no effort will
be required in daily interactions with devices. Using muscles
will not be necessary, except those ones responsible for vital
activities of organism, such as involuntary movements of
heartbeat.
We believe that, if HCI knowledge is associated from the
beginning, it is possible to advance toward more comfortable,
suitable and easy-to-use BCIs, so that users may have a higher
degree of satisfaction in interactive systems. Our research
group is exploring this area, in order to purpose new BCIs
guided by human factors, which are involved in this high
complex form of (brain)human-computer interaction. In the
same way, we believe that the area of VR plays a vital role in
brain-computer interactions, providing an immersive
environment, easing movement imagination and increasing
focus on visual stimulus.
VI. CONCLUSION
This paper presented a survey of BCI and additionally,
based on this review, we identified and discussed several
challenges for BCI in the interactive systems context. We
believe that these challenges must be addressed so that BCIs
may be adopted in interactive systems more effectively.
Furthermore, due to advancements and price reduction of
headsets, BCIs will be common in the near future as well as
other kinds of interface/interaction are today, for example, such
as those provided by mobile devices and by Kinect.
We are aware that thinking about interaction design in this
domain involves various areas of knowledge, especially to

12

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013

reach its full potential. However, it is important to point out
that HCI can contribute to the expansion of knowledge
frontiers; outcomes achieved with this study is a concrete
example since it enhances the importance of this review and
outstands the merit of literature works. Considering different
related areas and the diverse use possibilities of BCI, this
research topic deserves greater attention from both HCI and
VR communities in order to undertake further studies on
interaction in BCIs.
As future works we make an interaction design study and
we will implement a visual stimulus-based BCI game for use
with a low-cost noninvasive EEG headset.
ACKNOWLEDGMENT
This work was partially supported by the Brazilian Federal
Agency for Support and Evaluation of Graduate Education
(CAPES) and by the Physical Artifacts of Interaction Research
Group (PAIRG) at the Federal University of Rio Grande do
Norte (UFRN), Brazil. The present paper is a extended and
reviewed version of a previous work published at XI Simpósio
Brasileiro sobre
Fatores
Humanos
em
Sistemas
Computacionais (IHC’12) entitled “Interfaces CérebroComputador de Sistemas Interativos: Estado da Arte e Desafios
de IHC” [38]. The authors thank JIS editors for the invitation.
REFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

[8]

H.S. Al-Zubi, N.S. Al-Zubi, and W. Al-Nuaimy, “Toward inexpensive
and practical brain computer interface,” in Proceedings of the
Developments in E-systems Engineering (DeSE’11), IEEE, 2011, pp.
98–101, doi: 10.1109/DeSE.2011.116.
A. Campbell, T. Choudhury, S. Hu, H. Lu, M.K. Mukerjee, M. Rabbi,
and R.D.S. Raizada, “NeuroPhone: brain-mobile phone interface using a
wireless EEG headset,” in Proceedings of the 2nd ACM SIGCOMM
Workshop on Networking, Systems, and Applications on Mobile
Handhelds
(MobiHeld’10),
ACM,
2010,
pp.
3–8,
doi:
10.1145/1851322.1851326.
J.M. Carmena, M.A. Lebedev, R.E. Crist, J.E. O’Doherty, D.M.
Santucci, D.F. Dimitrov, P.G. Patil, C.S. Henriquez, and M.A.L.
Nicolelis, “Learning to control a brain-machine interface for reaching
and grasping by primates,” in PLoS Biology, vol. 1, n. 2, 2003, pp. 193–
208, doi: 10.1371/journal.pbio.0000042.
H. Cecotti, “A self-paced and calibration-less SSVEP-based braincomputer interface speller,” in IEEE Transactions on Neural Systems
and Rehabilitation Engineering, vol. 18, n. 2, IEEE, 2010, pp. 127–133,
doi: 10.1109/TNSRE.2009.2039594.
P. Coulton, C.G. Wylie, and W. Bamford, “Brain interaction for mobile
games,” in Proceedings of the 15 th International Academic MindTrek
Conference (MindTrek’11), ACM, 2011, pp. 37–44, doi:
10.1145/2181037.2181045.
P. Gergondet, S. Druon, A. Kheddar, C. Hintermuller, C. Guger, and M.
Slater, “Using brain-computer interface to steer a humanoid robot,” in
Proceedings of the IEEE International Conference on Robotics and
Biomimetics (ROBIO’11), IEEE, 2011, pp. 192–197, doi:
10.1109/ROBIO.2011.6181284.
M. Grierson and C. Kiefer, “Better brain interfacing for the masses:
progress in event-related potential detection using commercial brain
computer interfaces,” in Proceedings of the Extended Abstracts on
Human Factors in Computing Systems (CHI EA’11), ACM, 2011, pp.
1681–1686, doi: 10.1145/1979742.1979828.
C. Guger, G. Krausz, B.Z. Allison, and G. Edlinger, “Comparison of dry
and gel based electrodes for P300 brain-computer interfaces,” in
Frontiers in Neuroprosthetics, vol. 6, Frontiers, 2012, pp. 1–7, doi:
10.3389/fnins.2012.00060.

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

G. Hakvoort, H. Gürkök, D.P-O. Bos, M. Obbink, and M. Poel,
“Measuring immersion and affect in a brain-computer interface game,”
in Proceedings of the 13 th IFIP TC 13 International Conference on
Human-Computer Interaction (INTERACT’11), Springer, 2011, pp.
115–128, doi: 10.1007/978-3-642-23774-4_12.
C. Harrison, D. Tan, and D. Morris, “Skinput: appropriating the body as
an input surface,” in Proceedings of the ACM CHI Conference on
Human Factors in Computing Systems (CHI’10), ACM, 2010, pp. 453–
462, doi: 10.1145/1753326.1753394.
B.A.S. Hasan and J.Q. Gan, “Hangman BCI: an unsupervised adaptive
self-paced brain-computer interface for playing games,” in Computers in
Biology and Medicine, vol. 42, n. 5, Elsevier, 2012, pp. 598–606, doi:
10.1016/j.compbiomed.2012.02.004.
N.J. Hill and B. Schölkopf, “An online brain-computer interface based
on shifting attention to concurrent streams of auditory stimuli,” in
Journal of Neural Engineering, vol. 9, n. 2, pp. 1–13, 2012, doi:
10.1088/1741-2560/9/2/026011.
H-J. Hwang, J-H. Lim, Y-J. Jung, H. Choi, S.W. Lee, and C-H. Im,
“Development of an SSVEP-based BCI spelling system adopting a
QWERTY-style LED keyboard,” in Journal of Neuroscience Methods,
vol.
208,
n.
1,
Elsevier,
2012,
pp.
59–65,
doi:
10.1016/j.jneumeth.2012.04.011.
C. Kapeller, C. Hintermüller, and C. Guger, “Usability of videooverlaying SSVEP based BCIs,” in Proceedings of the 3 rd Augmented
Human International Conference (AH’12), ACM, 2012, doi:
10.1145/2160125.2160151.
T. Kaufmann, S. Völker, L. Gunesch, and A. Kübler, “Spelling is just a
click away - a user-centered brain-computer interface including autocalibration and predictive text entry,” in Frontiers in Neuroprosthetics,
vol. 6, Frontiers, 2012, pp. 1–10, doi: 10.3389/fnins.2012.00072.
D-W Kim, J-H Cho, H-J Hwang, J-H Lim, and C-H Im, “A vision-free
brain-computer interface (BCI) paradigm based on auditory selective
attention,” in Proceedings of the Annual International Conference of the
IEEE Engineering in Medicine and Biology Society (EMBC’11), IEEE,
2011, pp. 3684–3687, doi: 10.1109/IEMBS.2011.6090623.
S.S. Liu, A. Rawicz, S. Rezaei, T. Ma, C. Zhang, K. Lin, and E. Wu,
“An eye-gaze tracking and human computer interface system for people
with ALS and other locked-in diseases,” in Journal of Medical and
Biological Engineering, vol. 32, n. 2, pp. 37–42, 2012.
T. Liu, L. Goldberg, S. Gao, and B. Hong, “An online brain-computer
interface using non-flashing visual evoked potentials,” in Journal of
Neural Engineering, vol. 7, n. 3, 2010, pp. 1–9, doi: 10.1088/17412560/7/3/036003.
F. Lotte, J. Fujisawa, H. Touyama, R. Ito, M. Hirose, and A. Lécuyer,
“Towards ambulatory brain-computer interfaces: a pilot study with P300
signals,” in Proceedings of the International Conference on Advances in
Computer Enterntainment Technology (ACE’09), ACM, 2009, pp. 336–
339, doi: 10.1145/1690388.1690452.
J. Mankoff, A. Dey, U. Batra, and M. Moore, “Web accessibility for low
bandwidth input,” in Proceedings of the 5th ACM International
Conference on Assistive Technologies (ASSETS’02), ACM, 2002, pp.
17–24, doi: 10.1145/638249.638255.
M. Mauro, P. Francesco, S. Stefano, G. Luciano, and P. Konstantinos,
“Spatial attention orienting to improve the efficacy of a brain-computer
interface for communication,” in Proceedings of the 9 th ACM SIGCHI
Italian Chapter International Conference on Computer-Human
Interaction (CHItaly’11), ACM, 2011, pp. 114–117, doi:
10.1145/2037296.2037325.
J.D.R. Millán, R. Rupp, G.R. Müller-Putz, R. Murray-Smith, C.
Giugliemma, M. Tangermann, C. Vidaurre, F. Cincotti, A. Kübler, R.
Leeb, C. Neuper, K.R. Müller, and D. Mattia, “Combining braincomputer interfaces and assistive technologies: state-of-the-art and
challenges,” in Frontiers in Neuroprosthetics, vol. 4, Frontiers, 2010,
pp. 1–33, doi: 10.3389/fnins.2010.00161.
E.M. Mugler, C.A. Ruf, S. Halder, M. Bensch, and A. Kubler, “Design
and implementation of a P300-based brain-computer interface for
controlling an internet browser,” in IEEE Transactions on Neural
Systems and Rehabilitation Engineering, vol. 18, n. 6, IEEE, 2010, pp.
599–609, doi: 10.1109/TNSRE.2010.2068059.

ISSN: 2236-3297

SBC Journal on 3D Interactive Systems, volume 4, number 1, 2013
[24] Y. Nam, Q. Zhao, A. Cichocki, and S. Choi, “Tongue-Rudder: a
glossokinetic-potential-based tongue-machine interface,” in IEEE
Transactions on Biomedical Engineering, vol. 59, n. 1, IEEE, 2012, pp.
290–299, doi: 10.1109/TBME.2011.2174058.
[25] A. Nijholt, D. Tan, B. Allison, J.R. Milan, and B. Graimann, “Braincomputer interfaces for HCI and games,” in Proceedings of the
Extended Abstracts on Human Factors in Computing Systems (CHI
EA’08), ACM, 2008, pp. 3925–3928, doi: 10.1145/1358628.1358958.
[26] G. Pires, U. Nunesa, and M. Castelo-Branco, “Statistical spatial filtering
for a P300-based BCI: tests in able-bodied, and patients with cerebral
palsy and amyotrophic lateral sclerosis,” in Journal of Neuroscience
Methods, vol. 195, n. 2, Elsevier, 2011, pp. 270–281, doi:
10.1016/j.jneumeth.2010.11.016.
[27] G.M. Poor, L.M. Leventhal, S. Kelley, J. Ringenberg, and S.D. Jaffee,
“Thought cubes: exploring the use of an inexpensive brain-computer
interface on a mental rotation task,” in Proceedings of the 13 th
International ACM SIGACCESS Conference on Computers and
Accessibility (ASSETS’11), ACM, 2011, pp. 291–292, doi:
10.1145/2049536.2049612.
[28] A.B. Randolph, ”Not all created equal: individual-technology fit of
brain-computer interfaces,” in Proceedings of the 45 th Hawaii
International Conference on System Science (HICSS’12), IEEE, 2012,
pp. 572–578, doi: 10.1109/HICSS.2012.451.
[29] E.T. Solovey, A. Girouard, K. Chauncey, L.M. Hirshfield, A. Sassaroli,
F. Zheng, S. Fantini, and R.J.K. Jacob, “Using fNIRS brain sensing in
realistic HCI settings: experiments and guidelines,” in Proceedings of
the 22nd Annual ACM Symposium on User Interface Software and
Technology (UIST’09), ACM, 2009, pp. 157–166, doi:
10.1145/1622176.1622207.
[30] E. Solovey, P. Schermerhorn, M. Scheutz, A. Sassaroli, S. Fantini, and
R. Jacob, “Brainput: enhancing interactive systems with streaming
fNIRS brain input,” in Proceedings of the ACM CHI Conference on
Human Factors in Computing Systems (CHI’12), ACM, 2012, pp. 2193–
2202, doi: 10.1145/2207676.2208372.
[31] S. Vernon and S.S. Joshi, “Brain-muscle-computer interface: mobilephone prototype development and testing,” in IEEE Transactions on
Information Technology in Biomedicine, vol. 15, n. 4, IEEE, 2011, pp.
531–538, doi: 10.1109/TITB.2011.2153208.
[32] C. Vi and S. Subramanian, “Detecting error-related negativity for
interaction design,” in Proceedings of the ACM CHI Conference on
Human Factors in Computing Systems (CHI’12), ACM, 2012, pp. 493–
502, doi: 10.1145/2207676.2207744.
[33] Y-T. Wang, Y. Wang, and T-P. Jung, “A cell-phone-based braincomputer interface for communication in daily life,” in Journal of
Neural Engineering, vol. 8, n. 2, 2011, doi: 10.1088/17412560/8/2/025018.
[34] J.R. Wolpaw, N. Birbaumer, D.J. McFarland, G. Pfurtscheller, and T.M.
Vaughan, “Brain-computer interfaces for communication and control,”
in Clinical Neurophysiology, vol. 113, n. 6, Elsevier, 2002, pp. 767–791,
doi: 10.1016/S1388-2457(02)00057-3.
[35] H. Xu, T. Qian, B. Hong, X. Gao, and S. Gao, “Brain-actuated human
computer interface for google search,” in Proceedings of the 2nd
International Conference on Biomedical Engineering and Informatics
(BMEI’09), IEEE, 2009, pp. 1–4, doi: 10.1109/BMEI.2009.5305708.
[36] A. Yazdani, J-S. Lee, J-M. Vesin, and T. Ebrahimi, “Affect recognition
based on physiological changes during the watching of music videos,” in
ACM Transactions on Interactive Intelligent Systems, vol. 2, n. 1, ACM,
2012, pp. 1–26, doi: 10.1145/2133366.2133373.
[37] B.F. Yuksel, M. Donnerer, J. Tompkin, and A. Steed, “A novel braincomputer interface using a multi-touch surface,” in Proceedings of the

ISSN: 2236-3297

[38]

[39]

[40]

[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

[49]

[50]

13
ACM CHI Conference on Human Factors in Computing Systems
(CHI’10), ACM, 2010, pp. 855–858, doi: 10.1145/1753326.1753452.
A.L.S. Ferreira, L.C. Miranda, and E.E.C. Miranda, “Interfaces cérebrocomputador de sistemas interativos: estado da arte e desafios de IHC,”
in Anais do XI Simpósio Brasileiro sobre Fatores Humanos em Sistemas
Computacionais (IHC’12), SBC, 2012, pp. 239–248.
R. Leeb, M. Lancelle, V. Kaiser, D. Fellner, and G. Pfurtscheller,
“Thinking Penguin: multimodal brain-computer interface control of a
VR game,” in IEEE Transactions on Computational Intelligence and AI
in Games, vol. 5, n. 2, IEEE, 2013, pp. 117–128, doi:
10.1109/TCIAIG.2013.2242072.
M. Marchesi, “From mobie to Neu: 3D animated contents controlled by
a brain-computer interface,” in Proceedings of the Virtual Reality
International Conference (VRIC’12), ACM, 2012, pp. 1–3, doi:
10.1145/2331714.2331747.
R. Poli, C. Cinel, A. Matran-Fernandez, F. Sepulveda, and A. Stoica,
“Towards cooperative brain-computer interfaces for space navigation,”
in Proceedings of the International Conference on Intelligent User
Interfaces
(IUI’13),
ACM,
2013,
pp.
149–160,
doi:
10.1145/2449396.2449417.
D. Hood, D. Joseph, A. Rakotonirainy, S. Sridharan, and C. Fookes,
“Use of brain computer interface to drive: preliminary results,” in
Proceedings of the 4 th International Conference on Automotive User
Interfaces and Interactive Vehicular Applications (AutomotiveUI’12),
ACM, 2012, pp. 103–106, doi: 10.1145/2390256.2390272.
G. Edlinger, C. Holzner, and C. Guger, “A hybrid brain-computer
interface for smart home control,” in Proceedings of the 14 th
International Conference on Human-Computer Interaction (HCII’11),
Springer, 2011, pp. 417–425, doi: 10.1007/978-3-642-21605-3_46.
G. Edlinger and C. Guger, “A hybrid brain-computer interface for
improving the usability of a smart home control,” in Proceedings of the
ICME International Conference on Complex Medical Engineering
(CME’12),
IEEE,
2012,
pp.
182–185,
doi:
10.1109/ICCME.2012.6275714.
C. Escolano, J.M. Antelis, and J. Minguez, “A telepresence mobile robot
controlled with a noninvasive brain-computer interface,” in IEEE
Transactions on Systems, Man, and Cybernetics, Part B: Cybernetics,
vol.
42,
n.
3,
IEEE,
2012,
pp.
793–804,
doi:
10.1109/TSMCB.2011.2177968.
S.M. Grigorescu, T. Lüth, C. Fragkopoulos, M. Cyriacks, and A. Gräser,
“A BCI-controlled robotic assistant for quadriplegic people in domestic
and professional life,” in Robotica, vol. 30, n. 3, 2012, pp. 419–431, doi:
10.1017/S0263574711000737.
G. Pfurtscheller, R. Leeb, C. Keinrath, D. Friedman, C. Neuper, C.
Guger, and M. Slater, “Walking from thought,” in Brain Research, vol.
1071, 2006, pp. 145–152.
R. Leeb, D. Friedman, G.R. Müller-Putz, R. Scherer, M. Slater, and G.
Pfurtscheller, “Self-paced (asynchronous) BCI control of a wheelchair in
virtual environments: a case study with a tetraplegic,” in Computational
Intelligence and Neuroscience, vol. 2007, 2007, pp. 1–8, doi:
10.1155/2007/79642.
D. Friedman, R. Leeb, G. Pfurtscheller, and M. Slater, “Humancomputer interface issues in controlling virtual reality with braincomputer interface,” in Human-Computer Interaction, vol. 25, n. 1,
Taylor & Francis, 2010, pp. 67–94, doi: 10.1080/07370020903586688.
Lotte, Fabien, Josef Faller, Christoph Guger, Yann Renard, Gert
Pfurtscheller, Anatole Lécuyer, and Robert Leeb. “Combining BCI with
virtual reality: towards new applications and improved BCI,” Towards
Practical Brain-Computer Interfaces, pp. 197–220. Springer, 2013, doi:
10.1007/978-3-642-29746-5_10.

