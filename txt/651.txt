This is the authors’ original unrevised version of the manuscript. The final
version of this work (the version of record) is published by Springer in the
book Guide to Brain-Computer Music Interfacing, ISBN 9781447165835.
This text is made available on-line in accordance with the publisher’s policies. Please refer to any applicable terms of use of the publisher.
CHAPTER 10
On Mapping EEG Information into Music

Joel Eaton and Eduardo Reck Miranda

Abstract: With the rise of ever-more affordable EEG equipment available to musicians, artists and researchers, designing and building a Brain-Computer Music
Interface (BCMI) system has recently become a realistic achievement. This chapter discusses previous research in the fields of mapping, sonification and musification in the context of designing a BCMI system and will be of particular interest to
those who seek to develop their own. Design of a BCMI requires unique considerations due to the characteristics of the EEG as a human interface device (HID).
This chapter analyses traditional strategies for mapping control from brain waves
alongside previous research in bio-feedback musical systems. Advances in music
technology have helped provide more complex approaches with regards to how
music can be affected and controlled by brainwaves. This, paralleled with developments in our understanding of brainwave activity has helped push braincomputer music interfacing into innovative realms of real-time musical performance, composition and applications for music therapy.

Joel Eaton
Interdisciplinary Centre for Computer Music Research (ICCMR), Plymouth University
joel.eaton@postgrad.plymouth.ac.uk

Eduardo Reck Miranda
Interdisciplinary Centre for Computer Music Research (ICCMR), Plymouth University
eduardo.miranda@plymouth.ac.uk

2

10.1 Introduction
Articles on Brain Computer Music Interfacing (BCMI) research often open with a
sentiment on how far away we are from the science-fiction like dreams of thought
explicitly controlling computers. However, the on-going progress in this field in
the last decade alone indicates that this is becoming reality; we are not as far away
from such dreams as people tend to think.
In a climate where science and technology has the ability to translate primitive
emotional states of the brain, develop Brain-Computer Interfacing (BCI) for precise control of machinery and allow for non-speaking persons to communicate by
means of brain signals - or brainwaves - mediated by brain scanning technology, it
is easy to become enthused about the potentials within neuroscience, especially
when applied to the arts.
The possibility of BCI for direct communication and control was first seriously
investigated in the early 1970s and the notion of making music with brainwaves
(turning BCI into BCMI) is not new. Musicians and composers have been using
brainwaves in music for almost the last 50 years. Instrumental in this were a number of highly innovative people, the work of which is discussed in this chapter.
This period reflected a significant trend towards interdisciplinary practices within
the arts influenced by experimental and avant-garde artists of the time and a growing engagement with eastern music and philosophies by those in this field. It is
fair to say that brainwaves in music was initially explored by experimental composers and the area has been pioneered by a number of notable non-traditional
composers and technologists since, and this is reflected in the wide range of applications and research that has been undertaken over the last decade and a half.
Over the last twenty or so years the world of computer music has been waiting
for technology to interpret brainwave information in order to develop BCMI systems. Equipment costs, portability, signal analysis techniques and computing
power has rapidly improved over recent times, alongside a deeper understanding
of how the brain functions. Now that the line between these two areas is narrowing the playing field is becoming much larger enabling the two to flourish together. Brainwaves have long been considered to be one of the most challenging of biological signals from the human body (known as bio-signals) to harness, and
beginning to understand them through music and sound offers clinical as well as
creative rewards; for instance, BCMI systems are bound to benefit Music Therapy.
This chapter focuses on the pressing problem of mapping EEG information into
sonic and musical forms. That is, on how to use EEG to control algorithms for
synthesising sound or to produce music. A number of mapping methods that have
been devised to date are introduced. As we shall see further on, there are a number
of different approaches to making music with EEG and the choice of which to use
is dependent on the overall objectives of the system.

3

10.2 Mapping and Digital Musical Interfaces
The pursuit of control within musical systems controlled by the brain has been at
the forefront of research ever since it was viable. Control has been a key driver in
BCMI research as within it is the ability to convey expression and communication
through music. Mapping can be likened to a key that unlocks the creative potentials of control. Mapping allows us to translate an input signal so that it can be understood and used by a musical system. Put simply, mapping is the connection of
input controls (via EEG) to an output, which in the case of a BCMI is a musical
engine. In the pursuit of enhancing user interactivity in BCMIs, mapping plays a
key role in designing creative and practical applications. Even Alvin Lucier, the
first composer to perform using EEG signals, had a desire for more comprehensive mappings within his system to allow for greater musical control (Lucier
1976).
Research into mappings and digital instruments has largely focused on gestural
control and physical interaction (Miranda and Wanderley 2006). Goudeseune
(2002) presents a comprehensive framework of mapping techniques for digital instrument design, building on the proviso that performers can think of mappings as
containing the feel of an instrument; how it responds to the physical control.
Garnett and Goudeseune (1999) refer to the results of mapping as providing “consistency, continuity, and coherence”, key factors in the design of musical control
systems. Clearly, different strategies for mapping in instruments driven without
gestural input, known as integral interfaces, is needed to develop BCMI systems
(Knapp and Cook 2005).
Mappings can be defined based on the number of connections between the input and output parameters; one-to-one, one-to-many and many-to-many (combinations of one-to-one and one-to-many) (Hunt et al. 2000). Although this framework
is useful for evaluating system design, it does not take into account the relationship of the input control to the mapping, or any co-dependencies or rules a mapping may rely on. Goudeseune (2002) recognises the intricacy involved in mapping design, coining the term High Dimensional Interpolation (HDI) to define
mapping a large number of parameters to a small number of inputs where controls
can be interpolated and connected using a variety of rules and techniques.
The investigation of sophisticated mappings in BCMIs, in comparison with
other contemporary digital musical instruments and interfaces, has until recently
been stifled by the difficulties in eliciting control from EEG information. On the
one hand, simple mappings that exemplify EEG control have been favoured as
they suit this purpose well. Simple mappings, such as a linear control to modulate
a synthesiser’s pitch, have been designed to be very effective to facilitate performing and composing with BCMIs for non-musicians (Miranda et al. 2011). On the
other hand, new methods of EEG acquisition provide much more accurate realtime control than was previously available, and as a result can accommodate far
more advanced mapping techniques leading to complex compositional approach-

4

es. Eaton’s The Warren, a performance BCMI piece that will be discussed later in
this chapter, provides a useful example of complex mapping strategies.
As technologies for monitoring brain wave information have advanced so too
has the field of computational music. This correlated evolution of technologies
and understanding of EEG has shaped the direction of brainwave-controlled music. Both fields have produced knock on effects in this area, from the introduction
of MIDI that led to new applications of brainwaves with music, to the advancement of Brain-Computer Interfacing (BCI), allowing BCMI research to shift towards its engagement with cognitive control of EEG.
In order to elicit control over EEG, it is essential to be able to decipher meaning within EEG data that directly correlate with the subjective decisions (control
choices) of a user, be it a mental state or a cognitive task. This quest for accurate
meaning in EEG information has long been at the forefront of BCMI research, as
through precision in generating data comes accurate control. Note that the term
meaning here refers to understanding the correlation between a user’s mental process and an associated brainwave response. Meaning in this manner does not refer
embedded or implied thought patterns within brainwaves (unless otherwise stated
later on). Mappings are not necessarily dependant on control, as generative mappings that interpret unknown EEG information can produce interesting music, but
the two can feed off of each other in terms of complexity. When control is explicit, the ability to introduce complex mapping strategies for more advanced musical
control arises.
In this chapter we use the term secondary mappings to refer to a mapping as an
aside of an input’s primary connection. A secondary mapping may not necessarily
be directly presented to a user, it may be used for time-based data harvesting for
algorithmic rule based mapping, or it may just not take precedence over a primary
mapping.

10.3 Mapping and Approaches to BCMI
The BCMI systems presented in this chapter differ in terms of application, cost,
equipment type and signal processing, data handling and indeed mappings, but all
can be said to consist of the following elements (Figure 10.1):

• Stimuli. This element is optional, and in some cases where it is present provides the feedback link with the system, being part of or being affected by the
musical system.
• EEG Input. Electrodes placed on the scalp, either in the form of a brain cap or
a headband to fit them.

5

• Signal Processing. Amplification of electrical activity, and data extraction to
isolate meaningful information. Filtering and further data processing/analysis/classification are applied depending on the EEG technique
used.
• Transformation Algorithm. Transforming the EEG information into parameters within a musical system. This is where mapping of non-musical information to the music engine occurs. This can take various forms from a patch
cable from an EEG amplifier into an analogue synthesiser, to a generative
software program that triggers musical events.
• Musical Engine. The musical system receiving commands from the transformation algorithm. This may be external to the algorithm (e.g., a MIDI instrument) or built into it with the appropriate software.

Figure 10.1: The makeup of a typical BCMI system.

Miranda and colleagues (2003) identify three types of BCI systems, based on how
they interact with a user. BCMIs can also be observed using this categorisation as
systems have been developed within all three areas: user-orientated, computerorientated and mutually-orientated.

6

10.3.1 User-oriented systems
A user-orientated type of system is programmed to understand the meaning of user
input with in an attempt to adapt to its behaviour in order to achieve control. For
the piece In Tune, Richard Teitelbaum adapts his system in response to a performer’s alpha waves as well as injecting his own musical directions (Teitelbaum
1976). Building user-orientated BCMIs poses difficulties with understanding
meaning within EEG. When relying on interpretation, control can be harnessed far
better in mutually-orientated systems where this problem is addressed two-way.

10.3.2 Computer-orientated systems
In a computer-orientated system the user adapts to the functions of the computer.
The computer model stays fixed and the success of the system relies on the ability
of a user to learn how to perform control over musical events. A performance
piece conceived in 2011 by BioMuse Trio, called Music for Sleeping and Waking
Minds, uses this approach. The responses of performers’ brainwaves are mapped
to fixed musical parameters. Controlling their states of mind (or sleep in this case)
affects control over the music. Attempts to control musical systems with alpha
waves using, a technique called neurofeedback, have mostly fallen into this category as the user is required to learn how to control their EEG in certain ways in
order to produce desired sonic results.

10.3.3 Mutually oriented systems
Mutually-orientated systems combine the functions of both user and computer orientation whereby the two elements adapt to each other. This was the approach
used in Eaton’s The Warren. Here the system requires the user to learn how to
generate specific commands, and features mappings that adapt depending on the
behaviour of the user.
The majority of BCMIs fall into the category of computer-orientated systems. This
allows for fixed parameters to be built that respond to known user brain responses.
The use of mutually-orientated systems allows for two useful things. Firstly, more
sophisticated algorithms derived from EEG behaviour can be mapped onto music.
As the system learns the EEG behaviour of a subject over time, this information
can be used in series with primary mappings and in parallel through embedding
deeper secondary mappings. Secondly, a system where user and computer adapt
together increases the likelihood of obtaining accurate EEG as both elements are
effectively calibrated to optimise the system performance.

7

10.3.4 Brainwave data for BCMI
There are two types of EEG data used in the systems discussed in this chapter:
Event Related Potentials (ERP) and spontaneous EEG. ERPs are fluctuations of
EEG measured in response to events triggered by external stimuli. ERP data are
time locked to stimulus and are recognised as positive or negative amplitude deflections. ERPs are categorised by their response time post-stimuli and are associated with brain processing of event expectation and perception.
Systems monitoring spontaneous EEG look at on-going EEG data, often across
multiple frequencies for patterns or trends that correspond to specific brain activities. This can also be time locked to external stimuli and if so, windows of corresponding data are captured for analysis.
Significant work in using brainwaves for music has been developed with other
forms of measurement of brain activity. For instance, fMRI (functional Magnetic
Resonance Imaging) has been used to translate brain data as input to offline musical compositions, one example of which is discussed in Chapter 14 in this volume.
However, fMRI is currently impractical for developing a BCMI: it is expensive,
not portable and has poorer time-resolution than EEG, to cite but three encumbering factors.

10.3.5 Methods of music generation with brainwaves
When looking back on research into music and brainwaves we can separate systems into three categories; ones for EEG sonification, ones for EEG musification
and ones for BCI control. EEG sonification is the translation of EEG information
into sound, for non-musical and predominantly medical purposes. EEG musification is the mapping of EEG information to musical parameters, however the EEG
data is arbitrary and when possible can offer only loose forms of control. BCI
Control is inherent in systems where direct cognitive real-time control of music is
achievable. In some systems more than one of these approaches can be found, and
in others where one approach has been adopted for investigation of the technique,
the application could well be applied to another approach as a result.
It should also be noted that the mapping approaches discussed in this chapter
are not wholly comparative, as it charts development in a relatively infantile field,
where, as previously mentioned, progress is heavily reliant on the advances within
neuroscience. Where considered useful, areas are touched upon that draw parallels
between systems as a way of directing the reader through the different approaches
and ideas.
Although this chapter does not attempt to explicitly categorise the accuracy of
each system, due to the wide range of disparaging technologies and individuals incorporated, it should be carefully acknowledged that accuracy plays a very im-

8

portant part in the derivation of meaning within EEG data, and this is considered
of high importance.
The sonification of data offers an interesting way of to listening to the sounds
of non-musical sources of information. Data harvesting allows us to sonify a
world of unlikely information, such as the stock market or even the weather. In
sonification we are concerned with the sound of the information relative to itself,
it is a passive process and a way of hearing numerical or graphical data.
Sound has long been used as a way of interpreting biological information, from
the use of the stethoscope to the steady beeping of the heart rate monitor. Both of
these are methods of hearing the body, which when used in real-time to help affect control over the signal is known as bio-feedback. The visual complexities of
EEG has given reason to sonifying its information as a method for understanding
activity through the simplification and the natural intuition of discernably listening
to multiple elements contained within sounds. As such the mappings for direct data sonification should be straightforward in order to provide an intuitive correlation between brain activity and sound. Control of EEG in sonification (and some
musification) systems is largely passive, whereby the user has no direct control
over their EEG. EEG may be influenced external factors, such as tiredness or
mood, but in situations where brainwave control is not achieved by explicit
choice.
In contrast with sonification to musify data is to map the data into organised
musical form. This is rather different from sonification as one is not attempting to
understand the data through sonification per se, but rather attaching it to a musical
system. Therefore musical structures are connected to the EEG information based
on the patterns or variables apparent within the data. For example, if the EEG delivers five distinguishable data, then these can be directly mapped to five parameters within a pre-designed musical piece. A common factor within EEG musification is the use of generative musical approaches. In musification BCMI systems a
passive approach to EEG control is generally used. EEG data is generally limited
in its meaning, and the shift in focus lies heavily on mappings using advanced
techniques of interpreting data in useful ways to grant musical success. In summary the difference between sonification and musification are: a) sonification produces sounds from EEG data; the system would normally control a sound synthesiser; b) sonification is not, in principle, intended for an artistic purpose, but rather
as some sort of scientific auditory display of the EEG behaviour.
Both, sonification and musification afford no explicit control of the sound of
music, and as such, strictly speaking they could be regarded outside of the realms
of BCI research. This is because BCI research is based on the premise that a BCI
system allows for the active control of a device and/or software by the explicit
thought of the command, and the results of the mental activity are fed back to the
user in real-time (Wolpaw and Birbaumer 2006). This definition of BCI has been
harnessed within BCMI to the extent that subjective control over systems is now a
realisation. Here is where the challenge of being unable to translate musical

9

thought into direct action has been bypassed through embedding meaning into
cognitive processes. For example, where reading the explicit thought of ‘play the
note D#’, is not feasible, using learnt cognitive processes where a user understands
the outcomes may lead to a dedicated brain wave response that can be mapped to
play the note D#.

10.4 Observations on Musifying EEG
Musifying brainwave activity without a need for control can offer interesting possibilities with regards to mapping data to music. Although musification is not really BCI, it is nevertheless a valid approach for BCMI for artistic purposes. For instance, Miranda and Soucaret (2008) reported on a mapping method they
developed to produce melodies from the “topological” behavior of the EEG across
a configuration of electrodes on the scalp, or montage. In this case the EEG signal
of each individual electrode was analysed individually in order to infer possible
trajectories of specific types of EEG information across a montage of 14 electrodes, as listed in Table 10.1; see Figure 10.2 for placement scheme with labels
suggested by the International Federation of Societies for EEG and Clinical Neurophysiology.

Table 10.1: The montage of 14 electrodes used in EEG Melodies.

10

Figure 10.2: The 10-20 Electrode Placement scheme recommended by the International Federation of Societies for EEG and Clinical Neurophysiology.

As an example, let us assume that we are interested in tracking the behaviour of
the overall EEG amplitude. Figure 3 plots the amplitude of the EEG on each electrode for approximately 190 seconds. Each plot is divided into 5 windows of approximately 38 seconds each; the size of this window is arbitrary. The average
amplitude is calculated for each window and the electrode with the highest value
is singled out (shaded windows in Figure 10.3). The example in Figure 10.4 shows
how the power of the EEG has varied across the montage: the area with the highest EEG power moved from electrode 2 (Fp2) to 1 (Fp1), then it moved to electrode 5 (F4) followed by electrode 6 (F8), where it remained for two windows.

11

Figure 10.3: The varying amplitude of the EEG on 14 different electrodes for approximately 190
seconds.

12

Figure 10.4: Tracking the behaviour of the amplitude of the EEG signal across a montage of
electrodes. In this example, the area with the highest EEG power moved from electrode 2 (Fp2)
to 1 (Fp1), then it moved to electrode 5 (F4), followed by electrode 6 (F8), where it remained for
two windows.

The method to produce melodies works as follows: we associate each electrode
with a musical note (Table 10.2), which is played when the respective electrode is
the most active with respect to the EEG information in question. The associations
between notes and electrodes are arbitrary and can be customised at will.

13

Table 10.2: Associations between musical notes and the electrodes of a given montage.

In the case of our example, the trajectory shown in Figure 10.4 would have generated the melody shown in Figure 10.5. (Rhythm is allocated by means of a Gaussian distribution function, which is not relevant for discussion here.)

Figure 10.5: Melody generated from the behaviour of EEG power shown in Figure 10.4.

The authors reported that it was possible to produce interesting pleasant music
with the system by forging crafty associations of electrodes and notes, combined
with careful generation of rhythmic figures.
A number of analyses can be performed in order to track the behaviour of other
types of EEG information. For instance, they generated two concurrent melodies
by tracking the trajectory of Alpha rhythms and Beta rhythms simultaneously.
They also generated polyphonic music by tracking other types of EEG information
simultaneously, such as correlation between electrodes or sets of them, synchronisation between one or more electrodes, and so on.

14

Another example of musification was reported by Wu and colleagues. They
harnessed EEG data generated by variations in sleep to compose music (Wu et al.
2009). The pitch and duration of notes were derived from formulas that mapped
each EEG wave to a determinate pitch and its period to duration. Characteristics of
the music were explored through experiments with listeners attempting to associate the resultant music with levels of sleep. They developed mapping strategies in
their investigations into musical representation of mental states. Figure 10.6 shows
the relationships between EEG features and musical parameters. Here mappings
accumulate in order to build bars of musical phrases. For example, as time based
features of sleep stages differ, compositions derived from Slow Wave Sleep
(where activity is high in low frequency delta and theta rhythms; see Chapters 1, 3
and 9 for more on EEG rhythms), are higher in amplitude and lower in pitch than
compositions generated from Rapid Eye Movement EEG (where alpha activity is
more prominent, albeit with low amplitudes) (Wu et al. 2010). This ability to directly map time-based features, such as the prominent frequency and amplitude,
gives way for direct musical evocations of the mind’s state, allowing a listener to
hear, through music, brain states of arousal and relaxation.

Figure 10.6: Mapping diagram for musification of EEG proposed by Wu and colleagues (Wu et
al. 2010).

10.5 Early Research into Biofeedback and Music
In 1965 Alvin Lucier performed a piece for live percussion and brainwaves titled
Music for Solo Performer. The piece was inspired by Luciers’ experiments, with
the physicist Edmond Dewan, into controlling bursts of alpha activity with medita-

15

tive states. Brainwaves mapped to sounds, in real-time, created a neurofeedback
loop, allowing Lucier to affect sonic changes based on the feedback of the previous brainwave states as he heard them. Alpha waves, or alpha rhythms, is the term
given to describe brain activity within the range of 8Hz and 13Hz, and is commonly associated with relaxed states of attentiveness (Cahn & Polich, 2006).
During the performance Lucier amplified his alpha waves, read from two electrodes positioned on his forehead, through a series of loudspeakers. As the frequencies contained in alpha waves are below the threshold of human hearing the
loudspeakers were coupled with resonant percussive instruments including cymbals, gongs, bass drums and timpani as a way of musifying brainwave activity
(Lucier 1976).
This simple method of directly mapping brainwave intensity to instrument resonance was the first attempt of its kind to interpret brainwave activity in real-time
into a form of experimental music. The theatrical dramaturgy of a man on a darkened stage with wires on his head and his brain generating music was surely impressive enough, but Lucier was considerate in his approach applying deeper
mapping considerations to increase the sonic possibilities. The input to the system
was alpha rhythms produced in phrases of varying duration, and this one limited
parameter from the brain was carefully utilised. The amplitude was operated by a
manual control (either by an assistant or Lucier himself), and mixed between individual speaker channels. The known behaviour of these three parameters (duration, volume and channel mixing) in response to alpha activity was used to design
the output stages of the system, or the musical engine; instrument type, speaker
placement, and the involvement of extra materials, such as cardboard boxes or
metal bins. Additionally a threshold switch was used for alpha above a certain
amplitude level to trigger pre-recorded tape loops of alpha activity, transposed
upwards into the audible realm for the audience to hear.
In his reflections on the piece, Lucier recognises the importance of how his
mapping choices are linked to musical complexity. He even goes as far as to identify a further mapping strategy, unavailable to him at the time. He wished to be
able to store time-encoded sections of alpha activity and map patterns within them
to speaker channel mixing; a technique possible with today’s computing and not
too dissimilar from methods used in BCMIs discussed later in this chapter.
In contrast to Lucier’s desire to communicate the natural frequencies of brain
activity through acoustic and tangible sound sources, Richard Teitelbaim, a musician in the electronic ensemble Musica Elettronica Viva (MEV), began to incorporate bio-signals into his electronic compositions using modular analogue synthesisers in the 1970s. Taking inspiration from Lucier and new advances in synthesis
technology Teitelbaum integrated EEG signals alongside other bio-signals into his
pieces, many of which focused on the use of meditative states of mind. Performed
throughout 1967 Spacecraft was Teitelbaum’s first use of amplified EEG activity
as a control voltage (CV) signal for a Moog Synthesiser. Here the electrical activities of the brain were electronically sonified in real time, again providing a realtime bio-feedback loop for the performer (Teitelbaum 2006). Although Spacecraft

16

was a wholly improvised composition it provided a foundation for his later uses of
brainwaves that sought to investigate elements of control and musical interaction.
In Tune, perhaps Teitelbaum’s most popular work, was first performed in
Rome, 1967. What stands out in later versions of the piece (referred to by the
composer as the expanded version of the piece) is the introduction of a second performer’s EEG within his system. Alongside other bio-signals, including heartbeat
and amplified breathe, alpha activity was measured then split into two paths within a modular system comprised of analogue synthesis modules, a mixer and audio
effects. Before any audio processing took place a threshold gate was set to allow
only alpha signals generated with eyes closed to pass; the amplitude of alpha
rhythms is markedly increased by closing one’s eyes. This provided a simple control switch for performers; system ON with eyes shut and system OFF with eyes
open. Precise control within an ON state of the system’s parameters was largely
unattainable beyond basic changes of alpha amplitude increase and attenuation.
With the gate open the alpha of a performer was split from an envelope follower
into two directions within the system to provide a one-to-many mapping. The first
path allowed for a direct DC signal to be mapped to two voltage-controlled oscillators, thus modulating a pre-set centre pitch for each. The second path sent the
EEG signal to an envelope generator, which allowed for variable control of a voltage-controlled amplifier (VCA) and voltage controlled filter (VCF). This parallel
mapping of one EEG signal allowed for real-time modification of pitch, rhythm
and amplitude of the synthesised waveforms coupled with magnetic tape recordings being played back through the same VCA and VCF. Again, these mapping
choices were not arbitrary but were in keeping with Teitelbaum’s artistic aims for
the composition. The heavy breathing and sexualised moaning sounds played back
from one tape machine being rhythmically enveloped by the alpha was designed to
play alongside the live breath and vocal sounds from a throat microphone
(Teitelbaum 1976).
The method for signal processing was repeated for the second performer whose
alpha controlled a third and fourth oscillator via a second envelope generator for a
their amplification and that of a secondary tape machine (but no subsequent filter
in this path).
With two performers generating biological signals Teitelbaum performed the
role of conductor. He manually played the system controls (synthesis, reverb and
mixing parameters) in response to the performer’s alpha alongside injecting his
own musical intuition. Alongside its use of brain wave information as a control
input to an electronic musical system In Tune introduces the use of brainwaves as
a collaborative musical tool for performers and raises interesting questions regarding the potential influences of bio-feedback between individuals in shared musical
environments not just of brainwaves but from other bio-signals.
The fields of bio-feedback and aesthetic experience became increasingly popular in the late 1960s and early 70s. During his time at the Laboratory of Experimental Aesthetics, part of the Aesthetic Research Center of Canada, David Rosenboom conducted a thorough body of research into biofeedback and the arts,

17

definitively recorded in his 1990 writing Extended Musical Interface with the Human Nervous System (Rosenboom 1990).
Other artists at this time were also experimenting with alpha, such as Finnish
artist Erkki Kurenniemi’s instrument Dimi-T, where EEG was used to control the
pitch of an oscillator (Ojanen et al. 2007). Manfred Eaton’s ideas for an adaptive
bio-feedback instrument presented in his book Bio-Music (Eaton 1971) presented
his concept of a musical brain system powered by visual and auditory stimuli.
What is significant in his idea is that the images or sounds that are presented as
stimulus for generating brain wave activity can be semantically removed from the
music as long as the corresponding brain activity is one desired by the composer.
This concept is now a common tool in contemporary BCMI design, where stimuli
is used to generate specific brain wave information or meaning, but is unrelated to
the musical outcomes; this will be discussed in more detail further on.
The study of alpha rhythms in music offered a rich time of creative practice.
Ultimately musical and artistic works were restricted by the limits of control that
came with generating and analysing alpha. In order to use the brain for more advanced musical applications new methods of harnessing and interpreting brain information were required. Yet the work undertaken in using alpha waves to control
music was an important landmark in the field of BCMI, as it suggests that the notion of music controlled by thought was actually achievable.
In 1995 Roslaie Pratt and colleagues at the Biofeedback Research Laboratory
in Brigham Young University, reported on experiments where children with ADD
and ADHD used neurofeedback training with the aid of music containing discernable rhythms, to increase focused behaviour through the reduction of theta activity
(Pratt et al. 1995). These experiments provided benefits that were still discernable
six months later. Years later sound and music were the focus in Hinterberger and
Baier’s body of work in providing aural elements to an SCP (Slow Cortical Potential) driven communicative tools, such as rewarding musical jingles linked to successful EEG control, and in their system POSER, short for Parametric Orchestral
Sonification of EEG Rhythms (Hinterberger and Baier 2004). Spurred on by research indicating the superiority of audio over visual feedback in a system with
multiple inputs (Fitch et al. 1994), POSER applied musical mappings to assist real-time analysis of EEG information. In initial implementations of POSER features of multiple brain wave rhythms were mapped to MIDI instruments and presented to users. Continuous sounds were modulated in pitch and volume according
to changes within the bandwidth of a corresponding rhythm. Reports showed that
users were able to evoke control over individual EEG rhythms, as successfully as
85% during trials, using musical notes as real-time feedback for simultaneous
EEG data. This approach is later adopted in a system that screens EEG for dynamic characteristics (Baier et al. 2007), such as those prominent in diseases including
epilepsy and alzheimers (Jeong 2002). Here, events of interest within EEG are
mapped to digital synthesis parameters in Csound music software (Boulanger
2000), to aid in the distinction between normal and abnormal rhythms in patients.
By connecting expected EEG artefacts to synthesis features such as amplitude
modulation and harmonic content, a sonic real-time interpretation of meaningful

18

data is available. In another system the use of sound localisation via an array of
speakers is used to reflect the horizontal location, across the scalp, of the current
activity. Further work into these sonification techniques also addressed interaction
and user acceptance issues (de Campo et al. 2007).

10.6 Computer Music and the Brain
The mappings in early experiments with music and brainwaves were built in to the
hardware that was used. They were pre-determined by the equipment available,
they were fixed and they were difficult to change or undo. BioMuse, a hardware
and software system developed by Benjamin Knapp and Hugh Lusted in the
1990s, introduced a major departure from this, with the use of real-time digital
computing to process EEG data (Knapp and Lusted 1990).
BioMuse provided a portable kit for digitally processing bio-signals, but what
was ground breaking was that it was able to convert these signals into MIDI data.
Thus creating a MIDI controller based on bodily responses; BioMuse also measured eye movements, muscle movements and sound from a microphone input.
This use of the MIDI protocol allowed for an EEG signal to be mapped to the input of MIDI enabled equipment, such as a synthesiser, a drum machine or a sequencer. Furthermore the technology allowed for fine-tuning of input data. An input threshold switch and a channel sensitivity control meant that the system could
be calibrated for different users and different applications. Adjusting the threshold
allowed for amplitudes over a specified level to trigger a specified MIDI command and increasing the channel sensitivity increased the number of MIDI values
in a corresponding range. A demonstration of BioMuse presented at the New Music Seminar 1990 in New York City, showcased this method of mapping multiple
bio-signals to MIDI parameters.
The BioMuse software provided the ability to manipulate bio-signal to MIDI
mappings. With the large number of MIDI commands available this feature allowed alpha waves to be mapped to note specific MIDI commands such as Note
On or Note Off, or to affect sounds triggered by other bio-signals, such as Control
Change messages. From 1987 bursts of alpha activity were sonified via a MIDI
synthesiser (Lusted and Knapp 1996), and again the use of opening and closing
the eyes was incorporated into compositions to generate significant differences in
alpha activity.
Earlier we mentioned the piece Music for Sleeping and Waking Minds, which is
a more recent work using updated versions of these tools. This is an eight-hour
long composition intended for night-time listening. Four performers wearing EEG
sensors affect properties of tones using simple direct mappings, in order to project
basic changes in their brainwave activity to an audience. Alongside alpha activity,
delta rhythms and spindles are also measured and mapped to parameters of audio.
The contrast in input parameters is reflected through the resulting sound. Where
alpha rhythms are prominent during modes of light sleep and through closing of

19

the eyes, Delta rhythms, waves between approximately 0-4Hz, are associated with
deepest levels of sleep. A spindle is recorded as a spike in activity between 11-16
Hz with a duration ≥ 0.5 secs and combines with muscle twitches during periods
before deep sleep (Babadi et al. 2012). These three classes of brain activity associated with different stages of sleep are mapped to three musical parameters. Within
the composition are sixteen tones of differing spectra. Each performer controls parameters relating to four of these tones. An increase in alpha activity applies a
tremolo effect to the tones, prominent delta waves change the timbre of the tones,
and spindles trigger enveloped tones through a delay effect with feedback
(Ouzounian et al. 2011). Whereas delta activity and spindles are not wholly controllable these three elements of brain activity are effectively communicated
through the act of watching the performers sleep as well as listening to the resulting audio.

10.7 Event-Related Potentials and Auditory Stimuli

Research into using brainwave activity for musical purposes has not been limited
to translating alpha and other rhythms related to meditative states. Studies into
event-related potentials (ERPs) have led to BCMIs designed to measure brain activity as a direct result of sensory, cognitive or motor responses. The ability to actively generate brain activity using ERPs has led to BCMI systems whereby a user
has full control over the musical outcomes.
ERPs are electrophysiological brain responses produced by perception to
stimuli that is presented to a subject. They are locked in time to the event of the
stimuli and they are sources of controlled and visible variability in brain activity
(Donchin et al. 1978). ERPs highlight the role of anticipation within brain processing as they can be elicited by deviation from expected events provided by, on
the whole repetitive, stimuli.
In 1990 Risto Näätänen reported on a number of experiments in measuring
brain activity relating to attention using auditory stimuli. Even though attention research involving ERPs had been going on for over 50 years at the time, Näätänen
was keen to distinguish between the brain’s automatic responses to stimuli and responses derived from someone’s attention and their interpretation of the heard
stimuli (Näätänen 1990). The idea of a subject being able to shift their attention at
will to auditory stimuli opened up possibilities of BCMI systems controlled by a
user’s attention to elements of what they are hearing.
Research into attention and sound has long been investigated even before the
use of EEG, and earlier research observed a phenomenon known as dichotic listening in regards to how we focus our hearing attention. Dichotic listening is the
process of paying attention to sound from one ear whilst ignoring sound from the
opposite ear. When asked to focus on speech arriving at one ear, subjects were often unable to recall speech of the same volume from the opposite ear (Cherry

20

1953). In Näätänen’s experiments he found that the brain reacts to deviations from
repetitive sounds automatically, even when a listener focuses their attention away
from what they are hearing. This was measured with a P300 EEG response, where
the potential begins with a positive deflection and peaks at around 300ms after the
onset of the stimuli. This ‘oddball paradigm’ implied that when presented with recurring audio information the brain reacts automatically, and predictably, to deviations in audio patterns.
Throughout the 1990s and early 2000s further research into how the brain responds to auditory stimuli shed light on how the brain processes our perceptions
of music. A key area in this field is the study of meaning held within ERPs, building upon previous research into how the brain processes language (Besson and
Macar 1987). Here, the term meaning has more depth than mere EEG association
to input. Besson and Faïta (1995) demonstrated how different responses within
ERPs are elicited when subjects listen to musical phrases that end either congruently or incongruently, in pitch or rhythm. The results also show how differences
between musicians and non-musicians indicate that musical expertise can influence aspects of music processing, aside from mere perception.
In 2003 Besson and Schön reported that the P600 ERP response (a positive deflection peaking at around 600ms post-stimuli) is associated with syntactic violations in language and music such as grammatical errors and incongruously ending
musical phrases. Whereas increases in the N400 (negative deflection around
400ms) ERP are associated with unexpected semantic violations in language, such
as “The pizza is too hot to cry” (Besson and Schön 2003). The amplitude of the
ERP is relative to the degree of the violation; the more abstract the meaning results in a potential with higher amplitude.
This research indicates that there is a separate mechanism in the brain for processing music and although the P600 is a slower response that the N400 it nonetheless provided a basis for further research into applying auditory perception into
controlling music. A difficulty in using ERPs as a control source in BCMIs is the
issue of identifying potentials amongst non-related EEG information. To address
this epochs of ERPs are summed and averaged from many presentations of the
same stimuli in order to gauge whether the response is positive or not. This extra
time adds a delay to the signal processing, distancing control away from real-time
musical influence.

10.8 EEG classification and Auditory Stimuli

By the early 2000s there were several headband-based systems that could play
music from EEG data (Miranda 2001). The majority of these provided only two
electrodes and very limited tools for interpreting the raw EEG data. Moreover, the
quality of the EEG obtained with these less costly systems did not match the min-

21

imum standards required to implement reliable BCI system. Nevertheless, in 2001
Alexander Duncan, then a PhD candidate working under the guidance of Eduardo
Miranda and Ken Sharman at the University of Glasgow, proposed a BCMI system based on musical focusing through performing mental tasks whilst listening to
music, alongside EEG pattern classification (Duncan, 2001). Duncan proposed a
number of data classification methods for collecting a subject’s EEG profile to
create an offline neural network classifier, which is used for comparative analysis
of EEG readings. This system could effectively be trained to understand the brain
signals of a user so that in practice there was a built in model to apply ‘best-fit’
rules to derive the meaning within the EEG. Here, EEG was extracted through
power spectrum analysis, instead of ERPs. Power spectrum analysis uses fourier
transformations to observe the amplitudes of EEG frequencies. In this setup EEG
generated from external stimuli was analysed by a computer to create classifications of patterns over multiple trials. Building such a classification systems used
Artificial Intelligence to create models of expected users responses. A model is
built from the averages of many practice tests of an individual’s response to stimuli, which in effect trains the system. When the system is then engaged in an experiment, it reads an incoming EEG signal and classifies it against the artificial neural
network stored within its memory.
Researchers based at the Interdisciplinary Centre for Computer Music Research, University of Plymouth (ICCMR) implemented this approach in experiments that combined auditory attention with data classification to analyse features
within a short epoch of post-stimuli EEG. In 2003 Miranda and colleagues reported on three experiments that investigate methods of producing meaningful EEG,
two of which were deemed suitable for practical musical control. The first of the
two uses the technique of active listening, and the second uses musical focusing.
In the first experiment small epochs of EEG measured across 128 electrodes
were analysed to determine any difference between the acts of active listening (replaying a song in the minds ear) and passive listening (listening without focus).
Trials were multiplied and looped to build a portfolio of EEG readings. Musical
stimuli consisted of melodic phrases being played over rhythmic patterns. In different trials during a break between melodies subjects were asked to do three different things. In the first trial to replay the tune in their heads, in a second to try relax their minds without focusing on anything in particular, and in a third to count.
Trials were carried out in a number of orders for greater disparity and a mental
counting exercise was factored in as a test of whether musical concentration
through active and passive listening was extrinsic to standard methods of mental
concentration focusing (Miranda et al. 2003).
The second experiment set to determine whether EEG could identify if a subject was engaged in musical focusing (paying particular attention to an element of
music being heard) or holistic listening (listening to music without any effort).
During the musical focusing experiments subjects were asked to focus attention to
an instrument within the music, that was positioned either in the left or right stereo
field.

22

These tests suggested that it might be possible to accurately measure EEG differentiation between someone engaged in mentally focusing on music and holistic
listening. The second test suggested that it might be possible, although to a lesser
degree, to record whether a subject is focusing on sound arriving in the left ear or
the right ear, whilst in both experiments the counting exercise provided a different
response in the EEG indicating that musical focus uses different brain processing
mechanisms that other forms of concentration.
The experiments were conducted in blocks of multiple trials and the results
were derived offline. However their outcomes led to two initial concepts for
BCMIs. b-soloist is a BCMI system designed to detect active and passive listening. A continuous rhythm is presented to a subject with regular melodic phrases
overlaid. Straight after the melody is played the system looks for either an EEG
reading of active or passive listening. If the reading shows active listening has occurred then the next melody line will be a variation of the last. If the reading
shows passive listening occurred then the next melody played will be exactly the
same as the last (see also Chapter 3). b-conductor was designed to use musical focusing to affect changes in either left or right channels of music (Figure 4). When
presented with music in both channels a user selects a channel through attentively
focusing on the instrumentation it contains. At regular intervals the system detects
the channel of attention in the EEG and this recognition is mapped to the music,
turning up the volume of the focused channel. After a change is made the volume
then returns to a default value until the next command to change is received.
In 2004 Miranda and Stokes report on a further experiment that investigates
EEG derived from auditory imagery. In this they further the search for distinctions
between mental tasks looking for any distinguishable differences between active
listening and tasks based on motor imagery and spatial navigation, whereby a subject focus’ their attention to a physical movement whilst remaining still (Miranda
and Stokes 2004). Tests again used power spectrum analysis but with three pairs
of electrodes (7 in total with a reference electrode) to determine a classification
system through building a neural network. The three extra tasks assigned were for
a subject to imagine opening and closing the right or left hand (motor), and to imagine scanning the rooms of their home (spatial). A separate pair of electrodes
read EEG data corresponding to each task, and the voltage difference between the
pairs was derived. It was observed which pair produced EEG readings that could
be most easily discriminated against another. Again, results were very positive
with the largest distinction recorded between auditory imagery and spatial imagery.
Not only did this latter test minimise the number of electrodes for accurately
reading overall EEG, thus likely reducing interference and preparation time, but it
also narrowed the gap between BCMIs and EEG techniques within other BCI
fields such as assistive technologies, where patients already accustomed to motor
imagery would need less training.
Importantly, these experiments indicated that subjective choices can elicit expected brain responses. Unlike the previous experiments with auditory stimuli
they do not rely on the subject’s expectation or perception of stimuli but allow for

23

a user to impose a subjective decision that has the possibility of becoming separate
from the meaning within the music being used. This is a crucial step in the leap
towards BCI control of music through neurofeedback.
This element of subjective control aside, the systems discussed in this section
rely on an intrinsic link between the stimuli and resultant music. They are in effect
one and the same, creating the ultimate feedback loop. Attempting to implement
such a BCMI as an interoperable interface with musical systems outside brain related activity becomes extremely difficult when using auditory stimuli as the driver for generating EEG. Issues of attention become prominent when a user is required to focus on specific sounds to generate EEG, which then have a separate
effect as they produce or affect unrelated music as the result. BCMIs designed
specifically for utilising these features such as the b-soloist and b-conductor ideas,
rely on the use of the stimuli as the driver and the receiver of neurofeedback.
However, to design any systems outside such a tight link the element of neurofeedback can become confused and even lost, as the cause is disengaged from
the effect. To counter this either a compromise in neurofeedback loss is made,
heavy user training is required to reassign unrelated mappings through decision
making or as noted by Miranda and collegues (Miranda et al. 2003) higher levels
of intelligence is imparted in compositional algorithms detracting from cognitive
musical control.

10.9 Towards BCI control of Music
Currently there are a number of systems offering EEG detection linked to musical
functions commercially available; e.g., WaveRider, g.tec, Emotiv, to name but
three. These systems provide various methods of processing raw EEG that can be
mapped to musical engines, in effect providing the hardware for a BCMI system.
At the time of publication there are few systems that allow for mapping EEG directly to musical programs without direct access to APIs and designing bespoke
tools, however the Emotiv system offers the ability to map raw EEG into OSC
(Open Sound Control) data, and software such as Brainbay and WaveRider provide tools for mapping EEG to MIDI. We note however that the prices of EEG
equipment can differ enormously. The reader should exercise caution here because
cheaper equipment does not always match the quality of more expensive ones;
EEG requires good quality electrodes and decent amplifiers.
To develop sophisticated systems of BCI control relevant stimuli is required,
and unless using in-the-box methods of analysis and data processing, the appropriate means of data acquisition and methods of mapping to a musical engine is necessary, and this requires expertise.
In 2005 Miranda adopted the approach of designing the musical engine of a
BCMI with sufficient artificial intelligence in order to create sophisticated meaning from simpler EEG readings. Here, he applied a process known as Hjorth analysis, a second method of extracting EEG alongside power spectrum analysis.

24

Hjorth analysis is the extrapolation and measure of time based features within
short windows of EEG information. These are referred to as the activity, mobility,
and complexity within the reading, and measures of each are produced involuntarily as they lie within overall EEG data. Using these techniques the BCMI-Piano
attempts to guess the mental state of the user and performs real-time generative
piano music in response, with features based on the techniques of composers such
as Beethoven and Schumann, as discussed in Chapter 3.
The P300 oddball paradigm, earlier mentioned in relation to auditory stimuli
research, was used by Grierson (2008) for a BCMI controlled by focusing visual
attention to stimuli displayed on a computer screen. The P300 potential was found
to contain information relative to visual attention of repetitive stimuli. In the same
manner as deviations in auditory stimuli was found to trigger P300 responses
(Näätänen 1990) as an automatic response, the P300 could also be elicited by an
unexpected interruption within a repetitive visual pattern. In the case of P300
spelling devices, that allow a user to select letters to form words and sentences, the
deviant information contains the letter the user desires, and as such is injected with
the meaning that a BCI system can knowingly respond to. In the first incarnation
of his BCMI Grierson replaces letters for musical notes for a user to select via a
visual interface.
Over the course of trials Grierson recorded that four out of five subjects were
able to perform subjective decision making, with regards to specific note selection
and with no training, that were understood by the system 75% of the time. As
ERPs are difficult to detect within EEG, conducting multiple trials improves the
reliability of the system to detect these choices and increases the percentage of
success. The downside is the time lapse introduced from the initial cognitive decision being made to the end of the trials and the subsequent data processing. Grierson recognises this factor opting for a minimal trial approach in an attempt to link
control as close to cognition as possible. The stimuli in this system presented the
names of note values over three octaves. Each note name was displayed for approximately 50ms then removed for up to 1800ms, in a quasi-random order. A
subject was asked to select a specific note and count each time it was displayed,
generating the associated ERP information in synchronisation with each display.
Experiments recorded time delays of approximately 12 seconds, with one subject
successfully initiating control over approximately 7 seconds with less trials, where
total time = flash time x choices x trials; e.g., 50ms x 36 x 7 = 12.7 seconds.
Although these times are lengthy in comparison to EEG response times in other
BCMI devices, what (Grierson et al. 2011) accomplished with his system was the
ability to widen choice to a range of values. Instead of a ‘one or the other’ decision, the meaning within the stimuli was designed to visually represent many more
choices, up to 36 in this case example. Grierson and colleagues have since developed a suite of BCMI applications based upon the NeuroSky bluetooth headset
(Grierson et al. 2011) .
The research into ERPs also went as far as to indicate that BCMI control may
not need to rely on a subject training their brain to act accordingly to the intelligence of a BCMI. By relying on the ability of the brain to respond to the focus of

25

attention in a multi-variable environment, no training was necessary as long as the
user had the ability to recognise visual events and perform the counting task. As a
result of these factors this method for eliciting P300 for control was subsequently
utilised by the neurotechnology company g.tec in their commercial BCI system.
As previously mentioned, the ERP response to a single event is problematic to
detect on a single trial basis, as it becomes lost in the noise of ongoing brain activity. However, if a user is subjected to repeated visual stimulation at short intervals
(at rates approximately between 5Hz – 30Hz), then before the signal has had a
chance to return back to its unexcited state the rapid introduction of the next flashing onset elicits another response. Further successive flashes induce what is
known as the steady-state response in the brain’s visual cortex, a continuously
evoked amplification of the brainwave (Regan 1989). This removes a need for
performing numerous delayed trials as the repeated visuals are consistently
providing the stimuli required for a constant potential, translated as a consistent
increased amplitude level in the associated EEG frequency.
This technique, Steady State Visual Evoked Potential (SSVEP), was adopted in
a BCMI system designed for a patient with locked in syndrome (Miranda et al.
2011) as a tool for providing recreational music making. Here four flashing icons
were presented on a screen, their flashing frequencies correlating to the frequencies of corresponding brainwaves measured in the visual cortex. The user selects
an icon simply by gazing at it and the amplitude of the corresponding brainwave
frequency increases. Whilst EEG data is analysed constantly, the system looks for
amplitude changes within the four frequencies. The icons represent four choices,
always available to the user at the same time. These controls are in turn mapped to
commands within a musical engine, as well as being fedback into the display
screen to provide visual feedback to the user. The instantaneous speed of the EEG
response to the stimuli finally brought real-time explicit control to a BCMI, which
required no user or system training beyond the task of visual focusing. Please refer
to Chapter 3 for more information on this system.
As well as the selection of commands a second dimension of control was gathered through the level of focused gazing. This elicited a relative linear response
within the amplitude of the corresponding brain wave. This allows users to employ proportional control methods akin to intrinsically analogue tasks such as
pushing a fader or turning a dial. This differs from previous selective, more digital
tasks in BCMIs, such as a switch or a toggle function. In this system Miranda and
colleagues utilised this control to trigger a series of defined notes within a scale
(Miranda et al. 2011).
The SSVEP-based BCMI by Miranda and colleagues broke new ground in
BCMI research. This is the first instance of a system whereby a user can precisely
control note specific commands with real-time neurofeedback. It is interesting to
refer back to the BCI definition of Wolpaw and Birbaumer (2006) who may well
define such systems as outside the realm of true BCI as it relies on the EEG interpretation of eye position, and not pure thought processes. That said, in the pursuit
of real-time control of brainwaves so far SSVEP, in comparison with motor imagery and P300 BCIs, has been found to offer the quickest and most accurate EEG

26

response, and with the least amount of training (Guger et al. 2011). Also, the advantages of these types of systems over previous BCMIs are clear to see. One of
the outcomes of this initial SSVEP research was the use of BCMIs in collaborative
musical applications. In terms of music used as a real-time communicative tool
between people, this system allows a user to play along with a musician, or potentially, with another BCMI user. This was recognised as an important breakthrough
for the potential BCMI systems in therapeutic situations, and for potentially
launching the BCMI into a wider field of collaborative musical applications.
In 2012 we reported on further mapping and compositional techniques using
SSVEP within a BCMI (Eaton and Miranda 2012) for the composition of The
Warren, a multichannel electronic performance piece designed to explore the
boundaries of mapping strategies in a BCMI system to generate real-time compositional rules. Control of EEG performs generative functions that control macrolevel musical commands, such as shifts in arrangement, tempo and effects over the
master channel, alongside control of micro-level functions, such as control over
individual pitches or synthesis parameters (Figure 10.7). This approach provided a
framework for addressing performance considerations often associated with more
mainstream digital interfaces. The piece was engineered to communicate expressive musical control, and to provide a loose framework of musical elements for the
performer to navigate through, selecting areas for precise manipulation. An important feature of the design was to emulate the unpredictable nature of performing with acoustic instruments, so often safeguarded in performances with electronic instruments. Slight deviations of learnt control patterns, or miscalculations
when navigating through the piece could result in the wrong result, such as bringing the composition to an abrupt end or injecting unwanted silences or dissonance
into cacophonies of consonance. This approach forced the concentration of the
performer, underlying the importance of successfully interpreting the meaning
within the control EEG. To achieve the desired complexities and nuances, mapping rules were designed to suit the musical functions, a break away from previous
systems where compositional mappings were intrinsic to the meaning of EEG.
Here, the meaning of EEG was designed through the use of the stimuli, and therefore learnt or understood by the user. With such an abundant amount of meaningful data The Warren also makes musical use of non-meaningful data to provide
deeper complexity through secondary mappings. For example, ordering rules were
applied to control specific musical parameters through monitoring the performer’s
control behaviour. The order in which icons were selected over x amount of control changes would result in different generative rules being applied, the results
unbeknown to the performer who would be concentrating on the current primary
task. This harks back to Miranda’s technique and the integration of Hjorth analysis, adding intelligent feedback to the system as part of the compositional process,
and making the system learning between performer and computer mutually exclusive.
When designing mappings for a structured performance piece, as opposed to an
instrument or an improvised piece, the mappings need to adapt to the arrangement
of the composition and the functions. This reverse engineering method of mapping

27

design based on musical function and necessity provides an interesting arena for
creativity. As a result the mappings explored in The Warren vary widely depending on the compositional choices, the sonic intentions of composer (and performer) and the limitations of the input controls. Instead of summarising these mappings solely in numerical terms, the nature of how the control is governed can be
presented in parallel with Dean & Wellman’s (1991) Proportional-IntegralDerivative (PID) model. This approach defines control as the ‘effect’ of the input
signal onto the output’s value, regardless of the number of parameters connected.
Proportional control dictates that output values are relative to input; the output is
value X because the input is X. Integral control provides an output value based
solely upon the history of the input whereas derivative control gives an output
value relative to the rate of change of the input signal.
These principles are adopted in a number of ways in The Warren, and the inclusion of conditional rules and variations allow for an abundance of creative implementations. For example, in the first movement a cello sound can be played using the derivative measurement of the increment and decrement of one of the four
EEG input channels. Alongside this a second input channel has an integral control
to regulate a modulation index of the cello sound processing; an example of interpolating two different primary controls to manipulate one sound. To add further
control within these selection based mappings mapping rules were applied to the
four incoming EEG data streams, and used at various times during the piece depending on the required function. Here we look at three of these rules, Threshold
values, Timing and Ordering.
Threshold values
All four of the brainwave control signals can act as a single selector using mappings for when the amplitude is high or low. Beyond this each input signal can be
assigned to elicit a number of commands. In this technique user control of brainwave amplitude (of a specific channel) was mapped to a series of functions across
a range of evenly spaced threshold values scaled according to the input range.
When the input signal passes a threshold value a control command is triggered.
For example an input range of 1 - 25 could be treated with the following rules:
if input == 5 play note C2
if input == 10 play note D2
if input == 15 play note E2
if input == 20 play note F2
Without further consideration an input signal rising and falling through this range
would excite all of the notes on the way up and on the way down. The use of timing rules (below) provided the performer with the ability to make specific selections whilst avoiding triggering unwanted commands.
Timing

28

The majority of the mappings within The Warren are led by timing rules. Calculating the time a user takes to complete cognitive tasks allows for an added dimension of control. Expanding the simplified threshold example shown above the
speed at which a brainwave increases towards a threshold value would dictate
whether the in between mapping rules are accepted or not. This allows the performer to choose how many of the threshold values within one input range to select during any one command. For example, if the time between initial excitation
and input signal reaching a value of 15 is greater than X then only note E2 would
be played. If the time taken was less than x and more than 1/2X, then notes D2
and E2 would be played. Less than 1/2X then C2, D2 and E2, and so forth. In
practice the timing rules were used like this alongside the threshold values and also separately on their own. They were mapped to parameters ranging from audio
effects settings including delay, filter and distortion parameters, and audio playback sample chopping controls such as playback position, pan position, and texture blending.
Further complexity was added through exploiting the features of using timers.
A hold-and-release function allowed for a change in control to occur at the point
of release. The time between the hold command and the release command being
received provided selectable options. When an input value increases, a timer begins until the value decreases. Upon this decrease the value of time is compared
against a series of rules. In practice the accuracy of brainwave control can vary
due to a range of factors such as tiredness, environment, mood and electrical interference. To accommodate this instability when attempting to sustain brainwave
amplitude through SSVEP a further time-delay rule monitors the EEG. For example if we define a threshold input value of 5, so that when the input value increases
above 5 a hold command is activated. If the input stays above 5 then the hold
command stays on, and if the value decreases below 5 it is released. To add some
flexibility to this simple hold and release function, a time delay of three seconds is
added to the hold function. Therefore if the input decreases below 5 for less than
three seconds and then increases to above 5, the hold command remains on. If the
input decreases to below 5 for longer than three seconds then the release function
is activated. This technique creates a rule whereby an icon needs to be fixated on
constantly to generate a command sent to the performance system, akin to the constant attention required to play a sustained note on an acoustic instrument. Deviation from this attention is allowed for a time span of up to three seconds, allowing
for the performer to utilise other input commands to manipulate the sound via different parameters or to control other aspects of the music. This flexibility can able
help combat irregularities in the input signal. To help performer calculate times
during a performance a digital clock display was built into the visual interface.
Ordering
The mappings and structure of the The Warren were desgined to allow loose periods for the performer to ‘play’ the system. Within this it was unlikely that the exact manner in which the controls were used would ever be the same twice. To add
a layer of surprise and quasi-randomness to the piece, as well as to further engage

29

the concentration of the performer to adapt to the system secondary mappings
were dominated by applying rules to the order in which icons were selected and
which commands were triggered. At times these rules were mapped to stochastic
musical parameters ensuring a controlled level of unpredictability.
The level of depth attained within these mapping strategies requires a high level of mental concentration and awareness of time, external and in relation to the
music within a performance. Here the mappings had to be tested, learned, practiced and optimised for system performance and user ability.

Figure 10.7: A mapping diagram from a short section of The Warren. Here each icon (1 - 4) is
assigned to a number of commands based on the requirements of the composition.

The Warren demonstrated that BCMI technology could be used in place of more
traditional digital controllers as well as in a live performance setting. In 2013’s
Flex (Eaton and Miranda 2013a) this idea was taken a step further through a
BCMI built using affordable hardware and open source software. In an effort to
make music making with brainwaves more accessible Flex used SSVEP with an
EEG headset by Emotiv and two laptop computers, one providing the visual interface, EEG signal processing and transformation algorithms and the other the musical engine. The gap between compositional and mapping design used in The
Warren was disregarded here as the two elements were intertwined. Flex is designed to be between approximately 10 to 15 minutes long depending on the how
the controls are found and used. The composition combines sound sources recorded in surround sound, ranging from fairground ambience to bell chimes, with synthesised and heavily processed sounds. A key aim of the performance is to convey
the narrative of the composition whilst attempting to engage an audience with the
control tasks being undertaken by the performer.

30

Flex uses the idea of control as a key theme. Instead of merely providing control, Flex hides control and moves it around forcing the performer to adapt to the
system and learn control before it is taken away again. In effect, the controls corresponding to the icons are randomised; different elements of the composition are
presented without any way of the performer knowing in advance. Built in rules allow for the presence of mappings corresponding to the current sounds being
played, but the choice of parameters is selected at random from an array of predetermined functions. Performed in quadraphonic sound, mapping rules mix the
sounds across the four channels as well as control the arrangement of the piece.
Additional mapping rules control micro-level functions such as audio sample
playback and audio effects (Figure 10.8).

Figure 10.8: The system components for Flex, built using consumer grade EEG hardware, two
laptop computers and open source software.

Indeed, there are more mappings available that can be used in any one performance, which helps make every performance different. The line between active
and passive control becomes somewhat blurred here due to the manner in which
control is attained. Control in Flex can be difficult to establish, and this brings elements of the unexpected and even the undesired into a performance. Again, hidden secondary mappings are also built-in to add elements of surprise, in effect further flexing the rigidity of control throughout the piece. Overall, the mapping
system is designed for control to be manageable and, where control becomes lost,
it is relatively easy to recover. As such, certain safety features are implemented in
order to prevent complete chaos. Performing Flex becomes a musical game, where
the aim and reward is control (although the success rate of control is not a primary

31

concern), and where the audience is rewarded with the resulting music and performance.
One of the main issues with performing with a BCMI system is what could be
considered as a lack of obvious ‘performance’. EEG measurement requires motionless concentration and the use of visual stimuli requires staring at a computer
screen, both of which offer a rather disengaging spectacle for viewing however
sophisticated the underlying processes are. We are fortunate to now be at a stage
where the technology is no longer the only focus of attention, yet we need to be
mindful of how we communicate the practices of BCMI to an audience and the
aesthetic effects of the tools we choose. This has led recent work to move away
from brainwave control over electronic sounds towards integrating brainwave control and external musical bodies, including acoustic instruments and musicians.
In 2013 Eaton and Miranda reported on Mind Trio, a proof-of-concept BCMI
that allowed a user to control a musical score in real-time, choosing from short
pre-composed musical phrases (Eaton and Miranda 2013b). SSVEP provides
choice over fours phrases during a window of time. This windows is synchronised
with a metronome and dynamic score presented to a musician via a computer
monitor. Within each window the user selects the phrase that is displayed at the
next sync time. The musician is presented with the current phrase and is shown the
next phrase shortly before it becomes active. This extension of brainwave control
designed to accommodate the involvement of a third party is the basis of Activating Memory, an experimental piece for a string quartet and a BCMI quartet. It uses
the same principle as Mind Trio for users to choose phrases of music that a corresponding musician then performs. All four systems are synchronised via a master
clock across two movements. Activating Memory’s debut performance was at the
2014 Peninsula Arts contemporary music festival, Plymouth, UK.

10.10 Concluding Discussion
BCMI research has come a long way in recent years. Meaning in EEG is becoming more understood and easier to detect, as the necessary technologies and computer processing speeds have allowed. However, difficulties in retrieving useful
EEG data still remain and pose significant problems for systems intended to be
used outside of the laboratory. Signal interference from external sources, unpredictable EEG information, and noise from other physiological input, are issues
widely reported in BCMI research. These factors affect the stability and performance of a system, and need to be taken into account when designing and testing a
BCMI.
The progress in BCMI research has brought us to a very healthy and pivotal
stage. We find ourselves in a climate where constructing a BCMI has become a
relatively simple and affordable task. New systems of finite control have provided
a strong foundation for integrating BCMIs within wider areas of musical composition and performance, perhaps realised through musical collaborations or interac-

32

tions with live, external sources, such as dance, acoustic music or other forms of
media. Wider research into neurofeedback is also possible through assessing the
affects of multiple users of a single BCMI, or multiple BCMIs being played together. Now that the appropriate tools are available we anticipate an increase in
research activity across a wider playing field, with a particular emphasis on compositional integration. We are slowly beginning to see brain wave control creep into everyday tech culture, and as in all successful interdisciplinary areas we expect
it to be prominent in all of the clinical, therapeutic and recreational interpretations
of what a BCMI is.
Events bringing researchers and practitioners together have produced fruitful
experiments in the past, as evident in programs such as eNTERFACE (Arslan et
al. 2006) (Benovoy et al. 2007). In the current climate of expansion in BCMI research the dissemination of ideas and collaboration between practitioners linking
BCMI research and related areas together is an opportunity to be embraced to further accelerate work in this field and should not be ignored.
It can still be argued that more meaning within EEG is needed, not only in
BCMI research but in our overall understanding of the brain. As we have seen,
meaning leads to control and in turn complexity, and advances in this offer exciting prospects. One area of research that promises to widen the scope of interpreting meaning in EEG is the study of emotional responses in brain activity and
evolving research in this field is already uncovering very direct links with emotional responses and music (Crowley et al. 2010) (Kirke and Miranda 2011).
The use of modern BCMI systems for performance in concert settings has
marked the arrival of more accessible, responsive and sophisticated platforms for
designing and building successful BCMI systems, bringing brainwave control and
music full circle. In place of Lucier’s percussive instruments are dynamic scores
and complex musical engines. And instead of bursts of alpha activity there are
layers of sophisticated EEG control on offer. The importance of considering mapping strategies in the development of BCMI systems can be traced all the way
back to Alvin Lucier and his Music for Solo Performer; an interface that offered
such a unique and tangible interaction with brainwaves, from such limited input.
With this, and the availability of today’s tools, in mind we hope to see a rise in the
creative applications of brainwaves in music coming from composers as well as
researchers through approaches applying the complexity in compositional and
mapping strategies that have now become a reality.

10.11 Questions
1. What were the first type of brainwaves used for musical control and how were
they controlled by a subject?
2. What is the difference between the sonification and the musification of
braiwave signals ?
3. What is the function of the Transformation Algorithm in a BCMI system?

33

4. With todays technology in mind consider an approach to modernising the mappings in Alvin Lucier's Music for Solo Performer. How could the piece bereworked?
5. What features of Event Related Potentials (ERPs) make them useful for mapping to musical functions?
6. Design a concept for a BCMI that uses two techniques of EEG extraction as input signal. Do the techniques you have chosen fit the concept well? Could other
techniques be used instead?
7. What are the benefits of a user-orientated BCMI over a computer-orientated
BCMI?
8. Compare the mappings of the two pieces Music for Sleeping and Waking Minds
and In Tune. How would each piece differ if the systems were swapped for both
performances?
9. What are the main differences between the P300 and SSVEP techniques and
how do they affect musical control?
10. Consider a musical extension of a BCMI. How could integrate BCMI into another type of musical interface of your choice?

10.12 References
Arslan B, Brouse A, Castet J, Le ́hembre R, Simon C, Filatriau J, Noirhomme Q A (2006) Real
time music synthesis environment driven with biological signals. In: IEEE International
Conference on Acoustics, Speech, and Signal Processing, Toulouse, France.
Babadi B, McKinney SM, Tarokh V, Ellenbogen JM (2012) DiBa: A data-driven bayesian
algorithm for sleep spindle detection. Biomedical Engineering 59 (2):483-493
Baier G, Hermann T, Stephani U (2007) Multi-channel sonification of human EEG. Paper
presented at the 13th International Conference on Auditory Display, Montréal, Canada, June
26 - 29
Benovoy M, Brouse A, Corcoran TG, Drayson H, Erkhurt C, Filiatriau J, Frisson C, Gundogdu
U, Knapp B, Lehembre R, Mu ̈hl C, Ortiz Pe ́rez MA, Sayin A, Soleymani M, Tahiroglu K
(2007) Audiovisual content generation controlled by physiological signals for clinical and
artistic applications. In: Enterface’07, Istanbul, Turkey.
Besson M, Fai'ta F (1995) An event-related potential (ERP) study of musical expectancy:
comparison of musicians with nonmusicians. Journal Of Experimental Psychology 21
(6):1278-1296. doi:http://dx.doi.org/10.1037/0096-1523.21.6.1278
Besson M, Macar F (1987) An event-related potential analysis of incongruity in music and other
non-linguistic
contexts.
Psychophysiology
24
(1):14-25.
doi:10.1111/j.14698986.1987.tb01853.x
Besson M, Schön D (2003) Comparison between language and music. In: Peretz IaZ, R.J. (ed)
The Cognitive Neuroscience of Music. Oxford University Press., New York,

34
Boulanger, R. (2000). The Csound Book. Cambridge, MA: The MIT Press.
Cahn BR, Polich J (2006) Meditation states and traits: EEG, ERP, and neuroimaging studies.
Psychological bulletin 132 (2):180-211. doi:10.1037/0033-2909.132.2.180
Cherry EC (1953) Some experiments on the recognition of speech with one and with two ears.
Journal of the Acoustical Society of America 25:975-979
Crowley K, Sliney A, Pitt I, Murphy D (2010) Evaluating a brain-computer interface to
categorise human emotional response. In: Advanced Learning Technologies (ICALT), IEEE
10th International Conference on, 5-7 July 2010. pp 276-278. doi:10.1109/ICALT.2010.81
Dean, T. L Wellman, M. P (2010) Planning and Control. Morgan Kaufmann Publishers. San
Mateo, US.
de Campo A, Höldrich R, Wallisch A, Eckel G (2007) New sonification tools for EEG data
screening and monitoring. In: 13th International Conference on Auditory Display, Montreal,
Canada.
Donchin E, Ritter W, McCallum WC (1978) Cognitive psychophysiology: The endogenous
components of the ERP. In: Callaway E, Tueting P, Koslow SH (eds) Event- Related Brain
Potentials in Man. Academic Press.
Duncan A (2001) EEG Pattern Classification for the Brain-computer music interface. Glasgow
University,
Eaton J (2011) The Warren. http://vimeo.com/25986554. Access 3 Jan 2014
Eaton J, Miranda E (2012) New approaches in brain-computer music interfacing: mapping EEG
for real-time musical control. In: Music, Mind, and Invention Workshop, New Jersey, USA.
Eaton J, Miranda, E (2013a) Real-time brainwave control for live performance. 10th
International Symposium on Computer Music Multidisciplinary Research (CMMR): Sound,
Music and Motion, Marseille, France.
Eaton J, Miranda, E (2013b) Real-time notation using brainwave control. In: Sound and Music
Computing Conference (SMC) 2013, Stockholm, Sweden.
Eaton M (1971) Bio-Music: Biological feedback, experiential music system. Orcus, Kansas City,
USA
Fitch WT, Kramer G, (1994) Sonifying the body electric: superiority of an auditory display over
a visual display in a complex, multivariate system. In: Kramer G (ed) Auditory Display –
Sonification, Audification, and Auditory Interfaces. Addison-Wesley, Reading, UK, p 307
Garnett G, Goudeseune C (1999) Performance factors in control of high-dimensional spaces. In:
International Computer Music Conference (ICMC 1999), Beijing, China. pp 268-271
Goudeseune C (2002) Interpolated Mappings for Musical Instruments. Organised Sound 7 (02).
doi:10.1017/s1355771802002029
Grierson M (2008) Composing With brainwaves: minimal trial P300 recognition as an indication
of subjective preference for the control of a musical instrument. In: ICMC, Belfast, Ireland.
Grierson M, Kiefer C, Yee-King M (2011) Progress report on the EAVI BCI toolkit for music:
Musical Applications of Algorithms for use with Consumer Brain Computer Interfaces. In:
ICMC, Huddersfield, UK
Guger C, Edlinger G, Krausz G (2011) Hardware/software components and applications of BCIs.
In: Fazel-Rezai R (ed) Recent Advances in Brain-Computer Interface Systems. Rjeka,
Croatia, pp 1-24
Hinterberger T, Baier G (2004) POSER: Parametric orchestral sonification of EEG in real-time
for the self-regulation of brain states. In: International Workshop on Interactive Sonification,
Bielefeld.
Hunt A, Wanderley MM (2003) Mapping performer parameters to synthesis engines. Organised
Sound 7 (02). doi:10.1017/s1355771802002030
Hunt A, Wanderley MM, Kirke R (2000) Towards a model for instrumental mapping in expert
musical interaction. In: International Computer Music Conference, San Francisco, USA.
International Computer Music Association
Jeong J (2002) Nonlinear dynamics of EEG in Alzheimer's disease. Drug Development Research
56 (2):57-66. doi:10.1002/ddr.10061

35
Kirke A, Miranda E (2011) Combining EEG frontal asymmetry studies with affective
algorithmic composition and expressive performance models. In: International Computer
Music Conference (ICMC 2011), Huddersfield, UK.
Knapp RB, Cook PR (2005) The integral music controller: introducing a direct emotional
interface to gestural control of sound synthesis. Paper presented at the International Computer
Music Conference, Barcelona, Spain
Knapp RB, Lusted H (1990) A bioelectric controller for computer music applications. Computer
Music Journal 14 (1):42-47
Lucier A (1976) Statement on: Music for solo performer (1971). In: Rosemboom D (ed)
Biofeedback and the Arts: Results of Early Experiments. Aesthetic Research Center of
Canada, Vancouver
Lusted H, Knapp RB (1996) Controlling computers with neural signals. Scientific American 275
(4):82–87
Miranda E, Brouse A (2005a) Interfacing the brain directly with musical systems: on developing
systems for making music with brain signals. Leonardo 38 (4):331-336
Miranda E, Brouse A (2005b) toward direct brain-computer musical interfaces. In: 2005
International Conference on New Interfaces for Musical Expression (NIME), Vancouver, BC,
Canada.
Miranda E, Sharman K, Kilborn K, Duncan A (2003) On harnessing the electroencephalogram
for the musical braincap. Computer Music Journal 27 (2):80 - 102
Miranda E, Stokes M (2004) On generating EEG for controlling musical systems.
Biomedizinische Technik 49 (1):75-76
Miranda ER (2001) Composing music with computers. Focal Press, Oxford, UK
Miranda ER (2006) Brain-Computer music interface for composition and performance.
International Journal on Disability and Human Development 5 (2):119-125
Miranda ER, Magee WL, Wilson JJ, Eaton J, Palaniappan R (2011) Brain-computer music
interfacing (BCMI): from basic research to the real world of special needs. Music and
Medicine 3 (3):134-140. doi:10.1177/1943862111399290
Miranda ER, Soucaret V (2008). Mix-it-yourself with a brain-computer music interface. In 7th
ICDVRAT with ArtAbilitation. Maia/Porto, Portugal.
Miranda ER, Wanderley MM (2006) New digital musical instruments: control and interaction
beyond the keyboard. Computer Music and Digital Audio Series. A-R Editions
Näätänen R (1990) The role of attention in auditory information processing as revealed by eventrelated potentials and other brain measures of cognitive function. Behavioral and Brain
Sciences 13 (2):201-288
Ojanen M, Suominen J, Kallio T, Lassfolk K (2007) Design Principles and User Interfaces of
Erkki Kurenniemi’s Electronic Musical Instruments of the 1960’s and 1970’s. In: New
Interfaces for Musical Expression (NIME07), New York, USA.
Ortiz Perez MA, Knapp RB (2009) Biotools: Introducing a Hardware and Software Toolkit for
fast implemetation of Biosignals for Musical Applications. Paper presented at the Computer
Music Modeling and Retrieval. Sense of Sounds: 4th International Symposium, CMMR
Copenhagen, Denmark
Ouzounian G (2012) The Biomuse trio in conversation - an interview with R. Benjamin Knapp
and Eric Lyon. http://cec.sonus.ca/econtact/14_2/ouzounian_biomuse.html. Accessed 3 Jan
2014.
Ouzounian G, Knapp B, Lyon E (2011) Music for sleeping and waking minds. Concert Notes.
Science Gallery Dublin and The League of Consciousness Providers, Dublin, Ireland
Ox J (2010) A 21st-Century pedagogical plan for artists: How should we be training artists for
today? Leonardo 43 (1):
Pratt RR, Abel HH, Skidmore J (1995) The effects of neurofeedback training with background
music on EEG patterns of ADD and ADHD children. International Journal of Arts Medicine
4 (1):24 - 31
Regan D (1989) Human brain electrophysiology: evoked potentials and evoked magnetic fields
science and medicine. New York; London: Elsevier

36
Rosemboom D (1990) Extended musical interface with the human nervous system. Leonardo
MonoGraph Series
Rosenboom D (1972) Method for producing sounds or light flashes with alpha brain waves for
artistic purposes. Leonardo, The MIT Press 5 (2):4
Teitelbaum R (1976) In Tune: some early experiments in biofeedback music (1966-74). In:
Rosemboom D (ed) Biofeedback and the Arts: Results of Early Experiments. Aesthetic
Research Centre of Canada Publications, Vancouver
Teitelbaum R (2006) Improvisation, computers and the unconscious mind. Contemporary Music
Review 25 (5-6):497-508. doi:10.1080/07494460600990026
Wolpaw JR, Birbaumer N (2006) Brain–computer interfaces for communication and control. In:
Selzer M, Clarke S, Cohen L, Duncan P, Gage F (eds) Textbook of Neural Repair and
Rehabilitation: Volume 1, Neural Repair and Plasticity, vol 1. Cambridge University Press,
UK, pp 602-614
Wolpaw JR, McFarland DJ, Neat GW, Forneris CA (1991) An EEG-based brain-Computer
interface for cursor control. Electroencephalography and Clinical Neurophysiology 78
(3):252-259
Wu D, Li C, Yin Y, Zhou C, Yao D (2010) Music composition from the brain signal:
representing the mental state by music. Computational intelligence and neuroscience:267671.
doi:10.1155/2010/267671
Wu D, Li CY, Yao DZ (2009) Scale-free music of the brain. PloS one 4 (6):e5915.

