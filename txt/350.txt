Journal of Bionic Engineering 13 (2016) 491–503

Remote Navigation of Turtle by Controlling Instinct Behavior
via Human Brain-computer Interface
Cheol-Hu Kim1, Bongjae Choi2, Dae-Gun Kim1, Serin Lee3, Sungho Jo2, Phill-Seung Lee1
1. Department of Mechanical Engineering, Korea Advanced Institute of Science and Technology (KAIST), 373-1 Guseong-dong,
Yuseong-gu, Daejeon 34141, Republic of Korea
2. School of Computing Science, Korea Advanced Institute of Science and Technology (KAIST), 373-1 Guseong-dong, Yuseong-gu,
Daejeon 34141, Republic of Korea
3. Institute for Infocomm Research, 1 Fusionopolis Way, #21-01 Connexis (South Tower) 138632, Singapore

Abstract
Brain-Computer Interface (BCI) techniques have advanced to a level where it is now eliminating the need for hand-based
activation. This paper presents a novel attempt to remotely control an animal’s behavior by human BCI using a hybrid of Event
Related Desynchronization (ERD) and Steady-State Visually Evoked Potential (SSVEP) BCI protocols. The turtle was chosen
as the target animal, and we developed a head-mounted display, wireless communication, and a specially designed stimulation
device for the turtle. These devices could evoke the turtle’s instinctive escape behavior to guide its moving path, and turtles were
remotely controlled in both indoor and outdoor environments. The system architecture and design were presented. To demonstrate the feasibility of the system, experimental tests were performed under various conditions. Our system could act as a
framework for future human-animal interaction systems.
Keywords: brain-computer interface, turtle (Trachemys scripta elegans), remote navigation, instinct behaviour, escape behavior
Copyright © 2016, Jilin University. Published by Elsevier Limited and Science Press. All rights reserved.
doi: 10.1016/S1672-6529(16)60322-0

1 Introduction
A repeated theme in fiction involves people imagining themselves in the body of another human or that of
an animal. For example, the premise of the movie
“Avatar” was that a human can exist in another body,
with that body controlled by a remotely connected mind.
Of course, we cannot expect to realize the technology
described in the movie in the near future. However,
recent advances in electronics and computer technology
have allowed researchers to approach this appealing
topic. A novel technique for interfacing between humans
and machines, based on human thought or neural responses, has been developed. This development is called
a “Brain-Computer Interface” (BCI). Using this technique, it is possible to read human thought and use that
ability to control machines. Previous BCI studies have
successfully controlled a humanoid robot[1–5]. Rao et al.
Corresponding author: Sungho Jo, Phill-Seung Lee
E-mail: shjo@kaist.ac.kr, phillseung@kaist.edu
Cheol-Hu Kim and Bongjae Choi contributed equally to the work

demonstrated the possibility of sending information
extracted from one brain directly to another brain
through direct brain-to-brain communication[6]. Yoo et
al. created a “Brain-to-Brain Interface” (BBI) system
that combines a BCI with a “Computer-to-Brain Interface” (CBI) that could be used to establish a functional
link between the brains of different species (i.e. humans
and Sprague-Dawley rats)[7].
On the other hand, there have been several attempts
to control animals by stimulation in order to draw on
their high levels of locomotion and energy efficiency. In
general, animals exhibit superior locomotion and survival abilities as a result of their adapting to the environment over millions of years. Therefore, their bodies
are optimized in terms of locomotion and energy efficiency.
Some researchers have tried to control animal
movement by applying invasive control methods. Daly

492

Journal of Bionic Engineering (2016) Vol.13 No.3

et al. designed a wireless flight control system for moths
that consisted of a 3 GHz to 5 GHz non-coherent pulsed
ultra-wideband receiver system-on-chip[8]. Sato and
Mahabiz proposed a beetle flight control system which
provided electrical stimuli to the beetle’s wing muscles[9]. Tsang et al. suggested the possibility of the remote flight control of a moth by using micro-fabricated
Flexible Neuroprosthetic Probes (FNPs)[10]. Sun et al.
proposed the automatic navigation of rat-robots using
the General Regression Neural Network (GRNN)
method[11]. Sanchez et al. designed hybrid cockroach
robots which applied electrical stimuli to the prothoracic
ganglia via a remotely operated backpack system and
implanted electrodes[12].
There have also been studies for controlling an
animal’s movement through non-invasive control
methods. Holzer and Shimoyama proposed a bio-robot
system for controlling an insect (Periplaneta Americana)
with electrical stimuli[13]. Butler et al. suggested a virtual
fence system for containing cattle that used sound stimuli[14]. Britt et al. were able to navigate a well-trained
dog using commands provided through wireless devices[15]. Lee et al. succeeded in controlling an untrained
turtle’s walking paths by inducing obstacle-avoidance
behavior[16]. Pi et al. proposed a non-invasive remote
control system for rat-robot via ultrasonic, epidermal
and LED photic stimulators[17].
Using the technologies mentioned above, it is possible to develop a system to control an animal’s behavior
using human BCI technology. To realize this, however,
the system architecture and interfacing techniques require further development. In this paper, we propose a
conceptual system that is capable of remotely guiding an
animal’s moving path by controlling its instinctive behavior (e.g. escape behavior) using a simple stimulation
device controlled by a human’s brain signals. As the
target animal, the turtle was chosen because it has good
cognitive abilities, is capable of distinguishing the
wavelength of visible light[18]. It is known that turtles
recognize a white light source as an open space and so
move toward it[19,20]. Also, turtles show specific avoidance behavior patterns by external visual obstruction[16].
Further, it has a hard shell on which devices can be
mounted. Also, our objective was to invoke instinctive
behavior, specifically, the escape behavior that induces
the operant responses that cause the animal to move
away from an ongoing punishing or obstructing stimulus.

In particular, this reactive behavior is connected to those
instincts which protect the body and which must be
evoked and directed in a consistent manner by a stimulus[21–23]. In our previous research, this instinctive behavior was utilized to control the turtle’s path. As a
result, coherent patterns in the turtle’s trajectory were
observed[16].
In our concept system, a Head-Mounted Display
(HMD) is adopted as the user interface. The combination
of the wearable BCI and HMD enables users to become
more immersed in the control of the turtle. The human
operator wears the integrated BCI-HMD system, while
the turtle is equipped with devices for stimulation,
wireless communication, and imaging. Based on the
images acquired from the cyborg turtle, the human uses
thought to command the turtle. These thought commands are recognized by the wearable BCI system.
Using Wi-Fi, these commands are transmitted to a
stimulation device attached to the turtle’s upper shell.
Then, the turtle is induced to move by the stimulation
device that invokes the turtle’s instinctive behavior.
Finally, the human acquires updated visual feedback
from the camera mounted on the turtle’s upper shell. In
this way, the human can remotely navigate the turtle’s
trajectory.
To check our system’s operability and applicability,
three tests were conducted in both indoor and outdoor
environments. An indoor test was performed to confirm
the responsiveness of the stimulation device and to
check the basic operability of the cyborg system. Outdoor tests were also performed to check the availability
and applicability of the system under real-field conditions. All of the tests were successfully implemented and
the results were found to point to the usefulness of the
concept system for extended applications in a real environment.

2 System
2.1 System architecture
The principal objective of the proposed system is to
provide a control feedback loop for remotely guiding a
turtle by means of human thought alone. To close the
loop, the human operator is provided with visual information (such as a real-time video stream) from the cyborg turtle that he or she is controlling.
Fig. 1 illustrates the architecture of our proposed
system. The overall system consists of two main

Kim et al. Remote Navigation of Turtle by Controlling Instinct Behavior via Human Brain-computer Interface

493

Fig. 1 Architecture of the remote navigation system for turtle with human BCI.

subsystems: the pilot and cyborg turtle. The pilot part
consists of a BCI device, a HMD, and a human subject.
The cyborg turtle part consists of a stimulation device
with telecommunication services, a video recording
system, and the animal subject (turtle).
The overall procedure was as follows. In the first
instance, the human and the turtle are fitted with their
respective devices (BCI device, HMD, and stimulation
device). Then, through the HMD, the human views the
image being captured by the camera attached to the
cyborg turtle. Based on this visual information, the human provides electroencephalography (EEG) signal
orders to the BCI system. Using Wi-Fi communication,
the BCI system passes the commands to the stimulation
device to control the turtle’s moving path by inducing its
escape behavior in response to the human EEG signals.
As the turtle is responding to the stimulation device, the
attached camera records the turtle’s field of view and
sends the captured images back to the human’s HMD in
real-time. By viewing this visual feedback, the human

operator understands the progress of the turtle’s motion
and then issues BCI commands again. This procedure is
repeated until the turtle arrives at the desired position or
completes the assignment.
In particular, instead of a direct connection between
the human and the animal brains or nerves (e.g. BBI),
our animal control system relies on the animal’s instincts,
namely, its escape behavior. Through the use of this
scheme, our system offers advantages in terms of
adaptability and usability in comparison with a direct
connection due to its simple and non-invasive devices.
2.2 BCI System
An EEG-based BCI system was used for the
guidance of the turtle by human thought. EEG signals
have been studied because they have several practical
advantages over other brain signal modalities[24,25].
Preconditions for the practical usage of BCI system are
inexpensiveness, compactness and usability of the acquisition devices. Invasive brain signals and several

494

Journal of Bionic Engineering (2016) Vol.13 No.3

non-invasive brain signals, such as magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI), do not satisfy these conditions. Compared
with functional near-infrared spectroscopy (fNIRS),
which offers an inexpensive, portable and practical alternative to fMRI, an EEG has a much better temporal
resolution, but the spatial resolution is poor. EEG signals
are generally more suitable for controlling tasks using
BCI due to higher temporal resolution and rapid electrical response to neuronal activity.
We adopt the hybrid Event Related Desynchronization (ERD) and Steady-State Visually Evoked Potential (SSVEP) based BCI system which successfully
demonstrated the humanoid robot navigation using human though[3]. It can discriminate three mental states;
<left>, <right>, and <ERD> with an idle state. The BCI
system determines a command every 250 ms. We introduced a control algorithm to guide the path of the
turtle using these three commands.
The reactive SSVEP-based BCI is based on brain
responses to visual stimulation at specific frequencies.
The <left> and <right> commands indicate that the
brainwaves acquired from the visual cortex are synchronized with the left and right SSVEP flickering
stimuli, respectively. The <left> and <right> commands
are used to turn the black semi-cylinder with a slit (the
turtle stimulation device) on the turtle in the selected

direction by 12 degrees per decision. The maximum
range through which the semi-cylinder can be moved is
±36˚.
Since our approach relies on the animal’s instinct
behavior, the human subjects do not need to command to
the turtle continuously during navigation. The stimulation device on the turtle induces its instinct behavior
consistently until another command is received. In other
words, the human subjects do not need to provide ongoing BCI commands, which would be annoying and
fatiguing for the users[26]. Therefore, we needed a means
of activating the visual stimuli only when users needed it.
To solve this problem, we introduced the brain switch
approach of a hybrid BCI system. The hybrid BCI utilizes a combination of two or more BCIs to take advantage of the benefits of each protocol. A typical hybrid
BCI system is used to improve the accuracy of classification and distinguish more mental states[27]. Another
approach to hybridization, called brain switching, is to
turn off a BCI when the user does not intent to communicate[26]. It reduces the false positive rate of the BCI
system and minimizes the fatigue of the user from the
stimuli of reactive BCI.
In this study, an ERD-based BCI was used to control the stimuli of an SSVEP-based BCI. The state transition diagram of the ERD-based brain switch to turn
on/off the SSVEP BCI is shown in Fig. 2a. In this case,

Fig. 2 Depiction of the BCI system. (a) Proposed control algorithm for controlling the cyborg turtle; (b) the human pilot remotely controls
the cyborg turtle through the BCI and HMD. The user interface is displayed on the HMD and consists of a flickering checker board,
direction arrow, and video player.

Kim et al. Remote Navigation of Turtle by Controlling Instinct Behavior via Human Brain-computer Interface

<ERD> serves as a brain switch. The <ERD> command
is issued when the BCI system detects specific motor
imagery from the subject. Whenever the ERD-based
BCI translates the <ERD> commands, the state of the
BCI system is changed. If the visual stimuli are flickering, the system turns them off and then resets the angle
of the slit of semi-cylinder. Otherwise, the system turns
on the visual stimuli for SSVEP-based BCI. The SSVEP
flickering stimuli are initially turned off. This state
transition paradigm allows the subject to take his or her
attention off the system. As such, it can increase the
usability of the system and minimize the fatigue of the
human subject.
In our experiment, the subject sat comfortably and
wore the HMD and EEG acquisition device. The subject
obtained visual feedback from the environment surrounding the cyborg turtle. The real-time video stream
from the camera attached to the turtle was displayed in
the HMD at 15 fps. The video stream used the RealTime Streaming Protocol (RTSP) and thus incurred a
very slight delay of 0.5 s to 1.0 s, depending on the
quality of the Wi-Fi signal. To provide visual feedback,
the commands translated by the BCI system, as well as
the current angle of the semi-cylinder, were also displayed. Fig. 2b illustrates how the subject remotely
controlled the cyborg turtle through the BCI system
described above.

3 Materials and methods
3.1 Subjects
3.1.1 BCI subjects
Five healthy male subjects (age 29 ± 3 years) voluntarily participated in our experiment. All of the subjects were of the same gender (male), were of the same
laterality (right-handed), and were free of neurological
diseases. They provided their written informed consent.
The BCI experiments were approved by the Korea
Advanced Institute of Science and Technology (KAIST)
Institutional Review Board (Permit Number:
KH2014-08) and our personal experiment qualification
certifications are: Bongjae Choi (K-2014-12526414),
and Sungho Jo (K-2012-9135188).
3.1.2 Animal subjects
The turtles used in this study were red-eared slider
(Trachemys scripta elegans). Four turtles were grown
indoors in the laboratory at KAIST to a size of 15 cm to

495

20 cm. They were housed in a glass tank (91 cm × 61 cm
× 20 cm) with oxygenated freshwater with a recycling
system and a dry platform for basking. The water temperature was maintained at 20˚ to 25˚ Celsius. The turtles were provided with UV light for basking for 6 h to
7 h per day, and fed commercial pellets four times a
week.
The animal experiments were approved by the
KAIST Institutional Animal Care & Use Committee
Board (Permit Number: KA2014-26) and the personal
certification numbers are: Cheol-Hu Kim (2010-OE01),
Dae-Gun
Kim
(2011-OE01),
Bongjae
Choi
(2014-CS03), Sungho Jo (2014-CS01) and Phill-Seung
Lee (2014-OS01). Our target animals (turtle: Trachemys
scripta elegans) were manipulated in strict accordance
with the KAIST Animal Experiment Ethical Law
RR0303 (revised 24/07/2013) and all efforts were made
to minimize the suffering.
3.2 Apparatus
3.2.1 Human
EEGs were recorded using a wearable EEG acquisition device (Epoc neuroheadset, Emotiv Inc., USA)[28].
This is a consumer-level EEG acquisition wireless
headset which can acquire brain signals at a sampling
frequency of 128 Hz through 14 channels, namely, AF3,
AF4, F3, F4, F7, F8, FC5, FC6, P7, P8, T7, T8, O1 and
O2, which are designated according to the 10-20 system.
The headset’s ease of use, portability, and simplicity of
operation made it very attractive for application to this
study. In a previous study[3], we proposed a successful
SSVEP/ERD hybrid BCI system with which we navigated a humanoid robot using this device.
We also adopted an HMD system (MyBud, Accupix Co., Ltd., Korea)[29]. It consists of an 852 × 480
(WVGA) liquid crystal on silicon (LCoS) display in
front of each eye. It has a refresh rate of 60 Hz, a separation distance of 20 to 30 mm, and weighs 78 g. The
Field Of View (FOV) is 35 degrees diagonally. The
HMD display was thus perceived as a 100 inch screen at
a distance of 4 m from the subject. This was used to
provide the subject with a more realistic view of the
environment during navigation. Faller et al.[30,31] reported that SSVEP-based BCI can be successfully implemented in a virtual environment when combined with
an HMD. Fig. 2b shows the interface devices used by
human subjects during the experiment. The subjects

Journal of Bionic Engineering (2016) Vol.13 No.3

496

communicated with the turtle through the EEG acquisition device and the HMD.
All procedures performed in studies involving
human participants were in accordance with the ethical
standards of the institutional research committee and
with the 1964 Helsinki declaration and its later
amendments or comparable ethical standards.
3.2.2 Turtle
We designed a simple stimulation device to induce
the turtle’s escape behavior. It is well known that animals move away from an external obstruction by instinctive escape behavior[22,23]. Through our previous
research, we observed the turtle’s specific behavior
pattern, namely, that it recognizes a black object as an
obstacle and turns toward open space[16]. Based on these
findings, we designed a stimulation device for the turtle.
The stimulation device and embedded control module
(8.6 cm × 5.4 cm × 5.5 cm, 171.5 g) was mounted on the
turtle’s upper shell. It consisted of a servo-motor and a
black semi-cylinder with a slit to restrict the turtle’s view
(Fig. 3). By adjusting the orientation of the slit, we could
guide the turtle’s moving path.
The embedded control module was based on the
Raspberry Pi single-board computer with a Broadcom
BCM2835 system on a chip (SoC), a Video Core IV
GPU, 512 MB of RAM, and a 16 GB SD card. This

Compact camera
Servo motor
Wi-Fi transceiver
Control module
(Raspberry Pi)
Battery

Black semi-cylinder
Color tracking marker

Fig. 3 Depiction of cyborg system. The embedded control system
used to induce the turtle’s escape behavior is shown in the drawing. The device consists of a main computer (Raspberry Pi), servo
motor, battery, Wi-Fi transceiver, compact color camera, and
semi-cylinder with a slit. The servo motor controls the positioning
of the slit in the semi-cylinder (in the image, it is positioned directly in front of the turtle). The blue circle on the controller was
tracked by a simple tracking algorithm and was regarded as indicating the location of the turtle.

embedded module was connected to a servo motor
which moved the stimulation semi-cylinder, as well as a
2600 mAh Li-Po battery. Altogether, the device weighed
171.5 g, with the embedded control module weighing
85 g and the battery 86.5 g.
The controller unit received an angular value to
control the servo motor, thus rotating the black
semi-cylinder with the slit through ± 36˚ with respect to
the turtle’s body axis, from the PC control software via a
Wi-Fi connection. The user sent <left>, <right> and
<ERD> commands remotely through the BCI sensor on
the human BCI headset. The embedded control module
on the turtle’s upper shell demodulated the signal and
then passed it to the servo motor. The turtle’s field of
view was recorded using a compact color camera (2592
× 1944 pixels) mounted on the turtle’s upper shell. The
captured video stream data was returned to the human’s
HMD, again through the Wi-Fi connection. All of the
devices attached to the turtle were waterproofed to allow
their application to the outdoor field tests.
3.3 Experimental setup
3.3.1 EEG BCI training
To translate the human subject’s thoughts to
commands, the following procedures were performed to
build a training dataset for the SSVEP/ERD hybrid BCI
system.
First, we set two flickering stimulus frequencies,
each corresponding to either the left or right commands
for the SSVEP-based BCI protocol. The stimulus frequencies were selected from 6.67 Hz, 7.5 Hz, 8.57 Hz,
10 Hz, 12 Hz, 15 Hz, or 20 Hz because of the characteristics of the acquisition device and the LCD display.
These frequencies were determined by empirical
pre-tests for each subject. In this study, checkerboard
visual stimuli were used to evoke the SSVEP. The subjects were asked to look at each visual stimulus for 5 s.
These trials were repeated a total of 10 times. Then, a
dataset for each 2 s time window with 250 ms increments was obtained. SSVEP features based on Canonical Correlation Analysis (CCA)[32] were used to train a
linear Support Vector Machine (SVM) classifier.
For the ERD-based BCI protocol, EEG signals
were recorded while each subject remained at rest for 5 s
and imagined a specific motor imagery for 5 s. Each
subject selected their own motor imagery. Each subject
repeated this 10 times. Then, again, a sliding window of

Kim et al. Remote Navigation of Turtle by Controlling Instinct Behavior via Human Brain-computer Interface

497

Table 1 Cross-validation accuracy results and the Information Transfer Rates (ITR) for each protocol
Subject

A

B

C

ERD cross-validation accuracy (%)

93.3

94.2

85.0

ERD ITR (bits per min)

19.3

20.4

11.7

SSVEP cross-validation accuracy (%)

90.4

80.8

94.2

91.2

92.7

89.9 (± 5.3)

SSVEP ITR (bits per min)

16.3

8.8

20.5

17.1

18.7

16.3 (± 4.5)

Flickering stimuli frequencies (Left, Right) (Hz)

10, 12

10, 12

15, 20

12,15

10, 12

2 s with 250 ms increments was obtained. The Common
Spatial Pattern (CSP) algorithm[33] was used to extract
the features needed to train a linear SVM classifier.
A tenfold cross-validation was assessed to evaluate
the classification performance. Table 1 summarizes the
cross-validation accuracy results and the Information
Transfer Rates (ITR) for each protocol.
The ERD-based and SSVEP-based BCI protocols
achieved an overall accuracy of 91.2% and 89.9%, respectively. The ITRs of the ERD-based and SSVEPbased BCI protocols were 17.5 and 16.3 bits per min,
respectively. The worst performer in terms of the ERD
cross-validation accuracy was subject C who achieved
85.0%. With SSVEP, subject B produced the worst result of 80.8%. After confirming the classifiers for the
SSVEP- and ERD-based protocols, the hybrid classifier
for the SSVEP- and ERD-based protocols was built as
described in Ref. [3]. The average accuracy for the five
subjects was 77.1 (± 3.2) %. The worst performer was
subject A, whose accuracy was 75.2%, while subject D
achieved the highest accuracy of 82.3%.
3.3.2 Indoor test
This test was implemented on the floor of the laboratory (Fig. 4a). We placed four waypoints (white, red,
yellow, and black) at each corner of the test area. The
turtle’s responses, that is, its navigational paths were
continuously recorded by a simple color-based tracker.
To ensure that the turtles would only be affected by our
stimulus, other possible stimuli (olfactory and auditory
stimuli, room temperature, brightness, etc.) were all
controlled during the tests.
Each turtle’s path was tracked by an experimental
camera and a color-based tracker based on a MATLAB
(The Mathworks Inc., USA) image processing program
developed by Matpic. During the experiments, a Kalan
filter with linear models was used to describe the turtle’s
trajectory.

D

E

Overall (± Std)

95.4

88.3

91.2 (± 4.4)

21.9

14.4

17.5 (± 4.3)

3.3.3 Outdoor test
This test was performed in a natural environment
that was 5 km distant from the human pilot. Because this
test was done outdoors, it was not possible to control the
stimuli factors described for the indoor trial during the
test. As shown in Fig. 4b, the start/end position and
artificial obstacles were set on an uneven lawn. Again,
the tests were recorded using the color-based tracker.
3.3.4 Field test
This test was implemented in a natural field with a
range of geomorphological conditions. In this test, we
assigned the cyborg turtle a mission in more demanding

(a)

(b)

Fig. 4 Experimental setups. (a) An indoor test was performed on
the laboratory floor (of the dimensions indicated), as shown in the
drawing. The placement of the cyborg turtle, waypoints 1 to 4, and
the tracking system (camera) are shown; (b) an outdoor test was
performed on a lawn (of the dimensions indicated), as shown in
the drawing. The placements of the cyborg turtle, start/end position, and artificial obstacles are shown. Note particularly that this
area was 5 km distant from the pilot, to test the teleoperation
performance.

Journal of Bionic Engineering (2016) Vol.13 No.3

outdoor real-field conditions. Three mission points were
set between the start and end positions. We placed a
mission card at each mission point, in alphabetical order.
The intention was for the cyborg turtle to capture an
image of the printed letter at each mission point using
the compact color camera mounted on its upper shell.
During the test, we also recorded the turtle’s path using
color tracking.

4 Results
4.1 Indoor test
In this test, we verified how the turtles respond to
stimulation device and checked the operability in greater
detail. Fig. 3 illustrates the overall design of the cyborg
turtle. The simple device was designed to position the
black semi-cylinder (radius = 22 cm, height = 10 cm) at
any specific angle in the turtle’s line of sight. The slit at
the centre of the circumference can thus be varied from
+36˚ to −36˚ in the clockwise direction, relative to the
anteroposterior axis of the turtle. Since the turtle shows
little response to light emanating from ±180˚[18], it
moves towards the slit.
In Fig. 4a, the cyborg turtle moved within a 2.5 m ×
1.5 m area in which there were four waypoints and an
8.83 m optimal path which is a straight line between the
waypoints. The turtle passed through the four waypoints
in order and then came back to the first waypoint. The
pilot was able to guide the turtle to approach each
waypoint with an accuracy of about 15 cm, based on the
visual feedback information. Each experiment was performed for 5 to 10 minutes and then repeated five times
per person.
As shown in Fig. 5, all of the subjects attained
successful navigation trajectories, passing through all of
the waypoints without any omissions. During the experiment, there were several cases where the turtle
would not move from the start point due to fatigue.
These cases were excluded from consideration, and we
allowed the subject turtle to rest, replacing it with another. Also, for such experiments, we calculated the
average travel time, travel distance, speed, and
Cross-Track Error (CTE, the minimum distance between
the optimal path and the actual position) of the turtle
from each trajectory to check the operability (Table 2).
The average travel time and distance were found to
be 538.4 s and 907.5 cm, respectively. The average
speed of the cyborg turtle was 1.84 cm·s−1. The average

CTE over the five subjects was 24.45 cm. This value
means that an average error of 24 cm was incurred between the cyborg turtle and the optimal track position.
The worst performer was subject B whose CTE was
28.37 cm while subject D (who achieved the highest
accuracy in the EEG BCI training) achieved the lowest
CTE of 18.41 cm. The difference between the two was
9.96 cm (relative error: 35.1%). Also, a comparison
between the speed of an unstimulated turtle (2.53 cm·s−1)
and our average speed (1.84 cm·s−1), revealed a difference of only 0.69 cm·s−1 (relative error: 27.3%) between
them.
4.2 Outdoor test
This test was designed to check the availability of
our system when faced with outdoor conditions. In addition, to test the teleoperation performance, the test area
was set up 5 km away from the pilot. Fig. 4b illustrates
the outdoor test area. The straight line distance between
the start and end positions was 12 m. In Fig. 6, the cyborg turtles successfully reached the desired location by
following an S-shaped curve in spite of the changing
environment and telecommunication condition. The
average travel time and speed of the turtles were measured and found to be 430.4 s and 1.80 cm·s−1, respectively. These figures were very similar to those attained
in the indoor test (1.84 cm·s−1).

y (m)

498

Fig. 5 Controlled trajectories of the cyborg turtles with each
human pilot. The cyborg turtles were remotely controlled to move
between waypoints through the alternate provision of stimuli that
invoked the escape behavior (see text). The optimal (blue) and
actual (red) paths of the turtles are plotted. Each test was repeated
five times per person, although the turtle subjects were changed.
Despite the changes in the human and turtle subjects, each red
path passes through all of the waypoints without any omission.

Kim et al. Remote Navigation of Turtle by Controlling Instinct Behavior via Human Brain-computer Interface
4

499

2
Moving path

3

1

Start

Optimal path
End # Way point

Human A

Human B

Human C

Human D

Human E

Turtle 1

Turtle 2

Turtle 3

Turtle 4

Fig. 6 Results of the outdoor test. Trajectories of the cyborg turtles in outdoor test. The experiment was performed on an uneven lawn. The
paths followed that of the pilot’s intention.
Table 2 Results of the indoor test
Subject

A

B

C

D

E

Total

Average travel time (s)
(± Std)

497.2
(± 189.2)

758.6
(± 141.3)

531.6
(± 69.2)

425.2
(± 64.5)

479.4
(± 83.3)

538.4
(± 166.2)

Average travel distance (cm)
(± Std)

911.1
(± 3.0)

911.5
(± 14.5)

908.4
(± 10.4)

901.5
(± 10.4)

905.3
(± 9.6)

907.5
(± 10.9)

Average speed (cm·s−1)
(± Std)

2.07
(± 0.63)

1.27
(± 0.34)

1.74
(± 0.26)

2.17
(± 0.35)

1.95
(± 0.38)

1.84
(± 0.52)

Average CTE (cm)
(± Std)

27.99
(± 3.00)

28.37
(± 14.52)

25.32
(± 10.36)

18.41
(± 10.35)

22.17
(± 9.59)

24.45
(± 10.92)

Optimal travel distance = 883.10 cm
Average speed of comparison group (unstimulated turtle) = 2.53 (± 0.42) cm·s−1

Journal of Bionic Engineering (2016) Vol.13 No.3

500

During the tests, we sometimes remotely waved the
black semi-cylinder to encourage an immobile turtle to
move. Also, during early trials with the system, there
were several cases where the equipment failed. These
failures were typically caused by a Wi-Fi communications problem or the battery becoming dislodged. If the
equipment failure interfered with the turtle, that trial was
excluded from consideration.
4.3 Field test
To examine the applicability of the proposed system, we operated it in an actual field. Fig. 7a shows the
conditions presented by the experimental field and the
path followed by the turtle. The turtle covered a 40 m
route that presented various geomorphological conditions (gravelly field, soil, lawn-like surfaces, shallow-water hazards, etc.) and natural obstacles. As shown
in Fig. 7b, despite the relatively rugged geomor-

phological conditions, the cyborg turtle was able to carry
out the assigned mission and successfully captured images at three mission points. The total travel time was
2436 s and the average speed was 1.64 cm·s−1. There was
a 0.16 cm·s−1 (relative error: 8.89%) drop in speed relative to that attained in outdoor test (1.80 cm·s−1) and
0.2 cm·s−1 (relative error: 10.9%) relative to the indoor
test (1.84 cm·s−1). If we look at the zonal speeds, the
turtle achieved 1.34 cm·s−1 in the gravelly field,
1.42 cm·s−1 over soil, 1.81 cm·s−1 on the lawn, and
1.49 cm·s−1 in the shallow water hazard.

5 Discussion
We performed three kinds of test to verify the remote navigation system for a turtle with a human BCI
controller. An indoor test was done to check the operability of our system (Fig. 5 and Table 2). In particular,
the calculated CTE of navigation paths was only 2.77%
Moving path
Mission
point
Shallow water
hazard

Soil
ground
Natural
obstacle

C
GOAL

START

B
Tree

Gravelly
field
Lawn

(a)

0

20
x (m)

40

(b)

Fig. 7 Results of the field test. (a) The cyborg turtle’s trajectories when traversing a range of geomorphological conditions (gravelly field,
soil/dirt field, lawn, treed and shallow water hazard) and natural obstacles; (b) the letters were recorded by the cyborg turtle’s camera at
each mission point. Also, the flickering checker board for the SSVEP-based BCIs is located at the bottom of the screen. The <left> and
<right> commands indicate that the brainwaves acquired from the pilot’s visual cortex are synchronized with the left and right SSVEP
flickering stimuli, respectively, provided by these checker boards.

Kim et al. Remote Navigation of Turtle by Controlling Instinct Behavior via Human Brain-computer Interface

in the 8.83 m track. Then, two outdoor tests were implemented to show the applicability under more complicated natural environment conditions and performed
through long-distance (5 km) wireless real-time remote
control (Figs. 6 and 7). The results of these three tests
showed that our turtle navigation system can be operated
successfully under a wide range of environmental conditions. In particular, the field test showed that our
animal guiding scheme is still valid in outdoor conditions, and it demonstrated the feasibility of navigation
applications, such as mobile robots.
To obtain higher operability of the proposed system,
further experiments for augmenting the accuracy of the
guidance systems can be performed. The major trajectory errors come both from fatigue and external stimuli
of the turtles, and the misclassification of the BCI system. In order to investigate these errors, additional
cross-check methodologies might be necessary to distinguish which factors are more influential in the operability. For an example, the guidance through manual
controls, such as keyboard inputs, might be able to resolve the false-positives from the BCI controller. Also, it
is possible to perform the experiment with a mobile
robot, removing unexpected behaviors by turtles’ instincts. Such cross-check methods are able to determine
the source of the error in the current system.
Our proposed system constitutes an innovative approach to constructing a human-animal interaction system. Through a combination of simple BCI protocols,
we provide orders to control the subject turtle by means
of human thought alone. In BCI research as well as the
animal control research field, this is a very meaningful
result. Firstly, by inducing instinctive escape behavior,
our system can control the movement of a living animal
and perform a particular mission while minimizing the
danger to the animal. That is, a non-invasive method was
used. A disadvantage is that relatively large devices are
required compared to small implanted devices used in
invasive methods[8–12]. Secondly, this research widens
the range of application of BCI through the success of
controlling animal behavior using human BCI techniques. Thirdly, our wearable devices are more accessible than existing BCIs or animal control devices. Finally,
unlike previous attempts including our previous
study[8–17], we evaluated the applicability of our system
not only in an indoor but also in outdoor conditions.
In the future, with the development of BCI tech-

501

nology and an enhanced HMD system, our system can
be further improved in terms of its adaptability and usability. Moreover, we expect that more effective animal
control systems will integrate a positioning system and
improved Augmented Reality (AR)/Virtual Reality (VR)
techniques. Therefore, we could apply this animal control framework to other animals (such as rats, pigeons,
etc.) with more research into their behavior. Our system
allowed us to attain a wider range of experience and
information from different species of controlled animals.
In future work, we plan to study the behavior of other
animals in more detail and then apply our framework to
them.
Meanwhile, from an application viewpoint, this
system could be used in exploration or navigation applications like robotic probes. Through a connection
with animals which live in various environments (e.g.
underwater or in hazardous areas), a user could acquire
valuable visual information by using controlled animals.
Also, this concept system could have military applications such as reconnaissance and surveillance. In the
BCI area, this system could be used in unconventional
applications such as immersive virtual reality systems
that give the user a sense of oneness with the controlled
animal, as if it were their surrogate agent.

6 Conclusion
In this paper, we proposed an animal remote navigation system using a human BCI. We selected the turtle
as the first target animal and developed the human-turtle
interaction system. Using the turtle’s escape behavior
pattern and an ERD-/SSVEP-based BCI system, we
could guide the turtle’s moving path according to the
human brain signal. To demonstrate the feasibility of the
proposed system, three kinds of experiments (indoor,
outdoor and field tests) were implemented. The results
showed that the proposed system could be operated well
in real-time conditions, and the animal guiding scheme
can be used in outdoor applications. This study was the
first attempt to remotely guide the moving path of animals through a human BCI controller. In the future, our
system could be developed with a positioning system,
AR/VR techniques, and enhanced BCI technologies. We
expect that our technology will inspire the development
of an innovative framework for human-animal interaction systems.

502

Journal of Bionic Engineering (2016) Vol.13 No.3

Acknowledgment
This work was supported by the Ministry of Education under Basic Science Research Program through
the National Research Foundation of Korea (NRF)
funded by the Ministry of Science, ICT & Future Planning (No. 2013R1A1A2009378) and the Human Resources Development program (No. 20134030200300)
of the Korea Institute of Energy Technology Evaluation
and Planning (KETEP) grant funded by the Korea government Ministry of Trade, Industry and Energy. The
funders had no role in the study design, data collection
and analysis, decision to publish, or preparation of the
manuscript.

References
[1]

[11] Sun C, Zheng N G, Zhang X L, Chen W D, Zheng X X.
Automatic navigation for rat-robots with modeling of the
human guidance. Journal of Bionic Engineering, 2013, 10,
46–56.
[12] Sanchez C J, Chiu C W, Zhou Y, González J M, Vinson S B,
Liang H. Locomotion control of hybrid cockroach robots.
Journal of the Royal Society Interface, 2015, 12, 20141363.
[13] Holzer R, Shimoyama I. Locomotion control of a bio-robotic

and Systems, Grenoble, France, 1997, 1514–1519.
mals: Virtual fences for controlling cattle. The International

oid robot control. 2011 11th IEEE-RAS International Con-

Journal of Robotics Research, 2006, 25, 485–508.
[15] Britt W R, Miller J, Waggoner P, Bevly D M, Hamilton Jr J

2011, 199–204.

A. An embedded system for real-time navigation and remote

Choi B, Jo S. A low-cost EEG system-based hybrid

command of a trained canine. Personal and Ubiquitous

brain-computer interface for humanoid robot navigation and

Computing, 2011, 15, 61–74.

Cohen O, Druon S, Lengagne S, Mendelsohn A, Malach R,

[16] Lee S, Kim C H, Kim D G, Kim H G, Lee P S, Myung H.
Remote guidance of untrained turtles by controlling voluntary instinct behavior. PLoS ONE, 2013, 8, 0061798.

study. 2012 4th IEEE RAS & EMBS International Confer-

[17] Pi X, Li S, Xu L, Liu H, Zhou S, Wei K, Wanga Z, Zheng X,

ence on Biomedical Robotics and Biomechatronics (BioRob).

Wen Z, Jia Z. A preliminary study of the noninvasive remote

Rome, Italy, 2012, 314–319.

control system for rat bio-robot. Journal of Bionic Engi-

Honda Research Institute Japan Co., Ltd. (HRI-JP),

neering, 2010, 7, 375–381.
[18] Verheijen F J, Wildschut J T. The photic orientation of

http://world.honda.com/news/2009/c090331Brain-Machine

hatchling sea turtles during water finding behaviour. Neth-

-Interface-Technology/

erlands Journal of Sea Research, 1973, 7, 53–67.

Rao R P N, Stocco A, Bryan M, Sarma D, Youngquist T M,

[19] Arnold K, Neumeyer C. Wavelength discrimination in the

Wu J, Prat C S. A direct brain-to-brain interface in humans.

turtle Pseudemys scripta elegans. Vision Research, 1987, 27,
1501–1511.

Yoo S S, Kim H, Filandrianos E, Taghados S J, Park S.

[20] Witherington B E, Bjorndal K A. Influences of artificial

Non-invasive brain-to-brain interface (BBI): Establishing

lighting on the seaward orientation of hatchling loggerhead

functional links between two brains. PLoS ONE, 2013, 8,

turtles Caretta caretta. Biological Conservation, 1991, 55,

Daly D C, Mercier P P, Bhardwaj M, Stone A L, Aldworth Z

139–149.
[21] Arbib M A, Hanson A R. Vision, Brain, and Cooperative

N, Daniel T L, Voldman J, Hildebrand J G, Chandrakasan A

Computation. MIT Press Cambridge, MA, USA, 1990.

P. A pulsed UWB receiver SoC for insect motion control.

[22] Tinbergen N. The Study of Instinct. Oxford University Press,

IEEE Journal of Solid-State Circuits, 2010, 45, 153–166.
[9]

Hong Kong, China, 2010, 39–42.

Rao R P. An adaptive brain-computer interface for human-

e60410.
[8]

Conference on Micro Electro Mechanical Systems (MEMS).

Bryan M, Green J, Chung M, Chang L, Scherert R, Smith J,

PLoS ONE, 2014, 9, e111332.
[7]

flexible neuroprosthetic probe. IEEE 23rd International

[14] Butler Z, Corke P, Peterson R, Rus D. From robots to ani-

[2009-03-31],

[6]

control of a cyborg moth using carbon nanotube-enhanced

IEEE/RSJ International Conference on Intelligent Robots

Kheddar A, Friedman D. fMRI robotic embodiment: A pilot

[5]

Daniel T, Hildebrand J G, Levine R B, Voldman J. Remote

robots: Asynchronous direct control using an EEG-based

recognition. PLoS ONE, 2013, 8, e74583.
[4]

[10] Tsang W M, Stone A, Aldworth Z, Otten D, Akinwande A I,

system via electric stimulation. Proceedings of the

ference on Humanoid Robots (Humanoids), Bled, Slovenia,
[3]

2010, 4, 199.

Chae Y, Jeong J, Jo S. Toward brain-actuated humanoid
BCI. IEEE Transactions on Robotics, 2012, 28, 1131–1144.

[2]

radio control of insect flight. Frontiers in Neuroscience,

Sato H, Maharbiz M M. Recent developments in the remote

New York, USA, 1951.
[23] Alcock J. Animal Behavior: An Evolutionary Approach.

Kim et al. Remote Navigation of Turtle by Controlling Instinct Behavior via Human Brain-computer Interface
Sinauer Associates, Sunderland, MA, USA, 1993.

503

[29] Accupix Inc., [2015-12-04], http://www.accupix.com/

[24] van Gerven M, Farquhar J, Schaefer R, Vlek R, Geuze J,

[30] Faller J, Allison B, Brunner C, Schmalstieg D, Pfurtscheller

Nijholt A, Ramsey N, Haselager P, Vuurpijl L, Gielen S,

G. A software SSVEP BCI integrating stimuli within moti-

Desain P. The brain-computer interface cycle. Journal of

vating and immersive virtual and augmented reality envi-

Neural Engineering, 2009, 6, 041001.
[25] Naseer N, Hong K S. fNIRS-based brain-computer interfaces: A review. Frontiers in Human Neuroscience, 2015, 9,
3.
[26] Pfurtscheller G, Solis-Escalante T, Ortner R, Linortner P,

ronments. Real Actions in Virtual Environments (RAVE)
Conference, Barcelona, Spain. 2010.
[31] Faller J, Leeb R, Pfurtscheller G, Scherer R. Avatar navigation in virtual and augmented reality environments using an
SSVEP BCI. International Conference on Applied Bionics

Müller-Putz G R. Self-paced operation of an SSVEP-Based

and Biomechanics (ICABB), Venice, Italy, 2010, 1–4.

orthosis with and without an imagery-based “brain switch:”

[32] Bin G, Gao X, Yan Z, Hong B, Gao S. An online

a feasibility study towards a hybrid BCI. IEEE Transactions

multi-channel SSVEP-based brain-computer interface using

on Neural Systems and Rehabilitation Engineering, 2010, 18,

a canonical correlation analysis method. Journal of Neural

409–414.

Engineering, 2009, 6, 046002.

[27] Khan M J, Hong M J, Hong K S. Decoding of four move-

[33] Ramoser H, Muller-Gerking J, Pfurtscheller G. Optimal

ment directions using hybrid NIRS-EEG brain-computer

spatial filtering of single trial EEG during imagined hand

interface. Frontiers in Human Neuroscience, 2014, 8, 244.

movement. IEEE Transactions on Rehabilitation Engi-

[28] Emotiv Systems. Emotiv-brain computer interface technology, [2015-12-04], http://emotiv.com/

neering, 2000, 8, 441–446.

