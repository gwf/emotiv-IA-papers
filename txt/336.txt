Articles

Extending the Diagnostic
Capabilities of Artificial
Intelligence-Based
Instructional Systems
Santosh Mathan, Nick Yeung

n Active problem solving has been
shown to be one of the most effective
ways to acquire complex skills. Whether
one is learning a programming language by implementing a computer program, or learning calculus by solving
problems, context-sensitive feedback
and guidance are crucial to keeping
problem-solving efforts fruitful and efficient. This article reviews AI-based
algorithms that can diagnose student
difficulties during active problem solving and serve as the basis for providing
context-sensitive and individualized
guidance. The article also describes the
crucial role sensor-based estimates of
cognitive resources such as working
memory capacity and attention can
play in enhancing the diagnostic capabilities of intelligent instructional systems

omputer-based educational technology has had a
transformative impact in every imaginable educational context — from tablet-based educational games for
preschoolers, to massively open online courses for university
students and independent learners. Artificial intelligence (AI)
research solutions have the potential to boost the impact of
these systems by enhancing their diagnostic capabilities. For
example, unlike one-on-one human tutoring, it is rare to find
computer-based learning environments that not only provide opportunities for practice on complex problems (such as
working on multistep algebra problems, or practicing programming by writing multiline code), but also provide contextually relevant feedback and guidance based on an analysis of individual problem-solving actions. In contrast,

C

Copyright © 2015, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602

WINTER 2015 51

Articles

because of the technical challenge of making sound
diagnostic inferences, we typically find two extremes
represented in the design of instructional systems:
either systems that constrain the learner by restricting allowable actions during skill acquisition (for
example, multiple choice problems) or systems that
provide opportunities for open-ended practice, with
the only corrective feedback coming from the natural feedback inherent to the task environment (for
example, a programming problem that a student
solves on his or her own — with built-in feedback
from the interpreter or compiler). The problem with
the first extreme is that students develop skills in
context and complexity that are different from their
eventual application context. The problem with the
latter is that students may flounder with inadequate
feedback, or have incorrect or superficial strategies
reinforced in the absence of corrective feedback and
guidance.
These challenges point to the need for diagnostic
techniques that are (1) tied to problem solving —
capable of interpreting and assessing individual
actions in the context of relatively unconstrained
problem-solving activities (for example, writing a
computer program, flying a scenario in a flight training simulator); (2) automated — reducing the need
for manual intervention; (3) objective — minimizing
confounds stemming from subjective biases; and (4)
fine-grained — providing feedback at the level of
individual problem-solving actions, as opposed to
problem-solving outcome alone.

Cognitive Tutors:
A Promising AI Approach
Cognitive tutors, based on the adaptive control of
thought — rational (ACT-R) cognitive architecture,
represent one promising approach for boosting the
diagnostic capability of interactive learning environments (Anderson et al. 1995). The development of
the ACT-R theory of cognition, which describes how
humans perceive, think, and act, has been led by psychologist John Anderson and colleagues at Carnegie
Mellon University (Anderson and Lebiere 1998).
ACT-R is instantiated in the form of a programming
language with primitive constructs that embody specific assumptions about human cognition. It has
been used to model human performance in a broad
array of complex cognitive tasks, ranging from automobile driving (Salvucci 2001) and tactical decision
making (Anderson et al. 2011) to algebra and computer programming (Corbett, Anderson, and O’Brien
1995).
Cognitive model-based diagnosis is particularly
well suited for environments where learners acquire
skills through hands-on practice. In many complex
problem-solving domains, learners have access to a
broad range of problem-solving actions (operators)
that can be combined to transform a problem from

52

AI MAGAZINE

some initial state to a goal state through a set of intermediate problem states. In complex task domains,
the number of actions students have at their disposal can be quite large. By interacting with elements of
the learning environment, these actions can produce
a vast set of possible intermediate problem-solving
states. The primary task of the novice, learning to
navigate unfamiliar problem spaces, is to reach goal
states efficiently through one or more possible
sequences of states. Feedback allows learners to avoid
problem states that lead to dead ends and circumvent
inefficient transition paths to the goal. The lack of
feedback, however, can cause the student to experience frustration and confusion, and result in the
adoption of suboptimal performance strategies.
The provision of feedback in cognitive tutors is
based on a combination of an ACT-R model of the
successful and unsuccessful strategies relevant in a
given problem domain, and plan-recognition algorithms (a process referred to as model tracing). Automated plan-recognition systems try to infer an
agent’s plans or line of reasoning from its actions
(Kautz and Allen 1986). The detailed encoding of
knowledge embodied in a cognitive model allows the
plan-recognition process to map student actions to
successful and faulty problem-solving strategies.
While student actions remain consistent with one or
more fruitful problem-solving strategies, the system
remains unobtrusively in the background. However,
when student performance is indicative of ineffective
or inefficient strategies, the system intervenes with
assistance tailored to the actual issue or subtask causing difficulty for the student at that given moment.
A student may also ask a tutor for hints specific to the
learner’s immediate problem-solving context.
Many cognitive tutors incorporate functionality
known as knowledge tracing to pace students optimally through problems while distinguishing
between slips and errors (that is, disambiguating
between the possibility that a student knows the correct answer but presses a wrong key, and the possibility of substantive errors in understanding). Knowledge tracing relies on Bayesian estimation to
characterize a student’s strengths and weaknesses relative to the knowledge components in the cognitive
model (Corbett and Bhatnagar 1997). Based on student actions, the system estimates the probability
that a learner has mastered the component rules
required to perform a task. These estimates are
dynamically updated while the student performs
learning tasks. Estimates from knowledge tracing can
identify areas in which a student may need the most
practice, providing opportunities to remediate specific skill deficiencies. Additionally, students receive
feedback in the form of estimates of their mastery of
various knowledge and skill components through an
on-screen bar graph. Unlike many computer-based
environments, which guide all students through a
pre-specified sequence of problems, knowledge trac-

Articles

ing allows students to work on problems appropriate
to their competence level. Proficient students can
progress quickly through to challenging problems,
while students who need additional practice get to
work on problems that target their particular deficiencies.
ACT-R based cognitive tutors have been shown to
reduce training time by half and increase learning
outcomes by a standard deviation or more in rigorous
classroom and laboratory assessments (Anderson et
al. 1995). They support teaching concepts ranging
from programming to genetics and represent some of
the most broadly used educational systems (for
example, tens of thousands students in thousands of
schools across the United States use ACT-R based
tutors for algebra and geometry).

Opportunities for Enhancement with AI
While replications in numerous domains have
shown cognitive tutors to be highly effective computer-based learning platforms, their performance
falls short of one-on-one tutoring from skilled
human tutors. Researchers have argued that a critical
advantage of skilled human tutors over cognitive
tutors is the fine-grain access they have to their student’s behaviors. For instance, as Anderson and
Gluck (2001) noted, a human tutor can see frustration on a student’s face, hear uncertainty in an utterance and track the length of time a student takes to
work through a problem. Such broad access to a
learner’s emotive and cognitive state, they argue,
allows the skilled human tutor to display far greater
sensitivity and adaptability in the tutorial interaction
than current computer-based tutoring systems. To
address this gap, researchers have begun considering
the inclusion of capabilities to sense and estimate
cognitive states that play a fundamental roll in learning, including variation in attention and working
memory load. These capabilities could expand the
diagnostic functionality of cognitive tutors and provide a more effective basis for pedagogical diagnosis,
and guidance.

Cognitive State Sensing
As Just, Carpenter, and Miyake (2003) note, information-processing theories of cognition suggest that
cognitive processes operate in a capacity-constrained
environment (for example, Kahneman [1973]; Wickens [1984]). These theories posit that cognitive capacity varies across individuals and is affected by broad
range of physiological and psychological factors.
Capacity utilization, a metric that indicates the proportion of a resource pool engaged during a given
period, characterizes how hard the cognitive system
is working to produce observed performance. Meanwhile, task resource demands, the size of each individual’s resource pool, and the speed with which cognitive processes execute all affect resource utilization.

When demand for cognitive resources exceeds availability, cognitive efficacy is compromised: processes
slow down and partial products and results of computations begin to decay, worsening performance.
Individuals experience high capacity utilization as
more effortful — we define the term cognitive effort
as the perceptible effect of high cognitive capacity
utilization. Measuring cognitive effort or capacity
utilization is vital because performance that burdens
available capacity processing may be brittle, and the
addition of processing requirements could cause performance to fail (Wickens and Hollands 2000). Over
the years, the educational research community has
become aware of the negative implications of excessive cognitive load on learning outcomes. Real-time
measurements of the cognitive effort experienced by
an individual can guide dynamic modulation of
training complexity to match a student’s cognitive
capacity.
A promising avenue for estimating changes in cognitive efficacy uses measurement of cortical activity
to estimate the degree of cognitive effort required to
perform tasks. Researchers have noted that the
amplitude and spatial extent of cortical activity
increases as performance becomes effortful, even as
behavioral performance on tasks remains the same
(for example, Hill and Schneider [2006]; McAllister
et al. [2001]). This methodology could enable detection of challenging learning contexts, which require
broader participation of cortical areas to produce a
given level of performance. In addition to broad
changes in overall levels of cortical activity, increases in cognitive effort are associated with localized
changes in activity within specific cortical networks.
This suggests that cortical activity can serve as the
basis for making inferences about cognitive effort.
Our objective at the Honeywell Laboratories HumanCentered Systems Group has been to measure ongoing cortical activity to make inferences about cognitive effort using practical and unobtrusive sensors
that can be used in realistic applied and laboratory
task contexts. Given these requirements for practicality and effectiveness, our efforts have largely
focused on the use of electroencephalography (EEG)
as a biometric measure of the user’s cortical activity.
EEG measures scalp potentials generated by the
combined electrical activity of billions of neurons in
the brain, as tiny currents generated by signaling
between neurons sum together to produce measurable voltage changes at the scalp. These signals represent a rich source of information about a broad range
of neural processes. For example, analyses of EEG
voltage fluctuations that consistently occur in a
time-locked fashion with critical events can reveal
the dynamics of perceptual and decision-making
processes at a temporal resolution of one millisecond. Additionally, spectral measures of rhythmic
cortical activity within characteristic frequency
bands serve as a useful source of information about

WINTER 2015 53

Articles

Subjective Rating

EEG Estimate

3.8

1.8

1.0

easy

medium

hard

0.0

easy

medium

hard

Figure 1. Ratings by Pilots Landing Helicopters in Three Different Scenarios.
Left: Subjective cognitive effort by pilots. Right: EEG-based estimates of cognitive effort of the same pilots in the same scenarios.

temporally stable cognitive activity, such as attention
and working memory. While EEG provides information about cortical neural activity, several other
methods can also estimate changes in brain function.
Broadly used alternatives have major practical limitations, however. For example magnetic resonance
imaging (MRI) systems, which offer excellent spatial
resolution, but have high per-use costs, rely on slow
changes in blood flow (hemodynamics) as an indirect measure of neural activity, and are impractical
for use in most clinical and operational settings. Other measures of cerebral hemodynamics, such as functional near-infrared spectroscopy (fNIRS) and transcranial Doppler (TCD) are typically designed to
measure activity in relatively small regions of the cortex and, therefore, typically limit spatial coverage.
Additionally, systemic changes in vascular activity
can confound interpretation of signals from these
sources. Though electrophysiological sensors such as
electrocardiogram (ECG) and electro-dermal
response (EDR) can also provide insight into cognitive effort, noncognitive factors such as physical
activity and stress can strongly influence measures
derived from these information sources, and they
have relatively poor temporal resolution compared
to EEG.
Modern, lightweight, relatively inexpensive EEG
sensor systems can offer excellent temporal resolution capabilities for measuring ongoing cortical activity. Depending on the number of sensors and their
distribution on the scalp, EEG systems can provide
useful information about activity distributed across a
wide range of cortical regions. Researchers have

54

AI MAGAZINE

observed distinct patterns of EEG activity in response
to variations in task demands. For instance, activity
in the alpha band (8 to 12 hertz) is dominant when
an individual is awake, but resting. Activity in the
beta band (13 to 30 hertz) becomes prominent when
cognitive effort increases. While these general observations provide an approximation of brain activity in
response to task demands, complex interactions
between rhythmic brain activity at different frequencies and brain locations during cognitive task performance also exist. For example, Meltzer et al.
(2008) examined cognitive load-related changes in
the EEG power spectrum and its spatial distribution
in the context of the Sternberg task (a short-term
memory task requiring recall of a sequence of letters).
They noted that power increases in the gamma band
(> 40 hertz) occurred throughout the brain as the
length of the letter sequence increased, but observed
that these increases occur most prominently in the
occipital lobe. They also found that both alpha and
theta band activity increased in the frontal midline
cortex and decreased in the occipital cortex, concurrently with increased gamma range activity in these
regions. These observations argue against viewing
power in any given frequency band as a unitary phenomenon and call for viewing variations in activity
in different brain regions. Consequently, we use statistical machine-learning techniques to characterize
the unique pattern of discriminating activity for each
individual in response to variations in cognitive
effort. Given the complex distribution of spectral
power across frequency bands and cortical locations,
we rely on these techniques to create multivariate dis-

Articles

criminant functions that identify an optimal subspace projection of the data to discriminate between
different levels of effort. These discriminant functions, constructed using task calibration with systematically varied task demands, accept high-dimensional EEG data as input, and produce a
single-dimensional estimate of effort. EEG data collected in this context can then be used to create a discriminant function for use in similar contexts to estimate effort. This method can create both individual
and group data models; however, the discriminant
functions exploit the optimal features for a given
individual (increasing accuracy), while the latter
(group model) approach supports contexts where a
baseline may not be available.
We have applied these techniques to estimate cognitive effort in a range of complex operational environments — from commercial flight decks (Mathan,
Feyereisen, and Whitlow 2007) to ground-based
infantry operations in urban environments (Mathan
et al. 2007). For example, professional helicopter
pilots performed a flight simulator evaluation of a
display designed to help pilots land in poor visibility
conditions by executing landings in three different
landing contexts of varying difficulty. Pilots’ ratings
of perceived cognitive effort, as measured by the
NASA-TLX mental demand rating (figure 1, left) were
closely correlated with EEG-derived measures of cognitive effort (figure 1, right).
In another study, we examined whether EEG-based
measures of effort exceed the specificity and sensitivity of alternate methods of estimating cognitive
effort; these include subjective ratings, a secondary
reaction-time task, and heart-rate variability. Data
were collected in two conditions (with corporate jet
pilots as participants) in a flight simulator under four
conditions: two low-effort cruise conditions, and two
high-effort approach conditions. Scores associated
with all measures were normalized to lie between 0
and 1 (figure 2). Given the stark differences in the
information-processing requirements associated with
these two tasks, we would expect the two plots on the
left of each graph in figure 2 (cruise) to share little
overlap with the two bars on the right of each graph
(final approach). However, as the extent of overlap
between the Hi and Lo workload bars for each of the
conditions indicates, the EEG measure has much
higher specificity and sensitivity than other measures, as indicated by the receiver operating characteristic (ROC) values and overlap of box plots.
EEG-based estimates of cognitive effort can also
detect subtle variations in cognitive effort among
individuals coping with acquired (treatment-related)
cognitive impairment (Mathan et al. 2010). Five survivors of brain cancer and five survivors of breast cancer participated by reading text of varying complexity under different levels of time pressure as a
cognitive load. Unlike prior evaluations that provided stark difference in cognitive load stimuli to

improve classifier performance, the subtle workload
variations explored in this study more closely represented the variations in cognitive load one might
encounter in activities of daily living. Analysis
revealed that user workload could be classified using
an EEG-based index, with an accuracy (ROC area) of
0.84 (SD = 0.03). The analysis of this data also
showed that the EEG sites that contributed to the
peak classification outcome were most densely concentrated over the left hemisphere (figure 3), which
includes regions that play a critical role in language
processing.
Some of our earlier research has demonstrated that
even when outside the controlled environment of a
laboratory or flight simulator, we can reliably estimate cognitive effort when using modern biometric
sensing devices. Honeywell originally developed
technology aimed at EEG-based cognitive state estimation for operational settings in the context of the
Defense Advanced Research Projects Agency
(DARPA), improving warfighter information intake
under stress (also called Augmented Cognition) program. Using a combination of data from a wireless
EEG sensor system (Advanced Brain Monitoring,
Inc., Carlsbad, CA), and the machine-learning
approaches described earlier, we successfully demonstrated the efficacy of our approach at the U.S. Army
Aberdeen Proving Ground (figure 4) where military
trainers explicitly manipulated participant cognitive
load between high and low levels of effort in a highfidelity battlefield training exercise. The system classification accuracies exceeded 0.80 (ROC area) in this
within-subjects cross-validation analysis context
(Mathan et al. 2007).
Collectively, these results suggest that the EEG
index described here provides a sensitive and specific
measure of cognitive effort in a broad range of contexts, from battlefield simulations to aircraft piloting, as well as reading tasks. Spatial analyses indicate
that the features driving the observed results are consistent with neuropsychological theory, and comparisons to other measures of cognitive effort highlight the stronger performance of the EEG-based
metric.

Attention Estimation
The process of acquiring a novel skill typically begins
with a period of declarative instruction when a student learns about the central facts or concepts associated with a domain. Computer-based learning
environments typically facilitate declarative instruction through video clips or online textual expositions. Research has shown failure to manage and
appropriately direct attention toward processing
declarative information, which likely compromises
the robustness and accuracy of encoded declarative
knowledge (Anderson 1993). Poor encoding of
declarative knowledge can impede skill acquisition,
and declarative knowledge of the central concepts in

WINTER 2015 55

Articles

Subjective Rating Mental Demand TLX

Secondary Task
(ROC: 0.60)
1

1

0

PFD-LO

SVS-LO

PFD-HI

SVS-HI

0

PFD-LO

Heart Rate Variability
(ROC: 0.81)

PFD-HI

SVS-HI

EEG
(ROC: 0.97)

1

0

SVS-LO

1

PFD-LO

SVS-LO

PFD-HI

SVS-HI

0

PFD-LO

SVS-LO

PFD-HI

SVS-HI

Figure 2. Comparing EEG Measures of Effort to Subjective Ratings, Secondary Task.
HRV (heart rate variability).

a domain serves to structure early problem-solving
attempts (Anderson 1993). This encoding requires
both sustained attention and active elaboration
strategies such as self-explanation, both of which are
difficult to maintain over long periods of time. For
instance, Smallwood and Schooler (2006) observed
subjects over the course of a 45-minute reading task.
Subjects were interrupted at random and asked if
they were still on task. Their research revealed that
learner’s “zoned out” for close to 20 percent of the
time during reading tasks. Real-world estimates of

56

AI MAGAZINE

mind wandering tend to be even higher
(Killingsworth and Gilbert 2010). Such lapses in
attention over the course of reading text and watching video expositions can impair the skill-acquisition
process. Learners may miss critical information that
could be of importance in subsequent problem-solving efforts.
Research suggests that oscillatory EEG activity —
particularly in the alpha frequency band (8–12 hertz)
— can index variations in attention. Parieto-occipital
alpha power and subjective reports of attentional

Articles

S1 Brain 47 male

S2 Brain 28 ffemale
emale

S3 Breasst 49 ffemale
emale

S4 Breast 29 ffemale
emale

S5 Breast 50 ffemale
emaale

S6 Breast 57 ffemale
emale

S7 Br
B ain 47 ffemale
emale

S8 Brain 58 male

S9 Breast 43 ffemale
emale

S10 Brain 40 ffemale
emale

Figure 3. Contour Map of Sites Identified Using Feature Selection Techniques to
Discriminate Between Brain Activity Associated with High and Low Difficulty Text.
Study involved brain and breast cancer survivors.

Figure 4. Demonstration of Honeywell Cognitive State Sensing Technology at Aberdeen Proving Ground.
EEG activity was measured using low-density wireless EEG sensors (left).

state are both associated with visual attention and
awareness. For example, the authors of this article
explored the relationship between alpha activity and
participants’ introspective judgments of attentional
state as each varied from trial to trial during performance of a challenging visual detection task. We
collected participants’ subjective ratings of perceptual decision confidence and attentional state on continuous scales on each trial of a rapid serial visual
presentation detection task while recording EEG. We
found that confidence and attentional state ratings

were largely uncorrelated with each other, but both
were strongly associated with task performance and
poststimulus decision-related EEG activity (Macdonald, Mathan, and Yeung 2011). Crucially, attentional
state ratings were also negatively associated with
prestimulus EEG alpha power: periods of low attention were associated with high levels of alpha oscillations, and vice versa. Attesting to the robustness of
this association, we were able to classify attentional
state ratings through prestimulus alpha power on a
single-trial basis (figure 5). Moreover, when we

WINTER 2015 57

Articles

Z Score

P1

Z Score

P5

Z Score

P9

4

P2

–0.66***

4

P3

– 0.29***

4

P4

– 0.50***

4

2

2

2

2

0

0

0

0

-2

-2

-2

-2

-4

-4

-4

-4

200

4

400

600

800

P6

–0.18***

200

4

400

600

800

P7

0.08

200

4

400

600

800

P8

– 0.37***

2

2

2

0

0

0

0

-2

-2

-2

-2

-4

-4

-4

-4

4

400

600

–0.64***

800

P10

200

4

400

600

–0.40***

800

P11

200

4

400

600

– 0.36***

800

P12

2

2

2

0

0

0

0

-2

-2

-2

-2

200

400

600

Trial

800

-4

200

400

600

800

-4

Trial

200

400

600

800

-4

400

600

800

– 0.18***

200

4

2

-4

200

4

2

200

– 0.49***

400

600

800

– 0.29***

200

400

Trial

600

800

Trial
alpha power
att. state rating

Figure 5. Data from 12 Subjects Showing the Relationship Between Alpha Power and
Subjective Ratings of Attention During a Target Detection Task.
Black lines: alpha power. Red lines: subjective ratings of attention.

repeated these analyses after smoothing the time
series of attentional state ratings and alpha power
with increasingly large sliding windows, both the
correlations and classification performance improved
considerably, with the peaks occurring at a sliding
window size of approximately 7 minutes worth of trials. Therefore, our results suggest that slow fluctuations in attentional state in the order of minutes are
reflected in spontaneous alpha power. Because these
subjective attentional state ratings were associated
with objective measures of both behavior and neural
activity, they can provide a simple and effective estimate of task engagement as the basis for AI assistance
in operational settings that require human operators
to maintain a sustained focus of visual attention.

Conclusion
This review supports the feasibility of making reliable
estimates of cognitive state in a wide range of application contexts. However, the practicality of EEG
data collection in training and operational environments has limited the development of EEG-based

58

AI MAGAZINE

instructional applications. The time required to configure large arrays of sensors, restrictions on movement associated with cables, and mess associated
with conductive gel or saline electrolytes represent
significant user inconveniences that have largely
restricted the use of EEG systems to research settings.
However, in recent years several technology developments, including dry contact sensors from Wearable
Sensing, Inc. (San Diego, CA), a dry electrolyte gel
from Advanced Brain Monitoring, Inc., both of
which can provide a mess-free conductive medium,
and consumer-oriented systems from NeuroSky, Inc.
(San Jose, CA), and EMOTIV, Inc. (San Francisco, CA)
promise to support EEG data collection, signal classification, and estimation of cognitive effort and attention in training and tutoring environments. These
systems eliminate wires and transmit EEG data over
wireless radio connections, simplify donning and
doffing of sensors, improve comfort, and provide
integrated data logging and processing capabilities to
enhance the practicality of EEG systems.
Real-time estimates of cognitive state can help in
tutorial interaction in several ways. Attention often
lapses when learners watch video or read text pas-

Articles

sively, which hampers skill acquistion. Mitigation
strategies triggered by cognitive state classifiers could
minimize the negative impact of low attentional
states during declarative instruction. For instance,
when the intelligent automated cognitive tutor
detects low attentional state in a student during presentation of online text or video, the system could
intervene and step the student through the material
with interactive prompts. These prompts could present questions related to concepts just covered and
give students the chance to respond using multiplechoice responses. Additionally, the system could
index text or video segments presented during low
attentional states and prompt students to revisit
these segments at a later time.
Neurophysiological assessments of working memory can provide augmentation of hands-on practice
by dynamically matching working memory demands
imposed by the learning environment with a student’s working memory capacity. For instance, the
grain size of instructional content could dynamically vary the level of assistance or scaffolding provided
to students following errors during practice based on
neurophysiological assessments of cognitive state
(Anderson et al. 1995). Students experiencing high
levels of cognitive load would receive instructional
scaffolding to step interactively through the series of
subgoals necessary to accomplish a problem-solving
objective and maximize learning performance. In
contrast, students experiencing lower cognitive load
levels could simply be reminded of the overall problem-solving goal, leaving negotiation of the underlying problem space and its maintenance in memory
to the student. As a student becomes more proficient
at performing tasks, the working memory resources
associated with task execution normally diminish.
The intelligent tutor system could detect this change
and modulate workload levels to adapt the pace or
complexity of the task environment to the learner’s
working memory capacity to maximize learning and
retention.
These examples highlight promising avenues for
creating closed-loop neural feedback systems that
could accelerate learning through individually tailored training. Each example is based on current
technologies and existing theories about the cortical
underpinnings of cognitive functions. We can expect
the impact of neurotechnology on AI-based tutoring
systems to only increase as the technology is refined
— or revolutionized — as research uncovers ever
more sensitive and nuanced indices of the cognitive
processes of interest.

References
Anderson, J. R. 1993. Problem Solving and Learning. American Psychologist 48(1): 35. dx.doi.org/10.1037/0003066X.48.1.35
Anderson, J. R.; and Gluck, K. 2001. What Role Do Cognitive Architectures Play in Intelligent Tutoring Systems? In
Cognition and Instruction: Twenty-Five Years of Progress, ed. D.

Klahr and S. M. Carver. Mahwah, NJ: Psychology Press.
Anderson, J. R.; and Lebiere, C. 1998. Atomic Components of
Thought. Hillsdale, NJ: Erlbaum.
Anderson, J. R.; Bothell, D.; Fincham, J. M.; Anderson, A. R.;
Poole, B.; and Qin, Y. 2011. Brain Regions Engaged by Partand Whole-Task Performance in a Video Game: A ModelBased Test of the Decomposition Hypothesis. Journal of Cognitive Neuroscience 23(12): 3983–3997. dx.doi.org/10.1162/
jocn_a_00033
Anderson, J. R.; Corbett, A. T.; Koedinger, K.; and Pelletier,
R. 1995. Cognitive Tutors: Lessons Learned. The Journal of
Learning Sciences 4(2): 167–207. dx.doi.org/10.1207/
s15327809jls0402_2
Corbett, A. T.; Anderson, J. R.; and O’Brien, A. T. 1995. Student Modeling in the ACT Programming Tutor. In Cognitively Diagnostic Assessment, ed. P. Nichols, S. Chipman, and
B. Brennan, 19–41. Hillsdale, NJ: Erlbaum. dx.doi.org/
10.1007/978-3-7091-2670-7_25
Corbett, A. T., and Bhatnagar, A. 1997. Student Modeling in
the Act Programming Tutor: Adjusting a Procedural Learning Model with Declarative Knowledge. In Proceedings of the
Sixth International Conference on User Modeling, ed. A Jameson, C. Paris, and C. Tasso. Berlin: Springer.
Hill, N. M., and Schneider, W. 2006. Brain Changes in the
Development of Expertise: Neurological Evidence on SkillBased Adaptations. In Cambridge Handbook of Expertise and
Expert Performance, ed. K. A. Ericsson, N. Charness, P. Feltovich, and R. Hoffman, 653–682. New York: Cambridge
University Press. dx.doi.org/10.1017/CBO9780511816796.
037
Just, M.A.; Carpenter, P.A.; and Miyake, A. 2003. Neuroindices of Cognitive Workload: Neuroimaging, Pupillometric and Event-Related Potential Studies of Brain Work.
Theoretical Issues in Ergonomics Science. 4(1–2): 56–88.
dx.doi.org/10.1080/14639220210159735
Kahneman, D. 1973. Attention and Effort. Englewood Cliffs,
NJ: Prentice-Hall Inc.
Kautz, H. A., and Allen, J. F. 1986. Generalized Plan Recognition. In Proceedings of the Fifth National Conference on Artificial Intelligence (AAAI-86). Menlo Park, CA: AAAI Press.
Killingsworth, M. A., and Gilbert, D. T. 2010. A Wandering
Mind Is an Unhappy Mind. Science 330(6006): 932.
dx.doi.org/10.1126/science.1192439
McAllister, T.; Sparling, M. B.; Flashman, L.; Guerin, S. J.;
Mamourian A. C.; and Saykin, A. J. 2001. Differential Working Memory Load Effects After Mild Traumatic Brain Injury.
NeuroImage 14(5): 1004–1012. dx.doi.org/10.1006/nimg.
2001.0899
MacDonald, J. S. P.; Mathan, S.; and Yeung, N. 2011. Trialby-Trial Variations in Subjective Attentional State Are
Reflected in Ongoing Prestimulus EEG Alpha Oscillations.
Frontiers in Psychology 2 (82). dx.doi.org/10.3389/
fpsyg.2011.00082
Mathan, S.; Feyereisen, T.; and Whitlow, S. 2007. WorkSense: Exploring the Feasibility of Human Factors Assessment Using Electrophysiological Sensors. Paper presented
at the 4th International Conference of the Augmented Cognition Society. October, Baltimore, MD.
Mathan, S.; Whitlow, S.; Dorneich, M.; Ververs, P.; and
Davis, G. 2007. Neurophysiological Estimation of Interruptibility: Demonstrating Feasibility in a Field Context.
Paper presented at the 4th International Conference of the

WINTER 2015 59

Articles

I CWSM
Please Join Us for ICWSM-16!
The Tenth International AAAI Conference on Web and Social Media (ICWSM) will be held in Cologne, Germany,
from May 17–20. This interdisciplinary conference is a forum for researchers in computer science and social science to come together to share knowledge, discuss ideas, exchange information, and learn about cutting-edge
research in diverse fields with the common theme of online social media. This overall theme includes research
in new perspectives in social theories, as well as computational algorithms for analyzing social media. ICWSM
is a singularly fitting venue for research that blends social science and computational approaches to answer
important and challenging questions about human social behavior through social media while advancing computational tools for vast and unstructured data.
ICWSM-16 will include a lively program of technical talks and posters, invited presentations, and keynote talks
from prominent social scientists and technologists. The ICWSM Workshop program will return in 2016 and will
be held on the first day of the conference, May 17. Tutorials Day will also be May 17. Registration information
will be available at the ICWSM-16 website in March. For full details about the conference program, please visit
the ICWSM-16 website (icwsm.org) or write to icwsm16@aaai.org.

Augmented Cognition Society. October, Baltimore, MD.
Mathan, S.; Smart, A.; Ververs, P.; and Feuerstein, M. 2010.
Towards an Index of Cognitive Efficacy: EEG-Based Estimation of Cognitive Load Among Individuals Experiencing
Cancer-Related Cognitive Impairment. In Proceedings of the
32nd Annual IEEE Engineering in Medicine and Biology Society
Conference. Piscataway, NJ: Institute of Electrical and Electronics Engineers. dx.doi.org/10.1109/iembs.2010.5627126
Meltzer, J. A.; Zaveri, H. P.; Goncharova, I. I.; Distasio, M.
M.; Papademetris, X.; Spencer, S. S.; Spencer, D. D.; and
Constable, R. T. 2008. Effects of Working Memory Load on
Oscillatory Power in Human Intracranial EEG. Cerebral Cortex 18(8): 843–55. dx.doi.org/10.1093/cercor/bhm213
Salvucci, D. D. 2001. Predicting the Effects of In-Car Interfaces on Driver Behavior Using a Cognitive Architecture. In
Proceedings of the 2001 CHI Human Factors in Computing Systems, 120–127. New York: Association for Computing
Machinery. dx.doi.org/10.1145/365024.365064
Smallwood J., and Schooler J. W. 2006. The Restless Mind.
Psychological Bulletin 132(6): 946–958. dx.doi.org/10.1037/
0033-2909.132.6.946
Wickens, C. D. 1984. Processing Resources in Attention. In
Varieties of Attention, ed. R. Parasuraman and D. R. Davies,
63–102. New York: Academic Press.
Wickens, C. D., and Hollands, J. G. 2000. Engineering Psy-

60

AI MAGAZINE

chology and Human Performance. Englewood Cliffs, NJ: Prentice-Hall.

Santosh Mathan is a staff scientist in the Human Centered
Systems group at Honeywell Labs. His research lies at the
intersection of computational neuroscience and humancomputer Interaction. Mathan’s research, carried out in collaboration with academic and industry researchers around
the world, has led to the development of algorithms that
can estimate changes in cognitive function following brain
trauma, identify fluctuations in attention and working
memory load, and serve as the basis for hands-free robotic
control. He has a Ph.D. in human-computer interaction
from the School of Computer Science at Carnegie Mellon
University, and is an instrument-rated pilot.
Nick Yeung is a professor of cognitive neuroscience and
head of the Attention and Cognitive Control Laboratory at
the University of Oxford. He received his Ph.D. from the
University of Cambridge in 2000, and has held positions at
Princeton University and Carnegie Mellon University. His
research investigates human attention, memory, and decision making using a combination of behavioral, computational, and brain imaging techniques. He has published
more than 50 papers.

