From Auditory and Visual to Immersive
Neurofeedback: Application to Diagnosis of Alzheimer’s
Disease
Mohamed Elgendi1, Justin Dauwels2, Brice Rebsamen3, Rohit Shukla4, Yosmar
Putra2, Jorge Gamez5, Niu ZePing2, Bangying Ho6, Niteesh Prasad7, Dhruv
Aggarwal8, Amrish Nair2, Vasilisa Mishuhina9, Francois Vialatte10, Martin
Constable11, Andrzej Cichocki12, Charles Latchoumane13, Jaesung Jeong14, Daniel
Thalmann1, and Nadia Magnenat-Thalmann1
1

Institute for Media Innovation, Nanyang Technological University, Singapore
School of Electrical Engineering, Nanyang Technological University, Singapore
3
Temasek Laboratories, National University of Singapore, Singapore,
4
Indian Institute of Information Technology, India,
5
Universidad Nacional Autonoma de Mexico, Mexico
6
Hwa Chong Institution, Singapore
7
Drexel University, USA
8
BITS- Pilani, Goa Campus, India
9
Belarusian State University of Informatics and Radioelectronics, Belarus
10
ESPCI ParisTech, Paris, France
11
School of Art, Design and Media, Nanyang Technological University, Singapore
12
Lab. ABSP, RIKEN Brain Science Institute, Wako-Shi, Japan
13
Center for Neural Science, Korea Institute of Science and Technology, South Korea
14
Korea Advanced Institute of Science and Technology, South Korea
moe.elgendi@gmail.com, fvialatte@brain.riken.jp, mconstable@ntu.edu.sg,
cia@brain.riken.jp, jdauwels@ntu.edu.sg
2

Abstract. In neurofeedback, brain waves are transformed into sounds or music,
graphics, and other representations, to provide real-time information on
ongoing waves and patterns in the brain. Here we present various forms of
neurofeedback, including sonification, sonification in combination with
visualization, and at last, immersive neurofeedback, where auditory and visual
feedback is provided in a multi-sided immersive environment in which
participants are completely surrounded by virtual imagery and 3D sound.
Neural feedback may potentially improve the user’s (or patient’s) ability to
control brain activity, the diagnosis of medical conditions, and the rehabilitation
of neurological or psychiatric disorders. Several psychological and medical
studies have confirmed that virtual immersive activity is enjoyable, stimulating,
and can have a healing effect. As an illustration, neurofeedback is generated
from electroencephalograms (EEG) of Alzheimer’s disease (AD) patients and
healthy subjects. The auditory, visual, and immersive representations of
Alzheimer’s EEG differ substantially from healthy EEG, potentially yielding
novel diagnostic tools. Moreover, such alternative representations of AD EEG
are natural and intuitive, and hence easily accessible to laymen (AD patients
and family members), and can provide insight into the abnormal brainwaves
associated with AD.

Keywords: Brain Activity, Alzheimer’s Disease, Neurofeedback, EEG
Sonification, EEG Visualization, Immersive Room

1

Introduction

The brain is the main organ of the human body. It is responsible for controlling
virtually every cell in every group of organs that performs a certain task. This control
is occurs directly through neurohormones, such as endorphin, and rapid
neurotransmitters such as glutamate and gamma-aminobutyric acid (fast acting), and
neuromodulators (slower acting) [1]. The brain contains approximately 100 billion
neurons.
This large number of neurons conduct many electrical signals―action
potentials―measured in microvolt which can be amplified and heard via speakers [2].
Neurophysiologists usually listen or/and visualize neuronal electromagneticallygenerated communications that are produced by individual neurons [3].

Figure 1. René Laënnec investigates a child by means of a stethoscope [4]. He proved that
diagnosis can be done through listening to body sounds.

The idea of listening to internal sounds of human parts started in 1816, when René
Laënnec invented the stethoscope (cf. Figure 1). Three years later, he was able to
provide a detailed description of the acoustic signals perceived in many patients with
specific pathologies, malfunctions of physiology and anatomy. Since then, the rules
used in interpreting the sound have become a metalanguage of practice in medicine
[5]. A minor resurgence occurred in 1996, when Guyton and Hall wrote a textbook of
medical physiology, discussing the topic of heart sounds [6].
However, this method of diagnosis became less popular in the course of the
nineteenth century [5], especially after Wilhelm Röntgen discovered the X-ray
radiation as a means to examine human body. This was perhaps the first time in
history that human parts can be visualized as an image (see Figure 2).

Figure 2. The first visualization of a human part is done in 1895 by Wilhelm Röntgen, when he
invented the X-ray [7, 8]. This has inspired many scientists and researchers to develop new
techniques to visualize the human anatomy and physiology, in particular the human brain.

As the technology advances, especially in the development of biosensors, listening
and seeing to the human parts are moving from macro to micro scale. For example, in
here we are interested in representing the microvolt neural activity into different
forms.
In general, brain activity is displayed in real-time through electroencephalography
or functional magnetic resonance imaging (fMRI), with a goal of controlling central
nervous system activity. Because of the EEG’s economically-efficient and easy-to-use
properties, it is convenient to use EEG in our investigation rather than fMRI.
To date, it is still unclear whether the feedback presented to subject/volunteer
should be auditory, visual, audio-visual or immersive. Table 1 shows different types
of neurofeedback and their applications. However, It is well-known that fMRI
produces a two dimensional representation (an image) while the EEG is presented in a
single dimension. Therefore, visualizing the EEG signals in two-dimensional
presentations is intuitive and more informative.
This book chapter illustrates different mappings/representations of EEG signals to
be used in neurofeedback therapy as an alternative to mainstream medical treatment.
However, it is not clear how the visual feedback should be presented. Should the
visual feedback be a graph, map or even a virtual reality presentation? There is no
rule of thumb to answer this question yet, the same holds for auditory and audiovisual neurofeedbacks. Thus, this research took a place to provide some insight about
auditory, audio-visual and visualization neurofeedbacks.
To answer the question of how design an efficient neurofeedback application, there
is a need to understand the typical neurofeedback procedure first.
In a typical neurofeedback sitting, EEG signals are represented in the form of video
display and/or sound or music [9]. The user receives instantaneous, real time visual
and audio feedback about his/her brainwave activity, to reinforce specific brainwaves.
This process relies on the operant conditioning mechanism [10-12], where behavior is
adapted through positive reinforcement. After several sessions, certain brainwaves are
modified and maintained.

Table 1. Neurofeedback types and their applications.
Brain
Activity
Form

Neurofeedback Type

Neurofeedback Applications

 Cognitive:
 Auditory:
□ Sinusoidal tones [26]
□ Piece of harp or bongo
sounds [27]
□ Musical notes [28-30]

EEG
 Visual with (or without)
Auditory:
□ Games [31-33]
□ Virtual reality [34, 35]

 Visual:

fMRI

□ Video projection [43, 44]
□ Visual cues (i.e. text, color
bars, and icons) [45]
□ Visual thermometer bar [46,
47]
□ Virtual reality [47]

□ control the level of attention [13, 14]
□ improve cognitive functions [10, 15, 16]

 Psychiatric:
□ treat individuals who have attention deficit
hyperactivity disorder (ADHD) [11, 17, 36, 37]
□ treat alcoholic with depressive symptoms [38]
□ treat epilepsy [12, 18]
□ treat depression anxiety [19, 20] and bipolar
disorder [22]
□ treat schizophrenia [21]

 Neurological:

□ support motor imagery practice in people with
stroke [39]
□ improve autism disorders [40, 41]
□ treat substance use disorders [42]

 Cognitive:
□ perform cognitive imagery tasks (i.e. brain
computer interface) [47, 48]
□ self-regulate local brain activity [43, 44]

 Psychiatric:

□ promising
treatment
for
neuropsychiatric disorders [45]

patients

with

 Neurological:

□ treat Parkinson’s disease [23]
□ improve chronic tinnitus [46]

Several clinical studies suggest that neurofeedback may potentially be used to
improve cognitive performance and also to treat psychiatric and neurological
conditions (cf. Table 1). A number of studies have shown the potential of
neurofeedback to improve cognitive performance in healthy individuals. This simply
can be done by training healthy adults to either augment or suppress the amplitude of
EEG signal within a certain frequency range according to the level of vigilance [13].
In a study by Rasey et al. [14], subjects managed to increase the amplitude of EEG
within certain frequency range while suppressing it in a different frequency range
according to level of attention. Several other researchers have investigated
neurofeedback as a technique for improving cognitive functions [10, 15, 16].
For over 35 years, neurofeedback has been considered as a potential treatment for a
number of neurological and psychiatric disorders including ADHD [11, 17], epilepsy
[12, 18], depression [19, 20], schizophrenia [21] and bipolar disorder [22]. Recently,
visual neurofeedback has been explored for treating Parkinson's disease [23].
Moreover, Neurofeedback has been used to treat amputees' phantom limb pain
through an immersive virtual reality [24, 25].
Neurofeedback systems typically provide either auditory or visual feedback, or a
combination of both. Medical practitioners rely heavily on visual or/and auditory
inspection to examine patients. Thus, in this study, different types of neurofeedback

have been designed and implemented systems that in real-time translate EEG signals,
recorded from a wireless EEG headset, into sounds and graphics.
The sound representations have been assessed in an offline style, by applying our
sonification system to EEG collected from AD patients and from healthy subjects.
The sounds generated from AD EEG should be distinct from sounds extracted from
the EEG of healthy subjects. We investigate whether our EEG sonification system
improves diagnosis of AD, following an approach proposed earlier by Vialatte et al.
[38].
Visualization systems (topographic map, neuro-vortex and brain drops) have also
been developed which can be integrated with developed sonification system.
Moreover, we experience the output of the neurofeedback in an immersive manner.
This book chapter is structured as follows. In the next section we discuss
sonification types and its advantage. Section 3 elaborates on the brain
electroencephalogram signals. Section 4 explains how the features will be extracted
from brain activity. Section 5 proposes a new sonification system and its performance
(online and offline). Section 6 suggests three approaches (topographic map, neurovortex, and brain drops) for visualization. Section 7 demonstrates how sonification
and visualization can be integrated with examples. Section 8 describes the immersive
neurofeedback. Section 9 clarifies how to design a neurofeedback system
(sonification, visualization, sonification-visualization, and immersive). Finally a
conclusion and discussion is covered in Section 10.
Video captures of the examples described below are available online at:
http://www.youtube.com/user/ElgendiChannel
http://www.elgendi.net/apps/videos/channels/show/3689856-eeg-sonification

2 Sonification
The word sonification comes from the Latin word ‘sonus’, meaning ‘sound’.
Hence, the verb “sonify” means representing data by sounds and noises [49].
Sonification deals specifically with non-speech sounds and aims to provide the
listener with an informative output.
Hermman et al. [50] defines sonification as auditory display with the following
four properties:
1. The sound reflects objective properties or relations in the input data.
2. The transformation is systematic. This means that there is a precise
definition provided of how the data (and optional interactions) cause the
sound to change.
3. The sonification is reproducible: given the same data and identical
interactions (or triggers) the resulting sound has to be structurally
identical.
4. The system can intentionally be used with different data, and also be
used in repetition with the same data.
In [50], various advantages of sonification compared to visual feedback are
mentioned:
1) Sonification uncovers patterns masked in visual displays

2) Sonification can identify new phenomena current display techniques
miss.
3) Sonification improves data exploration of large multi-dimensional and
multi-dataset.
4) Sonification enables us to explore the data in frequency rather than
spatial dimensions
5) Sonification helps in analyzing complex, rapidly, or temporally
changing data.
6) Sonification complements the existing visual displays.
7) Sonification allows us to monitor the phenomena while looking at
something else (background event-finding)
8) Sonification improves the visual perception when accompanied by
audio cues.
Unlike the eyes, human ears are always active and never closed. Also, hearing is
omnidirectional (i.e. one does not need to be oriented in a particular direction).
Furthermore, sound is particularly useful for people with visual impairment or who
are non-visually oriented. Keller [51] has categorised sonification systems into three
types:
A. Iconic Sonification
One strategy for sonification is to associate sounds with certain
phenomena or events in the data. For example, weather data may be
sonified as the sound of raindrops, thunderstorms, etc. The mapping
process in iconic sonification is often straightforward.
B. Direct Conversion Sonification
Alternatively, time series data can directly be converted into sound
signals. It can be as simple as computing the dominant frequencies in the
time series, and generating sound waves with the same (or rescaled)
frequencies [51]. For example, aerospace scientists sometimes map
measurements of electromagnetic waves into sound waves, to explore the
measurements in an efficient manner [51].
C. Musical Sonification
Data may be mapped into musical structures. For instance, data may be
encoded as pitch, tempo or rhythm of the artificially generated music. The
musical rendering adds an aesthetic dimension to the sonification process.
Moreover, it greatly improves the listenability, as it prevents the listener
from experiencing exhaustion and fatigue. For those reasons, musical
sonification may be the most effective form of sonification for biomedical
and clinical applications, such as neurofeedback and diagnostics.

3 Electroencephalogram (EEG)
The neurons of the brain generate ionic current flows that have voltage fluctuation
property. These fluctuations―represent brain activities— can be measured using an
electroencephalography. The EEG was first recorded in 1924 by Hans Berger [52,

53]. Berger studied and described for the first time the nature of EEG alterations in
brain diseases such as epilepsy. Currently, EEG is a helpful tool in clinical
neuroscience, with several applications as:
 Monitor alertness, coma and brain death [54, 55]
 Locate areas of damage following head injuries [56, 57], stroke [58, 59] or
brain haemorrhage [60, 61]
 Detect AD [62-65] and brain tumour [66, 67]
 Investigate sleep disorders [68, 69] and epilepsy [70, 71]
 Monitor human brain development [72, 73]
 Measure the depth of anaesthesia [74]
 Test drug effects [75, 76].
Progressive developments in electrical engineering and the fascination with the
human brain have attracted researchers from different scientific fields to investigate
EEG recordings.
Hermann et al. investigated the sonification of brain activity, and concluded that
the most prominent criteria to describe an EEG signal are rhythmicity, intensity of
the rhythms and the location of those rhythms on the scalp [50]. The main EEG
features described in literature [30, 77-80] as follows:
 EEG signal amplitude (usually in specific frequency bands)
 EEG signal oscillatory rhythms (time-frequency study, e.g., wavelets)
 EEG synchronization (e.g., correlation).

Figure 3. The average power spectrum of EEG signals at 21 electrodes for 10 healthy subjects
(blue, dotted line) and 10 Alzheimer’s patient (red, solid line). The EEG data set is described in
Section 5.1.2.

EEG analysis is usually described in terms of its rhythmic activity, which is helpful
in relating the EEG to the brain function. The rhythmic activity in EEG is commonly
divided in specific frequency bands: 0.5−4 Hz (delta), 4−8 Hz (theta), 8−10 Hz (alpha
1), 10−12 Hz (alpha 2), 12−30 Hz (beta) and 30−100 Hz (gamma) [81].

As an illustration, Figure 3 shows the spectrum of EEG from AD patients and
healthy subjects (see Section 5.1.2 for a description of the EEG data set). Clearly, the
spectrum of AD EEG is, on average, quite different from the one of healthy EEG.
Motivated by this observation, we extract neurofeedback signals from the EEG
spectrum. As a test, we will then generate neurofeedback from AD EEG and healthy
EEG, and compare both.

4 Feature Extraction
EEG signals are often corrupted by noise and artifacts: 50/60 Hz power line
interference, motion and eye blinking artifacts, electromyogram (EMG) signals from
muscles, and artifacts due to changes in the electrode-skin interface [64, 82]. The
gamma range (30–100 Hz) has a particularly low signal-to-noise ratio, and therefore
will be excluded from our analysis. In this study, the frequency range (4−30 Hz) is
considered as it is the most informative frequency for AD.
The EEG spectrum is known to depend on the subject’s mental state (e.g.,
relaxation, sleep); moreover, abnormal EEG spectra seem to be associated with
neurological disorders, such as AD [64, 82] (see Figure 3).
The relative power of three different EEG frequency bands is considered as the
extracted features from the EEG spectrum. Relative power is a simple and informative
measure that can readily be computed in real-time. In future work, additional spectral
measures (e.g. synchrony measures) will be considered.

TH1
Relative Power

TH2

TH3

Figure 4. Distribution of relative EEG power for AD EEG (red bar indicates standard
deviation and circle depicts the mean value) and Control EEG (blue bar indicates standard
deviation and cross depicts the mean value), for three frequency bands (4−10 Hz, 10−20 Hz,
and 20−30 Hz).

The power spectrum P is calculated for each EEG channel, next relative power
features f1, f2, and f3 are calculated:

f1 

P(4  10Hz) ,
P(4  30Hz)

f2 

P(10  20Hz)
P(4  30Hz)

and

f3 

P(20  30Hz)
P(4  30Hz)

(1)

Those features are averaged across all channels. Figure 4 shows the distribution of
relative EEG power for AD patients and healthy subjects, for the same EEG data as in
Figure 3. It can be seen from Figure 4 that relative EEG power has substantially
different values in AD patients to those of healthy subjects.
To generate neurofeedback, we choose certain thresholds for the relative power in
each band. When the relative power in a certain band (e.g. 4−10 Hz) is above a
threshold, specific feedback will be triggered (e.g. a music note will be played and/or
an object will be shown on the screen). In other words, feedback will be generated
whenever relative power falls below and exceeds certain thresholds. In this way, the
feedback is shaped by the EEG spectrum. In this particular example, we will choose
the thresholds such that they maximally differentiate between AD and healthy EEG.
Obviously, for other applications, the threshold may need to be designed in a different
manner.
By appropriately choosing the thresholds THi, EEG signals of AD patients will
reach thresholds THi more or less frequently than EEG from healthy subjects, leading
to distinct sounds and/or visualizations. Following this reasoning, we determine the
thresholds THi as follows:
(μ (f )  σ A (f1 )) + (μ H (f1 ) + σ H (f1 )
,
TH1 = A 1
2
(μ (f )  σ H (f2 )) + (μ A (f2 ) + σ A (f2 )
TH 2 = H 2
,
2
(μ (f )  σ H (f3 )) + (μ A (f3 ) + σ A (f3 )
TH 3 = H 3
,
(2)
2
where μ A and σ A is the mean and standard deviation respectively of the features for
AD EEG, and likewise μ H and σ H for healthy (control) EEG. Those choices of
thresholds can be understood as follows. For example, relative power in the (4−10
Hz) band is clearly larger in AD patients. Therefore, we choose the corresponding
threshold TH1 below the mean value for AD EEG and above the mean value for
control EEG. With this choice of thresholds TH1, TH2 and TH3, the averaged features
f1, f2, and f3 are then mapped into music and visual display, where music notes and
graphical objects are triggered by above- or below-threshold values of relative power
respectively.

5 EEG Sonification
As far as we know, EEG sonification was for the first time attempted in 1965 by
Alvin Lucier (composer) and Edmond Dewan (physicist); in their composition called
Music for Solo Performer [83], human brain waves controlled percussion instruments.

Figure 5. A performer tries to control the sounds generated from his EEG, by adjusting the
mental states associated with those sound [84].

Later that same year, Richard Teitelbaum used various biological signals (including
EEG and electrocardiograms) to control electronic synthesizers [85]. In 1972, David
Rosenboom developed a bio-feedback system for producing sounds and light flashes
from brain waves [84], as illustrated in Figure 5. In 1990 he went on to develop a
musical system driven by EEG components believed to be associated with shifts of
the performer’s selective attention [86]. More recently, Miranda et al. [87] and
Hermann et al. [88] proposed novel sonification methods.
Although several researchers and musicians tried to generate sound from EEG
signals, there are still many open questions and challenges, and plenty of
opportunities for further research. For example, the recent advent of convenient
wireless EEG headsets [89-93] may further stimulate and advance the area of EEG
sonification.
In this study we design and implement a system that in real-time translates EEG
signals, recorded from a wireless EEG headset, into sounds. Offline we assess the
sound representations, by applying our sonification system to EEG collected from AD
patients and from healthy subjects. The sounds generated from AD EEG should be
distinct from sounds extracted from the EEG of healthy subjects. We investigate
whether our EEG sonification system improves diagnosis of AD, following an
approach proposed earlier by Vialatte and Chichocki [77].

5.1 Methodology
The proposed sonification system has two operating modes: offline and real-time.
In the offline mode, the system extracts sounds from EEG signals that have been
recorded earlier. In Section 5.3, we will apply our system to an EEG dataset from
Alzheimer’s patients and control subjects, recorded by a wired high-performance
EEG system. In real-time mode, EEG signals are acquired and immediately
transformed into sounds. In the following, we will elaborate on the EEG signal
acquisition.

Figure 6. Emotiv EEG wireless headset [91].

5.1.1. Data Acquisition
The process begins with a real-time collection of EEG signals using a wireless
EEG headset, specifically the Emotiv EPOC wireless headset [91], as shown in Figure
6, with a sampling frequency of 128 Hz. The headset has fourteen data collecting
electrodes and two reference electrodes. The electrodes are placed approximately at
the 10−20 locations AF3/4, F3/4, FC5/6, F7/8, T7/8, P7/8, and O1/2 (cf. Figure 7).
We use the software package BCI2000 [94] to interface with the Emotiv EPOC
wireless headset. The headset transmits encrypted data wirelessly to a laptop
computer.

Figure 7. The electrode positions of the Emotiv headset [91]. The headset contains 14
electrodes, not counting the ground and ref, with a sampling frequency is 128 Hz.

The Emotiv headset is originally designed mostly for entertainment (e.g., video
games) [91], however, it is inexpensive and user-friendly, and with suitable signal
processing, it may become useful for research and clinical purposes. The device
seems to be prone to various artefacts, such as eye blinking, ECG, EMG, body
movements and power sources. In ongoing work, we are developing real-time
algorithms for removing artefacts, which is a crucial step towards reliable real-time
EEG sonification by means of low-end wireless EEG headsets.
5.1.2 EEG data set
For offline benchmarking of our neurofeedback systems, we consider EEG data of
mild-AD patients and age-matched control subjects. The EEG data set has been
analyzed in previous studies [9-11]; the data was obtained using a strict protocol from
Derriford Hospital, Plymouth, U.K., and had been collected using standard hospital
practices [10]. EEGs were recorded during a resting period with various states:
awake, drowsy, alert and resting states with eyes closed and open. All recording
sessions and experiments proceeded after obtaining the informed consent of the
subjects or the caregivers and were approved by local institutional ethics committees.
EEG dataset is composed of 24 healthy Ctrl subjects (age: 69.4±11.5 years old; 10
males) and 17 patients with mild AD (age: 77.6±10.0 years old; 9 males). The patient
group underwent full battery of cognitive tests (Mini Mental State Examination, Rey
Auditory Verbal Learning Test, Benton Visual Retention Test, and memory recall
tests). The EEG time series were recorded using 21 electrodes positioned according to
Maudsley system, similar to the 10-20 international system, at a sampling frequency
of 128 Hz. EEGs were band-pass filtered with digital third-order Butterworth filter
(forward and reverse filtering) between 0.5 and 30 Hz. The recordings were
conducted with the subjects in an awake but resting state with eyes closed, and the
length of the EEG recording was about 5 minutes, for each subject. The EEG
technicians prevented the subjects from falling asleep (vigilance control). After
recording, the EEG data has been carefully inspected. Indeed, EEG recordings are
prone to a variety of artifacts, for example due to electronic smog, head movements,
and muscular activity. For each patient, an EEG expert extracted one segment of 20
seconds that is most informative. The EEG expert was blinded from the results of the
present study. These extracted 20-second segments are used in the analysis, as
described below.

5.2 Sonification
A diagram of our sonification system is shown in Figure 8. The system computes
the relative power features (f1, f2, f3) in three non-overlapping frequency bands (4−10
Hz, 10−20 Hz and 20−30 Hz), as discussed in Section 4. Next it generates notes from
the computed values, depending on whether the values are above or below threshold
THi (cf. Equation 1). We will now discuss this procedure in more detail.
To keep the generated sounds as simple and transparent as possible, we considered
only notes from one octave (MIDI Octave -1) with pentatonic scale (five notes per

octave), and we limited ourselves to only one instrument (acoustic bass). Obviously,
one could incorporate more music instruments and multiple octaves. However, the
extracted sound easily becomes cacophonic and difficult to parse. In the future, we
will explore alternative schemes to generate music from EEG relative power.

Figure 8. Sonification of EEG signals: sounds are synthesized from relative power in three
frequency bands (4−10 Hz, 10−20 Hz and 20−30 Hz); a single music instrument is used
(acoustic bass).

In the current sonification system, we consider the following three notes and
corresponding MIDI note number: (C,48), (E,52), and (A,57). Those 3 notes are
played according to the three values of relative power (f1, f2, f3). If feature fi is above a
threshold THi (cf. Equation 2), note i is played. More precisely, the notes are
generated as follows:
IF f1  TH1 THEN play low note 48
IF f 2  TH 2 THEN play intermediate note 52
IF f 3  TH3 THEN play high note 57.

(3)

The EEG is divided into consecutive 1-second segments. In each segment, the
features (f1, f2, f3) are computed and notes are generated according to the above rule.
Note that at most three notes can be generated for each EEG segment; that occurs
when all three features are above threshold. However, typically, one or two notes are
played during each segment, which leads to a simple musical arrangement. We
implemented our sonification system in Python (specifically, pyPortMidi [98] and
Numpy [99]). The generated MIDI sequences are synthesized by SyFonOne [100] in
conjunction with MIDI-YOKE [101]. The sound sequences are saved into MP3 files
for further offline analysis.
Currently, we are developing sonification systems that yield more melodic and
harmonic compositions. We will describe our progress in that direction in Section 5.4.
First, however, in Section 5.3 we will present results for the basic mapping shown in
conditional rules (3).

5.3 Evaluation
We applied the sonification procedure (2) to EEG signals of AD patients and
healthy subjects (described in Section 5.1.2). For AD EEG, we expect the threshold
TH1 to be reached more often, which will lead to more frequent low-pitch notes (bass
note 48). Similarly, we expect AD EEG to generate fewer high-pitch notes (E,52) and
(A,57).
10
9
8
False Positive

Answers

7
AD

6
5
4
3

Control

False Negative

2
1
0
1

2

3

4

5
6
Subjects

7

8

9

10

Figure 8. Result of the experimental human classification (10 subjects and 15 volunteers). A
classification accuracy of 95% was obtained.

We assessed the sonification procedure (2) as follows. We asked 15 volunteers to
listen to the generated sounds, and to guess whether they stem from AD patients or
healthy subjects. Particularly, we asked each volunteer to classify sound sequences
from 10 different subjects (one sequence per subject). Each volunteer was asked to
score the sound sequences from 0 to 10 (0: certainly healthy, 5: unsure, and 10:
certainly AD patient). We did not provide any further details about the sound files.
Prior to this test, each volunteer was trained with sound sequences from 4 subjects (2
AD patients and 2 healthy subjects), so that they can learn to appreciate how the
sounds differ between the two groups; we also briefly explained how the sounds were
generated, and emphasized that, in our sonification scheme, AD EEG tends to
generate more low-pitch notes than their healthy counterparts.
Figure 8 summarizes the result of our classification experiment. Subjects 1 to 5 are
AD patients while subjects 6 to 10 are age-matched control subjects. Subjects 2 and 4
have been classified correctly by all 15 volunteers, each time with the maximum
confidence (10). Subjects 1, 3, 7, and 9 have been misclassified by a few volunteers
(three of them are false negatives, one false positive, and three cases of ‘unsure’). The
other subjects were correctly classified by all volunteers, but not always with
maximum confidence. Overall, the volunteers were able to reliably label the sound
sequences; they correctly classified 95% of the subjects, with sensitivity of 93.3% and
specificity of 97.3%. Note that we tested just 10 subjects out of 41, and classification
on the entire database might not be as successful.

Figure 9. Linear discriminant analysis (LDA) and Quadratic discriminant analysis (QDA) for
pairs of features: (a) f1 and f2 (b) f1 and f3 (c) f2 and f3.

Table 2. Results for linear discriminant analysis (LDA) and discriminant analysis (QDA)
for the same 10 subjects as in the sonification test.

Feature

LDA
Accuracy

Features

LDA
Accuracy

QDA
Accuracy

4−10 Hz

65%

4−10 Hz and 10−20 Hz

90%

90%

10−20 Hz

90%

4−10 Hz and 20−30 Hz

85%

85%

20−30 Hz

90%

10−20 Hz and 20−30 Hz

90%

95%

Table 3. Results for linear discriminant analysis (LDA) and quadratic discriminant analysis
(QDA) for the entire data set (cf. Section 5.1.2).

Feature

LDA
Accuracy

Features

LDA
Accuracy

QDA
Accuracy

4−10 Hz

70%

4−10 Hz and 10−20 Hz

84%

86%

10−20 Hz

84%

4−10 Hz and 20−30 Hz

82%

80%

20−30 Hz

87%

10−20 Hz and 20−30 Hz

86%

86%

Nevertheless, this experiment demonstrates that the proposed sonification system
translates EEG into meaningful sounds, which can for example be used for detecting
EEG abnormalities and perhaps different mental states.
As a benchmark, we conducted linear discriminant analysis (LDA) and Quadratic
discriminant analysis (QDA) with the same features (f1, f2, f3) for the same 10
subjects; we average those features over the entire EEG segment of 20s. In other
words, we do not consider here individual EEG segments of 1s. We compute
classification rates through leave-one-out crossvalidation. The results are summarized
in Table 2. We considered the features individually and we also investigated pairs of
features (see Figure 9). It is noteworthy that through this approach, a maximum of
95% of the subjects are correctly classified using the quadratic nonlinear classifier as
the sonification system achieved. We also conducted LDA and QDA for the entire
data sets, with the same choice of features (see Table 3). As expected, the
classification results are slightly worse.
In this study we have developed a system that translates EEG signals (acquired by
a wireless headset) to sounds in real-time. Several similar systems [77, 102] have
been developed for high- quality wired EEG devices; the latter are more expensive
and less user-friendly. However, the low-end inexpensive wireless EEG devices such
as EPOC Emotiv (as used in this study) are more prone to artefacts. We have not
investigated the effects of artefacts on the generated sounds. We are now in the
process of developing real-time artefact detection and removal methods. Such
methods may enable reliable EEG sonification, even from low-end wireless EEG
head sets.

The proposed sonification system has been validated offline by means of a small
EEG data set, collected with high quality wired EEG headset. By listening offline to
the sonified EEG signals, the patients with mild AD can differentiated from control
subjects with 95% accuracy. Those results suggest that the proposed sonification
system generates sounds that are meaningful from a neurophysiological viewpoint.

5.4 Extensions
It is clear the simple sonification method introduced in section 5.2 and evaluated in
section 5.3 showed promising results in diagnosing AD. Using three notes, we
achieved 95% classification accuracy.

Figure 10. This is part of the Fur Elise by Beethoven. The dashed box refers to a note; the
solid box refers to a chord (multiple notes played simultaneously), while the dotted box refers
to a phrase (multiple notes and multiple chords).

In this section, we investigate the impact of adding a note, chord, phrase, as shown
in Figure 10. Moreover, we see the effect of manipulating the tempo of a played piece
of music.
To explore this effect, we run two tracks: one represents the normal subject and
followed by one which represents the AD patient. The probability of adding a note in
the first track is 25% while in case of AD is 75%.
In the experiment, we play the first track followed by the second track and ask a
total of 10 volunteers to distinguish them, which track is most probably AD?
Similarly, we tested adding a chord, phrase and decreasing the tempo.
SoundCipher library [103] has been used since it provides an easy way to create
music in the Processing development environment as follows:

We play a MIDI version of Fur Elise by Beethoven as a baseline as
sc.playMidiFile("furelise.mid",60). The song is played initially at a tempo of
60 beats per minute (bpm). Every second, a random value between 1 and 100 is
generated. Depending on the value, additions were made to the song simultaneously.
Two tracks are generated normal and AD. It is deemed that the normal track
contains AD signs by 25% while 75% for the AD track. The AD sign has been
presented by adding a note (middle C), a chord (C Major), a phrase (an 8-note
sequence of the song itself) and decreased the tempo (speed of playback) of the song.

Adding a note in case of AD is done by playing a Middle C (C4, MIDI Number 60)
while the Fur Elise song is running as follows:
Thus hearing multiple extra/abnormal notes over a short period of time would
indicate the presence of AD. In case of normal track, the probability of adding a note
is 25% as:

Similarly, adding a Chord to the running song. The added chord is a C Major
consists of C4, E4, and G4 (MIDI Numbers 60, 64, and 67 respectively) played
simultaneously as:

In case of normal track, p<25, the song is played with nearly no change (i.e. few
added chords).
Similarly, the phrase(s) is added. The played phrase consists of 8 notes sequence
of the song itself (C5, G5, G4, G5, A4, G5, B4, and G5 which correspond to the MIDI
Numbers 72, 79, 67, 79, 69, 79, 71, and 79 respectively) as:

For sure, hearing multiple extra/abnormal phrases over a short period of time
would indicate the presence of AD. In case of normal track, p<25, the song is played
with nearly no change (i.e. few added phrases).
Likewise, slowing the tempo in case of AD from the original of 60bpm to 30bpm
has been considered as a sign of AD. The following is an example of slowing the
tempo in case of AD:

However, in case of normal signs the tempo reverts back to the original 60bpm.
The slower the song tempo the more evidence of AD is.
In the training phase, the volunteers have been trained to hear the difference
between a normal and AD track for adding note, chord, phrase, and slowing the
tempo. While in the testing phase, they have been asked to classify 8 tracks (4 audio
features for two tracks) into normal or AD. The accuracy of their classification of the
algorithms was recorded and the results of the experiment are as follows:
Table 4. Classification performance of the developed sonification algorithms
Added/Changed Audio feature
Note (C4)

Accuracy of classification out of 10 volunteers (%)
Normal Track
AD Track
Average
70
70
70

Chord (C4, E4, G4)

70

80

75

Phrase (C5, G5, G4, G5, A4, G5, B4, G5)

70

100

85

Tempo

100

100

100

As shown in Table 4, notes scored the lowest classification accuracy while tempo
scored the highest. Notes and chords occasionally resulted in hesitation amongst the
volunteers during the process of classification and on more than one occasion did a
volunteer request for the replay of the music scores. There were no requests for the
replaying of the tracks with addition of phrases; however, there was still some
uncertainty. Tempo gave the best results. Interestingly, the volunteers’ distinguished
normal from AD tracks much faster compared to the other audio features without
having any doubts. Therefore, sonfiying EEG signals to detect AD disease by
manipulating the tempo is considered promising.

6 EEG Visualization
Visualization of brain activity, also known as visual neurofeedback, has been
investigated. In here, this section discusses three applications of the visual
neurofedback: real-time topographic maps, neuro-vortex and (b)rain drops.

6.1 Real-time topographic maps
Visualization of EEG features provides more intuitive interpretation of EEG
signals. With visualization techniques, doctors would have a better idea of the
characteristics of the current signals and how to diagnose a specific disorder more
accurately.
One of the most popular EEG visualization techniques is topoplot; a technique in
which a large number of EEG electrodes are placed onto the head, following a
geometrical array of even-spaced points (cf. Figure 10).
Special software is needed to plot the activity on a color screen by mapping the
amount of activity in a color scale starts from blue and ends with red ( the blue color
represents the lowest EEG amplitude, while the red color represents larger
amplitudes). The software used to generate the results in Figure 11 is PyMVPA [104].
It is an efficient tool that calculates the spatial points lying between electrodes using
natural neighbor interpolation based on Delaunay triangulation. Thus, a smooth
gradation of colors is achieved (cf. Figure 11).

Figure 11. Topoplot demonstration of EEG signals in healthy subject (Top) and AD patient
(bottom). The color scale from blue to red represents the relative EEG power value from 0 to 1
respectively. These topoplots are generated using the PyMVPA free-software [104].

It can be seen that the topographical representation of EEG succeeded in showing
the differences between AD patients and healthy controls; especially within the
frequency range 4−10 Hz. Certainly the PyMVPA does not generate the above results
by default or even with these specific frequency ranges. We enhanced the code by

introducing multi-threading technique to show the topoplots for three frequency
ranges simultaneously. Of course, the topoplot has been used offline over the two
records for healthy subject and AD patient.
In addition, we used the same methodology for real-time application, but we have
not test it on real patients yet.

6.2 Neuro-Vortex
We map the relative EEG power features (f1, f2, f3) into colour and speed to
visualize the EEG as a moving tunnel shown in Figure 12.

Figure 12. The EEG activity is represented as a vortex, where the blue vortex with low speed
represents a healthy subject while the red vortex with high speed represents an AD patient.

This graphic is used to diagnose whether a patient is suffering from AD or not
based on the color and speed of the graphic. This graphic shows a three dimensional
vortex and creates an affect of gliding through the vortex. The color of the vortex and
the gliding speed will change based upon the patient’s EEG data collected using the
emotiv headset. If the patient is healthy, then the speed of the vortex would become
low and the color of the vortex keeps on changing between light blue and dark blue. If
the patient is suffering from AD, the color of the vortex would change between light
red and dark red and the speed will be high.
To decide whether the subject is healthy or an AD patient, we look at the EEG
power features (f1, f2, f3). Each of these power features has a range of values and a
threshold that distinguishes a healthy subject from an AD patient. If the value of each
of these power features falls in the AD region, then the colour of the vortex will
become red and the speed will be high, otherwise the colour will be blue and speed
will be slow. The threshold values have been decided according to Figure 4 and
Equation 2.

Figure 13.The base image used for creating vortex graphic.
The base image used to create this vortex graphic is shown in Figure 13. The (R,
G, B) pixel values of the base image has been mapped in the range of 0 and 1. To
change the color of the vortex the (R, G, B) pixel values of the base image are added
with the (R, G, B) values stored in an array, that is shown in Table 5. The pixel values
of the graphic obtained after the addition are in between -1 to 1. The RGB values are
accessed from the array using an integer counter that acts as an index for the array.
Table 5. The (R, G, B) color array that change the vortex color between blue and red
0
RED

1

2

3

4

5

-0.16 -0.14 -0.12 -0.242 -0.286 -0.33

6

7

8

9

-0.374 -0.418 -0.462 -0.506

10

11

12

13

14

15

16

17

18

19

0.08

0.1

0.12

0.14

0.16

0.18

0.2

0.22

0.24

0.26

GREEN 0.096 0.084 0.072 -0.055 -0.065 -0.075 -0.085 -0.095 -0.105 -0.115 -0.048 -0.06 -0.072 -0.084 -0.096 -0.108 -0.12 -0.132 -0.144 -0.156

BLUE

0.256 0.224 0.192 0.082

0.097

0.112

0.127

0.142

0.157

0.172 -0.128 -0.16 -0.192 -0.224 -0.256 -0.288 -0.32 -0.352 -0.384 -0.416

The integer counter increases or decreases one value at a time to create the effect
of transitional color change. The color of the vortex changes from dark blue to light
blue and from light blue to dark blue when the integer counter changes from 0 to 9
and vice-versa, respectively. The color will vary between light red and dark red as the
integer counter changes from 10 to 19 and vice-versa. There are only 20 RGB values
in the array, so that the user is able to see a visible transition from blue to red or from
red to blue within one second.

Figure 14. An instance of the (B)Rain drops. Band 1, band 2, and band 3 represent the
frequency bands 4−10 Hz, 10−20 Hz and 20−30 Hz respectively. Red color is an indication of
Ad diseases while green color indicated normality.

6.3 (B)Rain Drops
In here, we map the relative EEG power features (f1, f2, f3) into three ball drops.
According to the value of the feature, the colour will be assigned (cf. Figure 14).
To reflect values of EEG relative power features f1, f2, and f3 graphically, balls of
different colors were generated in a 3D environment. Specifically, two distinct colors,
green to represent normal and red to represent AD. However, the intensity of the two
colors varies according to the relative power feature. Other physical elements of the
balls (diameter and amplitude) were kept constant.
First, to determine if the balls were to be either green or red, we checked the value
of the relative power feature against the appropriate threshold THi (cf. Equation 2). If
the value of the relative power feature represented the presence of AD, red balls
would be generated. On the contrary, green balls would be generated if there is an
absence of AD.

Figure 15. Colour intensity of balls varies proportionally with difference between EEG data
and threshold. ‘5’indicates certainly AD and ‘-5’ indicates certainly healthy.

Next, the intensity of the color was determined. The specific intensity of green or
red that the ball will be is calculated by taking a fifth of the difference between the
incoming relative power feature and the corresponding threshold value as follows:

 1 ( f 1  TH1 ) *100 , for 4  10 Hz
5

C   15 (TH 2  f 2) *100 , for 10  20 Hz
1
(TH 3  f 3) *100 , for 20  30 Hz

5

(4)

The larger the calculated value (and difference) is, the stronger the intensity of the
color. The calculated C value (which lies within the range -5 to 5) will mapped into its
corresponding colors as shown in Figure 15.

7 EEG Sonification and Visualization
Simultaneous sonification and visualization of brain activity, also known as
auditory-visual neurofeedback, has been investigated. In here, this section discusses
two applications of the auditory-visual neurofedback: (b)rain drops with music and
brain forest.

7.1 (B)Rain Drops with Music
The Fur Elise song (by Beethoven) is played simultaneously with the (B)Rain
drops application described in section 6.3. The tempo of the Fur Elise song varies
according to the relative power feature as follows:

60  ( f 1  TH1 ) *100 , for 4  10 Hz

tempo  60  (TH 2  f 2) *100 , for 10  20 Hz
60  (TH  f 3) *100 , for 20  30 Hz
(5)
3

The default tempo would be 60 beats per minute (bpm) and would decrease to
reflect the presence of AD and would increase to reflect the absence of it.

7.2 Brain Forest
The brain activity is represented as a forest, where trees grow and bloom, and birds
whistle, as illustrated in Figures 16 and 17.
The EEG electrodes have been divided into four regions, where each region is
represented by one tree (see Figure 16). Similar to the sonification mapping concept
discussed in section 5.2, we map the relative EEG power features (f1, f2, f3) into
colour, tree branching and birds sounds.

Figure 16. The EEG activity is represented as a forest, where trees grow and bloom, and birds
whistle. The activity of each brain area (Right) of the brain is represented by a different tree
(Left). Electrodes F7, F8, P7, and P8 are used to represent the trees at top left, top right, bottom
left, and bottom right respectively.

The relative EEG power for healthy subjects, according to Figure 4 and Equation 2,
accomplishes the following rule:

( f1  TH1 )  ( f 2  TH2 )  ( f3  TH3 )

(6)

Therefore, in case of healthy feature received the branching parameter gets
incremented; consequently the branching colour becomes greener, and the birds sound
would be played after 6 seconds if the healthy rule achieved at least 5 times in the first
6 seconds, as shown in Figure 16(left).
The tree has been set to grow up to 10 levels. Each level represents a one second of
EEG signals. Once the tree is fully grown (reached to the10th level after 10 seconds),
the branching parameter will be reset to zero, the bird sounds will stop and the tree
will start growing from beginning.
For creating the tree graphic, the concept of fractal trees has been used. The idea in
the brain forest is that instead of rotating the line segments by fixed angles; they have
been rotated at random angles, to give an effect of random branching happening in a
tree. However, the generated random angles do not correspond to the calculated EEG
features or the collected EEG signals.
The algorithm can be described as follows:
1. Select four electrodes of the EEG headset to represent four regions of the
scalp. The four electrodes used to represent the four trees shown in Figure
16 are (F7, top left), (F8, top right), (P7, bottom left), (P8, bottom right).
2. Check the relative EEG power features (f1, f2, f3) of the four electrodes.
3. If the relative power features satisfy the healthy condition (cf. Equation 6),
the branching parameter gets incremented by one and two branches will
sprout from the previous branch. The index attribute in Table 6 refers to the
branching parameter. If the trees keep on growing, the branching colour
becomes greener. Moreover, if the growing continues till it reaches last two
levels, the trees will flourish and a few flowers will sprout from them (cf.
Figure 17 (left)).

4. If the relative power features do not satisfy the healthy condition (cf.
Equation 6), only one branch will sprout from the tree and the branching
parameter will remain unchanged. Hence, there will not be any colour
change in the new branches. If EEG features continues in not satisfying the
healthy rule (cf. Equation 6), the tree will not flourish and it will look like a
dying tree (cf. Figure 17 (right)).
The (R, G, B) values of the line segment vary 0 between 255. The graphic starts
with the trunk of the tree having a brown colour. Every time a tree grows, the
branching parameter is incremented by one. This branching parameter acts as an
index for the index for the colour array as shown in Table 6. The lowest index of the
array stores the (R. G, B) value of brown colour and the highest index stores the (R,
G, B) value of bright green colour.

Table 6. The colour array of the tree graphic. The branching parameter is the index of this
array. Index (0) indicates certainly AD with brown colour, while index (8) indicates certainly
healthy with green colour.
Index

RED
GREEN
BLUE

0

1

2

3

4

5

6

7

8

139
69
19

100
70
25

80
120
35

70
140
50

60
170
53

45
190
30

20
190
20

10
210
0

0
255
25

When the trees have grown continuously for 8 seconds, it’ll start sprouting flowers
at the top. This flower is a regular circle with the border colour and fill colour having
(R, G, B) value (220, 20, 20), to give an effect of red colour.
Initially, when the trees start to grow, sound of insects is played in the background.
The bird sounds are played in the background whenever the healthy rule is achieved
at least 5 times in the first 6 seconds. All of the four trees have different bird sounds
so that when a particular bird sound is played we will be able to guess which tree is
flourishing. Hence, we can imply from which part of the brain the healthy rule is
satisfied.

1

3

2

1

4

3

2

4

Figure 17. Two examples of the brain forest display for healthy subject and AD patient. EEG
activity for a healthy subject (left) where the forest flourishes, while for an AD patient (right)
the forest is less active and developed. Note: the EEG signals are taken from the annotated
database described in Section 5.1.2.

Two examples of using the brain forest as an offline neurofeedback system are
shown in Figure 17. The EEG samples are taken from the database described in
Section 5.1.2. It is clear that in the case of a healthy subject, the forest flourishes: the
trees grow fast, they are in full bloom, and birds live happily in the forest. While in
case of Alzheimer’s patient, the forest is less active: the trees are less developed, there
are hardly any flowers, and there are few birds.

8 Immersive Neurofeedback
Healthcare organizations continue to pursue ways of offering higher-quality care
to face the demand and expectations in promoting and maintaining health and in
disease prevention. Currently, in neuroscience, there is an undergoing paradigm shift
towards immersive neurofeedback mechanism.
Immersive neurofeedback allows the user to participate in a comprehensive and
realistic experience [105]. This will improve the user’s (or patient’s) ability to control
brain activity, medical diagnoses, and rehabilitation of neurological or psychiatric
disorders. Indeed, several psychological and medical studies have confirmed that
virtual immersive activity is enjoyable, stimulating, and can have a healing effect
[106-108].

Figure 18. A 3D conceptual view of the 320 degrees dome screen which is located within the
Institute for Media Innovation at Nanyang Technological University (NTU) in Singapore. Its
dimensions are as follow: outer most diameter (6.58m), inner floor diameter (4.45m), inner top
opening diameter (6.37m), and overall height (2.68m). The immersive room consists of
projectors (5 DP Titan 1080p Dual 3D), screen (5 Fusion 3D warp and blend device), graphic
server (EON Icube graphic server: 5 dell T5500), audio system (Onkyo HT-S5300, 7.1
Surround Sound System), and 3D glasses (5 Long range emitter and 30 active stereo glasses).

We have developed a new paradigm that translates EEG signals, recorded from a
wireless EEG headset, into sounds and graphics mapped into a dome screen within an

immersive room (c.f. Figure 18). A visitor to the immersive room can see, hear and
touch his brainwaves, as if he is standing in the middle of his own brain, as illustrated
in Figure 19. A virtual human in the immersive room guides the visitor through
different applications and types of neural feedback. The visitor can customize the
demo in the immersive room on the fly: he can control certain parameters of the
virtual reality (e.g., colors, speed, angle of view, zoom), by talking to the virtual
human or/and by gesture.

Figure 19. The immersive room (inside view), a performer tries to control the sounds and the
vortex properties (i.e. colour and speed) generated from his EEG, by adjusting the mental states
associated with the heard sound and shown graphics. This is the real view of the

All types of sonification (such as brain forest, neuro-vortex, etc) can be generated
separately or simultaneously, resulting in a virtual reality that has been sculpted from
EEG signals. Such intense neural feedback may, for example, be used to treat
neurological diseases such as depression, epilepsy, bipolar disorder, cognitive
impairment, migraines, and autism spectrum disorders; alternatively, it can be used to
explore and investigate mental states (e.g., emotions, meditation). Indeed, several
psychological and medical studies have confirmed that virtual immersive activity is
enjoyable, stimulating, and can have a healing effect. These studies have also shown
that the effect is stronger with virtual reality (VR) feedback than with simple 2D
feedback [109-112].
Such a neurofeedback system may have a stabilizing effect on the brain [111], and
might be used in the long-term as treatment for a wide spectrum of neurological
diseases [37]. This virtual reality can also be generated offline: that would enable a
medical doctor to screen EEG signals of patients in a retrospective fashion.

9 System Design
Our system has three input devices: Emotiv headset (wireless non-invasive
acquisition of brain waves) [91], Kinect camera (gesture recognition) , and wireless
microphone (voice/speech recognition). These devices are connected to Processing
(3D graphics mapping), Dolby surround system, Python plotting software PyMVPA
[104] (brain topographic mapping), and SAPI5.4 (Speech Application Programming
Interface).

Figure 20. Emotiv (wireless EEG headset) [91], Kinect (gesture detection camera) and wireless
microphone are connected to Processing software (graphical rendering [113]), Topoplot
(research-oriented visualizations)[104], and Speech Application Programming Interface
(SAPI5.4). The programming language used for each component is indicated on the arrows.
The package BCI2000 serves as an interface between those different languages, and operates as
a meta-platform. The microphone is used to choose the neurofeedback application such
auditory, visual, or auditory-visual. While the Kinect camera is used for zooming or rotating the
3D applications in real-time.

The graphics output of these software packages can projected on computer screen,
TV, or demo screen of an immersive room.
According to the application requirement, the appropriate input and output devices
will be used. However, we propose a generic system (cf. Figure 20) that can generate
several types of neurofeedback applications (i.e. audio, visual, audio-visual or/and
immersive).
The proposed system integrates three general-purpose programming languages
(VC++, Java, and Python). The input devices are all processed in VC++, whereas the
output is generated in Java, VC++, or/and Python.
As the midi libraries, signal processing modules, and topoplot libraries [104] are
well-established in Python, therefore we used Python for extraction EEG features,
sonification, and topographic brain activity visualization.
For designing real-time artistic and visual context, we used Processing as an open
source programming language and environment to create images, animations, and
interactions. The Processing community has written over seventy libraries to facilitate

computer vision, data visualization, music, networking, and electronics. Therefore, it
is convenient to use Processing for sonification and visualization, and it generates
professional work as well.

9.1 Auditory neurofeedback systems
The sonification algorithms discussed in Section 5.2 have been developed
following the diagram of Figure 21.

Figure 21. Emotiv (wireless EEG headset [91]) is connected to Python through BCI2000 [94].
The package BCI2000 serves as an interface between those different languages, and operates as
a meta-platform. This schematic is part of the main schematic shown in Figure 16.

The EEG is collected via Emotiv headset and passed to Python for sonification;
we used PyPM packages for midi control. Although we only consider one instrument
in our sonification system, below we show an example of how to use multiple
instruments:

This example shows how to establish three channels for three different
instruments (bass, violin, and flute) for the three EEG power features (f1, f2, and f3)
respectively. According the conditional rules for the AD and Control subjects the
notes for each instrument will be played (cf. Equation 3). For example we can map f1
into note C (MIDI note 48), f2 into note E (MIDI note 52), and f3 into note A (MIDI
note 57) as follows:
MidiOut.WriteShort(0x90,48)
MidiOut.WriteShort(0x91,52)
MidiOut.WriteShort(0x92,57)

9.2 Visual neurofeedback systems
The visualization algorithms discussed in Section 6 are also designed following
the diagram of Figure 21. The EEG collected via Emotiv headset and passed to
Python for visualization. The PyMVPA software has been extended to work in realtime with the Emotiv 14-electrode configuration. We applied threading techniques to
display multiple topographic maps in real-time, one for each frequency range (4−10
Hz, 10−20 Hz and 20−30 Hz):

9.3 Auditory-visual neurofeedback systems
The sonification-visualization algorithms discussed in Section 7 have been
developed according to integration system shown in Figure 22. The EEG is collected
via Emotiv headset [91] and next passed to Processing for visualization.

Figure 22. Emotiv (wireless EEG headset) is connected to Processing through BCI2000
[94] and Python. The package BCI2000 serves as an interface between Emotiv [91] and
Python. The EEG signal analysis with feature extraction is carried out in Python, and then the
EEG features are transmitted to Processing [113] through sockets.

We used sockets in order to connect Python (client socket [114]) with Processing
(server socket [115]).
Specifically, the integration of the different modules is achieved through UDP
sockets. This allows modules running on the same or different computers through a
Local Area Network. UDP sockets were selected because they are faster and easier to
work with than TCP sockets. The client socket is implemented in Python as follows:

When the connect function completes, the socket s can be used to send the
relative EEG power features (f1, f2, f3) calculated in Python to Processing, in order to
map them into graphics. A server socket in Processing is listening to the client socket
output from Python. The server UDP socket is created in Processing as follows:
1

The server listens to the port number 60000 where the client transmits the
relative EEG power features (f1, f2, f3). The IP address is 127.0.0.1 because the
established socket connection is visible within the same computer.
We use multithreading technology and sockets to seamlessly integrate those
different languages; the same technology also allows us to run multiple applications
in parallel, and to connect and process additional sensors and input/output devices;
information can even be transmitted and received through the world wide web,
enabling various powerful extensions of our approach (e.g., multi-user applications

for study of social interactions). For such applications, we would need to replace UDP
sockets by TCP sockets, which is straightforward.

9.4 Immersive neurofeedback systems
Our system has three input devices: Emotiv headset (wireless non-invasive
acquisition of brain waves) [91], Kinect camera (gesture recognition) , and wireless
microphone (voice/speech recognition). These devices are connected to Processing
(3D graphics mapping), Dolby surround system, Python plotting software PyMVPA
[104] (brain topographic mapping), and SAPI5.4 (Speech Application Programming
Interface).
The graphics output of these software packages are projected on the dome screen
in the immersive room, and the sound is generated by a Dolby Surround System. As
shown in Figure 20, the system is developed in different programming languages: the
input devices are all processed in VC++, whereas the output is generated in Java,
VC++, and Python.

10 Discussion
Any EEG sonification or neurofeedback system depends mainly on how the brain
activities mapped into auditory or/and visual representation(s). The mapping should
be developed empirically with usability testing to ensure the message they are
intending to communicate will be perceived by the user.
Two mapping issues in sonification need to be discussed which are: polarity and
scaling. For example the polarity of the auditory sonification system introduced in
section 5.2 has been chosen as the increase of the number of low musical notes
represents the increase of AD indication (positive polarity). While the decrease of low
played musical notes means less indication of AD (negative polarity). Regarding the
scaling issue, it is important to determine how much change in the played notes (in
our case we used three notes as a scale). Certainly, the polarity and scaling differ from
application to another. However we are still investigating the appropriate polarity and
scaling for mapping EEG signals into sound and visual representations for AD
detection.
The key factor in developing any EEG sonification system is the clarity and
simplicity of the hidden message delivered to the user through sound or/and visual
representations. This sonic or/and visual message needs to be understood. Perhaps,
adding context cues (e.g. axes or marks for time and amplitude) to the visual display
may improve the readability and aid perception. Moreover, an efficient neurofeedack
system needs to be developed aesthetically in terms of sound or/and graphics.
However, including aesthetical sound or/and graphic in any neurofeedback system
remains an open area to explore.
In addition, understanding the perceptual and cognitive differences between the
participants is challenging. These differences depend mainly on the capability,
experience (training duration) and transient states (like mood and level of fatigue) of

the participant, which have an impact on the overall performance of the EEG
sonification or/ and neurofeedback system. Interestingly, Glueck and Stroebel [116]
recommended that various types of biofeedback techniques must be carefully tailored
to the needs of the individual patient.
After discussing the major issues and challenges in mapping EEG signals, it is vital
to discuss the possible future auditory or/and visual neurofeedback applications. For
example controlling the brain activation and pain learned by using one of the
proposed real-time neurofeeback as deCharms et al. [117] did it using functional
MRI. It is expected developing an efficient auditory-visual neurofeedback system will
help patients with severe, chronic clinical pain.
Another promising application is engaging the proposed neurofedback system with
socially assistive robotics on assisting people through social interaction. Marti et al.
[118] described an exploratory study related to the use of the seal robot Paro for the
treatment of dementia. Connecting the proposed neurofeedback systems with social
robots can improve the efficacy of the existing nonpharmacological treatments for
dementia.

11 Conclusions
The sonification study discussed in section 5 showed promising results in detecting
AD. Certainly, it is important to point out that the number of volunteers (15 in total)
and subjects (5 AD patients and 5 healthy control subjects) in our test is fairly small.
A larger sample size and a more diverse data set are needed in order to generalize the
findings of this study. Multiple types of dementia and other neurological disorders can
also be analyzed through our technique, which may further validate our results.
The ultimate objective of this line of research is to develop a reliable real-time
system that transforms EEG signals captured by a cost-effective and user-friendly
wireless headset, into various tangible representations, including:
 Sounds and music (sonification),
 Graphics (visualization),
 Vibrations or other tactile stimuli (haptification).
 Immersive neurofeedback
Such multimodal representation may prove to be useful for improved neurological
diagnosis and neurofeedback.
We also introduced the immersive neurofeedback system that maps brain signals in
real-time wirelessly. This new technology may be valuable for therapy, diagnosis,
entertainment, and arts.
Video captures of the examples described above are available online at:
http://www.youtube.com/user/ElgendiChannel
http://www.elgendi.net/apps/videos/channels/show/3689856-eeg-sonification

Acknowledgment
Mohamed Elgendi and Justin Dauwels would like to thank the Institute for Media
Innovation (IMI) at Nanyang Technological University (NTU) for partially supporting
this project (Grant M58B40020).

References
1.

Lubar, J.F., Neocortical Dynamics: Implications for Understanding the Role of
Neurofeedback and Related Techniques for the Enhancement of Attention.
Applied Psychophysiology and Biofeedback, 1997; 22(2): 111-126.
2. Novak, P., Daniluk, S., Ellias, S.A., and Nazzaro, J.M., Detection of the
subthalamic nucleus in microelectrographic recordings in Parkinson disease
using the high-frequency (> 500 Hz) neuronal background. Journal of
Neurosurgery, 2007; 106(1): 175-179.
3. Baslow, M., The Languages of Neurons: An Analysis of Coding Mechanisms by
Which Neurons Communicate, Learn and Store Information. Entropy, 2009;
11(4): 782-797.
4. Laënnec, R. [updated: Aug 13, 2008; cited: March 23, 2012]. Available from:
http://en.wikipedia.org/wiki/File:Rene-Theophile-Hyacinthe_Laennec_(17811826)_with_stethoscope.jpg.
5. Sterne, J., (eds). The Audible Past: Cultural Origins of Sound Reproduction
Duke University Press; 2003.
6. Guyton, A. and Hall, J., (eds). Textbook of Medical Physiology (9th),
Philadelphia, Pennsylvania: Saunders; 1996.
7. X-rays. The electromagnetic spectrum. [updated: March 27, 2007; cited: May 18,
2012]. Available from: http://science.hq.nasa.gov/kids/imagers/ems/xrays.html.
8. Kevles, B., (eds). Naked to the Bone Medical Imaging in the Twentieth Century.
Basic Books; 1996.
9. Hammond, D.C., What Is Neurofeedback? Journal of Neurotherapy:
Investigations in Neuromodulation, Neurofeedback and Applied Neuroscience,
2006; 10(4): 25 - 36.
10. Vernon, D., Egner, T., Cooper, N., Compton, T., Neilands, C., Sheri, A., and
Gruzelier, J., The effect of training distinct neurofeedback protocols on aspects
of cognitive performance. International Journal of Psychophysiology, 2003;
47(1): 75-85.
11. Lévesque, J., Beauregard, M., and Mensour, B., Effect of neurofeedback training
on the neural substrates of selective attention in children with attentiondeficit/hyperactivity disorder: A functional magnetic resonance imaging study.
Neuroscience Letters, 2006; 394(3): 216-221.
12. Egner, T. and Sterman, M.B., Neurofeedback treatment of epilepsy: from basic
rationale to practical application. Expert Review of Neurotherapeutics, 2006;
6(2): 247-257.

13. Beatty, J., Greenberg, A., Deibler, W., and O’Hanlon, J., Operant control of
occipital theta rhythm affects performance in a radar monitoring task. Science,
1974; 183: 871-873.
14. Rasey, H.W., Lubar, J.E., McIntyre, A., Zoffuto, A.C., and Abbott, P.L., EEG
biofeedback for the enhancement of attentional processing in normal college
students. Journal of Neurotherapy, 1996; 1: 15-21.
15. Angelakis, E., Stathopoulou, S., Frymiare, J., Green, D., Lubar, J., and Kounios,
J., EEG Neurofeedback: A Brief Overview and an Example of Peak Alpha
Frequency Training for Cognitive Enhancement in the Elderly. The Clinical
Neuropsychologist, 2007; 21: 110-129.
16. Hanslmayr, S., Sauseng, P., Doppelmayr, M., Schabus, M., and Klimesch, W.,
Increasing Individual Upper Alpha Power by Neurofeedback Improves Cognitive
Performance in Human Subjects. Applied Psychophysiology and Biofeedback,
2005; 30(1): 1-10.
17. Lubar, J.F., Swartwood, M.O., Swartwood, J.N., and O'Donnell, P.H.,
Evaluation of the effectiveness of EEG neurofeedback training for ADHD in a
clinical setting as measured by changes in T.O.V.A. scores, behavioral ratings,
and WISC-R performance. Applied Psychophysiology and Biofeedback, 1995;
20(1): 83-99.
18. Swingle, P.G., Neurofeedback treatment of pseudoseizure disorder. Biological
psychiatry, 1998; 44(11): 1196-1199.
19. Baehr, E., Rosenfeld, J., and Baehr, R., The clinical use of an alpha asymmetry
protocol in the neurofeedback treatment of depression. Two case studies. Journal
of Neurotherapy, 1998; 2(3): 10-23.
20. Hammond, D., Neurofeedback Treatment of Depression and Anxiety. Journal of
Adult Development, 2005; 12(2): 131-137.
21. Bolea, A.S., Neurofeedback Treatment of Chronic Inpatient Schizophrenia.
Journal of Neurotherapy, 2010; 14(1): 47-54.
22. Putman, J.A. and Othmer, S., Phase Sensitivity of Bipolar EEG Training
Protocols. Journal of Neurotherapy, 2006; 10(1): 73-79.
23. Subramanian, L., Hindle, J.V., Johnston, S., Roberts, M.V., Husain, M., Goebel,
R., and Linden, D., Real-Time Functional Magnetic Resonance Imaging
Neurofeedback for Treatment of Parkinson's Disease. The Journal of
Neuroscience, 2011; 31(45): 16309-16317.
24. Murray, C., Patchick, E., Pettifer, S., Caillette, F., and Howard, T., Immersive
Virtual Reality as a Rehabilitative Technology for Phantom Limb Experience: A
Protocol. CyberPsychology & Behavior, 2006; 9(2): 167-170.
25. Murray, C., Pettifer, S., Howard, T., Patchick, E., Caillette, F., and Kulkarni, J.,
The treatment of phantom limb pain using immersive virtual reality: three case
studies. Disability and Rehabilitation, 2007; 29: 1465-9.
26. Egner, T. and Gruzelier, J.H., EEG Biofeedback of low beta band components:
frequency-specific effects on variables of attention and event-related brain
potentials. Clinical neurophysiology : official journal of the International
Federation of Clinical Neurophysiology, 2004; 115(1): 131-139.
27. Nijboer, F., Furdea, A., Gunst, I., Mellinger, J., McFarland, D.J., Birbaumer, N.,
and Kübler, A., An auditory brain–computer interface (BCI). Journal of
Neuroscience Methods, 2008; 167(1): 43-50.

28. Elgendi, M., Rebsamen, B., Cichocki, A., Vialatte, F., and Dauwels, J. RealTime Wireless Sonification of Brain Signals. Proceedings of The International
Conference on Cognitive Neurodynamics (in press), 2011; Japan.
29. Rutkowski, T., Vialatte, F., Cichocki, A., Mandic, D., and Barros, A., Auditory
Feedback for Brain Computer Interface Management – An EEG Data
Sonification Approach Knowledge-Based Intelligent Information and
Engineering Systems, B. Gabrys, R. Howlett, and L. Jain, Editors. 2006, Springer
Berlin / Heidelberg. p. 1232-1239.
30. Hinterberger, T. and Baier, G., Parametric orchestral sonification of EEG in real
time. IEEE Multimedia, 2005; 12(2): 70-79.
31. Wang, Q., Sourina, O., and Nguyen, M., Fractal dimension based neurofeedback
in serious games. The Visual Computer, 2011; 27(4): 299-309.
32. Qiang, W., Sourina, O., and Minh Khoa, N. EEG-Based "Serious" Games
Design for Medical Applications. Proceedings of the International Conference on
Cyberworlds, 2010.
33. Sourina, O., Liu, Y., Wang, Q., and Nguyen, M., EEG-Based Personalized
Digital Experience
Universal Access in Human-Computer Interaction. Users Diversity, C. Stephanidis,
Editor. 2011, Springer Berlin / Heidelberg. p. 591-599.
34. Cho, B.H., Lee, J.M., Ku, J.H., Jang, D.P., Kim, J.S., Kim, I.Y., Lee, J.H., and
Kim, S.I. Attention Enhancement System using virtual reality and EEG
biofeedback. Proceedings of IEEE Virtual Reality, 2002.
35. Bayliss, J.D. and Ballard, D.H., A virtual reality testbed for brain-computer
interface research. IEEE Transactions on Rehabilitation Engineering, 2000;
8(2): 188-190.
36. Linden, M., Habib, T., and Radojevic, V., A controlled study of the effects of
EEG biofeedback on cognition and behavior of children with attention deficit
disorder and learning disabilities. Applied Psychophysiology and Biofeedback,
1996; 21(1): 35-49.
37. Arns, M., de Ridder, S., Strehl, U., Breteler, M., and Coenen, A., Efficacy of
neurofeedback treatment in ADHD: the effects on inattention, impulsivity and
hyperactivity: a meta-analysis. Clin. EEG Neurosci, 2009; 40(3): 180-9.
38. Saxby, E. and Peniston, E.G., Alpha-theta brainwave neurofeedback training: An
effective treatment for male and female alcoholics with depressive symptoms.
Journal of Clinical Psychology, 1995; 51(5): 685-693.
39. Prasad, G., Herman, P., Coyle, D., McDonough, S., and Crosbie, J., Applying a
brain-computer interface to support motor imagery practice in people with
stroke for upper limb recovery: a feasibility study. Journal of NeuroEngineering
and Rehabilitation, 2010; 7(1): 60.
40. Scolnick, B., Effects of electroencephalogram biofeedback with Asperger's
syndrome. International Journal of Rehabilitation Research, 2005; 28(2): 159163.
41. Kouijzer, M.E.J., van Schie, H.T., de Moor, J.M.H., Gerrits, B.J.L., and
Buitelaar, J.K., Neurofeedback treatment in autism. Preliminary findings in
behavioral, cognitive, and neurophysiological functioning. Research in Autism
Spectrum Disorders; 4(3): 386-399.

42. Sokhadze, T.M., Cannon, R.L., and Trudeau, D.L., EEG Biofeedback as a
Treatment for Substance Use Disorders: Review, Rating of Efficacy and
Recommendations for Further Research. Journal of Neurotherapy: Investigations
in Neuromodulation, Neurofeedback and Applied Neuroscience, 2008; 12(1): 5 43.
43. Weiskopf, N., Mathiak, K., Bock, S.W., Scharnowski, F., Veit, R., Grodd, W.,
Goebel, R., and Birbaumer, N., Principles of a brain-computer interface (BCI)
based on real-time functional magnetic resonance imaging (fMRI). Biomedical
Engineering, IEEE Transactions on, 2004; 51(6): 966-970.
44. Weiskopf, N., Scharnowski, F., Veit, R., Goebel, R., Birbaumer, N., and
Mathiak, K., Self-regulation of local brain activity using real-time functional
magnetic resonance imaging (fMRI). Journal of Physiology-Paris, 2004; 98(4–
6): 357-373.
45. Zotev, V., Krueger, F., Phillips, R., Alvarez, R.P., Simmons, W.K., Bellgowan,
P., Drevets, W.C., and Bodurka, J., Self-Regulation of Amygdala Activation
Using Real-Time fMRI Neurofeedback. PLoS ONE, 2011; 6(9): e24522.
46. Haller, S., Birbaumer, N., and Veit, R., Real-time fMRI feedback training may
improve chronic tinnitus. European Radiology, 2010; 20(3): 696-703.
47. Sitaram, R., Caria, A., Veit, R., Gaber, T., Rota, G., Kuebler, A., and Birbaumer,
N., fMRI brain-computer interface: a tool for neuroscientific research and
treatment. Intell. Neuroscience, 2007; 2007: 1-10.
48. Yoo, S.-S., Fairneny, T., Chen, N.-K., Choo, S.-E., Panych, L.P., Park, H., Lee,
S.-Y., and Jolesz, F.A., Brain-computer interface using fMRI: spatial navigation
by thoughts. NeuroReport, 2004; 15(10): 1591-1595.
49. Kramer, G., (eds). Auditory Display: Sonification, Audification, and Auditory
Interfaces. Santa Fe Institute Studies in the Sciences of Complexity Westview
Press; 1994.
50. Hermann, T., Hunt, A., and Neuhoff, J., (eds). The Sonification Handbook (1st
edition), Berlin: Logos Publishing House; 2011.
51. Keller, J. Sonification for Beginners. [updated: 25 Jan, 2010; cited: 25 March,
2012].
Available
from:
http://cse.ssl.berkeley.edu/stereo_solarwind/sounds_programs.html.
52. Berger, H., Über Das Elektrenkephalogramm Des Menschen. Archiv für
Psychiatrie und Nervenkrankheiten, 1929; 87: 527-570.
53. Berger, H., On the Electroencephalogram of Man. Electroencephalography and
Clinical Neurophysiology, 1969: 28:133.
54. Wijdicks, E.F.M., The Diagnosis of Brain Death. New England Journal of
Medicine, 2001; 344(16): 1215-1221.
55. The Electroencephalogram in the Determination of Brain Death. New England
Journal of Medicine, 1979; 300(9): 502-502.
56. Gütling, E., Gonser, A., Imhof, H.-G., and Landis, T., EEG reactivity in the
prognosis of severe head injury. Neurology, 1995; 45(5): 915-918.
57. Thatcher, R.W., Biver, C., McAlaster, R., Camacho, M., and Salazar, A.,
Biophysical Linkage between MRI and EEG Amplitude in Closed Head Injury.
NeuroImage, 1998; 7(4): 352-367.
58. Jordan, K.G., Emergency EEG and Continuous EEG Monitoring in Acute
Ischemic Stroke. Journal of Clinical Neurophysiology, 2004; 21(5): 341-352.

59. Jackel, R.A. and Harner, R.N., Computed EEG topography in acute stroke.
Neurophysiologie Clinique/Clinical Neurophysiology, 1989; 19(3): 185-197.
60. Vespa, P.M., Nuwer, M.R., Juhász, C., Alexander, M., Nenov, V., Martin, N.,
and Becker, D.P., Early detection of vasospasm after acute subarachnoid
hemorrhage using continuous EEG ICU monitoring. Electroencephalography
and clinical Neurophysiology, 1997; 103(6): 607-615.
61. Claassen, J., Mayer, S.A., and Hirsch, L.J., Continuous EEG Monitoring in
Patients With Subarachnoid Hemorrhage. Journal of Clinical Neurophysiology,
2005; 22(2): 92-98.
62. Dauwels, J., Vialatte, F., and Cichocki, A., Diagnosis of alzheimers disease from
EEG signals: Where are we standing? Current Alzheimer Research, 2010; 7:
487-505.
63. Dauwels, J., Vialatte, F., Musha, T., and Cichocki, A., A comparative study of
synchrony measures for the early diagnosis of alzheimer’s disease based on
EEG. NeuroImage, 2010; 49: 668-693.
64. Dauwels, J., Srinivasan, K., Reddy, R., Musha, T., Vialatte, F., Latchoumane, C.,
Jeong, J., and Cichocki, A., Slowing and loss of complexity in Alzheimer’s EEG:
Two sides of the same coin? International Journal of Alzheimer's Disease,
2011((in press)).
65. Vialatte, F.-B., Solé-Casals, J., Maurice, M., Latchoumane, C., Hudson, N.,
Wimalaratna, S., Jeong, J., and Cichocki, A., Improving the Quality of EEG Data
in Patients with Alzheimer’s Disease Using ICA. Advances in Neuro-Information
Processing , Lecture Notes in Computer Science, 2009; 5507/2009: 979-986.
66. Karameh, F.N. and Dahleh, M.A. Automated classification of EEG signals in
brain tumor diagnostics. Proceedings of American Control Conference, 2000.
Proceedings of the 2000, 2000.
67. Silipo, R., Deco, G., and Bartsch, H., Brain tumor classification based on EEG
hidden dynamics. Intelligent Data Analysis, 1999; 3(4): 287-306.
68. Benca, R., Obermeyer, W., Larson, C., Yun, B., Dolski, I., Kleist, K., Weber, S.,
and Davidson, R., EEG alpha power and alpha power asymmetry in sleep and
wakefulness. Psychophysiology, 1999; 36(04): 430-436.
69. Merica, H., Blois, R., and Gaillard, J.M., Spectral characteristics of sleep EEG
in chronic insomnia. European Journal of Neuroscience, 1998; 10(5): 18261834.
70. Kannathal, N., Choo, M.L., Acharya, U.R., and Sadasivan, P.K., Entropies for
detection of epilepsy in EEG. Computer Methods and Programs in Biomedicine,
2005; 80(3): 187-194.
71. Jerger, K.K., Netoff, T.I., Francis, J.T., Sauer, T., Pecora, L., Weinstein, S.L.,
and Schiff, S.J., Early Seizure Detection. Journal of Clinical Neurophysiology,
2001; 18(3): 259-268.
72. Marshall, P.J., Bar-Haim, Y., and Fox, N.A., Development of the EEG from 5
months to 4 years of age. Clinical neurophysiology : official journal of the
International Federation of Clinical Neurophysiology, 2002; 113(8): 1199-1208.
73. Meyer-Lindenberg, A., The evolution of complexity in human brain
development: an EEG study. Electroencephalography and clinical
Neurophysiology, 1996; 99(5): 405-411.

74. Zhang, X.S., Roy, R.J., and Jensen, E.W., EEG complexity as a measure of depth
of anesthesia for patients. Biomedical Engineering, IEEE Transactions on, 2001;
48(12): 1424-1433.
75. Salinsky, M.C., Oken, B.S., and Morehead, L., Intraindividual analysis of
antiepileptic drug effects on EEG background rhythms. Electroencephalography
and clinical Neurophysiology, 1994; 90(3): 186-193.
76. Bruhn, J., Röpcke, H., and Hoeft, A., Approximate Entropy as an
Electroencephalographic Measure of Anesthetic Drug Effect during Desflurane
Anesthesia. Anesthesiology, 2000; 92(3): 715-726.
77. Vialatte, F., Musha, T., and Cichocki, A., Sparse Bump Sonification: a New Tool
for Multichannel EEG Diagnosis of Brain Disorders. Artificial Intelligence in
Medicine, 2010.
78. Vialatte, F. and Cichocki, A., Sparse Bump Sonification: A New Tool for
Multichannel EEG Diagnosis of Mental Disorders; Application to the Detection
of the Early Stage of Alzheimer’s Disease. Lecture Notes in Computer Science,
Neural Information Processing, Springer, 2006; LNCS-4234(92-101).
79. Rutkowski, T., Vialatte, F., Cichocki, A., Mandic, D., and Barros, A., Auditory
Feedback for Brain Computer Interface Management - An EEG Data
Sonification Approach. Lecture Notes in Computer Science, Knowledge-Based
Intelligent Information and Engineering Systems, Springer, 2006; LNCS-4253:
1232-1239.
80. Hinterberger, T. Orchestral Sonification of Brain Signals and its Applications to
Brain-Computer Interfaces and Performing Arts. Proceedings of the 2nd
International Workshop on Interactive Sonification, 2007; York, UK.
81. Nunez, P. and Srinivasan, R., (eds). Electric fields of the brain Oxford
University press; 2006.
82. Vialatte, F., Cichocki, A., Dreyfus, G., Musha, T., Rutkowski, T.M., and
Gervais, R., Blind Source Separation and Sparse Bump Modelling of Time
Frequency Representation of Eeg Signals: New Tools for Early Detection of
Alzheimer's Disease, in IEEE Workshop on Machine Learning for Signal
Processing, 2005.
83. Lucier, A., Statement on: music for solo performer. Biofeedback and the Arts:
Results of Early Experiments 1967; (Vancouver, Canada: Aesthetic Research
Centre of Canada).
84. Rosenboom, D., Method for Producing Sounds or Light Flashes with Alpha
Brain Waves for Artistic Purposes. Leonardo, 1972; 5(2): 141-145.
85. Teitelbaum , R., In tune: Some early experiments in biofeedback music (19661974). Aesthetic Research Center of Canada Publications, 1976.
86. Rosenboom, D., Computer Music Journal. 1990; 14(1): 48-66.
87. Miranda, E.R., Sharman, K., Kilborn, K.a., and Duncan, A., On Harnessing the
Electroencephalogram for the Musical Braincap. Computer Music Journal,
2003; 27(2): 80-102.
88. Baier, G. and Hermann, T. The Sonification of Rhythms in Human
Electroencephalogram. Proceedings of ICAD 2004, 2004; Sydney.
89. NeuroFocus. [updated: 2012; cited: April 23, 2012]. Available from:
http://www.neurofocus.com/.

90. Biopac. [updated: 2012; cited: 15 March, 2012]. Available from:
http://www.biopac.com/researchApplications.asp?Aid=23&AF=437&Level=3.
91. EmotivSystems. Emotiv - brain computer interface technology. [updated: April
21, 2012 cited: April 23, 2012]. Available from: http://emotiv.com.
92. Imec. [updated: Feb 8, 2011; cited: April 23, 2012]. Available from:
http://www2.imec.be/be_en/press/imec-news/imecEEGMDMWest.html.
93. Neurobelt. Medical Computer Systems. [updated: March 30, 2011; cited: April
20, 2012]. Available from: http://www.mks.ru/eng/Products/EEG/Neurobelt/.
94. BCI2000. General-Purpose System for Brain Computer Interface. [updated:
2010;
cited:
2012].
Available
from:
http://www.bci2000.org/BCI2000/Home.html.
95. Goh, C., Ifeachor, E., Henderson, G., Latchoumane, C., Jeong, J., Bigan, C.,
Besleaga, M., Hudson, N., Capotosto, P., and Wimalaratna, S., Characterisation
of EEG at different stages of Alzheimer’s disease (AD). Clinical
Neurophysiology, 2006; 117: 138-139.
96. Henderson, G., Ifeachor, E., Hudson, N., Goh, C., Outram, N., Wimalaratna, S.,
Del Percio, C., and Vecchio, F., Development and assessment of methods for
detecting dementia using the human electroencephalogram. IEEE Transaction on
Biomedical Engineering, 2006; 53: 1557-1568.
97. Dauwels, J., Vialatte, F., Latchoumane, C., Jeong, J., and Cichocki, A., EEG
synchrony analysis for early diagnosis of alzheimer’s disease: A study with
several synchrony measures and EEG data sets, in 31st Annual International
Conference of the IEEE EMBS, 2009: Minneapolis, Minnesota, USA.
98. Harrison, J. pyPortMidi. [updated: March 15, 2010; cited: April 23, 2011].
Available from: http://alumni.media.mit.edu/~harrison/code.html.
99. numpy. [updated: 2011; cited: Feb 21, 2011]. Available from:
http://numpy.scipy.org/.
100. SynthFont. SynthFont and other tools for Midi and SoundFonts. [updated: March
22, 2012; cited: April 23, 2012]. Available from: http://www.synthfont.com/.
101. MIDI-OX. [updated: Jan 29, 2011; cited: Feb 18, 2011]. Available from:
http://www.midiox.com/.
102. Baier, G., Hermann, T., and Stephani, U., Event-based sonification of EEG
rhythms in real time. Clinical Neurophysiology, 2007; 118(6): 1377-1386.
103. Brown, A. SoundCipher: A music and sound library for Processing. [updated:
Feb 18, 2010; cited: May 14, 2012]. Available from:
104. Hanke, M., Halchenko, Y.O., Sederberg, P.B., Hanson, S.J., Haxby, J.V., and
Pollmann, S., PyMVPA: A Python toolbox for multivariate pattern analysis of
fMRI data. Neuroinformatics, 2009(7): 37-53.
105. Dede, C., Immersive Interfaces for Engagement and Learning. Science, 2009;
323(5910): 66-69.
106. Kizony, R., Katz, N., and Weiss, P.L., Adapting an immersive virtual reality
system for rehabilitation. The Journal of Visualization and Computer Animation,
2003; 14(5): 261-268.
107. Gorini, A., Gaggioli, A., Vigna, C., and Riva, G., A second life for eHealth:
prospects for the use of 3-D virtual worlds in clinical psychology. Journal of
Medical Internet Research, 2008; 10(3).

108. Hoffman, H., Richards, T., Bills, A., Oosstrom, T., Magula, J., Seibel, E., and
Sharar, S., Using fMRI to study the neural correlates of virtual reality analgesia.
CNS Spectr, 2006; 11: 45-51.
109. Leeb, R., Friedman, D., ller-Putz, G.R., Scherer, R., Slater, M., and Pfurtscheller,
G., Self-Paced (Asynchronous) BCI Control of a Wheelchair in Virtual
Environments: A Case Study with a Tetraplegic. Computational Intelligence and
Neuroscience, 2007; 2007.
110. Leeb, R., Lee, F., Keinrath, C., Scherer, R., Bischof, H., and Pfurtscheller, G.,
Brain-Computer Communication: Motivation, Aim, and Impact of Exploring a
Virtual Apartment. IEEE Transactions on Neural Systems and Rehabilitation
Engineering, 2007; 15(4): 473-482.
111. Ros, T., Munneke, M.A.M., Ruge, D., Gruzelier, J.H., and Rothwell, J.C.,
Endogenous control of waking brain rhythms induces neuroplasticity in humans.
European Journal of Neuroscience, 2010; 31(4): 770-778.
112. Gruzelier, J., Inoue, A., Smart, R., Steed, A., and Steffert, T., Acting
performance and flow state enhanced with sensory-motor rhythm neurofeedback
comparing ecologically valid immersive VR and training screen scenarios.
Neuroscience Letters, 2010; 480(2): 112-116.
113. Fry, B. and Reas, C. Processing. [updated: May 15, 2011; cited: May 2, 2012].
Available from: http://processing.org/.
114. McMillan, G. Socket Programming HOWTO. [updated: Jan 3, 2011; cited: May
2, 2012]. Available from: http://docs.python.org/howto/sockets.html.
115. Fry, B. and Reas, C. Server. [updated: Aug 30, 2010; cited: May 2, 2012].
Available from: http://processing.org/reference/libraries/net/Server.html.
116. Glueck, B. and Stroebel, C., Biofeedback and meditation in the treatment of
psychiatric illnesses. Comprehensive Psychiatry, 1975; 16(4).
117. deCharms, R.C., Maeda, F., Glover, G.H., Ludlow, D., Pauly, J.M., Soneji, D.,
Gabrieli, J.D.E., and Mackey, S.C., Control over brain activation and pain
learned by using real-time functional MRI. Proceedings of the National
Academy of Sciences of the United States of America, 2005; 102(51): 1862618631.
118. Marti, P., Bacigalupo, M., Giusti, L., Mennecozzi, C., and Shibata, T. Socially
Assistive Robotics in the Treatment of Behavioural and Psychological Symptoms
of Dementia. Proceedings of The First IEEE/RAS-EMBS International
Conference on Biomedical Robotics and Biomechatronics (BioRob 2006), 2006.

