Accessing Tele-Services using a
Hybrid BCI Approach
Chris Brennan1 , Paul McCullagh1 , Gaye Lightbody1 , Leo Galway1 ,
Diana Feuser2 , José Luis González3 , and Suzanne Martin4
1

2

Computer Science Research Institute, Ulster University, UK, BT37 0QB
{brennan-c15}@email.ulster.ac.uk; {p.j.mccullagh}@ulster.ac.uk
http://www.compeng.ulster.ac.uk/csri.php
Institute of Automation, University of Bremen, Otto-Hahn-Allee 1, 28359 Bremen,
Germany {diana.feuser}@uni-bremen.de
3
Telefonica de España, Ronda de la Comunicación, s/n, 28050 Madrid, Spain
4
Institute of Nursing and Health Research, Ulster University, UK, BT37 0QB

Abstract. Brain Computer Interface (BCI) technology has achieved
limited success outside of laboratory conditions. This technology is hindered by practical considerations of set up, lack of robustness and low Information Transfer Rate (ITR). There are two interfaces in a BCI system:
the brain’s interface with the computer and the computer-environment
interface, which provides access to applications for the user. Three user
services were implemented: control of the smart home, entertainment
and communication. These may be accessed through a graphical user interface controlled by a BCI. The paper contrasts the performance of an
SSVEP based system with a hybrid BCI comprising eye gaze and muscle response (measured at the scalp). The hybrid developed utilizes the
EPOC for recording electrical potential and an EyeTribe gaze tracker;
these can be combined to provide more robust interaction with applications. Average ITR for the eye tracker and hybrid approaches (190-200
bpm) are higher than for our SSVEP approach (approx. 15 bpm), for the
same applications. The poor performance of our SSVEP system was due
to the temporal duration of the stimulation (7s) and partly because not
all participants could achieve an accuracy of greater than 50%. The current challenge is the replacement of the scalp recorded muscle component
with a reliable user modifiable EEG measure.
Keywords: Applications, Brain-Computer Interface (BCI), Communication, Control, Entertainment, Eye-tracking, hBCI, Hybrid

1

Introduction

The Brain-Computer Interface (BCI) is a paradigm that aims to empower a
user’s capabilities by providing an input modality to the computer that does
not require the involvement of the user’s peripheral nerves and muscles [1]. In
recent years, our increased understanding of human brain function, coupled with
advances in technology, has led to significant advances in BCI technology. In

2

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

particular signal processing algorithms utilize specific patterns in brain activity
across both spatial and temporal domains and translate these into classified
commands for action by the computer. Increasingly as the algorithms become
more complex, the resulting systems may be endowed with more intelligent,
contextual processing, thereby leading to augmentation for the user.
The most commonly employed BCI approach, Electroencephalogram (EEG),
measures electrical activity recorded along the scalp, resulting from ionic current
flows within the neurons of the brain [2]. In this case, the mental state of the user
is probed to generate the brain activity patterns that facilitate control, which
will vary in accordance with the operating protocol [3]. The most commonly
employed approaches include the P300 component of Event-Related Potential
(ERP), Event-Related Desynchronisation/Synchronisation (ERD/ERS), motor
potentials, Steady State Visually Evoked Potentials (SSVEP) and Slow Cortical Potentials (SCP) [4]. Current limitations include slow Information Transfer
Rates (ITR), high inter/intra-subject variability, inconvenient set up procedures,
and the need for carefully controlled environments [5]. Such restrictions adversely
affect those that could potentially benefit most from this technology, individuals
who suffer from disease and traumatic brain injury [6].
To compensate for these limitations and utilize the advantages of different
BCI methods, recent emphasis has been placed upon ‘hybrid’ architectures [7].
One possible architecture may combine BCI approaches (e.g. motor imagery
and sensory stimulation) with separate technologies, such as eye tracking [6].
Hybrid BCIs (hBCI) have the potential to increase transfer rates and classification accuracy, thereby promoting the acceptance and adoption of the technology.
An hBCI, which combines EEG with eye tracking, may employ inputs sequentially or collaboratively, with each modality responsible for distinct aspects of
user input processing or for cooperative command classification [6]. As conventional BCI systems are hindered by an extensive list of limitations, which have
prevented exploitation, a hybrid BCI has the potential to overcome these and
promote widespread deployment. Such advances could see this technology being
utilised in other application areas such as entertainment, neurorehabilitation,
neuroprosthetics, immersive education, and other specialist areas. By improving
performance and usability through hybrid implementations, advances in other
areas can be made, resulting in improvements to the technology overall, which
will cycle back to and improve conventional BCI’s as an assistive technology for
the severely disabled. Current P300 systems, which are used by individuals that
have severe mobility reducing conditions such as ALS, can achieve a spelling rate
of between 5-10 characters per minute. For some people this is their only means
of communication and so, it is of importance to improve upon this technology,
thus improving social inclusion, quality of life, and well-being.
This work supports the hypothesis that a hybrid sensor fusion approach will
facilitate better human-computer interaction because performance and utility
will be improved. An architecture initially developed for the EU FP7 BRAIN
project [8] has been extended to facilitate an hBCI approach. The architecture consisted of a set of tools for interface development and testing, along

Accessing Tele-Services using a hBCI

3

with SSVEP and ERD/ERS recording. An Intuitive Graphical User Interface
(IGUI) tool has been extended to support the combination of eye tracking and
pseudo-EEG control through the use of inexpensive, easily deployed devices. It
is anticipated that the use of commercially available technology will facilitate
better human-computer interaction within the smart environment, permitting
access to a number of computer-mediated services. Three trial user services were
implemented: entertainment, control of the environment, and communication by
iconography.

2

Architecture and Interfaces

In BRAIN the BCI pipeline comprised data acquisition, signal processing (BCI2000), and user interface control using classified UDP packets. It has since been
updated to receive input from multiple input modalities, see Fig 1. The Intuitive User Interface (IUI) is realized as two components; the user interface itself
and a programmatic application interface, known as the Universal Application
Interface (UAI). The IGUI provides a user interface that interacts with a virtual
representation of a domestic environment, providing a BCI channel for domotic
control, entertainment, and communication. The IGUI, implemented using the
Java programming language, permits operating system independent deployment
and provides an interactive menu system that has been specifically structured
to provide four-way directional control. Each menu item controls an event that
can be triggered within the smart environment.

Fig. 1. The pipeline components of the BCI and ET collaborative system for domotic,
entertainment and communication control.

The UAI interacts with the BCI device through the IGUI for the purpose of
providing access to entertainment and communication packages as well as domotic device control. Device interaction may implement a relevant interface such
as: X10, Universal Plug and Play (UPnP). UPnP was selected as the communication protocol used in middleware layer to provide interoperable specification
with common protocols to other technologies, offering the possibility of wrapping other technologies. The architecture offers pervasive peer-to-peer network

4

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

connectivity of PCs, intelligent appliances, and wireless devices. The UPnP architecture is a distributed, open networking architecture that uses TCP/IP and
HTTP to control data transfer among networked devices in the home. The design aim was to provide an easily extensible interface that could support various
input modalities (BCI paradigms, eye tracking, etc.). The UAI is the second
component of the IUI and is responsible for providing a generic platform that
forms the bridge between the BCI platform, IGUI, and applications and devices.
This interface also facilitates leisure activities such as painting with the users
brain waves [9].

3

Control of the IGUI

The first phase of experimentation incorporated a series of preliminary tests,
which aimed to evaluate the IGUIs performance in terms of quality assurance.
In the first experiment, the IGUI was controlled by features classified from EEG;
recorded using the TMSi amplifier, Porti7, and EasyCap combination. Both
devices are research or medical-grade products which, when coupled together,
provide high quality EEG acquisition. The sampling frequency for the Porti7
amplifier was set to 256Hz and the electrodes were placed at positions PZ, PO3,
PO4, OZ, O9, O10 and referenced at site AFZ.
For the BCI paradigm, SSVEP was utilized as the control mechanism and
calibration was handled by the BCI-Wizard, which facilitates both frequency and
threshold calibration. The calculation of a threshold was computed in BCI2000
for each calibrated or selected frequency. As a quality criterion, an Area Under ROC Curve (AUC) was calculated and if the AUC exceeded a predefined
threshold, the calibration was regarded as suitable for further use. The SSVEP
signal processing approach takes advantage of the Minimum Energy Combination (MEC) method to estimate power changes in the acquired EEG signals
and classify the signal with the highest signal-to-noise ratio (SNR) if the predetermined threshold was exceeded. This algorithm, implemented in C++ as a
BCI2000 signal processing module, commissions a spatial filter that readjusts
the input channels in order to minimize interference. As a result, electrodes
that transmitted poor signals were ignored. Moreover, the combination matrix
is constantly adapted to signal changes over time, which is executed on-line in
BCI2000. Classification was always based on a two second sliding window and
signals were processed every 125 ms.
Stimulation was provided by four Light Emitting Diodes (LEDs) mounted
around the outside of a visual interface. Each LED corresponded to a specific
direction, left, up, right, down and flickered at 13Hz, 14Hz, 15Hz and 16Hz
respectively. The user was required to make a cognitive link between a particular
direction and the corresponding LED. For example, to move left the user had to
focus on the LED located to the left of the display. This method of control was
restrictive, due to the location of the LEDs. When navigating around the menu,
the user did not have a full view of the changes they were affecting. In addition to
the physical constraints, some users found this as a cognitive challenge since the

Accessing Tele-Services using a hBCI

5

directional command arrows and manipulation of menu items caused confusion
and difficulties when attempting to navigate to a specific item. Nevertheless,
for a control group of five users (N=5), the mean accuracy achieved was 69.6%
±32.9%. However, as a method for measuring the minimum level of tolerated
accuracy is yet to be established within the BCI community, we contrasted the
achieved results by conducting a BCI accessibility assessment, which provided
an indication of the minimum level of accuracy for each user. The individual
accuracy values are further expanded in Table 1.

Table 1. Five subjects participated in the BCI accessibility assessment and then completed the command operation assessment. The results, shown in this table, contrast
user specific accuracy levels and the levels achieved using a BCI application. Desirable
accuracy is the minimum level of control, which a subject deems to be satisfactory and
the Acceptable Accuracy is minimum level of control before being considered unusable [10]
User
1
2
3
4
5

Desirable Acc. %
79
77
77
74
78

Acceptable Acc. % Achieved Acc. %
81
16
81
95
76
62
77
80
77
95

The IGUI was designed to provide four-way directional control, right, left, up
and down. A down command takes the user down a level and the up command
brings the user back up a level. However, when operating the IGUI at the top
level an ‘up’ command exits the application. When using a BCI paradigm, several
unintended exits due to miss-classifications are expected due to this design.
The effects of this can be mitigated by using other input modalities or hybrid
combinations. In order to provide more intuitive navigation, an eye tracking
system was implemented whereby a user could select a menu item by navigating
to it with their gaze and dwelling on it as the selection criteria. Essentially, this
approach attempts to simulate a more natural input method where the user’s
gaze acts as the cursor control and the dwell time simulates the mouse click.
The Tobii X60 eye tracker was incorporated as the input modality, which offers
high performance but is an expensive product (greater than 10K Euro). More
recently, however, the EyeTribe Tracker has been incorporated as an alternative
input modality because it is available at a much lower cost (less than 200 Euro),
and yet retains sufficient accuracy for this application. At first glance the eye
tracking-only system appeared to significantly outperform the BCI-only system
in terms of accuracy. After extensive testing, however, a number of false positives
were detected, especially when a user paused to read or think. This is a recognized
limitation of eye tracking technology and reduces the reliability and of course
usability of the IGUI.

6

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

The next iteration of the IGUI was developed to replace the dwell time component with a Brain-Neuronal Computer Interaction (BNCI) approach. BNCI
includes devices that monitor additional physiological signals; not only from
signals acquired directly from the brain, but from other measures of nervous
activity, such as Electrooculography (EoG), Electromyography (EMG) or heart
rate. By implementing the Emotiv EPOC BCI device to acquire EEG activity,
it was possible to replace the dwell time with a teeth clench/grind component;
as a proxy for an EEG feature. The EPOC was employed to extract the representative features from scalp locations AF3, AF4, F3, F4, F8, F7, FC5, FC6,
T7, T8, P7, P8, O1, and O2. The following sections of the paper cover details
of the protocol and provide an analysis of the results for each approach.

4

Methods

For each experiment the same stimulation was employed, which consisted of
three tasks to be completed by operation of the IGUI. The first represents a
domotic task; the second an entertainment task and the third a communication
task. The task details are as follows:
Task 1 domotic control: turn on the lamp (Critical Path = 13). For
successful completion, each user was required to navigate to the Dining Room
icon, select it and then navigate to the Lamp icon, select it and return to the
Back Garden icon. The shortest series of sequential operations (i.e. the least
commands for successful completion) is: Right-Right-Right-Right-Down-RightRight-Down(Lamp on)-Up-Left-Left-Left-Left.
Task 2 entertainment: multimedia control (Critical Path = 25). For
successful completion, each user was required to navigate to the Living Room
icon, select it, navigate to the Home Media icon, select it, navigate to the Home
Cinema icon, select it, navigate to a video icon, select it (play video), return
to the Home Media icon, navigate to the Controls icon, select it, navigate to the
Stop icon, select it (stop video) and return to the Back Garden icon at the top
level. The shortest series of sequential operations is: Left-Left-Left-Down-LeftDown-Right-Right-Down-Right-Right-Right-Down(Play)-Up-Left-Left-Down-Right-Down(Stop)-Up-Up-Up-Right-Right-Right.
Task 3 communication: indicate hunger (Critical Path = 7). For
successful completion, each user was required to navigate to the Talk icon, select
it, navigate to the Eat icon, select it and return to the Back Garden icon. The
shortest series of sequential operations is: Left-Down-Left-Left-Down(Eat)-UpRight.
The sequence of commands enacted for Task 1 (a total of 13 individual commands), for a representative subject, is represented in Table 2. Note that the
IGUI must be able to deal with repeated correct commands, due to the variable
nature of the subject interaction. It does this by dampening the responsiveness
of the interaction. Figure 2 shows a subject enacting Task 1 using the hybrid approach. The IGUI is being operated using the Emotiv EPOC and the EyeTribe
Tracker as collaborative input modalities.

Accessing Tele-Services using a hBCI

7

Table 2. Sequence of commands enacted for Task 1 for a representative subject. The
subject successfully navigated around the interface and turned on the light. They have
completed a total of 13 commands but some commands were repeated and disregarded
to slow the responsiveness of the application.
Task #1 started
3 Back Garden 0/0
3 Bathroom 1/1
3 Bathroom 1/1
3 Bedroom 1 2/2
3 Bedroom 1 2/2
3 Bedroom 2 3/3
3 Bedroom 2 3/3
3 Dining Room 4/4

3 Dining Room 4/4
4 Dining Room 4/4
4 Window 5/5
3 Window 5/5
3 Door 6/6
3 Door 6/6
3 Lamp 7/7
11 Lamp 7/7

4
4
4
1
2
2
1
2

Lamp 7/7
1 Dining Room 9/9
Lamp 8/8
1 Bedroom 2 10/10
Lamp 8/8
1 Bedroom 2 10/10
Lamp 8/8
1 Bedroom 1 11/11
Lamp 8/8
1 Bedroom 1 11/11
Dining Room 9/9 1 Bathroom 12/12
Dining Room 9/9 1 Bathroom 12/12
Dining Room 9/9 1 Back Garden 13/13

N.B. Task 1 finished with 13 successful commands and 0 invalid commands as repeated
commands were disregarded (e.g. command one was to select the bathroom icon, which
was activated twice but only executed once). The subject achieved an accuracy 100%
with an average ITR of 16.96 bits/min

5

Results

In BCI it is common to measure performance by calculating ITR in bits/min,
which takes into account values for both accuracy and time. According to the
literature, traditional BCI systems generally achieved an ITR between 10-25
bits/min [1] whereas conventional input devices such as a mouse and keyboard
have been known to achieve an ITR in excess of 300 bits/min [11]. More recent
SSVEP-based BCI systems, which use gel electrodes, have managed to achieve
an ITR of at least 60 bits/min, [12]. ITR as defined in [13] is shown below:
IT R = (log2 M + P log2 P + (1 − P )log2 [(1 − P )/(M − 1)]) ∗ (60/T )

(1)

where M is the number of choices, P is the accuracy of target detection, and T
(seconds/selection) is the average time for a selection. This metric can be used to
evaluate the performance of the input modalities and compare different experiments. In the BRAIN project motor imagery and SSVEP were used to navigate
the IGUI. Kus et al. [14] computed an ITR of 12.77 bits/min for a 3-way motor
imagery (ERD/ERS) selection task. SSVEP data was then recorded from a control group (N=23), which incorporated the TMSi amplifier and EasyCap for signal acquisition, the SSVEP paradigm as the control mechanism, screen-mounted
LEDs for stimulation and the same three tasks, domotic control, entertainment,
and communication by iconography. Table 3 presents the results of the data
analysis. An important point to note is that when operating the IGUI at the top
level an ‘up’ command exited the application and as such, several unintended
exits due to miss-classifications were categorized as inconclusive. In addition,
the ‘-’ symbol in the table indicates that a subject attempted to perform the

8

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

Fig. 2. A subject enacting Task 1 and operating the IGUI while using the EyeTribe
Tracker and Emotiv EPOC as collaborative input modalities. The subject has successfully executed a right command thus scrolling the IGUIs menu from the Back Garden
icon to the Bathroom icon.

task but could not complete it due to poor performance. Nonetheless, the mean
accuracy for all participants that completed all three tasks was 79.2% ±14.4%
with a mean ITR of 15.23 ±7.92 bits/min.
As mentioned previously, an eye tracking component was implemented to provide better navigational control of the IGUI. The third phase of experimentation
focused on a control group (N=10). In this experiment, the SSVEP paradigm
was replaced with an eye tracking-only approach. The Tobii X60 Eye Tracker
was incorporated as the input modality. When operating the IGUI at the top
level an ‘up’ command still exited the application. As the eye tracker produced a
number of false positives, several unintended selections were experienced, which
triggered an unintended exit or incorrect command. However, in this case if an
unintended exit occurred, the interaction was paused and the experimenter provided a manual override to restart the experiment at the point in which the
mistake occurred. If an unintended command occurred, which did not exit the
application, the user was required to rectify the error. These values were marked
as false positives and were factored in when calculating the accuracy and ITR.
Table 4 presents the results using the Tobii X60 eye tracker as the only input
modality. Successful completion of all three tasks resulted in a mean accuracy
of 78.39% ±10.21% and a mean ITR of 194.15 ±13.34 bits/min.
The fourth phase of experimentation focused on a further control group of
10 participants. In this experiment, the Tobii X60 based control mechanism
was substituted for an EyeTribe Tracker based control method. Similar to the
previous experiment, the EyeTribe Tracker was the only input modality and the
same protocol was employed once more. Again a number of false positives were
detected, which triggered an incorrect command that needed to be rectified
by the user or in the case of an unintended exit, by the experimenter. If an
unintended exit was experienced the interaction time was paused and a manual

Accessing Tele-Services using a hBCI

9

Table 3. Accuracy and Information Transfer Rate from SSVEP data collected for 23
healthy participants
Subject Domotic
ACC
ITR
(%)
(bpm)
T1
INCONCLUSIVE
T2
INCONCLUSIVE
T3
INCONCLUSIVE
T4
INCONCLUSIVE
T5
INCONCLUSIVE
T6
73.33
8.95
T7
T8
ERROR
T9
INCONCLUSIVE
T10
T11
68.80
9.89
T12
52.78
5.78
T13
T14
83.33
25.95
T15
70.59
5.53
T16
INCONCLUSIVE
T17
100
36.23
T18
45.16
1.82
T19
T20
90.00
23.70
T21
INCONCLUSIVE
T22
86.67
8.37
T23
100
28.44
Mean: 77.00
15.47
SD(±): 18.51
11.98

Entertainment
ACC
ITR
(%)
(bpm)
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
60.47
6.03
ERROR
INCONCLUSIVE
90.00
21.15
INCONCLUSIVE
93.33
18.57
65.38
6.56
91.67
17.23
INCONCLUSIVE
68.97
6.64
78.30
12.70
14.92
7.00

Communication
ACC
ITR
(%)
(bpm)
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
66.67
9.45
ERROR
INCONCLUSIVE
66.67
10.38
100
30.38
INCONCLUSIVE
88.89
21.69
70.00
8.10
83.33
24.06
INCONCLUSIVE
100
18.68
82.22
17.53
14.78
8.48

All tasks
ACC
ITR
(%)
(bpm)
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
INCONCLUSIVE
66.82
8.14
ERROR
INCONCLUSIVE
INSUFFICIENT
INSUFFICIENT
91.11
20.06
INSUFFICIENT
INCONCLUSIVE
94.07
25.5
60.18
5.49
88.33
21.66
INCONCLUSIVE
INSUFFICIENT
89.66
17.92
79.17
15.23
14.38
7.92

override provided to return to the point when the unintended exit occurred. All
unintended commands were counted as false positives and commands to rectify
a mistake were counted as true positives, since they were intended selections.
These values were factored in when calculating accuracy and ITR. Completion
of all three tasks for all users equates to a mean accuracy of 93.81% ±4.59% and
a mean ITR of 164.97 ±22.88 bits/min.
The final phase of experimentation included the same control group as the
previous experiment (N=10), which used the EyeTribe Tracker only. In this
phase, the Emotiv EPOC was implemented to function collaboratively with the
EyeTribe Tracker thus providing multimodal interaction. The same protocol was
used once more and a significantly reduced number of false positives were detected. As a result there were no unintended exits experienced by any participant
at any time. However, unintended commands still needed to be rectified by the
user. In addition, the hybrid system also experienced a rate of false negatives,

10

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

Table 4. Accuracy and Information Transfer Rate data collected for 10 healthy participants using the Tobii X60 as the only input modality
Subject Domotic
ACC
ITR
(%)
(bpm)
UU101 36.11
153.43
UU102 81.25
132.32
UU103 100
138.95
UU104 86.67
151.34
UU105 76.47
175.17
UU106 81.25
147.02
UU107 81.25
160.38
UU108 72.22
126.19
UU109 86.67
124.85
UU110 92.86
134.61
Mean: 79.48
144.43
SD(±): 17.18
16.07

Entertainment
ACC
ITR
(%)
(bpm)
58.70
161.20
61.36
185.21
90.00
173.04
87.10
128.01
84.38
178.11
67.50
167.40
58.70
163.17
62.79
163.31
62.79
169.85
69.23
168.45
70.26
165.78
12.20
15.16

Communication
ACC
ITR
(%)
(bpm)
84.62
129.01
84.62
158.78
100
150.13
91.67
132.95
84.62
147.44
78.57
152.38
73.33
157.52
78.57
122.75
100
128.69
78.57
163.66
85.46
144.33
9.15
14.68

All tasks
ACC
ITR
(%)
(bpm)
59.81
189.58
75.74
207.57
96.67
198.56
88.48
166.19
81.81
212.42
75.77
198.71
71.09
199.77
71.19
181.22
83.15
188.91
80.22
198.53
78.39 194.15
10.21
13.34

which were not present during the eye tracking-only experiments. A false negative is more tolerable as it does not trigger an incorrect command. Nevertheless,
such errors are factored in when calculating the accuracy and ITR. For all tasks
the mean accuracy was 96.04% ±3.56% and the mean ITR was 207.41 bits/min.
In our tests, the hybrid system achieved the highest performance in terms of
both accuracy and ITR. It is difficult to determine which eye tracking system
had the better performance since the Tobii X60 system had lower accuracy but a
higher ITR than the EyeTribe system. However, there exists a tradeoff between
accuracy and ITR, which means that a more accurate system may report a lower
ITR since unintended selections need to be rectified by an additional intended
selection; thereby increasing the overall bit rate.

6

Discussion

Previous research studies have focused largely on single paradigms in order to
facilitate control using BCI systems. These studies have accuracy, performance
and reliability limitations [10]. The combination of different input modalities
may improve upon these results, thus providing a greater level of control [10].
For instance, an active BCI paradigm, such as SSVEP and/or Motor Imagery
(MI), can deliver a method of direct control, whilst a passive BCI paradigm, using
error potentials, for example, can potentially provide implicit control [15]. Such
a system could benefit from reinforced decisions while autonomously correcting
false positive no-control states, thereby improving upon the performance that the
use of a single paradigm can offer. There is still a gap between what an acceptable
accuracy for a usable system is and what can be readily achieved outside the
specialist laboratory [16]. Indeed there is a further difference in the performance

Accessing Tele-Services using a hBCI

11

Table 5. Accuracy and Information Transfer Rate data collected for 10 healthy participants using the EyeTribe Tracker as the only input modality
Subject Domotic
ACC
ITR
(%)
(bpm)
UU201 100
65.39
UU202 100
143.43
UU203 93.33
135.50
UU204 76.47
87.59
UU205 93.33
114.42
UU206 93.33
107.27
UU207 87.50
164.74
UU208 88.24
163.45
UU209 100
127.04
UU210 88.24
122.59
Mean: 92.04
123.14
SD(±): 7.36
31.38

Entertainment
ACC
ITR
(%)
(bpm)
96.15
137.11
100
160.74
100
142.37
93.10
173.12
100
132.88
100
146.56
92.59
151.57
87.10
173.09
96.30
136.37
93.10
139.75
95.83
149.36
4.37
14.91

Communication
ACC
ITR
(%)
(bpm)
100
106.27
100
118.77
80.00
131.10
100
87.79
100
74.78
88.89
125.17
88.89
146.04
88.89
138.35
100
96.15
88.89
101.10
93.56
112.55
7.29
23.11

All tasks
ACC
ITR
(%)
(bpm)
98.72
127.55
100
183.89
91.11
170.75
89.86
153.85
97.78
143.71
94.07
159.85
89.66
191.28
88.07
203.85
98.77
157.07
90.08
157.85
93.81 164.97
4.59
22.88

of such systems when comparing users suffering from brain injury and control
subjects [15]. The motivation for this research is that solutions based on a hBCI
architecture could harness features from complementary technologies [6] thereby
closing these gaps.
Both eye tracking systems investigated outperformed pure BCI systems incorporating SSVEP and ERD/S approaches in terms of ITR. However eye tracking
as a technology suffers from false positives when the user is in a ‘no control’
state. Hence there is a need for a hybrid system, where the user can ‘initiate’ a
command in a predictable manner. At present the hBCI uses a teeth clench as
a ‘proxy’ for an EEG controlled switch. This is for pragmatic reasons because
the Emotiv suite comes preloaded with classification algorithms for teeth clench.
Furthermore, eye tracking technology, which can aid severely disabled individuals by providing them with a level of control, also experiences its own set of
limitations. In general, eye tracking uses both infra-red signals and reflected light
to determine the trajectory of a user’s gaze. This technology is known to provide efficient cursor control but suffers from intended selection restrictions, an
area of significant attention in BCI. Earlier research studies have focused on the
frequency of blinks and dwell times as the selection criteria, however, the results
are not yet promising [17], thus increasing the demand for further innovation.
Subsequently, the collaboration of both BCI and eye tracking as input modalities may mitigate the individual limitations of each and provide a more natural
and usable hybrid system. In 2010 Lee et al. [18] proposed a method to combine BCI and Eye Tracking for 3D interaction using a bespoke head-mounted
eye tracking device with attached EEG sensors. According to the experimental
results, the feasibility of the proposed 3D interaction method using eye tracking and BCI was confirmed [18]. The Tobii X60 eye tracker is able to provide

12

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

Table 6. Accuracy and Information Transfer Rate data collected for 10 healthy participants using the EyeTribe Tracker and Emotiv EPOC as collaborative input modalities
Subject Domotic
ACC
ITR
(%)
(bpm)
UU201 92.86
172.98
UU202 100
195.73
UU203 100
158.70
UU204 92.86
183.15
UU205 86.67
120.09
UU206 92.86
163.88
UU207 88.24
180.60
UU208 100
163.11
UU209 100
150.56
UU210 100
167.77
Mean: 95.35
165.66
SD(±): 5.30
20.69

Entertainment
ACC
ITR
(%)
(bpm)
100
204.08
100
153.66
96.15
187.24
89.29
214.30
89.66
132.76
100
192.07
89.29
157.78
100
207.32
96.15
179.75
89.29
199.42
94.98
182.84
5.04
26.70

Communication
ACC
ITR
(%)
(bpm)
77.78
151.20
100
179.52
100
134.64
100
134.64
100
122.40
100
107.71
100
122.40
100
141.73
100
128.23
100
134.64
97.78
135.71
7.03
19.42

All tasks
ACC
ITR
(%)
(bpm)
90.21
225.41
100
202.86
98.72
208.84
94.05
235.99
92.11
156.54
97.62
204.83
92.51
195.36
100
223.49
98.72
199.56
96.43
221.26
96.04 207.41
3.56
22.15

consistent and reliable control, albeit with some false positives. However it is expensive and therefore is only applicable to laboratory based testing. In addition
the Application Programming Interface (API) is not open source and as such,
this limits its value as a research tool. By combining the Eye Tribe eye tracker
with the EPOC (using a teeth clench for selection), the ITR increased to the
performance of the much more expensive eye tracker. We aim for an open source
system that incorporates inexpensive and commercially available eye tracking
and BCI hardware.
The performance improvement, which is evident from the results in table 6
compared with the results in tables 4 and 5, shows that the tested hybrid system
outperforms eye tracking only based approaches in terms of accuracy and ITR.
It also outperforms the SSVEP only system. When questioned all users reported
that the hybrid system provided greater control as it facilitated the ability to
pause, think and even talk in between attempting to execute commands. Unlike
the eye tracking only approaches the hybrid did not issue false positives, which
are intolerable when controlling events in a smart environment. In this case unintentional commands, which execute events such as opening and closing doors,
switching lights on and off, flushing the toilet, opening and closing the garden
gate, interacting with food preparation devices, and security systems could create a state of bedlam. Seemingly, the burden of montage of an EEG devices is
therefore less severe.

7

Conclusion

In this work we have shown the feasibility of using collaborative input modalities
based on a hybrid BCI approach. Both of the devices utilized are cost-effective,

Accessing Tele-Services using a hBCI

13

highly usable and sufficiently accurate, which makes them particularly suited
for deployment outside of laboratory conditions and into ‘the wild’. A benefit
of the hybrid approach lies in the system ability to interact only when desired
by the user. Each user reported full control of the system, i.e. they were able to
pause, think, read and talk in between each task without controlling the system,
unintentionally. We have also shown that the fusion of BNCI and eye tracking technology reaches the performance of conventional BCI systems and eye
tracking-only approaches. Our objective is to investigate how we can replace the
BNCI component with a ‘true BCI’ paradigm, such as SSVEP. It is of importance to maintain high ITRs while maintaining an efficient and robust system
that can be widely deployed. Although such components may not meet the performance of research-grade devices, in our tests the Eye Tribe proved sufficiently
accurate to provide the desired level of control. By utilizing devices such as the
Eye Tribe tracker and the Emotiv EPOC collaboratively, we believe that it is
possible to deploy this system outside of the laboratory.
In the next phase of work, low frequency SSVEP stimulation will be incorporated into the IGUI using on screen pattern reversal positioned at each control
point. Four different frequencies will be implemented in a collaborative manner
with eye tracking. Each selection can only be made if two conditions are true;
the gaze based co-ordinates are in an appropriate area and a frequency within a
pre-defined threshold is detected in the EEG. Signal acquisition, processing, and
classification will be handled within the OpenVibe environment and VRPN will
facilitate fusion between the Eye Tribe Tracker and EEG. At first the EEG signal will be acquired using g.Tec’s g.USBamp which, is also supported within the
current version of OpenVibe. Experimentation will focus on a control group of 20
subjects who will complete the same tasks twice, once using the EPOC and once
using the g.Tec device. A comparative study will contrast the two devices and
provide an indication of the difference in usability and performance. A further
study in conjunction with the Cedar Foundation will test our system on clients
with brain injury to establish it’s utility as a collaborative assistive technology.
A number of challenges may arise: 1) differentiating between frequency bands
2) poor performance by brain injury subjects; and 3) inconsistent frequencies
caused by the refresh rate.

References
1. Wolpaw, J.R., Birbaumer, N., Mcfarland, D.J., Pfurtscheller, G., Vaughan, T.M.:
Brain computer interfaces for communication and control. Clinical Neurophysiology 113(6) (2002) 767–791
2. Coyle, D., Principe, J., Lotte, F., Nijholt, A.: Guest Editorial: Brain/neuronal Computer game interfaces and interaction. IEEE Transactions on Computational
Intelligence and AI in Games 5(2) (June 2013) 77–81
3. Lotte, F., Faller, J., Guger, C.: Combining BCI with Virtual Reality: Towards
New Applications and Improved BCI. In: Proceedings of the 6th International
Conference on Foundations of Digital Games. (2013) 1–24

14

Brennan, McCullagh, Lightbody, Galway, Valbuenna, González & Martin

4. Volosyak, I., Valbuena, D., Malechka, T., Peuscher, J., Gräser, A.: Brain-computer
interface using water-based electrodes. Journal of neural engineering 7(6) (December 2010) 066007
5. McFarland, D.J., Miner, L.a., Vaughan, T.M., Wolpaw, J.R.: Mu and beta rhythm
topographies during motor imagery and actual movements. Brain topography
12(3) (January 2000) 177–86
6. Mccullagh, P., Galway, L., Lightbody, G.: Investigation into a Mixed Hybrid Using
SSVEP and Eye Gaze for Optimising User Interaction within a Virtual Environment. Universal Access in Human-Computer Interaction. Design Methods, Tools,
and Interaction Techniques for eInclusion 8009 (2013) 530–539
7. Allison, B.Z., Brunner, C., Kaiser, V., Müller-Putz, G.R., Neuper, C., Pfurtscheller,
G.: Toward a hybrid brain-computer interface based on imagined movement and
visual attention. Journal of neural engineering 7(2) (April 2010) 26007
8. Thomson, E., Mathews, S., Todd, D., McCullagh, P., Ware, M., Mulvenna, M.,
Martin, S.: Developing brain computer interfaces with rapid automated interfaces
for non experts. Gerontechnology 9(2) (April 2010) 4017
9. Todd, D.a., McCullagh, P.J., Mulvenna, M.D., Lightbody, G.: Investigating the
use of brain-computer interaction to facilitate creativity. Proceedings of the 3rd
Augmented Human International Conference on - AH ’12 (2012) 1–8
10. Ware, M.P., McCullagh, P.J., McRoberts, A., Lightbody, G., Nugent, C., McAllister, G., Mulvenna, M.D., Thomson, E., Martin, S.: Contrasting levels of accuracy
in command interaction sequences for a domestic brain-computer interface using
SSVEP. In: Biomedical Engineering Conference (CIBEC), 2010 5th Cairo International, Ieee (December 2010) 18–21
11. Krepki, R., Blankertz, B., Curio, G., Müller, K.R.: The Berlin Brain-Computer
Interface (BBCI) - Towards a new communication channel for online control in
gaming applications. Multimedia Tools and Applications 33(1) (2007) 73–90
12. Chen, X., Chen, Z., Gao, S., Gao, X.: A high-ITR SSVEP-based BCI speller.
Taylor & Francis: Brain Computer Interfaces 1(3 & 4) (2014) 181–191
13. Gao, S., Wang, Y., Gao, X., Hong, B.: Visual and auditory brain-computer interfaces. IEEE transactions on bio-medical engineering 61(5) (May 2014) 1436–47
14. Kus, R., Valbuena, D., Zygierewicz, J., Malechka, T., Graeser, A., Durka, P.: Asynchronous BCI based on motor imagery with automated calibration and neurofeedback training. IEEE Transactions on Neural Systems and Rehabilitation Engineering 20 (2012) 823–835
15. George, L., Lécuyer, A.: An overview of research on passive brain-computer interfaces for implicit human-computer interaction. In: International Conference on
Applied Bionics and Biomechanics ICABB 2010. (2010)
16. Allison, B., Luth, T., Valbuena, D., Teymourian, A., Volosyak, I., Graser, A.:
BCI Demographics : How Many ( and What Kinds of ) People Can Use an SSVEP
BCI? IEEE Transactions on Neural Systems and Rehabilitation Engineering 18(2)
(2010) 107–116
17. Lightbody, G., Ware, M., McCullagh, P., Mulvenna, M., Thomson, E., Martin,
S., Todd, D., Medina, V.C., Martinez, S.C.: A user centred approach for developing Brain-Computer Interfaces. In: Proceedings of the 4th International ICST
Conference on Pervasive Computing Technologies for Healthcare, Ieee (2010)
18. Lee, E.C., Woo, J.C., Kim, J.H., Whang, M., Park, K.R.: A brain-computer interface method combined with eye tracking for 3D interaction. Journal of Neuroscience
Methods 190 (2010) 289–298

