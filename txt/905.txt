Master thesis on Sound and Music Computing
Universitat Pompeu Fabra

Neural Correlates of Music and Emotion
in ASD and Typical Children
Natalia Delgado

Supervisor: Rafael Ramirez

September 2017

Master thesis on Sound and Music Computing
Universitat Pompeu Fabra

Neural Correlates of Music and Emotion
in ASD and Typical Children
Natalia Delgado

Supervisor: Rafael Ramirez

September 2017

Contents

1 Introduction

1

1.1

Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1

1.2

Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

2

1.3

Structure of the Report

3

. . . . . . . . . . . . . . . . . . . . . . . . . .

2 State of the Art

4

2.1

Autistic Spectrum Disorder . . . . . . . . . . . . . . . . . . . . . . . .

4

2.1.1 Definition and types . . . . . . . . . . . . . . . . . . . . . . . . . . . .

4

2.1.2 Theories and diagnosis . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.1.3 ASD Therapy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

5

2.2

ASD and music therapy . . . . . . . . . . . . . . . . . . . . . . . . . .

6

2.3

EEG and Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.3.1 Emotion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

9

2.3.2 Brain waves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
2.3.3 Three layer emotion experience . . . . . . . . . . . . . . . . . . . . . . 13
2.3.4 EEG for emotion representation . . . . . . . . . . . . . . . . . . . . . . 14
2.4

Research question and hypothesis . . . . . . . . . . . . . . . . . . . . . 17

3 Material and Methods

20

3.1

Materials . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

3.2

Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22

3.2.1 Stimuli Selection and Experiments . . . . . . . . . . . . . . . . . . . . 22
3.2.2 Data Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

3.2.3 Data Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
4 Results

32

4.1

Tables and graphics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

4.1.1

Average Classification and Average Score Representation for ASD subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

4.1.2 Correlation Matrix between EEG features and Audio features of ASD
and Typical Subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
4.1.3 Confusion Matrix of each set of stimuli . . . . . . . . . . . . . . . . . . 36
4.1.4 Valence-Arousal centroid plots compared to expected emotion in ASD
and typical subjects

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 37

4.1.5 Valence-Arousal centroid plots compared to given response emotion in
ASD and typical subjects . . . . . . . . . . . . . . . . . . . . . . . . . 39
5 Discussion
5.1

40

Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40

5.1.1 Expected and Given Response Classification from EEG features . . . . 40
5.1.2 Correlation between EEG features and Audio features . . . . . . . . . . 42
5.1.3 Confusion Matrix for Expected and Given Response . . . . . . . . . . . 44
5.2

Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45

6 Contributions

47

6.1

Workshop conference . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

6.2

Github repository . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

List of Figures

47

List of Tables

50

Bibliography

51

A Appendix

57

A.1

Added figures from Results section . . . . . . . . . . . . . . . . . . . . 57

A.1.1 Correlation Matrix between EEG features and Audio features of ASD
and Typical Subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57
A.1.2 Valence-Arousal centroid plots compared to expected emotion in ASD
and typical subjects

. . . . . . . . . . . . . . . . . . . . . . . . . . . . 57

A.1.3 Valence-Arousal centroid plots compared to given response emotion in
ASD and typical subjects . . . . . . . . . . . . . . . . . . . . . . . . . 57

Dedication
I dedicate this work to my family, who support me everyday, whether they are
present or not.

Acknowledgement
I would like to express my sincere gratitude to Rafael for being patient with me
and introducing me to the world of technology in autism and music therapy. I am
also very grateful to Julia, to the children of Carrilet and their families, and to the
teachers and students from the Principe Felipe school in Madrid, who made possible
my project and were all immensely kind.
Also, thank Emilia for listening when I had doubts and for taking care of all of
us. Finally thank my flat mates and master colleagues for their support, and for
making the days a little bit brighter. I’m glad I came across these wonderful people
at MTG.

Abstract
Autistic Spectrum Disorder (ASD) is characterized by a delay in expressing and
interpreting other emotions as well as having difficulties with social abilities. It
covers a very broad range of diagnosis and it affects 1% of the world population,
the differences between the various types are not well defined and the reason behind this disease is not well understood. Therefore, studies such as this are being
carried out towards better comprehending and explaining the unknown of this disease. In this work, a group of ASD children and a group of typical children were
observed by recording their EEG signal and verbal answers while they were exposed
to audio-visual stimuli with an emotional character and were asked to identify such
emotion. First, correlation between EEG features and verbal and expected response
was computed to confirm EEG may be used to interpret emotions. This was confirmed. Next, EEG features were correlated with audio features to observe which
audio category (harmony, melody, rhythm, or timbre) is most relevant when interpreting an emotion. Results from this study suggest timbre being most relevant.
This study is intended to obtain further information about the relationship between
music and ASD patients that could be employed to improve music therapy.

Keywords: Autistic Spectrum Disorder, EEG, EEG features, audio features, correlation, machine learning

Chapter 1
Introduction

1.1

Motivation

The Autistic Spectrum Disorder (ASD) is characterized by a delay in expressing
and interpreting others’ emotions, as well as by a lack of social engagement and
interaction initiation, refusal of new settings and repetitive behavioural patterns
[1]. However, studies have proven many ASD people have strong abilities for music
regarding pitch labelling, pitch direction and identification of pitch contours [2],
and suggest that although they tend to have difficulties interpreting the emotion
from a picture, ASD children usually find it easier to identify the emotion expressed
by a music piece [3]. The reasons behind this difference in abilities between ASD
and typical people is not strictly proven, however studies claim music helps autists
express and canalize emotions [4]. As a matter of fact, this knowledge is being
exploited more and more by psychologists and music therapists not only with autists,
but also involving music as a non-invasive treatment for people who have difficulties
in interpreting and displaying emotional states [5].
Being aware of the Autistic Spectrum Disorder and the difficulties it brings to the
individual and its family, it is a motivation to contribute to possible music therapy
improvement for these individuals. Comprehending their situation is a first and
important step. ASD is still a quite broad and vaguely understood disorder which
1

2

Chapter 1. Introduction

wasn’t even considered as such a century ago. This implies there is much to do in
terms of understanding why this happens and what is different compared to typical
children. As presented in the State of The Art section, there are theories proposed
to explain this [6] [7], nevertheless, there is still much research to be carried out
still. Hence, as well as better knowing this disorder, it is a motivation for this study
to work towards better understanding it and searching for more precise connections
between ASD and music, to provide therapists with new tools and information, and
therefore impulse the growth of music therapy for ASD patients.

1.2

Objectives

The fact that many ASD children can interpret the emotion expressed by music
implies that although in the case of visual stimuli it becomes a huge challenge, they
can comprehend external emotions. Hence, it would be highly useful to comprehend
at which point of processing visual stimuli emotion differs from auditory stimuli
emotions in ASD, and when and how this processing differs from typical to ASD, to
better treat patients with such disorder. This project proposes two objectives.
The first objective is to confirm, as previous studies suggest, that the difference
between typical and ASD children in interpreting emotion depends on the type of
stimuli they are exposed to. It is expected to find that ASD participants will obtain
similar results to typical participants in the case of audio stimuli, and poorer results
when being exposed to visual stimuli. The aim is to observe this by analysing both
the expected response and their verbal responses, i.e: the emotion they claim to
perceive, and the EEG features extracted from their EEG signal.
The second objective is to contribute to the search of understanding the reason
behind this difference between ASD and non-ASD children. To do so, we propose
searching for correlation between the EEG features and the audio features of the
audio except the subject was being exposed to while the latter EEG was being
recorded. The audio features will be divided into four categories: harmony, melody,
rhythm, and timbre; and correlation will be searched for between the EEG features

1.3. Structure of the Report

3

and the groups or categories of audio features. The idea behind this is to observe
whether a specific music category is more relevant to the subject than the other
category when interpreting the emotion of that music. This could be of use for
music therapy to refine the methods by better comprehending the patient.

1.3

Structure of the Report

This work is divided into five sections. This is the first section, an introduction of
the motivation and objectives of this work. The second section, State of The Art,
presents an overview of the most relevant concepts in which this work is based on
and which helps the reader understand the context. Materials and Methods in the
third section. It presents the material employed and the procedure followed for the
project. In fourth place, the results are presented in the form of graphs and plots,
and in the fifth section these are discussed and conclusions are made.

Chapter 2
State of the Art

2.1
2.1.1

Autistic Spectrum Disorder
Definition and types

Autistic Spectrum Disorder (ASD) is characterized by having difficulties in social development and in social communication, and usually having strong narrow interests
and repetitive behavior [8]. At the same time, however, many ASD individuals have
normal physical growth and development [9], and ASD abilities may range from
near-or-above average intellectual and communicative skills to absence of speech
and severe mental retardation [10]. Hence, Autistic Spectrum Disorder covers a
very broad and diverse group of individuals. In fact, it covers 1% of the world
population according to the autism society and the CDC [11].
When talking about ASD, all kinds of autism are included. Despite there being
many definitions of autism by different communities, many researchers refer to two:
classical autism and Asperger Syndrome (AS). Although the line between these two
tends to be blurry, AS individuals are usually diagnosed later in time and tend to
learn to speak on time and have on average a higher IQ [7], while Asperger Syndrome
could be considered as a lighter version and tend to achieve their professional goals
at the same pace as typical individuals. However, they still present many social
4

2.1. Autistic Spectrum Disorder

5

difficulties and frustrations regarding this sense.
Hence, a great part of the world population is affected by the Autistic Spectrum
Disorder, yet it is neither well defined, nor the causes of this disease are well understood, and it covers a very broad range of diagnosis. It gives reason to why
researchers have worked, and are working, on explaining some of these unknown
properties of ASD.

2.1.2

Theories and diagnosis

Baron Cohen, who has been researching on autism for many years, has proposed a
number of theories to explain and diagnose both cases of ASD. In 1997 he presented
the mind-blindness theory [12], in which he proposes that ASD children take longer
in developing the theory of mind (ToM): the ability to put oneself into someone else’s
shoe or imagining someone else’s thoughts or feelings. To confirm this, he conducted
many studies in which he compared ASD behaviours with typical behaviour at
different development stages, by using tests and employing neuroimaging techniques
to compare brain activity in reading tasks [7]. These studies have been highly useful
in objectively defining ASD and diagnosing it.
In 2002, Baren-Cohen published a new theory, the empathizing-systemising theory,
which suggests a low level of empathy and average or over average level of systemizing
indicate you are likely to develop an autism spectrum condition [6]. Empathy and
systemization tests were employed to evaluate these aspects and this theory is also
employed to diagnose ASD.

2.1.3

ASD Therapy

As stated by Myers et all [13], ASDs are generally not ’curable’, and they require
chronic management. Although situations can be improved, they tend to remain
within the spectrum as adults and continue experiencing problems with employment,
independent living, social relationships, and mental health. However, treatment
helps to minimize the most relevant features, to maximize their quality of life and

6

Chapter 2. State of the Art

their functional independence, and alleviate family distress.
Therapy treatments are diverse, and there is no consensus on how to improve the
situation and life quality of such individuals. Medical treatments don’t result as
an accepted alternative. Studies have been carried on the effectiveness of medical
treatments for ASD children and experts conclude "little evidence exists to support
the benefit for most treatments" [14]. Other treatment alternatives are educational
interventions or therapy both for the ASD and for its family. These interventions
work on behavioural strategies and habilitative therapies and work on communication, social skills, daily living skills, play and leisure skills, academic achievement,
and maladaptive behaviours. It is difficult to evaluate whether these techniques are
useful, due to subjectiveness, however, questionnaires filled in by patients, family
and therapists are employed to evaluate as objectively as possible and tend to results
as positive [13].
Furthermore, due to the preference of many ASD patients for auditory stimuli over
other stimuli [15], music therapy is becoming an attractive option for ASD and is
growing very fast in the past few years [16].

2.2

ASD and music therapy

It has been reported that auditory stimuli, when presented in the form of music,
tends to be preferred over other stimuli by individuals with autism [15]. The reason
behind this is still unknown, however, it is used as a benefit in music therapy.
In fact, in 2009, the National Autism Center [16] classified music therapy as an
emerging evidence based practice useful in teaching individual skills or goals by
initially targeting the therapy through a song or a rhythmic cuing.
Music therapy has recently been employed for ASD people, at any age [17], and an
increasing number of studies are being carried out to prove its positive influence in
their communicative behaviors and their social engagement with other individuals
[18]. Simpson and Keen [4] selected the 20 most precise and reliable articles out of
a compilation of 180 articles following a strict criteria, which give evidence base for

2.2. ASD and music therapy

7

the use of music as an intervention for children with autism to analyse their results.
They conclude agreeing with the National Autism Center in music intervention being
successful in facilitating social, behaviour and communicative skills. However, they
both state further research is required to determine treatment effectiveness.
Not only many ASD have a preference for musical stimuli, but also have a preserved,
and in some cases enhanced music perception compared to typical people. Applebaum proved in 1979 a superior identification and labelling by ASD [2], and Heaton
proved many have an enhanced sensititory for pitch direction [19], and for detection
of changes in pitch contours[20].
Furthermore, the use of music therapy has been justified from a neurobiological point
of view. Caria et al. [21] carried out a study in which both ASD and neurotypical
subjects (NT) were observed through functional magnetic resonance imaging (fMRI)
while processing happy and sad music excerpts. Their results gave a hint on the
neurobiological correlates of altered and preserved emotional processing in ASD
compared to NT. As expected and observed in other experimental studies, they
demonstrated that the perception of emotions when listening to music is relatively
intact in ASD. They also conclude that their results provide a neurobiological proof
of music therapy being able to enhance emotional skills and facilitate communication
in ASD patients, justifying the use of music therapy in these patients.
Other studies, such as Quintin’s [22], also explore the ability of ASD individuals
categorizing or recognizing different emotions in music by means of a forced-choice
experimental procedure and exploring the perception of emotional intensity in music.
This case is highly related to the work being presented. In this study, both ASD
and typical children were exposed to a musical extract followed by a visual extract
composed of four line-drawings of emotional faces - happy, sad, scared and peaceful.
While a music extract was being played, subjects had to match an emotional face to
it and indicate their confidence of correct answer. The results revealed there were
not significances between participants in both groups when recognizing music as
happy, sad, and scared. Nevertheless, participants with ASD were not as accurate
as control participants when recognizing peacefulness. The reason for this is not

8

Chapter 2. State of the Art

known, but the author hypothesizes it might be due to peacefulness being a more
complex emotion and that probably, ASD individuals have difficulty recognizing
complex emotions and mental states in the voice. Furthermore, Quintin suggests
it could be due to different boundaries or conventions for what is considered to be
peaceful music maybe being ambiguous.
She concludes suggesting that it seems music is a positive channel through which
emotions can be communicated to individuals with ASD. Moreover, she proposes
using psychophysiological measures such as electrodermal response, heart rate, etc.,
to further explore the previous and to observe whether ASD individuals could make
use of these psychophysiological measures to regulate their mood, reduce anxiety,
or increase concentration.
Another study highly related to the effect of music on ASD individuals is that carried
out by Matamoros [23]. Her research questions whether Autistic Spectral Disorder
individuals distinguish among emotional states in a more efficient way when they
are exposed to musical input representing an emotion. To do so, ASD children were
exposed to three conditions once a week during four weeks and results were recorded.
The conditions were the following: pictures without music (EFNM), pictures with
music (EFM), and different random pictures with no music (EFNMr). Both verbal
responses and EEG signals were recorded as results, the later using an Emotiv
Epoc Device, after considering Justin and Sloboda’s proposed model of comparing
obtained signals with self-reported states to verify consistency [24].
The results showed positive improvements in conditions one and two from the first
to the fourth week, and no positive improvement for the third condition. This research leads to concluding children integrate new skills and maintain them over time,
although it does not prove these skills will be applicable in a real-life context. Moreover, it is suggested that EEG signals do, to a certain degree, match participants’
verbal responses. Finally, since it seems that music did have a positive impact on
participants’ results, it is considered that, if replicated with a bigger sample, it could
be confirmed what is suggested in this work: Music has a positive impact on autistic
children’s social and emotional skills.

2.3. EEG and Emotion

9

In conclusion, studies suggest ASD children tend to be more sensitive to music and
have average or even enhanced musical habilities, however this does not include
all ASD cases. This has implied music therapy arising in the ASD therapy world.
Nevertheless, demonstrating and quantifying its efficiency is not easy. As presented
above, there are cases of or both experimental and neurobiological studies working
on demonstrating the positive effect of music on ASD.

2.3

EEG and Emotion

As presented above, music may have a positive effect on ASD children in terms of
improving social skills, behaviour and communication. Nevertheless, studies are still
being carried out to confirm music therapy being effective, as the Autism National
Centre already stated, more studies need to be done to be proven, and to go one
step further in attempting not only to prove it but also to understand the reason
behind it. For both of these objectives, various methods are being used: from
observational experiments to more technological experiments, which take advantage
of new technology such as fMRI, EEG, or PET.
EEG provides a noninvasive way of measuring brain activity with temporal resolution in milliseconds, and has been used in the past decades in the field of neuroscience
to investigate processing and regulation of emotion. Due to its advantages in relation to the present study, EEG recording will be used to observe emotion induced
by audio-visual stimuli, and will be discussed in this section after presenting basic
concepts on emotion.

2.3.1

Emotion

The limbic system, paralimbic association cortices, and the neocortex are the three
main layers through which neural mechanisms of emotion can be described. In the
following section, the most relevant functions of these three layers are presented.
The limbic system is in the centre of the brain, as may be observed in figure 2.
The amygdala, hippocampus and hypothalamus are the components in the limbic
system which are most relevant in emotion process. The hypothalamus takes part

10

Chapter 2. State of the Art

Figure 2: Paralimbic association cortices. Including (a) the cigulate gyrus, (b) retrosplenial cortex, (c) parahippocampal cortex, (d) temporal lobe, (e) insular cortex
and (f) orbitofrontal cortex. [28]
in the emotion process at a very basic level, triggering primitive, diffuse, undirected
and unrefined emotions, which make one "feel" happy or unhappy [25]. The hippocampus participates in both cognitive and emotional functions [26], maintaining
strong interconnections with the amygdala, complementing it in regard to attention, arousal, learning and memory. Finally, the amygdala is now known for being
an essential component of the emotional system. It is involved in higher order activities than the hypothalamus due to its rich interconnections with other parts of
the brain. One of its most relevant roles is to determine emotional significance to
external events, thus being involved in emotions such as fear, anxiety or anger.
At a second level lies the paralimbic association
cortices, shown in figure 3. In his work, Papez
suggested the cingulate cortex is vinculated to
emotion consciousness [27], and the insular cortex is also involved in processing interoceptive
information. Finally, the orbitofrontal cortex,
located in the medial and ventral surfaces of the

Figure 1: Limbic system

frontal lobe, is mainly concerned with general
arousal reaction.
It has been shown that the neocortex plays a fundamental role in interpreting many
stimuli that induce emotional experience [28]. It can be subdivided into two complementary ways: into cortical lobes, and laterally into left and right hemispheres. Out
of the cortical lobes, for this study, we are interested in the frontal lobe, which plays

2.3. EEG and Emotion

11

Figure 3: Neocortex [28]
an important role in motor and behaviour planning, and particularly the prefrontal
cortex is thought to be involved in emotion. In terms of hemispheric differences there
are two theories: the right hemisphere theories, which hypothesizes the right hemisphere is dominant in emotional processing; and the valence theories, which claims
the left hemisphere is associated with positive emotions, and the right hemisphere
with negative emotions [29]. The second theory is very relevant to this work.

2.3.2

Brain waves

Sammler et al. [30] argue that taking advantage of the high temporal resolution of
EEG makes possible studying the neuronal processes at different time scales. These
are frequency bands that are related to different mental functions and, as presumed
in the referenced paper, also to emotional states. Based on the latter, they present a
study in which they contemplate the possibility of frequency bands in the EEG being
able to raise reliable correlation with emotion processing and in such case, observe
which. The frequency bands searched for in the EEG are the five frequency bands
the human EEG power spectrum has been divided into traditionally [31]. These are
delta, theta, beta, and gamma. The first three, which are the most relevant to this
work, are presented and briefly explained below.
Theta (4-8 Hz) As Schacter proved [32], human theta rhythm is manifested in two
ways. The first is related to drowsiness and low-level alertness states, such as sleeping. The second, referred to as "frontal midline theta" (Fm Theta) has been observed
during many tasks, such as mental calculation, working memory and learning, error
processing, and meditation [30]. Putting together a number of studies, Sammler

12

Chapter 2. State of the Art

concludes that Fm if theta originates from the ACC (Anterior Cingulate Cortex),
the ACC is part of the neural "emotional circuit", and the ACC is activated with
pleasant music, it is reasonable to question whether Fm Theta may be related to
emotional processing. In their study, they find pleasant emotions are a sign of an
increase in Fm theta power, showing Fm theta power is "modulated by emotion
more strongly than previously believed".
Alpha (8-12 Hz) In the case of alpha EEG activity, reports show there are at least three different types of alpha activity [33]. The first, the
classical posterior alpha rhythm, which is dependent on attentional factors and vigilance of the
subject. The second, rolandic mu rhythm, is related to movement and movement preparation
[33]. And the third, tau rhythm, is modulated by
auditory stimulation. It is also important to note

Figure 4:
Representation of
that alpha power and brain activity are inversely where the Anterior Cingulate
Cortex and Pre Frontal Cortex
related. Alpha rhythm has also been linked to
are located
perceptual processing, memory tasks, and emotional processing [30]. A relevant hypothesis for this study is the "hemispheric
valence hypothesis" [34], which states that positive approach-related emotions are
mainly processed in the left frontal side of the brain, while negative withdrawalrelated emotions are processed on the right frontal side of the brain. This hypothesis is reflected in the EEG, where an asymmetrical decrease of alpha can be
observed according to the perceived emotion. This means that the left-frontal alpha power decreases during positive emotions and the right frontal alpha decreases
during negative emotions, as reflected in table 1. A number of studies have proven
this hypothesis is reflected in the EEG [34]. As Sammler et al. state in their paper,
"These studies have corroborated the relation between emotional state and forebrain
EEG asymmetry".
Betha (13-30 Hz) Betha power is thought to be related to increased alertness and

2.3. EEG and Emotion

13

Table 1: Arousal computation from alpha waves
Left-frontal alpha power
(F3, F7, T3, C3)
Right-frontal alpha power
(F4, F8, C4, T4)

Positive emotion
DECREASES
(more brain activity)
INCREASES
(less brain activity)

Negative emotion
INCREASES
(less brain activity)
DECREASES
(more brain activity)

cognitive processes [35], and to decrease during non-REM sleep. Not many studies
have investigated the relationship between beta power and emotional processing, but
some have been carried out [36] [37] and have mainly reported beta power increases
with an unspecified increase of emotional arousal.
In 1997, Krumhansl [38] reported how physiological patterns showed significant differences when listening to musical pieces representing sadness, happiness, and fear.
As some other recent studies [39] [40], Sammler et al. also observe a physiological
pattern of participants, the heart rate. Results show heart rate a significant index
for emotional processing. This type of data collection will not be carried out in
the present study, nevertheless, it is considered as a valuable possibility for future
research.
As Sammler states, alpha-power asymmetry is a strong indicator of emotional states,
which can be extracted by computing the spectral differences between the asymmetric electrode pair at the anterior areas of the brain. More detail about the
computation of arousal and valence is presented below.

2.3.3

Three layer emotion experience

Starting from the base that many individuals with high functioning autism and
Asperger syndrome are associated with having difficulties identifying and distinguishing one’s own feelings [41], Silani carried out a study in which she employed
alexithymia and empathy questionnaires, and used fMRI to asses this inability, comparing individuals with high functioning autism/Asperger syndrome with matched
controls.
To interpret and evaluate results, she proposes a three level model of emotional

14

Chapter 2. State of the Art

experience based on Lambie and Marcel’s two-layer model [42]. Lambie and Marcel’s
model proposes a first order experience is associated with neurophysiological arousal
associated with emotions, and in terms of brain activation, studies suggest there
is an increased activity in the amygdala and orbitofrontal cortex. Second order
experience is associated with the awareness of this arousal, or interoceptive arousal,
and is related to the anterior cingulate cortex and anterior insula activation. More
information about these components of the brain is presented in the previous section.
Silani adds a third level of experience which is associated with introspection or selfprocessing. This is, being aware of having an emotion. Results of her study show
that at first level experiences activation of its corresponding parts of the brain are
correlated with the degree of alexithymia or lack of empathy. In the case of the
second level experience, results suggest the level of activity in the anterior insula is
significantly and negatively related to the scores on both alexithymia and empathy
measures. This is, the higher the alexithymia results, the lower the activity in
the anterior insula. However, Silani claims a lack of bodily states awareness or
impairment in second-order emotional experience is "neither necessary nor sufficient
for autistic disorder as currently diagnosed". (Around 10% of the population has
alexythimia and it is not enough to demonstrate an autism diagnosis [43]). However,
her results do suggest that third-order emotional experience, being aware that you
are having an emotional experience, could be critically associated with autism.
Silani employs fMRI to carry out her study. Nevertheless, for the work presented,
EEG is employed due to its low cost, its high precision and the fact that it is non
invasive

2.3.4

EEG for emotion representation

The Electroencephalogram (EEG) has been used in cognitive neuroscience to investigate the regulation and processing of emotion for the past decades[40]. It is a
representation of the electrical activity of the large groups of neurons in the brain
obtained by recording the electric potential at the surface of the scalp [39] and its
advantages over other neuroimaging tools are being a non-invasive technique and

2.3. EEG and Emotion

15

being cheap. Typically, EEG is characterized by amplitudes ranging from about 10
to 100 µ V and frequencies from 0Hz to 60Hz. The main frequency bands into which
the EEG signal is usually divided into are explained below.
EEG was proved to be effective in interpreting from brain signals basic emotions
[34]. The psychologist Ekman [44] was who, in 1992, originally claimed emotions
could be classified into basic into basic emotions and he considered anger, disgust,
fear, joy, sadness, surprise were these basic emotions. However, the accepted basic
emotions vary from one psychologist to another. To extract these basic emotions
from brain signals, valence and arousal are computed, as shown in figure 5. Arousal
is considered a state of activation. High arousal is related to being awake, alert,
and prepared to process stimuli, while low arousal implies the opposite: a state of
relaxation, calmness, or even being asleep. On the other hand, valence is considered
as an indicator of how pleasant or unpleasant we evaluate something [34]. Arousal
is computed as the inverse of alpha, brain signal in the frontal cortex from 8-12Hz.
Further explanation is given in the next section. Valence is computed by subtracting
arousal in the left side of the brain from arousal in the right side of the brain. It
is computed this way since, as studies have shown[45], the left hemisphere is activated with positive emotions while the right hemisphere is activated with negative
emotions. In recent years, this method has been used by various researchers such as
Chopin [39], Lin [40], or Ramirez [3].
Apart from these examples, the closest case to
this work is Ramirez’s [3] in which he proposes a
method to confirm that "comparing hemispherical activation seems to be a reasonable method
to detect valences" by classifying EEG signals
recorded from subjects exposed to auditory stimuli into arousal and valence values, and comparing them to the labels of the stimuli.
In his study, a number of subjects are exposed Figure 5: Computation of bato auditory stimuli, situated in the extremes on sic emotions from arousal and valence values

16

Chapter 2. State of the Art

the arousal-valence emotion plane, while their
response EEG activity is being recorded. Afterwards, only the EEG signals in the prefrontal cortex (AF3, AF4, F3, F4) are selected
to be filtered and processed with openVibe [46], which are the most used positions
for looking for alpha and beta. Obtaining these two signals, arousal and valence
are computed, which are then contrasted with the original labels of the stimuli, obtained from a library of emotion-annotated sounds publicly available for emotional
research. Two different machine learning algorithms, LDA and SVM, are employed
to evaluate the classifications. The results obtained show accuracies up to 83.35%
and 86.33% with the best algorithm, which allows concluding "EEG data obtained
with Emotiv Epoc device contains sufficient information to distinguish these emotional states, and machine learning techniques are capable of learning the patterns
that distinguish these states".
Y. Lin et al., [40] carried out an experiment on a proposed method to detect emotion when listening to music by means of EEG signals, and proves this is an effective
method, in the conditions they have carried it out. They also apply machine learning algorithms to categorize EEG dynamics according to subject self- reported emotional states during music listening. They claim, as compared with audio - and/or
visual-based methods, the responses of biosignals tend to provide more detailed
and complex information as in indicator for estimating emotions [47]. Nevertheless,
recording brain activity using EEG has the advantage of providing non-invasive
measurement with temporal resolution in milliseconds.
While Chopin [39] also employs EEG for emotion extraction, he focuses on Amyotrophic lateral sclerosis (ALS). It is a devastating neuromuscular disease that increasingly reduces the voluntary control of muscle movement, including facial muscles, which when paralyzed, make hard to “read” a person’s feelings in his face. This
is why Chopin’s research focuses on developing a new type of Brain-Computer interface based on an EEG analysis, with the purpose of helping patients to express
emotion. In his study, a patient suffering from a motor-neuron disease was followed
while expressing emotions through the built interface, which was designed based on

2.4. Research question and hypothesis

17

EEG analysis. Furthermore, more experiments were carried out on nine subjects
to assess the validity of the built interface analysing their EEG. He concluded that
"neural networks could efficiently be used in an emotion expressing system for mapping EEG features onto emotion". He also concludes further research is required and
results could be improved both selecting EEG features specifically for each patient
and by improving the neural networks.

2.4

Research question and hypothesis

After presenting a summary of relevant research studies and information, three hypothesis for the thesis are explained below.
First, it is hypothesized that there exist correlations between the reported and expected emotional responses among participants. On the basis of ASD being characterized by a delay in expressing and interpreting others’ emotions [3], when observing
results we expect typical children obtaining a higher correlation between reported
emotion and expected emotion than ASD children. This is hypothesized due to believing that typical children will process emotion similarly to what the stimuli was
created for.
During the collection of data, ASD children and controls are exposed to audio and
visual stimuli, first separately, then combined coherently and finally combined noncoherently. It is believed combining both types of stimuli will result in an unexpected
outcome. However, we hypothesize coherent combination of stimuli will affect both
ASD and controls in their reported answers becoming closer to the expected ones,
but the impact will be stronger on ASD. In the case of non-coherent combination
of stimuli, the expected result is not obvious, but we hypothesize in the case of
ASD individuals, the audio stimuli will have a stronger influence on their reported
answers.
In second place, we hypothesize that there will be a correlation between EEG and
reported emotion. From the previous research, it is understood ASD individuals
can understand emotions, simply not in the same way as typical individuals. It is

18

Chapter 2. State of the Art

believed comprehending this difference in emotion processing will give tools to music
therapists and ASD families to enhance social, communicative and behavioural skills
in ASD individuals. Moving towards this direction in research will allow working
towards building the bridge between typical and ASD individuals in the emotional
sense, in a more rational way.
This project also aims at contributing to this field of investigation. ASD individuals
and controls will be exposed to audio and visual stimuli while reporting the emotion
each of these stimuli transmits to them. During the whole of this process, the EEG
signal of the subject will be recorded with an EEG device, emotiv Epoc. Once the
data is collected, the correlation will be searched for between the EEG signal and the
reported answer for all subjects. It is expected that the results will show a similar
correlation for audio and visual stimuli in typical children, and a higher correlation
of the EEG with audio than with visual stimuli in ASD children.
Hypothesis 3: Audio features and EEG correlation
Finally, it is believed there will be a correlation between audio features and the
EEG. To contribute to the understanding of this expected phenomenon, the EEG
signal will also be correlated with audio features of the audio stimuli. The reason
behind this is that, as mentioned in the previous sections of the state of the art,
many autists are able to interpret emotions from music as well, or even better, than
typical children, but have many difficulties with visual stimuli such as faces. The
aim of this part of the study is to search for correlations between the processing of
the emotion in the music, which we expect to be similar to that in typical children as
expressed in the previous paragraph, and which is contained in the EEG signal; and
groups of features of the music fragment in matter. Hence, the idea is to search for
specific groups of features such as rhythm, timbre, harmony, or melody that could
have more relevance than the rest when processing emotion. The result of this could
give a queue in how to focus more specifically some exercises in music therapy.
Searching for correlation between EEG signal and audio features will also be done on
ASD individuals and on controls. In this case, due to the difficulties ASD children

2.4. Research question and hypothesis

19

have with images, we expect the results will show stronger correlations between
audio features and EEG signal in ASD than in typical subjects

Chapter 3
Material and Methods

3.1

Materials

In this work, the EEG signals of patients are recorded while they are exposed to a
set of stimuli. To carry out this part of the work, stimuli is selected and built into a
video, an EEG device, emotiv Epoc, is used to record the EEG signal, and openVibe,
a software, is employed to receive and preprocess that data. Moreover, this data
is then analysed using Weka, scikit library in Python and Essentia libraries. The
material mentioned is further explained below.
Stimuli
As explained above, this project is carried out by exposing subjects to a set of stimuli
while recording their EEG and their verbal answer, i.e. the emotion they claim to
interpret from the stimuli. The stimuli are a set of audio excerpts and images
characterized by one of four basic emotions: happy, sad, calm or aggressive. The
audio stimuli are 10 second excepts of soundtracks which were originally composed
to evoke an emotion. It was selected from the IADS library of emotion-annotated
sounds [48], which has been widely used in studies similar to this. The images were
obtained from the IAPS affective ratings of pictures [49]. The manual that comes
with this set of pictures gives an arousal and valence value for each of them, and
the pictures with extreme values and acceptable for children were selected.
20

3.1. Materials

21

Figure 6: Stimuli
The stimuli are presented in the form of a video, which is composed by four different
parts or sections as presented in figure 6. Moreover, each part or section is conformed
by twelve stimuli: three of each of the four emotions. First, only twelve audio excepts
are presented with a blanc image, three happy, three sad, three aggressive, and three
calm (audio stimuli). Second, twelve images are presented with no audio (visual
stimuli). Third, twelve audios and images are presented together in a coherent
way (audio-visual coherent stimuli). This is, both the image and the audio evoke
the same emotion. Finally, the fourth section presents twelve images and audios,
however this time non-coherently (audio-visual non-oherent stimuli). Hence, the
emotion the image evokes is different to the soundtrack emotion each time. The
objective of presenting different combinations of sets of stimuli is to observe which
type of stimuli, audio or image, is dominant in each group of subjects.
Each stimuli, both audio and image, lasts 10 seconds, and a 10 second pause with
white image is inserted in between each stimuli to give the child time to answer.
Amongst each group or set of stimuli, the stimuli are presented in a randomized order
and two videos were generated to avoid a determined order affecting the answers of
the subjects. To automatize the order of stimuli and to facilitate changing stimuli
and pause durations, a script was written in matlab to generate the video.
Emotiv Epoc
The Electroencephalogram device employed in this work is called emotiv epoc [50].
It is a 14 channel wireless EEG, designed for contextualized research and advanced
brain computer interface (BCI) applications. It connects to openVibe software
through bluetooth, and it offers a high resolution, multi-channel EEG system, which
is both effective and economic.
OpenVibe

22

Chapter 3. Material and Methods

OpenVibe is a software platform dedicated to designing, testing and using braincomputer interfaces. OpenVibe has a number of applications from which the test
bench, acquisition server, designer are used in this work. The designer is the main
page from which the code is uploaded, edited and executed. The acquisition server is
necessary to connect with the EEG device, and the test bench is used once they are
connected. It allows observing the signals collected by the electrodes and to confirm
all electrodes are connected to the scalp of the participant during the experiment.
[46]
Essentia
Essentia is an open-source C++ library with Python bindings for audio analysis and
audio-based music information retrieval [51]. It was used to extract audio features
from the audio stimuli employed, using its Music Extractor. Essentia offers the
possibility of extracting individual low-level or mid-level audio features, and also
offers the possibility of extracting many low level features together. In this case,
the second option was used, i.e. the "Music Extractor" was used, to then select the
most relevant features out of all of these.
Weka
Weka is "a collection of machine learning algorithms for data mining tasks" [52].
It offers configuring machine algorithms, and selecting features before running the
algorithms. For the present work Weka is employed for the sake of feature selection
to avoid over-fitting.

3.2
3.2.1

Methods
Stimuli Selection and Experiments

Stimuli
The first step of this process was selecting the stimuli to be used. As mentioned
before, audio excerpts and images were taken from the IADS library of emotion-

3.2. Methods

23

annotated sounds [48] and from the IAPS of affective ratings of images [49] and the
most "representative" were selected. This selection was done subjectively in the case
of audios, and in the case of images, they were selected by searching for extreme
cases of arousal and valence, which were then filtered subjectively and taking into
account children would be exposed to these.
For each set of stimuli (the four different set combinations are shown in figure 6),
three stimuli were selected for each of the four basic emotions (happy, sad, calm,
aggressive). For example, in the case of only images, three happy images, three
sad images, three calm images, and three aggressive images are selected. Hence,
twelve images. In total, 36 audio excerpts and 36 images were selected. These were
combined in random order to generate two different videos, with the aim of reducing
possible influences of the order of the stimuli in the subject’s answer. As explained
above, each stimuli, audio or image, lasts 10 seconds and they are separated by 10
second pauses. Moreover, there is a silence of 30 seconds at the beginning of the
video for possible need of a reference and to give the subject a moment to relax and
accommodate to the new setting. The video was generated by a script written in
matlab to allow doing modifications easily, to assure randomization and to assure
precision in the duration of stimuli for future processing.
Participants
Once the stimuli was generated, the experiments were carried out, and before starting with any procedure, an authorization form was signed by the parents or legal
tutors of all participants. Two sets of subjects were involved in this process: seven
typical children from the school "Principe Felipe" in Madrid, and four ASD children
from the Carrilet center in Barcelona. Both groups have similar IQ and are between
8 and 13 years old. In the case of ASD children, Carrilet center gave their IQ test
results, which show they all have an IQ over 70%. Similar conditions were searched
for, to avoid these influencing results.
Experiment
The experiment is carried out in a calm room, where the child is explained the

24

Chapter 3. Material and Methods

process. In the case of ASD children they were accompanied by a parent or a tutor.
Once the openVibe software is connected to the EEG device, and the electrodes
are wet and set on the device, it is placed over the child’s head. All electrodes are
confirmed to be reading the child’s EEG signal and the code is started. This code
triggers the stimuli video and the recording of EEG signal, which is stored in the
form of a .gdf file, which is then processed and converted into .csv in openVibe. This
was carried out, as explained above, in "Pricipe Felipe" school of Madrid with seven
children, and in Carrilet center in Barcelona with four ASD children. Both groups
of children had similar age (eight to thirteen years old) and similar IQ to make sure
this would not affect the results.

3.2.2

Data Processing

EEG feature extraction
During the experiment, a .gdf file is generated by openVibe for each subject. This file
is processed in the designer section of openVibe, obtaining as a result a nine column
.csv file containing the following information: alpha, arousal 1, beta, arousal2, theta,
gamma, valence1, valence2, and valence3. This result is as shown in figure 8, except
for the last two columns, which are added afterward. This result is generated by
running a code in the designer of openVibe which first separates and selects the
channels desired. As presented in the section of the State of The Art, for this work
we are interested in analyzing the information from the pre-frontal cortex. This is
why the channels AF3, AF4, F3, and F4 are selected. These can be observed in
relation to the rest of channels in figure 7.
To compute arousal, the signal is filtered with a butterworth band pass filter order 4 to divide the signal of
the four channels into frequency bands: 4-8 Hz to obtain
theta waves, 8-12 Hz to obtain alpha waves, 12-28 Hz to
obtain beta waves, and 28-64 Hz to obtain gamma waves.
Then, windowing is applied: 1 second window size with
0.1 second hop-size. FFT is computed for each window
Figure 7: EEG map

3.2. Methods

25

and all channels are averaged. Finally, beta and alpha
are combined to compute two types of arousal. The first,
computed as 1/ alpha, the second computed as beta/alpha. The values of alpha,
beta, theta, gamma, arousal1 and arousal2 are sent to be stored in the csv.
To compute valence values, the same four channels are filtered into band frequencies,
but this time only alpha and beta are employed. Furthermore, windowing and FFT
are also done in the same way. However, this time the average between them is not
computed because, as explained in the State of the Art section, valence is computed
by subtracting the left side arousal from the right side of the brain arousal. Since
the average is not computed, there is alpha and beta for each side of the brain,
which allows generating four possible arousals: 1/alpha and beta/alpha for the left
side of the brain, which we will call for the sake of this explanation leftarousal1 and
leftarousal2 respectively. In the same way we obtain rightarousal1 and rightarousal2
for the signals of the right side of the brain. By combining the four variations of
arousal, three types of valence are computed: valence1 as rightarousal1 - leftarousal1,
valence2 as rightarousal2 - leftarousal2, and valence 3 as right beta - left beta. The
values of valence1, valence2, and valence3 are also sent to be stored in the csv.
Table 2: Summary table of arousal and valence computation
EEG feature

Computation

Arousal 1

1/alpha

Arousal 2

Beta/Alpha

Valence 1

(1/alpha)right - (1/alpha)left

Valence 2

(beta/alpha)right - (beta/alpha)left

Valence 3

(beta)right - (beta)left

The data processed from openVibe is stored in a .csv file. The result is one file
per subject, each containing 16 minutes 30 seconds of information. In this step,
each column is assigned to its corresponding attribute name, presented in section
above. This is, alpha, beta, gamma, theta, arousal1, arousal2, valence1, valence2,
and valence3. After, two new columns are added: one with the "correct" or expected

26

Chapter 3. Material and Methods

corresponding emotion, and another with the corresponding emotion response of the
subject. An example of the result is shown in figure 8. This is done by knowing the
duration of each section of the video and matching it with the assigned order, which
was randomized when generating the video to avoid any types of preferences by the
participants. Since the video is composed by 10 seconds of stimuli and 10 seconds of
pause for the participant to think about the emotion, the blanc spaces in between
each stimuli are deleted from the data. Also, the 30 second initial black out used
for reference is omitted. Finally, each subject’s data is divided into five files, one
for each class: only audio stimuli (class 1), only visual stimuli (class 2), audio and
visual coherent stimuli (class 3), audio and visual non-coherent stimuli with audio
"correct answer" (class 4), and audio and visual non-coherent stimuli with image
"correct answer" (class 5). Although the initial idea was to have only four classes,
the last one was divided into two classes to facilitate work when comparing results
with one "correct answer" or the other, since the last class would have in fact two
"correct answers". From this point onwards, these classes are treated separately
in the classification task to then compare results for the various situations. Also,
each subject is treated separately and then averaged within each group (typical and
ASD) to be compared.
Some problems were encountered in this process: dealing with comas and semi
colons when reading .csv files. There is an incompatibility between regional systems
in terms of column separation in .csv files. For future work, it is recommended to
make sure openVibe generates the .csv files with semi colons separation to avoid
further problems.
EEG feature selection
As seen in figure 8, nine EEG features are obtained from the original data. To avoid
over fitting, some of these are discarded, and weka is employed to select which. Weka
offers three methods to select attributes: best first, greedy stepwise, and ranker. For
this study, ranker was employed. It gave the following order and weights: 0.1373
valence1, 0.1009 arousal1, 0.0999 alpha, 0.0676 theta, 0.0436 betha, 0.0361 valence3,
0.0191 arousal2, 0.0138 valence2. Hence, attributes valence 1, arousal 1, alpha, theta

3.2. Methods

27

Figure 8: Example of EEG data in csv format
and betha were selected, and valence3, valence 2, arousal 2 were eliminated.
Audio feature extraction
To later search for correlation between EEG and audio features, the latter must
be extracted first. To maintain a consistency between EEG features and audio
features, same window size and hop-size are used. This is, one second of window
size, and 0.1 seconds of hop-size. However, the number of frames did not match
by around 90 frames, which was considered too many. It is believed this is due to
the sampling frequency in openVibe, 128. Since 128 won’t give an exact number
of samples per frame size, in some occasions this value is rounded upwards, and
in others downwards. This implies varying by one sample the number of samples
in a frame across the whole data which, by the end of it, results in an significant
inconsistency. To solve this, it was proposed to calculate the hop-size openVibe is
in fact taking, instead of the value inserted in this software. This could have been
also solved by readjusting the size of the window, but it was decided to be done
readjusting the hop-size instead. The obtained hop-size was 0.9346 instead of 0.1.
Audio feature selection
After having features extracted, feature selection must be carried out. 980 features
are initially extracted and only 12 features are desired to avoid over fitting. There
are around 100 samples per audio extract, hence around 10 features (10% of the
total) would be desirable to avoid over fitting. Since four groups of features are
desired: rhythm, melody, harmony, and timbre; features are categorized into these

28

Chapter 3. Material and Methods

four groups and only three features per group are used in each case.
To divide into the four categories, music extractor labels are followed. Its labels
are organized in the following way: they are either rhythm.*, tonal.*, or lowlevel.*.
Rhythm and harmony were clear: rhythm is assigned to rhythm.* and harmony to
tonal.*. However, both timber and pitch features correspond to low-level features.
In the case of melody, only pitch salience was taken account: mean values. Studies
[53] focused on pitch extraction employ pitch salience and other parameters to extract pitch contour such as frequency distribution, energy distribution, or even take
advantage of values related to timbre. However these were thought to be too broad,
that they were not specifically or not only characteristic of melody but could give information about many high-level features depending on the way they are employed,
therefore only various statistical values of pitch salience was used. To select timbre
features, a study [54] regarding timbre selection was followed and their features used
for timbre detection are also employed in this work. These are: mfccs, spectral centroid, spectral flatness, spectral spread, spectral roll-off, spectral skewness, spectral
flux, spectral kurtosis, spectral complexity, and spectral decrease.
Once audio features are categorized into the four main categories (melody, harmony,
timbre and rhythm), three features from each category are selected. As explained
before, this is done to avoid over-fitting and three features from each are selected
to obtain 12 features in total, a close number to 10% of the 100 samples we have
of each audio extract. To select the features, the libraries "feature selection" and
"SelectKBest" function from scikit learn were employed. Each feature category was
ranked both for the arousal and valence selected in the EEG feature selection step.
Only these two features were employed because they are considered the most relevant
features, since the combination of them is used to predict emotions. Once the two
ranks were done, the three first features in the intersection were selected. To assure
the most relevant features were selected, this process was done by initially selecting
the top ten of both ranks, and slowly increasing this value if there were not enough
features in the intersection, until three were found.
Since we don’t have any evidence of common features being relevant to all subjects,

3.2. Methods

29

this process was carried out individually for each subject. Therefore, each subject
has a different selection of audio features, while maintaining having three features
of each category.

3.2.3

Data Analysis

Classification
Once data is prepared, the analysis begins. The first objective of this study is
to confirm that the difference between typical and ASD children in interpreting
emotion depends on the stimuli they are exposed to, and that in the case of ASD
participants, audio stimuli emotions will be easier to interpret than in visual stimuli.
Hence, it is proposed to compare the EEG features with the expected response and
the EEG features with the given response of both groups (typical and ASD), and to
observe if there is correlation between these. When talking about expected response
it is referred to the "correct" emotion the stimuli was originally assigned to by the
previously mentioned libraries [48] [49].
To observe whether there is correlation, this problem is treated as a classification
problem where the EEG features are the data and the labels are either the given
response or the expected response.
To improve the validity of the results, the classification carried out by three different
types of classifiers: Support Vector Machine, k Nearest Neighbor, and Decision
Trees, and the scikit learn library for Python is employed in the process. First, the
data is normalized. Then, for each classifier, the data is divided into a train set and
test set randomly using the "train test split" function from scikit learn. In all cases,
25% of the data is assigned to the test set and 75% of the data is assigned to the
train set. Once the data is divided into two sets, the train set is trained and fit for
each classifier. After, the test set is tested and a score is obtained employing the
scikit function "score". When observing results, the higher the score, the higher the
correlation. In the case of SVM, the default parameters were employed.
This process is carried out individually for all subjects, and afterwards, the average

30

Chapter 3. Material and Methods

is computed for typical and for ASD groups to then be represented and compared.
Correlation
The second objective of this work is to search for possible correlations between
music categories (melody, harmony, rhythm, and timbre) and EEG features to give
valuable information to be used in music therapy. Since different audio features
were selected for each participant, a correlation matrix is computed for each one
of them and no average is computed amongst them. For this work, "pearsonr"
function from scikit learn is employed. This function calculates a Pearson correlation
coefficient, which measures the linear relationship between two datasets, and the pvalue for testing non-correlation. In this case, only the Pearson correlation coefficient
is considered. Before calculating, the data is normalized, since Pearson’s correlation
requires that each dataset be normally distributed. Afterward, it is important to
take into consideration that this correlation coefficient varies between -1 and +1
with 0 implying no correlation. Due to our interest on a measure of a degree of
correlation and not whether the data is directly or inversely correlated, the absolute
value of the correlation coefficients originally obtained is computed. Finally, the
correlation matrix computed are represented through heatmaps.
Confusion Matrix Another proposed observation is to compare expected response
with given response to evaluate which group answered "better". As well as presenting the scores of both groups for each set of stimuli, a correlation matrix is computed
to observe which emotions tend to be confused with which. This was suggested due
to Chopin’s observations in her study [39], in which she comments how she observed
ASD children were in average similar to typical children when interpreting an emotion from a music excerpt, except for peaceful music, which could be considered as
equivalent to calm emotion in this study. Something similar was observed during
the experiments, and therefore, the matrix is computed to obtain objective results.
Valence Arousal Plots
To observe how well the EEG signal allows interpreting the expected or the interpreted emotion of the stimuli, it is proposed to plot these values. Initially, the

3.2. Methods

31

valence-arousal points of the data for each emotion are ploted for each set of stimuli.
To better interpret these representations, the centroid of each group of points was
computed and only these are represented. This is, the valence-arousal points are
grouped into their assigned emotion, whether is the expected or the given response,
and these were condensed into one centroid point to allow interpreting this data.
Ten plots are computed for ASD and for typical children: five plots, one for each
stimuli set, presenting arousal-valence compared to expected response, and five plots
presenting arousal-valence points compared to the given response.

Chapter 4
Results
Below are shown the graphics and tables representing results of this work. In first
place, we can observe how well expected response and given response were classified
from the EEG data for the three different classifiers: SVM, k-NN, and DT. Afterward, is presented the average score of how well the given emotions matched the
expected emotions for each set of stimuli type. The objective of this graph is to
observe which type of stimuli is more relevant when interpreting an emotion from it
for each group (ASD and typical). This group of three graphs is displayed first for
ASD subjects and after for typical subjects.
After this set, results for the correlation matrix between EEG features and the audio
features of the audio excepts the subject was being exposed to are presented. In
this case, the audio feature selection was done individually for each subject, both
in the case of typical and ASD individuals. This implies that we have a correlation
matrix for each child and that it does not make sense to compute any type of average
for this set of results. To avoid presenting such amount of individualized data, a
histogram representing the most relevant information contained in these correlation
matrix is also presented.
Finally, the confusion matrix between expected and given response is shown for
both groups: typical and ASD individuals. The idea here is to observe in each case
which emotions they tend to confuse with which, and to observe whether Chopin’s
32

4.1. Tables and graphics

33

[39] observations occur in this case too: that ASD children are similar to typical
children when extracting an emotion from music except for peaceful or calm music,
which is significantly more difficult for ASD children.
With the plots mentioned above, the average results of the information obtained is
presented. Nevertheless, it is also useful to observe, for each type of stimuli and for
each group of subjects, how well their EEG was matched to their given response
and expected response. The plots of these are shown after the general data.

4.1

Tables and graphics

4.1.1

Average Classification and Average Score Representation for ASD subjects

Figures 9a and 9b represent how well the expected emotion for the stimuli can
be classified from the EEG data. From the score obtained from the test set, one
can evaluate whether these parameters are correlated, and for this, three different
classifiers are employed: Support Vector Machine, k-Nearest Neigbour and Decision
Trees. No-correlation or a random guess of answers would result as a score of around
0.25, since there are possible answers: happy, sad, calm, and aggressive. Therefore,
consistent scores over 0.25 will suggest there is correlation.

(a) ASD subjects

(b) Typical Subjects

Figure 9: Average classification of expected response from EEG features
Figures 10a and 10b represent how well the given response for the stimuli can

34

Chapter 4. Results

be classified from the EEG data. From the score obtained from the test set, once
again, one can evaluate whether these parameters are correlated where no-correlation
would give a score of around 0.25 consistent scores over 0.25 will suggest there is
correlation.

(a) ASD subjects

(b) Typical Subjects

Figure 10: Average classification of given response from EEG features

Figures 11a and 11b show how well each group (typical and ASD children) identified
the emotions in the stimuli compared to the "correct" answer. From this data one
can observe to which type of stimuli each group is more sensitive to, both when
presented alone and combined, and to observe whether the first hypothesis is in fact
being confirmed.

(a) ASD subjects

(b) Typical Subjects

Figure 11: Average score of expected vs given response

4.1. Tables and graphics

4.1.2

35

Correlation Matrix between EEG features and Audio
features of ASD and Typical Subjects

ASD children
In the figures 15a to 15d, correlation matrix between EEG features and audio
features for ASD children are presented. Since the audio features are selected individually for each child, it would not be coherent to compute an average matrix for
each group, hence an individual matrix is shown per subject in the appendix. From
here one may observe whether we can find some correlation, and in that case, which
audio features tend to be correlated with the EEG features. To observe whether
there are some consistent patterns from one subject to the next, a histogram was
computed representing the data from all the ASD subjects.
Typical Children
In the figures 16a to 16g contained in the appendix, correlation matrix between
EEG features and audio features for typical children are presented. As explained
above, since the audio features are selected individually for each child, it would
not be coherent to compute an average matrix for each group, hence one matrix
per individual is shown. From here one may observe whether we can find some
correlation, and in that case, which audio features tend to be correlated with the
EEG features. Moreover, to extract global conclusions from this data, a histogram...
Average Histograms Below is presented the data contained in the correlation matrix
shown in the appendix in a different format. The first histogram in figure 12a is
a representation of the distribution of correlation of each musical category for both
typical and ASD children. From this figure one can observe the general distribution
of correlation, however, we may not observe which tends to be the most relevant
audio category from child to child in each group. Hence, another histogram is
computed and presented in figure 12b. For this case, the three maximum correlated
audio features were selected for each child, and the distribution of these is plotted.
This way, we observe to which audio category the children tend to be most affected

36

Chapter 4. Results

by.

(b) Percentage of maximum audio features
(a) Average correlations for ASD and typical
correlated to EEG features for Typical and
children
ASD

Figure 12: Correlation between EEG features and audio features statistics

4.1.3

Confusion Matrix of each set of stimuli

From figures 11a one may observe how well the ASD group interpreted the emotions
in comparison to the the "correct" or expected answer, however, this does not allow
observing which values tend to be confused with which. Figures 13a to 13e show
the average confusion matrix for each set of stimuli for ASD subjects. The idea of
computing this was due to reading about Chopin observing in her work that ASD
individuals were similar to typical children when interpreting emotions from music
except for peaceful, which ASD found more difficult to interpret. In this study, a
similar behavior was observed during the experiments with calm emotion, which
could be considered as similar, and it was desired to compute confusion matrix to
compare this in a more object manner.
From figures 11b one may observe how well the typical group interpreted the emotions in comparison to the the "correct" or expected answer, however, this does
not allow observing which values tend to be confused with which. Figures 14a to
14e show the average confusion matrix for each set of stimuli for ASD subjects. It
interesting to compare these results with the ASD results.

4.1. Tables and graphics

37

(a) Only Audio Stimuli

(b) Only Visual Stimuli

(c) Coherent Audio-Visual Stimuli

(d) Non-Coherent Audio-Visual Stimuli, Audio Response

(e) Non-Coherent Audio-Visual Stimuli, Visual Response

Figure 13: Confusion Matrix of each set of stimuli for ASD subjects

4.1.4

Valence-Arousal centroid plots compared to expected
emotion in ASD and typical subjects

To observe how well the arousal and valence values can be used to compute emotion,
these are plotted in comparison to the expected or "correct" emotion for each set of

38

Chapter 4. Results

(a) Only Audio Stimuli

(b) Only Visual Stimuli

(c) Coherent Audio-Visual Stimuli

(d) Non-Coherent Audio-Visual Stimuli, Audio Response

(e) Non-Coherent Audio-Visual Stimuli, Visual Response

Figure 14: Confusion Matrix of each set of stimuli for typical subjects

stimuli and for both groups, where red represents aggressive emotion, cyan represents
happy emotion, blue represents calm emotion, and green represents sad emotion.
To better visualize this data, the centroids are computed to show just the average
valence-arousal for each emotion in each situation. The expected, or correct arousalvalence value for each emotion should be, as presented in figure 5 positive arousal

4.1. Tables and graphics

39

and negative valence for aggressive emotion, positive arousal and positive valence for
happy emotion, negative arousal and positive valence for calm emotion, and finally,
negative arousal and valence for sad emotion.
This set of representations is presented in the appendix.

4.1.5

Valence-Arousal centroid plots compared to given response emotion in ASD and typical subjects

Like in the case of the previous subsection, to observe how well the arousal and
valence values can be used to compute emotion, these are plotted in comparison to
the given response for each set of stimuli and for both groups, where red represents
aggressive emotion, cyan represents happy emotion, blue represents calm emotion,
and green represents sad emotion. To better visualize this data, the centroids are
computed so only the average valence-arousal for each emotion in each situation
is shown. The expected, or correct arousal-valence value for each emotion should
be, as presented in figure 5 positive arousal and negative valence for aggressive
emotion, positive arousal and positive valence for happy emotion, negative arousal
and positive valence for calm emotion, and finally, negative arousal and valence for
sad emotion.
This set of representations is also presented in the appendix.

Chapter 5
Discussion

5.1
5.1.1

Discussion
Expected and Given Response Classification from EEG
features

ASD children results
From the results section we can observe in figure 9a how well the expected response,
or "correct" response, was classified from the EEG features of the ASD children in
average. Since they had four possible answers (happy, sad, calm or aggressive), in the
case of being no correlation and representing a random choice, around a 25% score
would be expected. In this case, however, for all types of classifications more than
this value can be observed. SVM shows the lowest score and shows scores of around
0.4 to 0.6. K Nearest Neighbor and Decision Trees show better scores, between 0.6
and 0.8. These results suggest there is a correlation between the expected response
and the EEG features of typical children.
Despite already observing correlations, when plotting the average expected response
respect to the arousal and valence computed by the EEG, not always the correct
emotion is predicted from the EEG recorded data. In this case, and in all cases,
results would be more precise if the data was "cleaned". Blinking, talking, moving,
40

5.1. Discussion

41

and other actions introduce artifacts in the signal which are very difficult to avoid.
These can be removed, sometimes manually, although they cannot be removed completely. In the case of this study, this step was not done due to time limitations.
Nevertheles, we can still observe valuable results, despite them probably being more
precise if that step were included.
Moreover, in the case of analyzing how well the given response was classified from
the EEG features of the ASD children in average, as presented in figure 10a, similar
results are observed. SVM classification shows around 0.4 or 0.5 score, and k-NN
and DT around 0.7 or 0.8. These results are slightly lower than in the case of EEG
features correlated to expected response, but the difference is almost imperceptible.
They also suggest there is a correlation between EEG features and the expected
response in ASD children.
Finally, when comparing the given responses with expected responses in ASD children we observe the "easiest" stimuli set to interpret emotions is the Visual stimuli.
This was completely unexpected. In the case of ASD children, according to the
literature, we would exoect emotions are not as easy to detect from visual stimuli
compared to audio stimuli. However, in the case of this study, the opposite was
observed: ASD children were more sensitive to visual stimuli both when presented
separately and when presented combined.
Typical children results
For typical children, we observe similar results to ASD children when observing
whether there is correlation between EEG features and given or expected response.
Both for expected and for given response, all classifications obtain scores over 0.25.
However, in the case of typical children all scores are a little lower than in the case
of ASD children. The reason behind this is unknown, however, while carrying out
the experiments it could be observed how ASD children are usually more sensitive
to any type of stimuli and tend to react stronger or in a more intense way. Perhaps,
this could be related with the fact that their EEG signal shows more correlation
with the emotion, because the emotion they feel or perceive is "more intense".

42

Chapter 5. Discussion

Finally, when comparing the given responses with expected responses in typical
children we observe the "easiest" stimuli set to interpret emotions is the Audio
Visual stimuli. This was expected, since combining two types of stimuli the effect of
these is reinforced. In the case of typical children, we have observed they were also
more sensitive to visual stimuli both when presented separately and when presented
combined.

5.1.2

Correlation between EEG features and Audio features

Individual Correlation Matrix Analysis
From the correlation matrix extracted we observe there is correlation between some
EEG features and audio features. The first ASD subject shows strongest correlations
of EEG features with harmony features. Although they are not consistent along all
EEG features, there is a clear tendency for harmony to have higher correlations in
comparison to other features. There is also some correlation with rhythm but not as
much. From the color we may observe around 0.3-0.4 of correlation with harmony
in some cases. For the second subject, correlation is observed along almost all EEG
features with harmony and one of timbre features. In this case, the correlation seems
to be around 0.3. Subject 3 shows high correlation with many features, especially in
the case of correlation with theta. The most significant seems to be harmony with
correlation coefficients from 0.3 to around 0.5. Moreover, timbre seems to show quite
high correlations, and some rhythm and melody features too. On the other hand,
subject 4 seems to show much less correlation between EEG and audio features in
general. Once again, the strongest correlations seem to be with harmony and timbre
features, however, they are not very strong.
In the case of typical subjects, the first does not show strong correlations in general
either. The strongest correlation seems to be with harmony, rhythm and timbre,
however quite light. The second subject does show higher correlations. In this
case, the subjects tends to show correlations with harmony and rhythm features
with around 0.3 to 0.5 correlation. Subject 3 of the typical group shows higher
correlation with harmony features, and is slightly correlated with timbre features

5.1. Discussion

43

too. Subject 4 suggests to have highest correlation with timbre features and rhythm
features, from around 0.3 to 0.5 correlation coefficient values too. On the other hand,
subject 5 shows quite high correlation with some melody features, a rhythm feature
and a timbre feature. The highest, the melody feature, has around a 0.6 correlation
coefficient. Subject 7 shows stronger correlation with harmony, timbre and rhythm
features, although not as high as the previous. Finally, the last feature shows quite a
high correlation of many audio features with the gamma wave. However, no specific
audio feature seems to have a strong correlation with all EEG features.
The overall observation is that there are correlations between audio features and
EEG features. However, there is not a consistent pattern observed where all subjects
have a stronger correlation with a specific audio category over the rest. In terms
of comparing the ASD group with the typical group no significant differences are
observed. The amount of correlation varies from one subject to another, but both
ASD and typical subjects results are in the same range of values: from around 0.1
to 0.6. The fact that correlation with a specific audio feature category changes from
subject to subject is also consistent from the ASD group to the typical group.
Correlation Matrix Histogram Analysis
Looking at figure 12a, one may observe the musical category with most correlation
with EEG features overall was timbre for both typical and ASD children. After that,
although rhythm seems to be lowest, there is not a significant different between the
rest of categories. In any case, it is considered an histogram representing a general
distribution of correlations between audio categories and EEG features is too broad
to make conclusions. Hence, a second histogram was computed with more precise
data.
In figure 12b, the histogram shows how the three most representative categories for
each child are distributed. It is believed this shows a better view of what category
is more influential in children of both groups when detecting emotions. From this
histogram we observe the most relevant category is, with a significant difference,
timbre. This happens for both groups, although it is a little higher in the case of

44

Chapter 5. Discussion

ASD children. Harmony is the second most influential category, although it has
much less relevance. Finally, melody and rhythm have very poor relevance in the
’top-three-rank’ being none in the case of ASD subjects.
Despite looking at the confusion matrix individually lead to interpreting that the
most influential aspect of music depends on the child, whether it is ASD or typical,
the histogram results show there is an aspect of music which tends to be more
relevant to children when interpreting emotion which is timbre. More research is
required to confirm this, however, this could be very useful information to be taken
into account in music therapy. In any case, it would be interesting to take the most
common influential category in mind for therapies, but it would be recommended
to carry out tests individually to confirm which is the most relevant aspect of music
for each child specifically.

5.1.3

Confusion Matrix for Expected and Given Response

To observe which emotions were confused with which, and in what set of stimuli,
confusion matrix were computed. Hence, this could be considered as a way of
unfolding the information shown in figures 11a and 11b "Average score of expected
vs given response".
ASD children results
In the case of ASD subjects, emotion in audio stimuli is interpreted reasonably well.
Calm emotions are confused with happy and sad, and sad with calm, although not
as much. In the case of visual stimuli, a stronger difference is observed: emotion
from aggressive, happy, and sad images were almost perfectly identified, while calm
emotions tend to be confused with happy and sad emotions, mainly happy. When
observing audio visual coherent stimuli, happy, sad and aggressive are very well
interpreted, while calm emotions tend to be confused with happy. In the case of
audio visual non coherent stimuli, in answers contrasted with the expected answers,
only happy emotions are identified, the rest are confused with other emotions, mostly
happy. It is important to remember that this set of stimuli presents a combination of

5.2. Conclusions

45

two different emotions, therefore, most of these are not identified well because visual
stimuli was predominant in the case of ASD children. An interesting observation
is that happy is also the best identified emotion when comparing with the visual
expected response. Therefore, probably not only the type of stimuli (audio or visual)
affects on how well the emotion is interpreted, but also the emotion itself. ASD seem
to interpret happy emotions easily, and even confuse other emotions with happy,
while they tend to have difficulties in identifying calm emotions, which are usually
confused with happy or sad.
Typical children results
We will now comment figures 14a to 14e, where confusion matrix for typical subjects
are presented. Overall, audio stimuli emotions are well defined, although sometimes
calm is confused with sad, and sad with calm. Visual stimuli emotions are also well
interpreted in general by typical children, and audio-visual coherent stimuli emotions
are almost perfectly interpreted. However, as in the case of ASD children, the noncoherent audio-visual stimuli compared with audio responses show poor results.
Once again, typical children are more influenced by visual stimuli, hence figure 14d
doesn’t show a strong diagonal line in the correlation matrix but doesn’t show any
tendencies to confuse with other emotions either. The most relevant observation
is that calm emotions do tend to be confused with sad emotions. Finally, when
observing figure 14e we see happy is perfectly identified, calm and sad emotions
are in overall well defined, and aggressive emotions tend to be confused with sad
emotions. From these we may conclude typical children are also more sensitive to
visual stimuli, and although they also have some tendencies to confuse calm with
sad or happy, it is not as strong as in the case of ASD children. Moreover, we don’t
observe a strong inclination for interpreting happy emotions, like ASD children do.

5.2

Conclusions

In opposition to the expected, ASD children who participated in this study appear
to be more sensitive to visual stimuli than to audio stimuli. According to previous

46

Chapter 5. Discussion

studies, ASD children tend to be more sensitive to audio stimuli than to visual,
but it was not the case in this study. It could be because the ASD participants had
already been through various years of therapy and maybe they are not representative
of ASD individuals.
Also, from this work it is concluded that results do show a correlation between
EEG features and expected and given response for both groups. The ASD group
shows a slightly higher correlation, which is suggested could be due to them being
more sensitive and expressive with stimuli in general. However, this reason is only
a possible explanation.
Observing correlation matrix individually, it was concluded there is not an audio
category that is consistently correlated with the EEG data from subject to subject,
and therefore that it seemed to be a personal preference. Nevertheless, observing
the statistical results of this data, a different conclusion was drawn: the category
of timbre is more relevant than the rest of categories when interpreting emotion
both for ASD and typical children. Therefore, as a final conclusion we observe the
category which is overall most relevant is timbre, however, it would be recommended
to analyze each patient individually if intending to use this information for music
therapy since timbre is not the most relevant in all cases.
As presented in the State of the Art section, Quintin [39] observed ASD individuals
in her study could interpret well emotion from music except for detecting peacefulness, for which they had difficulties. In this work, results also show difficulty in
ASD children from identifying calm emotions, which could be considered as similar
to peaceful. As a matter of fact, one of the ASD participants never assigned calm
to any emotion. Typical children also show difficulties in interpreting this emotion
in comparison to others, but the difference is not as strong as in ASD participants,
hence we conclude in this sense our results confirm Quintin’s observations. Furthermore, analyzing the confusion matrix one can also observe ASD individuals tend
to interpret emotions as happy more than any other emotion. Therefore, it could
be that their ability interpreting an emotion does not only depend on the type of
stimuli it is presented in, but also in the emotion itself.

Chapter 6
Contributions

6.1

Workshop conference

The results of this work have been presented and published in the 2nd International

Workshop on Quantitative and Qualitative Music Therapy Research http://quantitativemusictherapy.

6.2

Github repository

The code generated during this thesis is included in the following github repository:
https://github.com/nataliadelgadogalan

List of Figures
2

Paralimbic association cortices. Including (a) the cigulate gyrus, (b)
retrosplenial cortex, (c) parahippocampal cortex, (d) temporal lobe,
(e) insular cortex and (f) orbitofrontal cortex. [28]

. . . . . . . . . . 10

1

Limbic system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10

3

Neocortex [28] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
47

48

LIST OF FIGURES
4

Representation of where the Anterior Cingulate Cortex and Pre Frontal
Cortex are located . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

5

Computation of basic emotions from arousal and valence values . . . 15

6

Stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21

7

EEG map . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24

8

Example of EEG data in csv format . . . . . . . . . . . . . . . . . . . 27

9

Average classification of expected response from EEG features . . . . 33

10

Average classification of given response from EEG features . . . . . . 34

11

Average score of expected vs given response . . . . . . . . . . . . . . 34

12

Correlation between EEG features and audio features statistics . . . . 36

13

Confusion Matrix of each set of stimuli for ASD subjects . . . . . . . 37

14

Confusion Matrix of each set of stimuli for typical subjects . . . . . . 38

15

Correlation Matrix between EEG features and Audio features of ASD
subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58

16

Correlation Matrix between EEG features and Audio features of Typical subjects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

17

Valence-Arousal centroid plots compared to expected emotion for only
audio stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

18

Valence-Arousal centroid plots compared to expected emotion for only
visual stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60

19

Valence-Arousal centroid plots compared to expected emotion for coherent audio-visual stimuli . . . . . . . . . . . . . . . . . . . . . . . . 60

20

Valence-Arousal centroid plots compared to expected emotion for
non-coherent audio-visual stimuli, audio response . . . . . . . . . . . 61

21

Valence-Arousal centroid plots compared to expected emotion for
non-coherent audio-visual stimuli, visual response . . . . . . . . . . . 61

22

Valence-Arousal centroid plots compared to given response emotion
for only audio stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

23

Valence-Arousal centroid plots compared to given response emotion
for only visual stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . 62

LIST OF FIGURES
24

49

Valence-Arousal centroid plots compared to given response emotion
for coherent audio-visual stimuli . . . . . . . . . . . . . . . . . . . . . 62

25

Valence-Arousal centroid plots compared to given response emotion
for non-coherent audio-visual stimuli, audio response . . . . . . . . . 62

26

Valence-Arousal centroid plots compared to given response emotion
for non-coherent audio-visual stimuli, visual response . . . . . . . . . 63

List of Tables
1

Arousal computation from alpha waves . . . . . . . . . . . . . . . . . 13

2

Summary table of arousal and valence computation . . . . . . . . . . 25

50

Bibliography
[1] Lai, M.-C., Lombardo, M. V., Chakrabarti, B. & Baron-Cohen, S. Subgrouping
the autism “spectrum": Reflections on dsm-5. PLoS Biol 11, e1001544 (2013).
[2] Applebaum, E., Egel, A. L., Koegel, R. L. & Imhoff, B. Measuring musical
abilities of autistic children. Journal of autism and developmental disorders 9,
279–285 (1979).
[3] Ramirez, R. & Vamvakousis, Z. Detecting emotion from eeg signals using the
emotive epoc device. In International Conference on Brain Informatics, 175–
184 (Springer, 2012).
[4] Simpson, K. & Keen, D. Music interventions for children with autism: narrative
review of the literature. Journal of autism and developmental disorders 41,
1507–1514 (2011).
[5] Edgerton, C. L. The effect of improvisational music therapy on the communicative behaviors of autistic children. Journal of music therapy 31, 31–62
(1994).
[6] Baron-Cohen, S. The extreme male brain theory of autism. Trends in cognitive
sciences 6, 248–254 (2002).
[7] Baron-Cohen, S. Autism: the empathizing–systemizing (e-s) theory. Annals of
the New York Academy of Sciences 1156, 68–80 (2009).
[8] APA, D. statistical manual of mental disorders. 4th. Text Revision Ed, American Psychiatric Association (2004).
51

52

BIBLIOGRAPHY

[9] Zager, D. B. Autism: Identification, education, and treatment (Lawrence Erlbaum Associates, 1999).
[10] de Boer-Ott, S. et al. Autism spectrum disorders: Interventions and treatments
for children and youth (Corwin Press, 2004).
[11] Christensen, D. L. Prevalence and characteristics of autism spectrum disorder
among children aged 8 years—autism and developmental disabilities monitoring
network, 11 sites, united states, 2012. MMWR. Surveillance Summaries 65
(2016).
[12] Baron-Cohen, S. Mindblindness: An essay on autism and theory of mind (MIT
press, 1997).
[13] Myers, S. M., Johnson, C. P. et al. Management of children with autism spectrum disorders. Pediatrics 120, 1162–1182 (2007).
[14] McPheeters, M. L. et al. A systematic review of medical treatments for children
with autism spectrum disorders. Pediatrics 127, e1312–e1321 (2011).
[15] Blackstock, E. G. Cerebral asymmetry and the development of early infantile
autism. Journal of Autism and Developmental Disorders 8, 339–353 (1978).
[16] Green, G., Ricciardi, J., Boyd, B. et al. The national standards project–
addressing the need for evidence-based practice guidelines for autism spectrum
disorders. National Autism Center. Massachusettes. National Standards Report
(2009).
[17] Whipple, J. Music in intervention for children and adolescents with autism: A
meta-analysis. Journal of music therapy 41, 90–106 (2004).
[18] Alvin, J. Music therapy for the autistic child (Oxford University press, 1978).
[19] Heaton, P. Interval and contour processing in autism. Journal of autism and
developmental disorders 35, 787 (2005).

BIBLIOGRAPHY

53

[20] Heaton, P., Hudry, K., Ludlow, A. & Hill, E. Superior discrimination of speech
pitch and its relationship to verbal ability in autism spectrum disorders. Cognitive neuropsychology 25, 771–782 (2008).
[21] Caria, A., Venuti, P. & de Falco, S. Functional and dysfunctional brain circuits underlying emotional processing of music in autism spectrum disorders.
Cerebral Cortex bhr084 (2011).
[22] Quintin, E.-M., Bhatara, A., Poissant, H., Fombonne, E. & Levitin, D. J. Emotion perception in music in high-functioning adolescents with autism spectrum
disorders. Journal of autism and developmental disorders 41, 1240–1255 (2011).
[23] Matamoros, E., Ramírez, R., Mirabell, J. & Sánchez, E. Encoding emotional
states in asd: Can music help autistic children identify emotions in faces? .
[24] Juslin, P. N. Communicating emotion in music performance: A review and a
theoretical framework. (2001).
[25] Joseph, R. The limbic system: emotion, laterality, and unconscious mind.
Psychoanalytic review 79, 405 (1992).
[26] Derryberry, D. & Tucker, D. M. Neural mechanisms of emotion. Journal of
consulting and clinical psychology 60, 329 (1992).
[27] Papez, J. W. A proposed mechanism of emotion. Archives of Neurology &
Psychiatry 38, 725–743 (1937).
[28] Heilman, K. M. & Gilmore, R. L. Cortical influences in emotion. Journal of
Clinical Neurophysiology 15, 409–423 (1998).
[29] Borod, J. C. Interhemispheric and intrahemispheric control of emotion: a focus
on unilateral brain damage. Journal of consulting and clinical psychology 60,
339 (1992).
[30] Sammler, D., Grigutsch, M., Fritz, T. & Koelsch, S. Music and emotion: electrophysiological correlates of the processing of pleasant and unpleasant music.
Psychophysiology 44, 293–304 (2007).

54

BIBLIOGRAPHY

[31] Niedermeyer, E. et al. The normal eeg of the waking adult. Electroencephalography: Basic principles, clinical applications, and related fields 167, 155–164
(2005).
[32] Schacter, D. L. Eeg theta waves and psychological phenomena: A review and
analysis. Biological psychology 5, 47–82 (1977).
[33] Hari, R. & Salmelin, R. Human cortical oscillations: a neuromagnetic view
through the skull. Trends in neurosciences 20, 44–49 (1997).
[34] Heilman, K. The neurobiology of emotional experience. The neuropsychiatry
of limbic and subcortical disorders 133–142 (1997).
[35] Steriade, M. Cellular substrates of brain rhythms. Electroencephalography basic
principles, clinical applications, and related fields. Philadelphia: (2005).
[36] Sebastiani, L., Simoni, A., Gemignani, A., Ghelarducci, B. & Santarcangelo, E.
Autonomic and eeg correlates of emotional imagery in subjects with different
hypnotic susceptibility. Brain research bulletin 60, 151–160 (2003).
[37] Aftanas, L., Reva, N., Savotina, L. & Makhnev, V. Neurophysiological correlates of induced discrete emotions in humans: an individually oriented analysis.
Neuroscience and behavioral physiology 36, 119–130 (2006).
[38] Krumhansl, C. L. An exploratory study of musical emotions and psychophysiology. Canadian Journal of Experimental Psychology/Revue canadienne de
psychologie expérimentale 51, 336 (1997).
[39] Choppin, A. Eeg-based human interface for disabled individuals: Emotion
expression with neural networks. Unpublished master’s thesis (2000).
[40] Lin, Y.-P. et al. Eeg-based emotion recognition in music listening. IEEE Transactions on Biomedical Engineering 57, 1798–1806 (2010).
[41] Frith, U. Emanuel miller lecture: Confusions and controversies about asperger
syndrome. Journal of child psychology and psychiatry 45, 672–686 (2004).

BIBLIOGRAPHY

55

[42] Lambie, J. A. & Marcel, A. J. Consciousness and the varieties of emotion
experience: a theoretical framework. Psychological review 109, 219 (2002).
[43] Linden, W., Wen, F. & Paulhus, D. L. Measuring alexithymia: reliability,
validity, and prevalence. Advances in personality assessment 10, 51–95 (1995).
[44] Ekman, P. Are there basic emotions? (1992).
[45] Ahern, G. L. & Schwartz, G. E. Differential lateralization for positive and
negative emotion in the human brain: Eeg spectral analysis. Neuropsychologia
23, 745–755 (1985).
[46] Renard, Y. et al. Openvibe: An open-source software platform to design, test,
and use brain–computer interfaces in real and virtual environments. Presence:
teleoperators and virtual environments 19, 35–53 (2010).
[47] Kim, J. & André, E. Emotion recognition based on physiological changes in
music listening. IEEE transactions on pattern analysis and machine intelligence
30, 2067–2083 (2008).
[48] Bradley, M. & Lang, P. J. The International affective digitized sounds (IADS)[:
stimuli, instruction manual and affective ratings (NIMH Center for the Study
of Emotion and Attention, 1999).
[49] Lang, P. J., Bradley, M. M. & Cuthbert, B. N. International affective picture
system (iaps): Technical manual and affective ratings. NIMH Center for the
Study of Emotion and Attention 39–58 (1997).
[50] https://www.emotiv.com/. Emotive epoc.
[51] http://essentia.upf.edu/documentation/documentation.html. Essentia.
[52] https://weka.wikispaces.com/. Weka.
[53] Salamon, J. & Gómez, E. Melody extraction from polyphonic music signals
using pitch contour characteristics. IEEE Transactions on Audio, Speech, and
Language Processing 20, 1759–1770 (2012).

56

BIBLIOGRAPHY

[54] Zhang, X. & Zbigniew, W. R. Analysis of sound features for music timbre
recognition. In Multimedia and Ubiquitous Engineering, 2007. MUE’07. International Conference on, 3–8 (IEEE, 2007).

Appendix A
Appendix

A.1
A.1.1

Added figures from Results section
Correlation Matrix between EEG features and Audio
features of ASD and Typical Subjects

A.1.2

Valence-Arousal centroid plots compared to expected
emotion in ASD and typical subjects

A.1.3

Valence-Arousal centroid plots compared to given response emotion in ASD and typical subjects

57

58

Appendix A. Appendix

(a) ASD Subject 1

(b) ASD Subject 2

(c) ASD Subject 3

(d) ASD Subject 4

Figure 15: Correlation Matrix between EEG features and Audio features of ASD
subjects

A.1. Added figures from Results section

59

(a) Typical Subject 1

(b) Typical Subject 2

(c) Typical Subject 3

(d) Typical Subject 4

(e) Typical Subject 5

(f) Typical Subject 6

(g) Typical Subject 7

Figure 16: Correlation Matrix between EEG features and Audio features of Typical
subjects

60

Appendix A. Appendix

(a) ASD subjects

(b) Typical Subjects

Figure 17: Valence-Arousal centroid plots compared to expected emotion for only
audio stimuli

(a) ASD subjects

(b) Typical Subjects

Figure 18: Valence-Arousal centroid plots compared to expected emotion for only
visual stimuli

(a) ASD subjects

(b) Typical Subjects

Figure 19: Valence-Arousal centroid plots compared to expected emotion for coherent audio-visual stimuli

A.1. Added figures from Results section

(a) ASD subjects

61

(b) Typical Subjects

Figure 20: Valence-Arousal centroid plots compared to expected emotion for noncoherent audio-visual stimuli, audio response

(a) ASD subjects

(b) Typical Subjects

Figure 21: Valence-Arousal centroid plots compared to expected emotion for noncoherent audio-visual stimuli, visual response

(a) ASD subjects

(b) Typical Subjects

Figure 22: Valence-Arousal centroid plots compared to given response emotion for
only audio stimuli

62

Appendix A. Appendix

(a) ASD subjects

(b) Typical Subjects

Figure 23: Valence-Arousal centroid plots compared to given response emotion for
only visual stimuli

(a) ASD subjects

(b) Typical Subjects

Figure 24: Valence-Arousal centroid plots compared to given response emotion for
coherent audio-visual stimuli

(a) ASD subjects

(b) Typical Subjects

Figure 25: Valence-Arousal centroid plots compared to given response emotion for
non-coherent audio-visual stimuli, audio response

A.1. Added figures from Results section

(a) ASD subjects

63

(b) Typical Subjects

Figure 26: Valence-Arousal centroid plots compared to given response emotion for
non-coherent audio-visual stimuli, visual response

