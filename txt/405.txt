2012 Brazilian Symposium on Collaborative Systems

Exploring a Brain Controlled Interface for Emotional Awareness
Mauro C. Pichiliani and Celso Massaki Hirata

Tania Fraga

Department of Computer Science
ITA - Instituto Tecnológico de Aeronáutica
São José dos Campos, Brasil
{pichilia,hirata}@ita.br

IMA - Instituto de Matemática e Artes de São Paulo
São Paulo, Brasil
tania.fraga@gmail.com

condition can provide positive or negative impact. These
impacts can somehow influence the user experience of an
application and also can increase the importance of the
computer-mediated affective awareness as a factor in the
design and construction of CES.
The goal of this paper is to explore the use of an
electroencephalograph system to provide awareness
information about the users’ affective state in synchronous
CES. Our research proposal employs a low-cost off-the-shelf
electroencephalograph (EEG) system to collect emotional
awareness information in a relevant manner to CSCW
research. We also investigate how to effectively represent
emotional awareness in CES by designing visual widgets
coupled with user interfaces.
The research method is based on the proposition of a set
of different visual awareness widgets and the evaluation of
them in a controlled experiment that mimics real world
collaboration scenarios. Our hypothesis is that some of the
visual widgets used to provide emotional awareness
contribute to the collaboration more than others and are
suitable to provide meaningful emotional awareness in
synchronous CES. Our main research effort aims to provide
answers to the question: How emotional awareness can
affect collaboration aspects in synchronous CES?
The rest of the paper is organized as follows. Section 2
describes the definition and applications of emotional
awareness in collaborative systems. Section 3 presents the
use of Brain Controlled Interfaces to capture emotions
directly from the brain. Section 4 discusses how to represent
emotional awareness in CES. Finally, Section 5 presents the
conclusions, comments and future work.

Abstract— Emotions play an important role in human
interaction, communication, coordination, and cooperation.
The perception of emotional and affective state in present and
past group’s activities is a relevant issue since emotional
awareness influence the outcome of cooperative work. While
there is a growing interest in providing computational support
for recognition and representation of emotions, few research
efforts explore the potential of Brain Controlled Interfaces
(BCI) to collect and present emotional awareness in
synchronous collaborative systems. In this paper, we present
an initial step in exploring how a low-cost off-the-shelf
electroencephalograph (EEG) system can be used to provide
emotional awareness information in synchronous collaborative
editing systems.
Keywords-component; Affective Computing; Emotional
Awareness; Computer Supported Cooperative Workd; ComputerMediated Communucation; Brain controlled interface.

I.

INTRODUCTION

The CSCW (Computer Supported Cooperative Work)
area has many goals including the search for ways in which
information technology effectively supports group work.
One alternative to achieve this goal is the exploration of
awareness resources that provide information on the
participants’ actions, sense of presence, and emotions during
group work.
The resources used to provide awareness information in
Collaborative Editing Systems (CES) allow the participants
to obtain knowledge of the group activities to know what
happened, what is happening and what will happen, and also
to provide details about the work and the group. However,
most awareness information provided in collaborative
sessions relies on the sense of presence and task activities
instead of the users’ affective state. The reason to investigate
emotional awareness in CES is that this type of information
is known to affect the design solutions, guide social
implications, and influence organizational effects [2].
We choose synchronous CES because this type of
application requires immediate response that may impact
participant’s perception and behavior. We think that
synchronous CES has potential for the exploration of
emotional awareness. Moreover emotional awareness can be
considered a new layer of information employed to
understand users’ behavior. It also has the potential to
enhance the sense of group presence to remote participants.
The motivation for the study of emotional awareness
includes the need to identify and better understand how the
insertion of such information about personal human affective

978-0-7695-4890-6/12 $26.00 © 2012 IEEE
DOI 10.1109/SBSC.2012.34

II.

EMOTIONAL AWARENESS

The study of emotions involves many disciplines such as
neurology, psychology, and physiology. From the computer
science point of view, the Affective Computing area explores
how computing relates to, arises from, or deliberately
influences emotions [8] arguing that by conferring the
computers with abilities to recognize, model, and synthesize
emotions, human-human and human-computer interaction
will be improved.
The Computer Supported Cooperative Work area studies,
among other topics, the necessary factors to create a
common context shared by the participants during
collaboration. This context prevents that a specific
participant feels isolated from the group, thus reducing his
contributions and distancing himself from the work being
accomplished. The information provided by other

49

participants' presences and their roles during collaboration is
classified as Collaboration Awareness and is the most
common type of awareness available in CES. The
Collaboration Awareness is responsible to provide the sense
of presence and actions of the group to remote participants.
This means that awareness allows each participant of the
group to coordinate and organize his/her work, since he/she
has information that allows the understanding of what others
are doing. The awareness also provides the opportunity to
both enhance communication, either informal or formal and
support the social protocol used while the work is being
produced. The design and construction of groupware that
relies on efficient communication technology to support
transparent distributed interaction, to the point of achieving
the same communication faculty of a face-to-face interaction,
are still a challenge.
García et. al [2] state that at the same way collaboration
awareness can be used to bring sense of identity and colocation, the use of mechanisms that represent emotions can
also provide awareness of their collaborator's emotional state
and act accordingly to achieve better results in their joint
work. This specific type of collaboration awareness is named
Emotional Awareness and follows recent research efforts for
Sentiment Analysis and Opinion Mining [7].
The possibilities for applications of emotional awareness
in groupware system include: the enhancement of electronic
meeting systems with the level of audience interest, better
recommendation systems based on opinions and sentiments,
availability of information for initialization of casual
interactions, aggregation of sentimental information in order
to support collaborative decision making, detection of
willingness for collaboration on a specific task, and
empirical evaluation of user intention preservation when
concurrent controls techniques based on the CCI
(Convergence, Causality-preservation, and Intentionpreservation) [11] model are employed.
The development of emotional-aware groupware is not
considered a trivial task since conception, specification and
design decisions require the manipulation of sensors and
input devices, emotional models, pattern recognition
algorithms, and widgets for representing the user’s emotional
state [3]. Here we focus on the representation of awareness
information in collaborative systems that people use to
perform group work.
The traditional approach employed to display
information about the sense of presence and workspace
awareness is visual awareness devices that provide
information about the participants and their actions. The
interface elements are called awareness widgets and are
designed as components of the user interface that offer the
opportunity to understand the meaning of the actions and
allow the participants to coordinate their activities and
enhance the communication. At the current state of
development the problem of emotion representation by
means of an user interface widget is an open problem since
the interpretation of the complexity and richness of such a
subject affective state is still at an early stage of development
by CSCW research.

The literature of the CSCW area contains many
awareness widgets used to represent different types of
awareness including informal, group-structural, social, and
workspace awareness. Gárcia et. al [2] suggest experimental
interfaces to display emotion representation in collaborative
systems including: emotional awareness graphs, affective
icons and avatars, facial texture mappers, and a mood-drive
availability wheel. Although these interfaces present some
form of emotional awareness, they do not link the emotional
state with artifacts, task or the workspace where the users are
collaborating. These links are a relevant issue once they
allow the participants to associate their emotional state with
a specific action he/she is performing that can be used to
understand decisions, organize the tasks and its performers
according to mood, and inform how the coordination and
communication may guide the flow of the collaboration as a
whole.
III.

BRAIN CONTROLLED INTERFACES

There are many types of sensors that capture biological
signals including: Electromyography (EMG) to measure
muscle
activity,
Temperature
(TEMP),
Electroencephalography (EEG) used for brain-wave
monitoring, Skin Conductance (SC) to measure
electrodermal activity, Electrocardiogram (EKG) for heart
activity
monitoring,
Respiration
(RESP),
Photoplethysmography (BVP/HR) used for blood volume
pulse and heart rate monitoring.
In this paper, we focus on the Electroencephalograph
(EEG) that is the most commonly used technology in
contemporary noninvasive BCI research. EEG uses
electrodes placed on the scalp to measure the weak electrical
potentials generated by brain activity. The signal provided by
an EEG is at best a crude representation of brain activity due
to the nature of the detector. Scalp electrodes are only
sensitive to macroscopic and coordinated firing of large
groups of neurons near the surface of brain, and then only
when they are directed along a perpendicular vector relative
to the scalp. Additionally, because of the fluid, bone, hair,
and skin that separate the electrodes from the actual
electrical activity, the already small signals are scattered and
attenuated before reaching the electrodes.
Although BCIs are employed mostly in the HCI (Human
Computer Interaction) area by exploring its capability to
detect brain patterns in order to manipulate elements directly
with the brain, we concentrate our attention on EEG work
related to the ability to transform raw EEG data streams into
emotional and affective states. Considering the available
commercial noninvasive EEG, we opted for the low-cost offthe-shelf Emotiv EEG headset [1] due to the resourceful
standard SDK that allows three event-based classification
suites. The Expressive suite tries to detect the wearer’s facial
expression; the Affective suite tries to detect mood and
emotions; and the Cognitive suite tries to detect occurrences
of user-defined cognitive events.
Since we focus on the capture and representation of
emotional awareness our research efforts concentrate on the
Affective suite, which allows the capture of quantitative data
for
three
affective
states:
excitement/calm,

50

engagement/disinterest and meditation. The detections
provided by the Affectiv suite search for brainwave
characteristics that are universal in nature and do not require
an explicit training or signature-building step on the part of
the user. Figure 1 shows two users wearing the Emotiv EEG
headset viewed by four different positions during our initial
exploration of this device.

a) Right

b) Front

c) Back

IV.
CES

REPRESENTING EMOTIONAL AWARENESS IN

In this work we propose the exploration and study of
emotional awareness in order to conceive, design, create and
evaluate new widgets that represent emotional awareness
collected directly from an EEG. To guide the design of our
awareness widget we follow basic awareness principles
proposed by previous work [5, 9] which include: no explicit
actions required, least effort, no additional space, and focus
of attention. We also investigate how existing awareness
frameworks [6] can provide useful foundations for the design
and evaluation of the widgets that display emotional
awareness.
We chose to use the Emotiv EEG headset as a device to
collect emotional state in order to achieve two main goals:
(1) understand how this emotional information can influence
general collaboration aspects (communication, coordination
and cooperation); and (2) use this information to provide
additional awareness for the users in remote collaboration
scenarios aiming to provide a further sense of presence.
Considering the second goal, the criterion, results,
insights, and findings from previous work [5, 6, 9] are
related to emotional studying from the awareness perspective
as a guide to how model, present, convey, and evaluate
emotional information in the context of remote CES. We did
not expect to influence, suppress or manipulate human
emotions in any way during our study.
We believe that new technologies for collecting
emotional data can provide further explanations for the main
research question of this work compared to traditional
emotional gathering techniques employed for emotional
monitoring found on the Affective Computing research
efforts [8], such as the analysis of body language, voice, and
face emoticons. The base for this argument is the fact that
traditional emotional detection techniques require complex
sensors and apparatus, demand high level of manual and ad
hoc data gathering, do not provide non subjective reliable
emotions information, and lack real time quantitative data
suitable for awareness widgets found on CES.
Although it is possible to research emotional data from
theoretical models that focus on simulation of emotions or
predicted data [4], the complexity and unexpected
interactions usually found on remote CES sessions suggest
that emotional data provided by simulated collaboration
sessions do not represent well how the users behave in such
situations where communication, cooperation and
collaboration aspects assume different forms.
Besides the existing widgets that represent affective state
associated with artifacts manipulated by participants on a
synchronous collaborative session, we are exploring
alternative representations of emotions in the user interface.
The visual cues and widgets we are exploring include the use
of color-coded and size-coded words to represent emotional
in multi-user text editors, heat-map superimposition on a
new
layer
of
the
virtual
workspace,
and
Telepointer/Telecaret augmentation with icons for real-time
mood representation. We are also looking at the possibility to
augment the video/audio stream [10] used as external

d) Left

Figure 1. Two users wearing the Emotiv EEG headset
viewed by different positions: a) Right, b) Front,
c) Back and d) Left

The excitement/calm state is experienced as an awareness
or feeling of physiological arousal with a positive value. The
excitement/calm detection is tuned to provide output scores
that more accurately reflect changes in excitement over time
periods as short as several seconds. The emotions related to
the excitement/calm affective state includes titillation,
nervousness, and agitation.
Engagement/disinterest is experienced as alertness and
the conscious direction of attention towards task-relevant
stimuli. It is characterized by increased physiological
arousal and beta waves (a well-known type of EEG
waveform) along with attenuated alpha waves (another type
of EEG waveform). The greater the attention, focus and
cognitive workload, the greater the output score reported by
the engagement/disinterest detection. The emotions related
to the engagement/disinterest affective state include
alertness, vigilance, concentration, stimulation, and interest.
Although the meditation state recognition is provided by
the Emotiv EEG headset, it is not associated directly with
any emotion that indicates a noticeable user mental status
usually presented in the normal use of computational
systems. Moreover, this detection is provided to guide users
to a relaxed state of mind that avoids the expression of
emotions instead of the explicit detection of them.
The Emotiv EEG headset contains many differences
regarding complexity, cost and operational use compared to
invasive and noninvasive medical EEGs employed to exam
and study the brain. However, the information it provides is
adequate to the study of emotional aspects for computer
interaction and emotional state by employing the many EEG
concepts, techniques, models, and other resources originated
for the several years of advancements that come for the
medical study of the brain.

51

presentation of this paper and the individuals involved in the
initial exploration of the Emotiv EEG headset.

channel in collaborative sessions to better convey the
affective state of the participants, thus improving the
communication aspect of the collaboration.
The design of a future experiment for evaluating the
proposed widgets that display emotional information in CES
include the monitoring of user emotional state while using
traditional interaction devices (e.g. mouse and keyboard) to
perform common actions such as moving objects, changing
shapes, editing text, and selecting objects required to execute
simple collaborative tasks supported by a CES .
V.

REFERENCES
[1]

Emotiv
EPOC
Neuro
heaset,
http://www.emotiv.com/store/hardware/epoc-bci/epoc-neuroheadset/.
[2] O. García, J. Favela, and R. Machorro. “Emotional awareness in
collaborative systems”. Proc. of the String Processing and
Information Retrieval Symposium & International Workshop on
Groupware (SPIRE ’99), 1999, pp. 296-303.
[3] O. García, J. Favela, and R. Machorro. “Extending a collaborative
architecture to support emotional awareness”. In Workshop on
Emotion-Based Agent Architectures, Autonomous Agents ’99, 1999,
pp. 46-52.
[4] J. Hernandez, A. Sano, M. S. Goodwin, and R. W. Picard. “AMA, an
application for Annotation, Monitoring, and Analysis of behavioral
activity”. Extended Abstract of IMFAR 2012, 2012, pp. 17-19.
[5] S. Junuzovic, P. Dewan, and Y. Rui. “Read, write, and navigation
awareness in realistic multi-view collaborations”. Proc. of the 3rd
IEEE Conference on Collaborative Computing: Networking,
Applications and Worksharing, 2007, pp. 494-503.
[6] M. Kirsch-Pinheiro, J. V. Lima, and M. R. S. Borges. “A framework
for awareness support in groupware systems”. Computers in Industry
- Special issue: Knowledge sharing in collaborative design
environments 52, 1, Sep. 2003, pp. 47-57.
[7] B. Pang, and L. Lee. “Opinion mining and sentiment analysis.”
Foundations and Trends in Information Retrieval 2, 1-2, Jan. 2008,
pp.1-135.
[8] R. W. Picard. Affective computing. MIT Press, 1997.
[9] M. C. Pichiliani, C. M. Hirata, F. S. Soares, and C. H. Q. Forster.
“TeleEye: an awareness widget for providing the focus of attention in
collaborative editing systems”. Proceedings of the 4th IEEE
Conference on Collaborative Computing: Networking, Applications
and Worksharing, 2008, pp. 258-270.
[10] S. Sakurai, T. Narumi, T. Tanikawa, and M. Hirose. “Augmented
emotion by superimposing depiction in comics”. Proceeding ACE '11
Proceedings of the 8th International Conference on Advances in
Computer Entertainment, 2011, technology article No. 66.
[11] C. Sun, X. Jia, Y. Zhang, Y. Yang, and D. Chen. “Achieving
convergence, causality-preservation, and intention-preservation in
real-time cooperative editing systems”. ACM Transactions on
Computer-Human Interaction, 5, Mar. 1998, 63-108.

CONCLUSION AND FUTURE WORK

In this paper, we propose the exploration of Brain
Controlled Interfaces (BCI) aiming to study the possibilities
to collect and represent emotional awareness in synchronous
collaborative editing systems. We reviewed some of the
existing literature in the CSCW area that discusses the
importance of emotions in rational thinking and human
interaction and the need to support emotional awareness in
the design and implementation of groupware.
The main contribution of this paper concentrates in the
novel use of the EEG technology combined with awareness
widgets to provide emotion feedback from the biological
signals generated by the participants of a synchronous
collaborative session. The central question we want to
answer is How emotional awareness can affect collaboration
aspects in synchronous CES? The possible answers to this
question can increase our understanding of the groupware
domain, lead to more focused research, and influence
commercial products.
We believe this work represents an initial step in
exploring how brain sensing technologies can be applied in a
relevant manner to contemporary groupware emotional
research area. With the new awareness information provided,
it is possible to enhance the coordination and communication
of actions between the participants of collaborative sessions,
giving them awareness information somehow beyond what is
found in face to face meetings.
Future work includes the evaluation of how the
introduction of users’ emotional awareness in a collaborative
application affects the group work in order to assess the
effectiveness of new visual awareness widgets. Future work
also includes a comparison of the existing widgets
traditionally employed to display awareness in CES
providing valuable information on how the participants
communicate and coordinate their activities when they
possess emotional awareness of each participant of the
group.
Finally, this work represents a starting point of research
work that explores how computers can collect emotional
state within our minds to help us coordinate, communicate
and cooperate better while producing group work. We hope
this work will inspire and encourage other researchers in the
CSCW community.
ACKNOWLEDGMENT
The authors want to thank the anonymous reviewers
that made suggestions to significantly improve the

52

