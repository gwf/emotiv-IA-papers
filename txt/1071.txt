Copyright © 2015, Society of Photo-Optical Instrumentation Engineers. One print or electronic copy may be made for personal use only.
Systematic reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes, or modification of
the content of the paper are prohibited.

Emotion-prints: Interaction-driven Emotion Visualization
on Multi-touch Interfaces
Daniel Cerneaa,b , Christopher Webera , Achim Eberta , and Andreas Kerrenb
a University

of Kaiserslautern, Computer Graphics and HCI Group
P.O. Box 3049, D-67653 Kaiserslautern, Germany;
b Linnaeus University, Computer Science Department, ISOVIS Group
Vejdes Plats 7, SE-35195 Växjö, Sweden
ABSTRACT
Emotions are one of the unique aspects of human nature, and sadly at the same time one of the elements that
our technological world is failing to capture and consider due to their subtlety and inherent complexity. But
with the current dawn of new technologies that enable the interpretation of emotional states based on techniques
involving facial expressions, speech and intonation, electrodermal response (EDS) and brain-computer interfaces
(BCIs), we are finally able to access real-time user emotions in various system interfaces. In this paper we
introduce emotion-prints, an approach for visualizing user emotional valence and arousal in the context of
multi-touch systems. Our goal is to o↵er a standardized technique for representing user a↵ective states in the
moment when and at the location where the interaction occurs in order to increase a↵ective self-awareness,
support awareness in collaborative and competitive scenarios, and o↵er a framework for aiding the evaluation
of touch applications through emotion visualization. We show that emotion-prints are not only independent
of the shape of the graphical objects on the touch display, but also that they can be applied regardless of the
acquisition technique used for detecting and interpreting user emotions. Moreover, our representation can encode
any a↵ective information that can be decomposed or reduced to Russell’s two-dimensional space of valence and
arousal. Our approach is enforced by a BCI-based user study and a follow-up discussion of advantages and
limitations.
Keywords: Emotion visualization, emotion fingerprints, touch events, multi-touch interface, human-centered
design, brain-computer interface

1. INTRODUCTION
Emotions are one of the omnipresent aspects of our lives: whatever we do, we are always accompanied, influenced
and even defined by the emotional states that we experience. As such, it is not really surprising that scientists,
technicians and developers are often seen as “cold” or rather emotionless, as informational systems are one of
the few fields of our modern lives where feelings have gained minimal importance in the last years. This is partly
due to the fact that IT systems are generally oriented towards information, speed and efficiency, but also because
the interpretation of human emotions is as elusive and subtle as the emotional states themselves. Should this
mean that software applications are doomed to contain only “dry”, quantitative information?
In recent years, di↵erent technologies have been developed that made it easier to capture and interpret the
various emotions that users might experience based on facial expressions, speech and intonation, electrodermal
response, brain signals, etc. While these techniques that can indicate the presence of certain emotions are slowly
finding their way into software systems, most of the accent falls on a↵ective applications1 that are able to detect
a set of user emotions and change their behavior based on these readings (i.e., emotional states as subconscious
data input). Further examples for this include applications that react to emotions extracted from speech,2 as
well as systems that change the style of a painting based on the emotional state of the viewer.3
Further author information: (send correspondence to D.C.)
D.C.: E-mail: cernea@cs.uni-kl.de; C.W.: E-mail: chweber@rhrk.uni-kl.de;
A.E.: E-mail: ebert@cs.uni-kl.de; A.K.: E-mail: andreas.kerren@lnu.se
Daniel Cernea, Christopher Weber, Achim Ebert, and Andreas Kerren, "Emotion-Prints: Interaction-Driven Emotion Visualization on
Multi-Touch Interfaces", in Visualization and Data Analysis 2015, edited by David L. Kao, Ming C. Hao, Mark A. Livingston, and Thomas
Wischgoll, Proceedings of SPIE-IS&T Electronic Imaging, SPIE Vol. 9397, 9397-0A (2015). DOI: 10.1117/12.2076473

In the following, we propose a visualization approach, called emotion-prints, for incorporating real-time
information about user emotions in new or already designed multi-touch interfaces. Similar to fingerprints,
where touching an object would leave a marker on the object uniquely identifying the person, emotion-prints
aim at introducing a standardized visualization of emotions by marking the virtual object that a user touches
with a representation of the user’s current emotional state. The power of this representation lies in the fact that
it is applicable to any multi-touch screen and any interface element—regardless of its shape, size or color—as
long as at least parts of its margins are inside the interface. Furthermore, our representation can encode and
display a variety of emotions in terms of a↵ective valence (pleasant or unpleasant) and arousal (excited or calm),
while at the same time being entirely independent of the technique employed for acquiring the user’s a↵ective
states.
The goal of emotion-prints is to improve the user’s emotional self-awareness, the awareness of other users
emotions in collaborative and competitive scenarios—be it games, visualization systems or any collaborative
touch-based application—and support the evaluation of touch systems and user experience through the visualization of emotional involvement.
Awareness of and reflection on emotional states has proven particularly important in collaborative contexts, as “group self-awareness—of emotional states, strengths and weaknesses, modes of interaction, and task
processes—is a critical part of group emotional intelligence that facilitates group efficacy”.4 Furthermore, awareness of emotional states inside a group has been correlated with improved communication,5 decision support
and mitigation of group conflicts,6 while o↵ering an additional level of rich contextual information7 and also
supporting collaborative learning.8 At the same time, lower values of emotional intelligence (EI) and negative
emotion contagion inside groups have been linked to declining intra-group communication and collaboration,4, 9
suggesting the need for emotion awareness solutions.
In the next section we highlight relevant related work, followed by the design considerations that an emotion
visualization for multi-touch interfaces would need to observe. Then, we describe the details of our approach,
both in the context of real-time visualization of user emotions and their post-task analysis. The results we
obtained with emotion-prints are presented in a user study, where user emotional states are acquired through
wireless BCI headsets. Building on this, a discussion section addresses benefits as well as current limitations of
emotion-prints. Finally, we present our conclusion and potential future improvements.

2. RELATED WORK
The visualization and perception of user emotions has been inspected in multiple contexts and from various
angles over the last decades. In the work of Liu et al.,10 an a↵ect color bar reflects the emotional structure of
loaded text documents, while Stahl et al.11 focus on a creative tool for communicating emotional states through
sub-symbolic expressions (colors, shapes, etc.) as an additional channel in text-based communication. In the
area of emotion annotation, Ohene-Djan et al.12 highlight a visualization tool that allows the user to express
his current a↵ective state in order to reflect his emotional perception about what s/he is currently watching or
doing. Closer connected to our work, McDu↵ et al.13 devise a visualization for supporting long-term emotional
self-awareness, called A↵ectAura. This representation encodes valence, arousal and engagement levels through
color, shape, size and opacity. However, A↵ectAura is a visualization system in itself, contrary to our approach
that aims at o↵ering a generalizable representation for enhancing touch interface elements.
Besides abstract representations, emotions have also been depicted in interfaces through avatars and a↵ective
icons,6, 14, 15 as well as through custom interface widgets that would support user emotional awareness.6 Further
related to UI widgets, Cernea et al.16 present an approach for fostering emotional awareness in the context of
desktop computing. In terms of emotional awareness in a collaborative setting, Saari et al.17 describe an emotion
visualization system for mobile devices in order to address issues like emotional awareness within groups and the
influence of emotion visualization on group performance.
Focusing further on group psychology, emotional awareness—be it on a personal or group level—has been
shown to support the collaboration process by improving verbal and non-verbal communication,4, 5 support group
decisions6, 7 and foster collaborative learning.8 Research on emotion contagion further highlights the importance
of emotional awareness in groups, as a↵ective influences between group members can deeply impact the dynamic
of the group, with positive emotional contagion resulting in increased efficiency and less conflicts, and vice versa.9

3. DESIGN CONSIDERATIONS
As a vital starting point, we have composed a list of requirements that a real-time visualization of user emotions
on multi-touch interfaces would have to satisfy in order to o↵er an e↵ective representation of a↵ective states.
The elements of the list are based both on related research and the previous research of the authors:
1. Less standardized than graphical user interfaces (GUIs) designed for desktop environments, graphical
elements on a multi-touch device can have almost any shape. Accordingly, the representation has to be
generally applicable for any interface element regardless of shape and size, and at the same time refrain
from displacing or repositioning any of these interface objects.
2. The visualization of the emotions should be in the appropriate context, ideally at the physical location of
the touch event. This would support easy identification of the operation-emotion couple as well as, in some
cases, identify the corresponding user.
3. Human emotions are rarely precisely defined and complex in nature. A visualization that tries to convey
emotional states or attributes should consider these inherent properties and o↵er representations for hinting
at the emotional state of the user instead of trying to visualize exact values.
4. The representation needs to be as intuitive as possible and support fast processing. This is particularly
important as users need to perceive and interpret the displayed emotions during the real-time touch-based
interaction.
5. Various levels for emotional involvement have to be distinguishable. Also, users should have the possibility
to compare the di↵erent emotions that are displayed at a point in time.
6. The representations should support the real-time nature of the interaction and also ensure that the interface
does not succumb to visual clutter.

4. EMOTION-PRINTS
As highlighted previously, di↵erent sets of user emotions can nowadays be detected through a relatively wide
range of techniques, including facial expressions, speech,19, 20 electrodermal response or galvanic skin response
(EDS or GRS),21, 22 and brain-computer interfaces (BCI).15, 23, 24 The importance of detecting and classifying
these emotions increases as one considers that user emotional states are usually generated by stimulus events,25
events which in our context can be related to the applications the users interact with. As a result, emotions
might be closely connected to the visual systems users interact with (e.g., frustration about an unintuitive
representation), to the task they have to execute (e.g., excitement after having an insight), or even to the other
users they collaborate with (e.g., group tensions).
In this paper, we aim at creating an emotion representation that is independent of the emotion acquisition
technique and is thus combinable with any approach that can interpret a↵ective states in real-time. In order to
demonstrate and evaluate our concept, our implementation and corresponding study employs emotion reports
obtained through BCI readings. Details about the emotion acquisition process are highlighted in the User Study
section.
Once the emotional states are established, the design considerations could be addressed in order to generate
the most fitting visualization. To satisfy the first two requirements, we decided to employ a metaphor based on
the actual fingerprints people leave when interacting with touch displays. More precisely, virtual objects that
would be touched and acted upon by users would receive a colored halo that would closely follow their outline.
Thus, the emotion-prints would be included in the representation spatially right at the position where the touch
event took place and without influencing the previous distribution of elements on the interface.
To model the halo e↵ect around the objects we had to generate an o↵set-curve following the underlying shape
(Figure 2). These halos would extend the outline of the virtual objects by a constant distance a, computed along
the normal to the object’s outline. At the object’s edges, the halos would be rounded in order to o↵er a smoother
contour capable of sustaining an additional layer of information: outline texture. With this approach, colored

Figure 1. Russell’s circumplex model of a↵ect18 extended by the visual metaphors employed by emotion-prints. On the
horizontal axis of valence, positive and negative emotions are encoded by outline texture. Positive emotions are represented
by halos with an increasing number of wavy, round shapes, while negative ones have halos with jagged, angular outlines.
The intensity of each emotion—positive or negative—increases with the number of shapes that define the outline of the
halo, e.g., see arrow pointing at the state of delight and its corresponding emotion-print. On the vertical axis, high arousal
is encoded by intense red color and a fast pulsation of the halo, while low arousal is encoded by a blue halo and almost
no pulsation at all.

halos could be displayed for virtually any object on the display, regardless of shape and size. Note that the
developers of multi-touch applications can control which of the virtual objects will be able to represent an
emotion-print around their outline by adding the ID of those objects to a list of touch items. Through this,
developers have the option of defining a subset of virtual objects that can display the emotion halos, as not all
interface elements might be relevant for enhancing user awareness or evaluating the interface and user experience.
Considering the inherent complexity and imprecision of emotions, we addressed the third design consideration
by employing Russell’s circumplex model of a↵ect (Figure 1), a two-dimensional space that positions a set of
a↵ective states in terms of emotional arousal (excited or calm) and valence (pleasant or unpleasant). The model
follows the dimensional theory of emotions that states that di↵erent emotions can be considered as dissimilar
only in terms of one or more distinct dimensions.26 Widely accepted in the literature of a↵ective systems,18, 27
Russell’s model thus allows us to take any emotion included in this domain and convert it to a valence-arousal

Figure 2. Computing the outline of the emotion-prints halo: (left) the basic shape of the halo is computed by extending
the outline of the touched object by a length a along the normal of the object shape; (center) in the case of convex edges,
the edge of the halo is rounded, following the outline of a circle with the center in the edge of the object and a radius of
a; (right) for concave edges, the edge of the halo is again rounded by drawing a circle with the center X 0 and radius a.
X 0 is computed as the point that is on the line defined by the object edge X and the virtual edge of the halo XEP and
positioned such that the following segments are equal: X 0 XEP = XXEP .

pair, which can function as the core data for our representation. Visually encoding valence and arousal instead of
specific user emotions enables the use of a wider range of techniques for detecting user a↵ective states. As long as
these emotions would be part of the model, they could be visualized in terms of valence-arousal pairs. Note also
that the valence-arousal emotional space has been used previously for increasing awareness in communications
scenarios, where participants have rated the encoding as understandable and useful.28
The visual representation of emotion-prints is inspired by the interface of the eMoto mobile application,11
that focuses on emotion expressivity and communication by using color, shapes and animation to encode the
two-dimensional emotional space defined by Russell’s model. Similarly to this approach, emotion-prints employ
color, shapes and motion to represent various levels of valence and arousal (Figure 1). However, in contrast to
eMoto, where the color attribute changes over the entire Russell model, our solution is intended as a simpler
visualization, where users should be able to more easily and quickly distinguish the di↵erent levels of arousal
and valence.
For the vertical axis of our representation model, arousal is double encoded through color and motion. As
arousal increases from boredom towards excitement, the halos start to pulsate increasingly fast, in a manner that
is meant to mimic the human heart rate in terms of frequency. At the same time, the color of the halos changes
from blue to red as the perceived arousal levels increase. This is consistent also with Ryberg’s theory,29 that
suggests that emotional energy can be represented as increasing from blue to red. The two attributes of color
and pulsation are meant to combine their e↵ects on the human perception in order to transmit the metaphor of
thawing and becoming excited or calming down and freezing, depending on the extremity of the axis.
On the other hand, the horizontal line of valence is encoded in the emotion-prints representation through a
variation of the halos’ outline. More exactly, positive emotions are captured through an increasing number of
waves forming on the outline of the halo, while negative emotions are displayed as halos with increasingly many
jagged, sharp corners. Figure 1 highlights this representation of emotion-prints for the valence-arousal space.
By employing di↵erent attributes for each of the two dimensions, as well as by using separable dimensions
like color, curvature and motion,30, 31 we addressed the forth design consideration by supporting the intuitive
processing of the emotion-prints. Furthermore, the attributes allow users to clearly distinguish between multiple
levels of valence and arousal, thus also tackling the fifth design requirement. While it is clear that color and
motion do not allow for a very fine-grained classification, our study has shown that users are capable of clearly
distinguishing at least three distinct levels of arousal: blue for low arousal, violet for medium arousal and red
for high arousal.
Finally, in order to address the last design consideration and support the real-time character of the visualization, all emotion-prints slowly dissipate in a preset time interval after the touch event occurred (Figure 3).
Additionally, one can set an upper limit for the number of fingerprints that can be displayed simultaneously.

Figure 3. Example of emotion-prints dissipating and being overwritten. The first touch of the user generates a halo that
suggests low arousal and a slightly positive valence. This emotion-print starts to dissipate over time by increasing the
alpha channel of the halo. Before it can become completely transparent, the user touches the same object again and the
old emotion-print gets overwritten. This time, the emotional readings suggest high levels of arousal and an increased
positive valence. Finally, the new halo starts to dissipate again.

These two features—dissipation and maximum number of displayed halos—ensure that emotion-prints can be
customized for the particular needs of any multi-touch application, such that the visual clutter introduced by
this additional layer of a↵ective information is minimal.

5. POST-TASK ANALYSIS OF EMOTION-PRINTS
As stated in the beginning of this paper, one of the goals of our visualization is supporting the evaluation of
touch-enabled applications by o↵ering visual feedback about the a↵ective states of the interacting users. However,
as previously described, emotion-prints are aimed mainly as a real-time emotion visualization approach that
requires the representations to dissipate shortly after each touch event of the user in order to reduce display
clutter and maintain the visualization up-to-date. Thus, to further support evaluation and post-task analysis of
user interactions and corresponding emotions, we extended the concept behind emotion-prints to incorporate an
additional visualization that allows users to inspect the temporal and spatial distribution of the touch-emotion
couples.
After a session in which one or multiple users have interacted with an application on a touch interface, the
emotion-prints histogram can be displayed, as shown in Figure 4. The histogram o↵ers a temporal overview of
the touch events of a user and the corresponding valence-arousal pair for each such instance. To establish this
correspondence, our current implementation is able to track the hands and arms of multiple users around the
tabletop based on the infrared image captured by its camera and the orientation of the users’ arms over the
table. At the same time, a down-facing camera tracks the motion of the tabletop users around the display for the
entire length of the session, ensuring the correct user identification based on their location and arm movement.
While the initial design conventions have been considered also for the histogram, there are a couple of
di↵erences in the way emotion is encoded. In order to better di↵erentiate between emotional states with positive
and negative valence but also due to the inherent design of histograms, the valence corresponding to each touch
is encoded by the orientation and height of the vertical bars. The two sides of the histogram—positive and
negative—are indicated by a circle and a triangle that make the connection to the round and sharp edges of the
emotion-prints, as highlighted in the previous section. Moreover, touching one of these two glyphs will hide the
corresponding half of the histogram, allowing user, for example, to display and more easily compare only the
touch instances that represented a positive valence. In terms of arousal, the color of each bar encodes the arousal
levels for the touch instance, following the same color mapping from Figure 1. Again, the selected attributes for
encoding the two-dimensional emotion data are some of the more separable ones.31
As sometimes multiple touch events can be executed by a user in a very short period of time (e.g., fast
tapping in a game), we decided to discretize the temporal axis and convolute all emotional readings for time
intervals smaller than a second. For example, if a user executed three touch events in a one second interval, the
valence and arousal values for these three instances would be averaged and only one bar would be displayed in

Figure 4. Histogram representation of touch events and their associated arousal-valence values for two users. Users can
inspect their own temporal distribution of emotion-prints and compare it to other users by snapping together two views,
like in this figure. The bars are positioned along the temporal axis, and their orientation and size encode the valence and
its intensity. At the same time, the color of a bar encodes the arousal level experienced at that point in time, during that
interaction.

Figure 5. Emotion-prints histogram can be displayed with constant time intervals (top) or can be compressed to eliminate
time intervals where the user did not execute any touch events (bottom). Switching between these two representations
is done through the double-ended arrow at the right-bottom corner of the histogram. The curved arrow at the right-top
corner allows users to resize and reposition the view.

Figure 6. Screenshot of the eSoccer multi-touch game supporting up to four users. The background of the field is covered
by a blue-violet-red heatmap that encodes the arousal level of a user during the last gaming session. An emotion-prints
histogram is displayed for the same user. By selecting a bar in the histogram, the system displays the quasi-simultaneous
locations where the user has executed the corresponding touch operations on the screen. Besides location information,
the histogram instance further correlates with the user ID, the ID of the touched object and the executed operation (e.g.,
tap, drag or resize).

the histogram (see Figure 6 where one histogram bar corresponds to two touch events). This allows us to reduce
clutter, especially as emotional signals do not fluctuate with such a high frequency.
In terms of interaction, the histograms have a custom scrollbar attached, that allows users to both move to a
certain location in the history and to see an overview of a user’s valence during an entire session. As presented in
Figure 4, two histograms can also be coupled in order to facilitate comparison. More precisely, when a histogram
is positioned next to another one, the one that is being moved resizes to the same dimension as its counterpart
and snaps into place next to it. Additionally, if there are common periods of interaction for the two histograms,
the time axes align automatically.
Sometimes the density of touch events fluctuates heavily during a session. To allow for a more comprehensive
exploration of tendencies and patterns, emotion-prints histograms can be compressed to discard the temporal
segments where the user did not interact with the interface (Figure 5, bottom). To maintain a frame of reference,
the vertical interval lines are shifted such that areas were the lines are closer together suggest periods of time
when the touch events were more sparse, and vice versa.
While the histogram encodes the temporal distribution of touch-emotion events for a selected user, it gives no
information about what areas of the display or what objects have been interacted with. In order to achieve this,
the individual bars of a histogram can be selected highlighting the spacial distribution of the corresponding touch
instances (Figure 6). This means that for each valence-arousal pair displayed in a histogram, one can inspect
the ID of the issuing user, the location on the screen where the touch event/s took place and the operation that
the user executed (e.g., touch, drag, resize).
Further, to obtain an overview of the spatial distribution of emotion-prints during an entire session, a heatmap
can be displayed over the area of the application. Again, this heatmap employs the same color mapping from

Figure 7. Two players interacting with the eSoccer game on a tabletop while their emotional cues are being interpreted
via BCI and represented through emotion-prints, all in real-time.

Figure 1, and combined with the histogram visualization o↵ers a comprehensive solution for user post-task
analysis and evaluation support. However, one drawback of the heatmap representation is that it can only use
colors to convey information. As such, one can only show the spatial distribution of either arousal or valence at
a certain moment.

6. USER STUDY
When working with concepts like a↵ective states, it is difficult to devise a study that can quantitatively inspect
the merits of a visualization. In such a context, the issue of “correct” and efficient representation can be defined
only vaguely, as it seems more important to evaluate whether the emotion-prints were perceived by the users as
intended in the design considerations, and if the representation had an impact in terms of emotion awareness
and user experience analysis.
In order to validate our visualization, we incorporated emotion-prints in two tabletop applications: a soccer
game supporting up to four players (Figure 7) and a collaborative semi-hierarchical history visualization (Figure 8). In the soccer game, the only objects that users could manipulate were virtual soccer players, which
were then registered as touch items. Similarly in the history visualization, the nodes of the graph were tagged
as touch items on which emotion-prints could be displayed. The two applications were selected in order to
investigate how emotion-prints are perceived in a competitive and a collaborative scenario.
One di↵erence between the two applications in terms of emotion visualization was that for the collaborative
scenario, touches corresponding to a negative valence would not be displayed. The motivation for this is given by
the fact that reinforcement and feedback on positive emotions has been shown to foster the collaboration process
as well as contribute to the development of positive group a↵ective tone.32, 33 Note that the group a↵ective tone
is defined as a consistent emotional state throughout the members of a group. More importantly, the presence of
a (positive) group a↵ective tone can also influence the e↵ectiveness of said group. This has also been addressed
by Saari et al.,17 who state that in a group, negative emotions can lead to withdrawal, while positive emotions
can foster the interaction and collaboration of the team members.

Figure 8. User interacting with the browser history visualization on a tabletop. Emotion-prints are being displayed each
time a user touches a rectangular segment, corresponding to the nodes of the history that represent the websites that
have been accessed.

In both applications, whenever the user would interact with a virtual object, his current levels for arousal
and valence would be determined based on brain signals obtained from an electroencephalograph (EEG) device,
transmitted to the tabletop computer and represented through an emotion-print around the corresponding UI
object (Figure 9). In order to detect the two dimensions of the Russell’s emotional space, we employed the
Emotiv EPOC BCI system⇤ to ensure real-time detection of emotions and a high level of mobility for the work
around the tabletop. The Emotiv EPOC is a wireless EEG-based headset that enables the detection of electrical
brain signals on the surface of the scalp. These electrical signals are recorded through 14 electrodes positioned
at key locations on the head of the user.24
The headset is accompanied by a software framework that can interpret the raw EEG signals and classify
them in terms of a predefined set of facial expressions, emotional states and mental commands (Figure 10). Note
that the BCI-based detection of user emotional states is beyond the scope of this paper, and that the Emotiv
technology and its corresponding framework have been previously evaluated in the context of emotion reporting.16, 24 Also, the EPOC headset and similar BCI systems have been used successfully in various subjectivity
and state-based evaluations, including some in the field of visualization.23, 34 In our study, coupling both the
facial expression readings and the classification of a↵ective states generated by the EPOC framework supplied us
with values for the emotions that are closest to the extremes of the arousal-valence axes in Russell’s circumplex
model, and which can be employed to cover the entire corresponding 2D space.
Twelve subjects took part in our study, all of which had some level of experience with multi-touch devices or
tabletops. The participants were split into six pairs, where each pair was asked to play a game of virtual soccer
against each other and to collaboratively find a node with a particular set of attribute values in the history
⇤

http://www.emotiv.com

Figure 9. User playing on a tabletop while wearing the BCI headset. Each time the user touches the multi-touch surface,
his EEG signals are read and interpreted as a↵ective states. Subsequently, emotion-prints are displayed on the tabletop
around the object that the user touched. The images on the right highlight—from top to bottom—how the emotion-prints
dissipate in time.

Figure 10. Computing the emotion-prints representations: (left) gathering data that can encode information about user
a↵ective states (e.g., EEG signals, facial expressions, galvanic skin response); (center) interpreting this data as a↵ective
states that can be decomposed into valence and arousal; (right) representing the valence and arousal information on the
multi-touch surface as emotion-prints.

visualization. Before the tasks commenced, each team got introduced to the functionality of the two applications
until they felt confident to use them. All participants were equipped with an Emotiv EPOC headset, but only
after the abilities and limitations of this system has been highlighted to them, and they agreed to share the
readings and interpretations of the device.
In order to compare the impact of emotion-prints, the six groups were divided in two, where three groups
would complete the two tasks enhanced by the emotion representation obtained through the users BCI readings,
and three groups that could employ the same two applications without emotion-prints representations. However,
the members of the groups that did not have the benefit of emotion representations did still wear the BCI
headsets. This allowed us to inspect and compare user behavior in cases where similar valence-arousal readings
would be reported. The members of the pairs that would work with emotion-prints were given a thorough
presentation of its design and functionality. Furthermore, all users were encouraged to express themselves freely
during the tasks.
During the study, almost all participants engaged in verbal communication, allowing us to analyze their
interaction relatively to the emotion-prints. In the context of the soccer game, there were multiple instances
where the groups that had the emotion visualization at their disposal managed to use this information, in some
cases even gaining an advantage. For example, a player that was leading 4 to 3 after being down 1 to 3 saw
that his opponent’s emotion-prints were very edgy and red, thus making the following statement: “Wow, you
must be really annoyed”. He later was able to extend his lead also by directly addressing the frustration of his
opponent. On the other side, in the collaborative history visualization, users that had access to viewing the
emotion-prints would often notice the positive excitement of their colleague, suggested by phrases like “Did you
find something?” and “You think it’s in this area?”. In contrast, in the pairs that did not have access to the
emotion representation, the awareness of a↵ective attributes was minimal. This is also supported by the fact
that although users in these three groups experienced similar emotional configurations, there were almost no
discussions addressing real-time user experiences.
After the two tasks, the participants that worked with emotion-prints were asked to fill out a questionnaire
about their design and utility. The subjects considered that the design was intuitive and distinguishable. However, some would have liked to adjust the time interval in which the prints would dissipate. All participants
considered the opportunity of enriching the interface of multi-touch systems with emotional states a good idea.
The most powerful feedback was related to the impact on collaborative work and competitive gaming on multitouch devices (“I really want to see this in some of my iPad games”). At the same time, the collaborative aspect
brought up issues like privacy and property of the represented emotions. These are further discussed in the
following section.
Furthermore, all 12 participants had the opportunity to inspect and analyze their emotional readings in the
emotion-prints histograms and heatmaps. Their verbal iterations during the analysis process suggested that most
participants managed to gather valuable insights about their emotional states, increasing the overall awareness
of their own and their colleagues experience (e.g., “I really got frustrated after that, didn’t I?”, “What did you
see there?”). When inquired directly, all participants felt that emotion-prints increased their a↵ective awareness
in the two tasks. More precisely, users considered themselves aware especially of the opponents emotions in the
soccer game, while in the history visualization the focus was more equally distributed between emotional selfawareness and emotional awareness of their collaborators. At the same time, the heatmaps gave them insights
both about the areas that they most interaction one and their emotional experience during those touch events.
For example, one user preferred executing his attacks on the side of the field in the soccer game, and was mostly
successful in this approach. Thus, the heatmap for this case presented more intense coverage and positive valence
in those areas of the field.

7. DISCUSSION
As briefly mentioned in the previous section, the participants of our study have addressed the issue of emotional
privacy. Perhaps similarly to social media and other approaches to digital sharing, emotion visualization both
in single-user and collaborative context has to commence by informing and empowering the user. Irrespective
of how user emotions are interpreted and represented, users need to first be informed about the particularities

and capabilities of the acquisition technique, be it emotion recognition through facial expressions, BCI, or
physiological responses. Only through this, users will be able to take a pre-task informed decision whether they
want to share this data and to what extend. This is the reason why applications that are enhanced with emotionprints should inform the user about the interpreted emotional states and require them to agree with those terms,
as in any virtual sharing activity that involves private data. More precisely, users should be made aware that
the visualization approach does not inform other participants about concrete a↵ective states, but rather converts
this information to the space of valence and arousal. This conversion has also implications in terms of resolution,
as two distinct discrete emotions will be encoded similarly in the valence-arousal representation (e.g. sad and
bored in Figure 1). As such, this limitation needs to be considered when applying emotion-prints to a particular
scenario or task where only a certain set of emotional readings is relevant. Furthermore, as in the collaborative
history visualization, Russell’s two-dimensional space can be further restricted to filter out a set of values. Note
that the visual representations introduced by emotion-prints are consistent for any emotion detection or signal
interpretation technique that outputs either directly user valence and arousal levels, or reports the presence
of one or more emotions included in Russell’s circumplex model of a↵ect and that can be decomposed to the
two-dimensional encoding.
Other concerns that have been raised include display clutter and a corresponding increased cognitive workload.
To address this, the last design consideration has been added to our concept. Limiting the period of time that
an emotion-print stays visible has multiple advantages: the presented emotional information is never outdated,
the display objects can quickly revert to their original form in order to minimize clutter, and the identity of the
user that generated the print can be better recalled compared to an emotion-print that has been displayed for a
longer period of time.

8. CONCLUSION
In this paper we introduce the emotion-prints, a visualization framework for representing a↵ective states in new
of existing multi-touch system. In the past, emotions have been considered mostly in a↵ective systems that try
to react to the users emotions. Still, to our knowledge, no research has been undertaken on correlating user
emotions with the touch interface elements the user acts on and visualizing these states in real-time on the
corresponding objects.
The aim of emotion-prints is to aid user emotional self-awareness and the awareness of other users’ emotions
in collaborative and competitive touch-enabled applications, like games or collaborative visualization. User
awareness in collaborative scenarios is of particular importance, as awareness of and reflection on emotional
states has been linked to improved group communication and efficiency. Additionally, we have highlighted how
our approach can support the evaluation of user experience and multi-touch applications. Our user study suggests
that the selected representation can capture and convey user emotional valence and arousal. At the same time,
emotion-prints are independent of the technique through which user emotions are reported, as well as generally
applicable irrespective of the shape of the virtual object.
Even if most emotion detection techniques are not fully mature yet and our current work only addresses
the two-dimensional space defined by valence and arousal, we believe it is crucial to pave the way in fields like
visualization for the day when a large set of emotional states will be more easily and more precisely detectable
by dedicated systems.

REFERENCES
[1] Mähr, W., “eMotion: An outline of an emotion aware self-adaptive user interface,” in [Proc. of the 2nd
Scandinavian Student Interaction Design Research Conference (SIDER ’06) ], 25–26, Gothenburg, Sweden
(2006).
[2] Liikkanen, L. A., Jacucci, G., Huvio, E., Laitinen, T., and Andre, E., “Exploring emotions and multimodality
in digitally augmented puppeteering,” in [Proc. of the working conference on Advanced Visual Interfaces
(AVI ’08) ], 339–342, ACM Press (2008).
[3] Shugrina, M., Betke, M., and Collomosse, J., “Empathic painting: Interactive stylization through observed
emotional state,” in [Proc. of the 4th international symposium on Non-photorealistic animation and rendering (NPAR ’06) ], 87–96, ACM Press (2006).

[4] Druskat, V. U. and Wol↵, S. B., “Building the emotional intelligence of groups,” Harvard Business Review 79(3), 80–90 (2001).
[5] Charlton, B., “Evolution and the cognitive neuroscience of awareness, consciousness and language,” in
[Psychiatry and the human condition ], Radcli↵e Medical Press, Oxford, UK (2000).
[6] Gárcia, O., Favela, J., and Machorro, R., “Emotional awareness in collaborative systems,” in [String Processing and Information Retrieval Symposium (1999), International Workshop on Groupware ], 296–303
(1999).
[7] Gárcia, O., Favela, J., Sandoval, G., and Machorro, R., “Extending a collaborative architecture to support
emotional awareness,” in [Emotion Based Agent Architectures (EBAA ’99) ], 46–52, Seattle, USA (1999).
[8] McConnel, D., [Implementing Computer Supported Cooperative Learning ], Psychology Press, second ed.
(2000).
[9] Barsade, S. G., “The ripple e↵ect: Emotional contagion and its influence on group behavior,” Administrative
Science Quarterly 47(4), 644–675 (2002).
[10] Liu, H., Selker, T., and Lieberman, H., “Visualizing the a↵ective structure of a text document,” in [CHI ’03
Extended Abstracts on Human Factors in Computing Systems (CHI EA ’03) ], 740–741, ACM Press, New
York, NY, USA (2003).
[11] Ståhl, A., Sundström, P., and Höök, K., “A foundation for emotional expressivity,” in [Proc. of the 2005
Conference on Designing for User eXperience (DUX ’05) ], Article 33, AIGA: American Institute of Graphic
Arts, New York, NY, USA (2005).
[12] Ohene-Djan, J., Sammon, A., and Shipsey, R., “Colour spectrum’s of opinion: An information visualisation
interface for representing degrees of emotion in real time,” in [Proc. of the Conference on Information
Visualization (IV ’06) ], 80–88, IEEE Computer Society, Washington, DC, USA (2006).
[13] McDu↵, D., Karlson, A., Kapoor, A., Roseway, A., and Czerwinski, M., “A↵ectAura: An intelligent system
for emotional memory,” in [Proc. of the SIGCHI Conference on Human Factors in Computing Systems (CHI
’12) ], 849–858, ACM Press, New York, NY, USA (2012).
[14] Huisman, G., van Hout, M., van Dijk, E., van der Geest, T., and Heylen, D., “LEMtool: measuring emotions
in visual interfaces,” in [Proc. of the SIGCHI Conference on Human Factors in Computing Systems (CHI
’13) ], 351–360, ACM Press, New York, NY, USA (2013).
[15] Liu, Y., Sourina, O., and Nguyen, M. K., “Real-time EEG-based human emotion recognition and visualization,” in [Proc. of the 2010 International Conference on Cyberworlds (CW ’10) ], 262–269 (2010).
[16] Cernea, D., Olech, P.-S., Ebert, A., and Kerren, A., “Measuring subjectivity – supporting evaluations with
the Emotiv EPOC neuroheadset.,” Journal for Artificial Intelligence (KI) 26(2), 177–182 (2012).
[17] Saari, T., Kallinen, K., Salminen, M., Ravaja, N., and Yanev, K., “A mobile system and application for
facilitating emotional awareness in knowledge work teams,” in [Proc. of the 41st Annual Hawaii International
Conference on System Sciences ], 44–53 (2008).
[18] Russell, J. A., “A circumplex model of a↵ect,” Journal of Personality and Social Psychology 39, 1161–1178
(1980).
[19] Castellano, G., Kessous, L., and Caridakis, G., “Emotion recognition through multiple modalities: Face,
body gesture, speech,” in [A↵ect and Emotion in Human-Computer Interaction ], Peter, C. and Beale, R.,
eds., Lecture Notes in Computer Science 4868, 92–103, Springer Berlin Heidelberg (2008).
[20] Kessous, L., Castellano, G., and Caridakis, G., “Multimodal emotion recognition in speech-based interaction
using facial expression, body gesture and acoustic analysis,” Journal on Multimodal User Interfaces 3(1-2),
33–48 (2010).
[21] Coopera, J. B. and Siegela, H. E., “The galvanic skin response as a measure of emotion in prejudice,” The
Journal of Psychology: Interdisciplinary and Applied 42(1), 149–155 (1956).
[22] Westerink, J. H., Broek, E. L., Schut, M. H., van Herk, J., and Tuinenbreijer, K., “Computing emotion
awareness through galvanic skin response and facial electromyography,” in [Probing Experience], Westerink,
J. H., Ouwerkerk, M., Overbeek, T. J., Pasveer, W. F., and Ruyter, B., eds., Philips Research 8, 149–162,
Springer Netherlands (2008).

[23] Anderson, E. W., Potter, K. C., Matzen, L. E., Shepherd, J. F., Preston, G., and Silva, C. T., “A user study
of visualization e↵ectiveness using EEG and cognitive load,” in [Proc. of the 13th Eurographics / IEEE VGTC conference on Visualization (EuroVis’11) ], Hauser, H., Pfister, H., and van Wijk, J., eds., 791–800,
Eurographics Association, Aire-la-Ville, Switzerland (2011).
[24] Cernea, D., Olech, P.-S., Ebert, A., and Kerren, A., “EEG-based measurement of subjective parameters in
evaluations,” in [Proc. of the 14th International Conference on Human-Computer Interaction (HCII ’11),
poster paper, volume 174 of CCIS ], 279–283, Springer, Orlando, FL, USA (2011).
[25] Scherer, K., “What are emotions? and how can they be measured?,” Social Science Information 44(4),
695–729 (2005).
[26] Larsen, R. and Diener, E., “Promises and problems with the circumplex model of emotion,” Review of
personality and social psychology 13, 25–59 (1992).
[27] Gökçay, D., “Emotional axes: Psychology, psychophysiology and neuroanatomical correlates,” in [A↵ective
Computing and Interaction: Psychological, Cognitive and Neuroscientific Perspectives, Information Science
Publishing, IGI Global ], 56–73 (2010).
[28] Sanchez, J., Kirschning, I., Palacio, J., and Ostrovskaya, Y., “Towards mood-oriented interfaces for synchronous interaction,” in [Proc. of the 2005 Latin American Conference on Human-Computer Interaction ],
1–7, IEEE Computer Society, Washington, DC, USA (2005).
[29] Ryberg, K., [Levande färger ], ICA Bokförlag, Västerøas, Sweden (1999).
[30] Treisman, A., “Features and objects in visual processing,” Scientific American 255(5), 114–125 (1986).
[31] Ware, C., [Information Visualization: Perception for Design ], Morgan Kaufmann Publishers, second ed.
(2004).
[32] George, J., “Personality, a↵ect, and behavior in groups,” Journal of Applied Psychology 75, 107–116 (1990).
[33] George, J., “Group a↵ective tone,” Handbook of work group psychology 75, 77–93 (1996).
[34] Cernea, D., Ebert, A., and Kerren, A., “Detecting insight and emotion in visualization applications with a
commercial EEG headset,” in [Proc. of the SIGRAD11 Conference on Evaluations of Graphics and Visualization ], 53–60 (2011).

