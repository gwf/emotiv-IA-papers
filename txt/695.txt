Real-time EEG-based Human Emotion Recognition and Visualization

Yisi Liu, Olga Sourina, and Minh Khoa Nguyen
Nanyang Technological University
Singapore
e-mail: LIUY0053 | EOSourina | RaymondKhoa@ntu.edu.sg
Abstract—Emotions accompany everyone in the daily life,
playing a key role in non-verbal communication, and they are
essential to the understanding of human behavior. Emotion
recognition could be done from the text, speech, facial
expression or gesture. In this paper, we concentrate on
recognition of “inner” emotions from electroencephalogram
(EEG) signals as humans could control their facial expressions
or vocal intonation. The need and importance of the automatic
emotion recognition from EEG signals has grown with
increasing role of brain computer interface applications and
development of new forms of human-centric and humandriven interaction with digital media. We propose fractal
dimension based algorithm of quantification of basic emotions
and describe its implementation as a feedback in 3D virtual
environments. The user emotions are recognized and visualized
in real time on his/her avatar adding one more so-called
“emotion dimension” to human computer interfaces.
Keywords- emotion recognition; EEG; emotion visualization;
fractal dimension; HCI; BCI

I.

INTRODUCTION

Nowadays, new forms of human-centric and humandriven interaction with digital media have the potential of
revolutionising entertainment, learning, and many other areas
of life. Since emotions play an important role in the daily life
of human beings, the need and importance of automatic
emotion recognition has grown with increasing role of
human computer interface applications. Emotion recognition
could be done from the text, speech, facial expression or
gesture. Recently, more researches were done on emotion
recognition from EEG [1-6]. Traditionally, EEG-based
technology has been used in medical applications. Currently,
new wireless headsets that meet consumer criteria for
wearability, price, portability and ease-of-use are coming to
the market. It makes possible to spread the technology to
applications in emotion recognition in many other areas such
as entertainment, e-learning, virtual worlds, cyberworlds, etc.
Automatic emotion recognition from EEG signals is
receiving more attention with development of new forms of
human-centric and human-driven interaction with digital
media. In this paper, we concentrate on recognition of
“inner” emotion from EEG signals as humans could control
their facial expressions or vocal intonation.
There are different emotion classifications proposed by
researchers. We follow two-dimensional Arousal-Valence
model [7]. This model allows mapping discrete emotion
labels in the Arousal-Valence coordinate system. One of

emotion definitions is as follows: “The bodily changes
follow directly the perception of the exciting fact, and that
our feeling of the changes as they occur is the emotion” [8].
Our hypothesis is that the feeling of changes can be noticed
from EEG as fractal dimension changes. We focused on
study of fractal dimension model and algorithms, and
developed one fractal based approach to emotion
recognition.
To evoke emotions, different stimuli could be used:
visual, auditory, and combined. They activate different areas
of the brain. Our hypothesis is that emotions have spatiotemporal location. There is no easily available benchmark
database of EEG labeled with emotions. But there are
labeled databases of audio stimuli for emotion induction International Affective Digitized Sounds (IADS) [9] and
visual stimuli - International Affective Picture System
(IAPS) [10]. Thus, we proposed and carried one experiment
on emotion induction using IADS database of labeled audio
stimuli. We also proposed and implemented an experiment
with music stimuli to induce emotions by playing music
pieces and prepared questionnaire for the participants to label
the recorded EEG with corresponding emotions.
There are a number of algorithms for recognizing
emotions. The main problem of such algorithms is a lack of
accuracy. Research is needed to be carried out to evaluate
different algorithms and propose algorithms with an
improved accuracy. As emotion recognition is a new area, a
benchmark database of EEG signals for different emotions is
needed to be set up, which could be used for further research
on EEG-based emotion recognition. Until now, only limited
types of emotions could be recognized. Research could be
done on more types of emotions recognition. Additionally,
most of the emotion recognition algorithms were developed
for off-line data processing. In our paper, we target real-time
applications in 3D virtual environment. The user emotions
are recognized and visualized in real time on his/her avatar.
We add one more so-called “emotion dimension” to human
computer interfaces. Although in this paper, we describe a
standalone implementation of emotion recognition and
visualization, it could be easily extended for further use in
collaborative environments/cyberworlds.
In Section II A, review on emotion classification is given.
In Section II B, emotion recognition algorithms from EEG
are reviewed. In Section II C, a fractal dimension algorithm
proposed by Higuchi is described. Our approach, emotion
induction experiments, a novel fractal-based emotion
recognition algorithm, data analysis and results are given in

Section III. Real-time emotion recognition and visualization
of human emotions on 3D avatar using Haptek system [11] is
discussed in Section IV. In Section V, conclusion and future
work are given.
II.

RELATED WORKS

A. Emotion Classification
There are different emotion classification systems. The
taxonomy can be seen from two perspectives, dimensional
and discrete one [12]. Plutchik defines eight basic emotion
states: anger, fear, sadness, disgust, surprise, anticipation,
acceptance and joy. All other emotions can be formed by
these basic ones, for example, disappointment is composed
of surprise and sadness [13].
From the dimensional perspective, the most widely used
one is the bipolar model where arousal and valence
dimensions are considered. This emotion classification
approach is advocated by Russell [7]. Here, the arousal
dimension ranges from not aroused to excited, and the
valence dimension ranges from negative to positive. The
dimensional model is preferable in emotion recognition
experiments due to the following advantage: dimensional
model can locate discrete emotions in its space, even when
no particular label can be used to define a certain feeling [12]
[14].
B. Emotion Recognition Algorithms
There are an increasing number of researches done on
EEG-based emotion recognition algorithms. In work [1],
short-time Fourier Transform was used for feature extraction
and Support Vector Machine (SVM) approach was
employed to classify the data into different emotion modes.
The result was 82.37% accuracy to distinguish the feeling of
joy, sadness, anger and pleasure. A performance rate of
92.3% was obtained in [2] using Binary Linear Fisher’s
Discriminant Analysis and emotion states among
positive/arousal,
positive/calm,
negative/calm
and
negative/arousal were differed. SVM was applied in work [3]
for emotion classification with the accuracy for valence and
arousal identification as 32% and 37% respectively. By
applying lifting based wavelet transforms to extract features
and Fuzzy C-Means clustering to do classification, sadness,
happiness, disgust, and fear were recognized in work [4]. In
work [5], optimization such as different window sizes, bandpass filters, normalization approaches and dimensionality
reduction methods were investigated, and it achieved an
increase in accuracy from 36.3% to 62.07% by SVM after
applying these optimizations. Three emotion states: pleasant,
neutral, and unpleasant were distinguished. By using
Relevant Vector Machine, differentiation between happy and
relaxed, relaxed and sad, happy and sad with a performance
rate around 90% was obtained in work [6].
Although some researches achieved relatively high
accuracy of emotion recognition, the proposed algorithms,
were used only off-line. The number of electrodes used in
emotion recognition is another problem, since the equipment
with more electrodes is more expensive and it needs more
time to set up the experiment.

Additionally, as the brain is a complicated system, the
EEG signal is nonlinear and chaotic [15-16]. However, little
has been done to investigate chaos of brain for emotion
recognition. Works [1-6] were based on linear analysis,
however, linear analysis such as Fourier Transform only
preserves the power spectrum in the signal, but destroys the
spike-wave structure [17].
A fractal dimension analysis is suitable for analyzing
nonlinear systems and could be used in real-time EEG signal
processing [18]. Early work such as [19] showed that fractal
dimension could reflect the change of EEG signal; [20]
showed that fractal dimension varied for different mental
tasks; a more recent work like [15] revealed that when brain
processed tasks which were of emotional difference only,
fractal dimension can be used to differ these tasks. Work
[21-22] used music as stimuli to elicit emotions, and applied
fractal dimension for the analysis of the EEG signal. All
these works show that fractal dimension is a potentially
promising approach to investigate EEG-based emotion
recognition. In our research, fractal dimension model is
investigated to provide better accuracy and performance in
EEG-based emotion recognition.
C. Fractal Dimension Model
In fractal analysis, fractal dimension (FD) values of
geometric objects could reveal geometric complexity. For
calculation of fractal dimension value, we implemented and
analyzed two well-known algorithms Box-counting [23] and
Higuchi [24]. Both of them were evaluated using Brownian
and Weierstrass functions where “true value” is known [25].
Higuchi algorithm gave a better accuracy as it was closer to
the theoretical FD values [26].
Let us describe the Higuchi algorithm as we apply it for
FD calculation in our proposed fractal-based emotion
recognition algorithm shown in Section III.
Let X (1), X (2),..., X ( N ) be a finite set of time series
samples, the new time series is constructed as follows:
⎡N − m⎤
m
X k : X ( m), X (m + k ), X (m + 2 k ),..., X ( m + ⎢
⋅ k ) (1)
⎣ k ⎥⎦
where m = 1, 2,..., k , m is the initial time and k is the
interval time.
Then, k sets of Lm (k ) are calculated as follows:
⎧⎛ ⎡ N − m ⎤
⎫
⎞
⎪⎪⎜ ⎢⎣ k ⎥⎦
⎟
N − 1 ⎪⎪
⎨⎜ ∑ X ( m + ik ) − X ( m + ( i − 1) ⋅ k ) ⎟
⎬
N − m⎤ ⎪
⎪⎜ i =1
⎟⎡
k
⋅
⎠ ⎢⎣ k ⎥⎦ ⎭⎪
⎩⎪⎝
Lm ( k ) =
k

(2)
L( k ) denotes the average value over k sets of Lm (k )

and the relationship exists as follows:
L( k ) ∝ k − D

(3)

Finally, the fractal dimension can be obtained by
logarithmic plotting between different k and its associated
L( k ) [24].
III.

FRACTAL DIMENSION BASED APPROACH TO
EMOTION RECOGNITION
In this paper, we proposed a fractal dimension based
approach to EEG-based emotion recognition. First, we
designed and implemented emotion induction experiments
using two-dimensional model to describe emotions. Then,
we analyzed EEG recordings with Higuchi algorithm and our
proposed algorithm for online emotion recognition.
A. Emotion Induction Experiments
As we mentioned in Introduction, there is no easily
available benchmark database of EEG recordings with
labeled emotions. We designed two experiments to elicit
emotions with audio stimuli. Five truncated songs which
lasted for 1 minute each were used in experiment 1, and the
target emotions were negative/low aroused (sad),
positive/low aroused (pleasant), negative/high aroused (fear),
positive/high aroused (happy) and negative/high aroused
(angry). Sound clips selected from the International
Affective Digitized Sounds (IADS) were used in experiment
2. All the sounds in the IADS database are labeled with their
arousal and valence values. 27 clips were chosen to induce
five emotional states: 3 clips for neutral with mean arousal
ratings ranging between 4.79 and 5.15, mean valence rating
ranging from 4.83 to 5.09; 6 clips for each of positive/low
aroused, positive/high aroused, negative/low aroused and
negative/high aroused emotions with mean arousal rating
ranging between 3.36 and 4.47; 6.62 and 7.15; 3.51 and 4.75;
7.94 and 8.35 respectively, and mean valence rating ranging
between 6.62 and 7.51; 7.17 and 7.90; 4.01 and 4.72; 1.36
and 1.76 respectively.
10 participants, 2 female and 8 male students whose ages
ranged from 23 to 35, participated in the first music
experiment.
12 subjects, 3 female students and 9 male students whose
ages ranged from 22 to 35, participated in the second IADS
experiment. None of the subjects had history of mental
illness.
After a participant was invited to the project room, he or
she was briefly introduced to the experiment protocol and the
use of self-assessment questionnaire. Then, the participant
was seated in front of the computer which played the audio
stimuli. He or she was required to keep still and eyes closed
during experiment sessions to avoid muscle movement and
eye blinking artifacts.
Experiment 1 was consisted from five sessions. Each
session targeted one type of emotion induction. There was a
60 seconds silent period at the beginning of each session.
After that, one piece of music truncated to 1 minute duration
was played to the subject.
For experiment 2 using stimuli from IADS, 5 sessions,
namely: session 1 - neutral, session 2 - positive/low aroused,
session 3 - positive/high aroused, session 4 - negative/low
aroused, session 5 - negative/high aroused were prepared. In

each session, there was a 6 seconds’ silent break, then 6 clips
of IADS stimuli aiming at one particular emotion were
played to the subjects. For neutral state, since only three
sounds clips were available, each clip was played twice in
order to keep the same session duration.
For both experiments, the subjects needed to complete
the questionnaire after listening to music/sounds. In the
questionnaire, the Self-Assessment Manikin (SAM)
technique [27] with two dimensions: arousal and valence and
five levels indicating the intensity of both dimensions, was
employed for emotion state measurement. Additionally, the
subjects were asked to write down their feelings in a few
words.
Emotiv device [28] and Pet 2 [29] were considered for
carrying out experiments. Finally, Emotiv device with 14
electrodes locating at AF3, F7, F3, FC5, T7, P7, O1, O2, P8,
T8, FC6, F4, F8, AF4 following
the American
Electroencephalographic Society Standard [30] was used in
the experiments. The sampling rate is 128Hz.
B. Data Analysis and Results
The data collected in our two experiments using the
Emotiv headset was analyzed to find spatio-temporal
emotion patterns of high and low arousal level with positive
and negative valence level. 2 to 42 Hz band-pass filter was
applied to the raw data as the major EEG waves (alpha, theta,
beta, delta, and gamma) lie in this band [31-32]. Then,
Higuchi fractal dimension algorithm described in section II C
was applied for FD values calculations. We implemented the
algorithm with a sliding window where the window size was
1024 samples and 99% overlapping was applied to calculate
FD values of the filtered data.
In the first experiment using music stimuli, the data from
13th to 24th seconds of recording was processed. In the
second experiment using IADS clips, the data from 2nd to
the 13th seconds of recording was processed.
The arousal level could be identified from different
electrode locations. FC6 was selected for the arousal level
recognition as the FD values computed from it gave better
arousal difference compared to other channels. The mean of
FD values computed from FC6 aiming at recognizing the
arousal level for all subjects in music and IADS experiments
is shown in Table I and Table II. Two FD values for high
arousal level with negative and positive valence, and two FD
values for low arousal level with negative and positive
valence are presented in the Tables as follows: negative high
aroused (N/HA), positive high aroused (P/HA), negative low
aroused (N/LA), and positive low aroused (P/LA). In Table I,
it is shown that 10 subjects participated in the Music
experiment. In Table II, it is shown that 12 subjects
participated in IADS experiment. Based on the selfassessment questionnaires, 46 pairs of comparisons from
different subjects between high aroused and low aroused
states regardless of the valence level were used. 39/46
(84.9%) showed that the higher arousal was associated with
the larger FD values. This phenomenon is illustrated in Table
I and II as the mean of FD values for the high aroused states
(N/HA and P/HA) is larger than the low aroused states
(N/LA and P/LA). For example, for the subject #1 N/HA

value 1.9028 is larger than N/LA value 1.7647 and P/LA
value 1.8592, and P/HA value 1.9015 is larger than N/LA
value 1.7647 and P/LA value 1.8592. In Table I and II, we
denoted the FD value as X if the subject’s feeling was
different from the targeting emotion by self-assessment
questioner report. Thus, we eliminated such cases from our
analysis.
TABLE I.

Music

FD VALUES FOR AROUSAL LEVEL ANALYSIS OF MUSIC
EXPERIMENT
Emotion State FD Value
N/HA

P/HA

N/LA

P/LA

Subject #1

1.9028

1.9015

1.7647

1.8592

Subject #2

X

1.9274

X

1.9268

Subject #3

1.9104

1.9274

1.7579

1.8426

Subject #4

1.9842

1.956

1.8755

1.9361

Subject #5

1.7909

1.8351

1.8242

X

Subject #6

1.9111

X

X

1.9127

Subject #7

1.9352

1.9231

1.9106

1.9204

Subject #8

X

X

X

X

Subject #9

1.9253

1.939

X

1.9203

Subject #10

1.8507

1.8842

X

1.8798

a. “X“ denotes the data was not used according to self-report

TABLE II.

IADS

FD VALUES FOR AROUSAL LEVEL ANALYSIS OF IADS
EXPERIMENT
Emotion State FD Value
N/HA

P/HA

N/LA

P/LA

Subject #1

1.8878

1.8478

X

1.8042

Subject #2

1.9599

1.922

1.938

1.8237

Subject #3

1.9418

1.9507

1.9201

X

Subject #4

1.9215

1.917

1.9265

1.9118

Subject #5

1.7215

1.9271

X

1.8659

Subject #6

1.8898

1.8902

1.888

X

Subject #7

X

X

X

X

Subject #8

X

X

X

X

Subject #9

1.9261

1.9241

1.7437

X

Subject #10

1.9223

1.9333

X

1.9026

Subject #11

1.8796

1.8543

1.7183

X

Subject #12

X

X

X

X

a. “X“ denotes the data was not used according to self-report

Frontal brain area plays an important role in the
reflection of the valence level. In works [33-34], it shows
that there is frontal lateralization for positive and negative
emotions. Generally, right hemisphere is more active during
negative emotion, and left hemisphere is more active during

positive emotion. However, there are also studies such as
works [35-36] oppose this hypothesis.
In our study, the difference between FD values from
electrode pair AF3 (left hemisphere) and F4 (right
hemisphere) were used to identify valence level and test the
lateralization theory.
For valence level, the difference between FD values
(AF3-F4) was processed for each subject. It was found that
more stable pattern can be achieved by differentiating
valence level within the same arousal level, either high
aroused or low aroused. The results revealed partial support
for the asymmetric frontal hypothesis. Although not all the
subjects’ dominant hemisphere for positive and negative
emotion was the same as expected in the asymmetric
hypothesis, the pattern of lateralization for a particular
subject was consistent among different experiments with
similar arousal level. 10 subjects’ data was available for
comparison of positive and negative emotion states with
similar arousal level. 9/10 (90%) has shown the consistent
pattern as follows. For example, subject’s EEG data showed
the larger difference between AF3 and F4 values for negative
emotions than for positive emotions in all experiment trails
with different valence levels but similar arousal levels. Five
subjects have the larger difference AF3-F4 value during the
experience of negative emotion, while 4 subjects have larger
AF3-F4 value when having positive emotions. This
phenomenon may indicate that the frontal lateralization
exists with individual differences, and it may not be
applicable for everyone that the left hemisphere is more
active for positive and right hemisphere is more active for
negative emotions. It could be opposite for some individuals,
and this outcome complies with the conclusion made in work
[37] that individual difference may affect the processing of
emotion by brain.
Based on the result of our analysis, we developed the
following real-time emotion recognition algorithm described
in the next section.
C. Real-Time Emotion Recognition Algorithm
As it was mentioned in Introduction, we follow twodimensional model Arousal-Valence described in section II
A. This model allows the mapping of the discrete emotion
labels in the Arousal-Valence coordinate system as shown in
Fig. 1.
The advantage of using this model is that we can define
arousal and valence levels of emotions with the calculated
FD values. For example, the increase in arousal level
corresponds to increase of FD values. Then, by using ranges
of arousal and valence level, we could obtain discrete
emotions from the model. Finally, any emotion that can be
represented in the Arousal-Valence model can be recognized
by our emotion recognition algorithm.

subject is consistent, but FD values may vary among
different subjects, training session needs to be introduced in
order to improve the accuracy.
The training session scheme is illustrated in Fig. 3. The
procedure is similar with the real time scheme, except the
input is EEG data of anticipated emotion. Thresholds are set
and the lateralization pattern is found out based on the data
collected from the training session for the user.
When the subject wants to use this system after training,
the procedure is illustrated as the lower part below the dash
line in Fig. 3. The pattern of newly collected EEG data is
recognized based on comparison with the training session,
and the emotion state is classified.

Figure 1. Emotion labels in arousal-valence dimension
Russell’s circumplex model [38]).

(Adapted from

The emotion recognition algorithm for real time is
illustrated in Fig. 2. The raw EEG data gathered from AF3,
F4 and FC6 are the input to the 2 to 42 Hz band-pass filter.
Then, Higuchi fractal dimension algorithm with a sliding
window of window size 1024 and 99% overlapping is
applied to the filtered data. The benefit of the usage of the
sliding window is that it enables real time processing.
The FD value calculated from FC6 is used to distinguish
the arousal level independently by comparing with default
threshold extracted from our experiments’ results described
in section III B. As shown in Fig. 1, the change of FD could
be mapped along the arousal axis since our experiments
revealed that higher arousal level was associated with larger
FD values. Based on this observation, continuous recognition
of changing emotion from low arousal to high arousal is
enabled.
The difference of FD values between left hemisphere and
right hemisphere (AF3-F4) is computed simultaneously.
After the arousal level has been identified, the valence level
of emotions is recognized within the similar arousal level by
comparing the difference of FD with another threshold
which was set for valence level recognition.
Finally based on the arousal level and valence level, the
emotions are mapped into 2D model.

Figure 2. The emotion recognition algorithm overview.

In the algorithm, we set default thresholds for real time
emotion recognition based on the experiments’ results.
However, because of the existence of individual difference
which means the pattern of emotion for one particular

Figure 3. An emotion recognition scheme with training session.

IV.

REAL TIME APPLICATION

In order to visualize the recognized emotions, we
implemented our algorithm with Haptek activex control
system [11]. Microsoft Visual C++ 2008 was used in this
project.
Haptek software is a 3D model with predefined
parameters for controlling facial muscles visualization,
thereby enables users to create customized emotions and
expressions. Haptek supports stand-alone and web-based
application.
A. Data Acquisition
EEG data is acquired using Emotiv headset at 128 Hz.
We used Emotiv Software Development Kit for acquiring
raw data from the device. Three out of fourteen Emotiv’s
channels at locations AF3, F4 and FC6 are fed into the
algorithm for the emotion recognition process.
B. Data Processing
A data stream from the Emotiv device is stored in a
buffer. Every time a read command is triggered, all the
samples in the buffer are taken out and the buffer is cleared.

Therefore, the number of data obtainable at a time depends
on how long the samples have accumulated in the buffer.
The fractal algorithm requires data to be fed in a bunch of
1024 samples at a time for one channel. Therefore, we use a
queue to buffer the data from Emotiv’s buffer to the
algorithm. The queue is refreshed by the current number of
samples in Emotiv’s buffer every time the read command is
triggered as shown in Fig. 4. In the algorithm, those obsolete
values in the queue are replaced by latest values in the
Emotiv buffer at the time.

Figure 5. Illustration of discrete arousal and valence levels.

Mapping of discrete values of (Arousal level, Valence
level) into 6 emotions is shown in Table III.
TABLE III.

MAPPING OF COMBINATIONS OF (VALENCE, AROUSAL)
AND CORRESPONDING EMOTIONS

(Valence, Arousal)

Figure 4. An illustration of the implementation process of the application.

C. Emotion Mapping to Haptex
Haptek Activex control provides functions and
commands to change facial expressions of 3D avatars. We
used an avatar available with free version of Haptek
development package for this application. We defined six
emotions by changing the parameters controlling the facial
muscles of the Haptek emotion avatar. Those emotions are:
fear, frustrated, sad, happy, pleasant and satisfied. The above
emotions can be recognized by the proposed emotion
recognition algorithm described in the section III.
For the mapping, arousal and valence levels are
transformed into discrete values using thresholds. After this
step, arousal level can only take one of the following values
0, 1 or 2 and valence 0 or 1 as shown in Fig. 5. Combination
of discrete values of arousal and valence level gives six types
of emotions.

Emotion

(0,0)

Sad

(0,1)

Frustrated

(0,2)

Fear

(1,0)

Satisfied

(1,1)

Pleasant

(1,2)

Happy

Picture of the user with headset and pictures of six
emotions created using Haptek are shown in Fig. 6 and Fig. 7
respectively.

Figure 6. User with Emotiv headset.

calculation allows recognize any emotion that can be defined
in 2 dimensional Arousal-Valence model.
Currently,
real-time
emotion
recognition
and
visualization is implemented as a standalone application. The
next step of the project is an integration of our tools in CoSpaces on the Web targeting an entertainment industry.
A short video about the emotion recognition algorithm
implemented in real time with the Haptex system is
presented at http://www3.ntu.edu.sg/home/EOSourina/.
ACKNOWLEDGMENT
This project is supported by grant NRF2008IDMIDM004-020 “Emotion-based personalized digital media
experience in Co-Spaces” of National Research Fund of
Singapore.
REFERENCES
[1]

[2]

[3]

[4]

Figure 7. Six visualized emotions with Haptek (a) Fear (b) Frustrated (c)
Sad (d) Happy (e) Pleasant (f) Satisfied.

V.

CONCLUSION AND FUTURE WORK

In this paper, emotion classifications, emotion
recognition algorithms, and emotion evoking experiments
are reviewed. We proposed and implemented a novel fractal
dimension based algorithm for recognition of emotions from
EEG in real time. We implemented our algorithm with free
downloaded system Haptek that is also commercially
available. The system allows visualization of emotions as
facial expressions of avatars in 3D collaborative
environments in real time.
Compared with other works, our algorithm uses fewer
electrodes. We recognized emotions with AF3, F4 and FC6
electrodes, however, for example, in works [4] and [5], 63
and 16 electrodes were used respectively. Until now, to our
best knowledge there is no real-time EEG-based emotion
recognition algorithms reported. We implemented a novel
real-time emotion recognition algorithm based on fractal
dimension calculation. In this paper, we implemented
recognition of six emotions: fear, frustrated, sad, happy,
pleasant and satisfied. However, our approach based on FD

[5]

[6]
[7]

[8]
[9]

[10]

[11]
[12]
[13]

Y. P. Lin, C. H. Wang, T. L. Wu, S. K. Jeng, and J. H. Chen,
“EEG-based emotion recognition in music listening: A
comparison of schemes for multiclass support vector
machine,” in ICASSP, IEEE International Conference on
Acoustics, Speech and Signal Processing - Proceedings,
Taipei, 2009, pp. 489-492.
D. O. Bos, “EEG-based emotion recognition,” 2006. [Online].
Available:
http://hmi.ewi.utwente.nl/verslagen/capitaselecta/CS-Oude_Bos-Danny.pdf.
R. Horlings, “Emotion recognition using brain activity,”
Department of Mediamatics, Delft University of Technology,
2008.
M. Murugappan, M. Rizon, R. Nagarajan, S. Yaacob, I.
Zunaidi, and D. Hazry, “Lifting scheme for human emotion
recognition using EEG,” in Proceedings - International
Symposium on Information Technology 2008, ITSim, 2008.
K. Schaaff, “EEG-based emotion recognition,” Diplomarbeit
am Institut fur Algorithmen und Kognitive Systeme,
Universitat Karlsruhe (TH), 2008.
M. Li, Q. Chai, T. Kaixiang, A. Wahab, and H. Abut, “EEG
emotion recognition system,” ed, 2009, pp. 125-135.
J. A. Russell, “Affective space is bipolar,” Journal of
Personality and Social Psychology, vol. 37, 1979, pp. 345356.
W. James, “What is an emotion,” Mind, vol. 9, April 1984,
pp. 188-205.
M. M. Bradley and P. J. Lang, “The International Affective
Digitized Sounds (2nd Edition; IADS-2): Affective ratings of
sounds and instruction manual,” University of Florida,
Gainesville, 2007.
M. M. Bradley, P. J. Lang, and B. N. Cuthbert, “International
Affective Picture System (IAPS): digitized photographs,
instruction manual and affective ratings,” University of
Florida, Gainesville, 2005.
Haptek. Available: http://www.haptek.com.
I. B. Mauss and M. D. Robinson, “Measures of emotion: A
review,” Cognition and Emotion, vol. 23, 2009, pp. 209-237.
R. Plutchik, Emotions and life : perspectives from
psychology, biology, and evolution, 1st ed. Washington, DC:
American Psychological Association, 2003.

[14] G. Chanel, “Emotion assessment for affective-computing

[15]

[16]

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

based on brain and peripheral signals,” Computer Science
Departement, University of Geneva, Geneva, 2009.
V. Kulish, A. Sourin, and O. Sourina, “Human
electroencephalograms seen as fractal time series:
Mathematical analysis and visualization,” Computers in
Biology and Medicine, vol. 36, 2006, pp. 291-302.
V. Kulish, A. Sourin, and O. Sourina, “Analysis and
visualization of human electroencephalograms seen as fractal
time series,” Journal of Mechanics in Medicine & Biology,
World Scientific, 26(2) , 2006, pp.175-188.
C. J. Stam, “Nonlinear dynamical analysis of EEG and MEG:
Review of an emerging field,” Clinical Neurophysiology, vol.
116, 2005, pp. 2266-2301.
A. Accardo, M. Affinito, M. Carrozzi, and F. Bouquet, “Use
of the fractal dimension for the analysis of
electroencephalographic time series,” Biological Cybernetics,
vol. 77, 1997, pp. 339-350.
N. Pradhan and D. Narayana Dutt, “Use of running fractal
dimension for the analysis of changing patterns in
electroencephalograms,” Computers in Biology and
Medicine, vol. 23, 1993, pp. 381-388.
W. Lutzenberger, T. Elbert, N. Birbaumer, W. J. Ray, and H.
Schupp, “The scalp distribution of the fractal dimension of the
EEG and its variation with mental tasks,” Brain Topography,
vol. 5, 1992, pp. 27-34.
O. Sourina, V. Kulish, and A Sourin, “Novel tools for
quantification of brain responses to music stimuli,” In Proc of
13thInternational Conference on Biomedical Engineering
ICBME 2008 3–6 December 2008 Singapore, 2008, pp. 411414.
O. Sourina, A Sourin, and V. Kulish, “EEG data driven
animation and its application,” in Proc of International
Conference Mirage 2009, Springer, LNCS 5496, May 4-6,
2009, pp. 380–388.
A. Block, W. Von Bloh, and H. J. Schellnhuber, “Efficient
box-counting
determination
of
generalized
fractal
dimensions,” Physical Review A, vol. 42, 1990, pp. 18691874.
T. Higuchi, “Approach to an irregular time series on the basis
of the fractal theory,” Physica D: Nonlinear Phenomena, vol.
31, 1988, pp. 277-283.
P. Maragos and F. Sun, “Measuring the fractal dimension of
signals: morphological covers and iterative optimization,”

[26]

[27]

[28]
[29]
[30]

[31]
[32]

[33]

[34]

[35]

[36]

[37]

[38]

IEEE Transactions on Signal Processing, vol. 41, 1993, pp.
108-121.
Q. Wang, O. Sourina and M. K. Nguyen, "Fractal dimension
based algorithm for neurofeedback games," in Proc. CGI
2010. SP25, Singapore, 2010.
M. M. Bradley, “Measuring emotion: The self-assessment
manikin and the semantic differential,” Journal of Behavior
Therapy and Experimental Psychiatry, vol. 25, 1994, pp. 4959.
Emotiv. Available: http://www.emotiv.com.
PET
2-channel
bipolar.
Available:
http://www.brainquiry.com/NeurofeedbackHardware.html.
“American electroencephalographic society guidelines for
standard electrode position nomenclature,” Journal of Clinical
Neurophysiology, vol. 8, 1991, pp. 200-202.
S. Sanei and J. Chambers, EEG signal processing. Chichester,
England ; Hoboken, NJ: John Wiley & Sons, 2007.
D. Sammler, M. Grigutsch, T. Fritz, and S. Koelsch, “Music
and emotion: Electrophysiological correlates of the processing
of pleasant and unpleasant music,” Psychophysiology, vol.
44, 2007, pp. 293-304.
N. A. Jones and N. A. Fox, “Electroencephalogram
asymmetry during emotionally evocative films and its relation
to positive and negative affectivity,” Brain and Cognition,
vol. 20, 1992, pp. 280-299.
T. Canli, J. E. Desmond, Z. Zhao, G. Glover, and J. D. E.
Gabrieli, “Hemispheric asymmetry for emotional stimuli
detected with fMRI,” NeuroReport, vol. 9, 1998, pp. 32333239.
R. D. Lane, E. M. Reiman, M. M. Bradley, P. J. Lang, G. L.
Ahern, R. J. Dividson, and G. E. Schwartz, “Neuroanatomical
correlates of pleasant and unpleasant emotion,”
Neuropsychologia, vol. 35, 1997, pp. 1437-1444.
J. V. Pardo, P. J. Pardo, and M. E. Raichle, “Neural correlates
of self-induced dysphoria,” American Journal of Psychiatry,
vol. 150, 1993, pp. 713-719.
S. Hamann and T. Canli, “Individual differences in emotion
processing,” Current Opinion in Neurobiology, vol. 14, 2004,
pp. 233-238.
J. A. Russell, “A circumplex model of affect,” Journal of
Personality and Social Psychology, vol. 39, 1980, pp. 11611178.

