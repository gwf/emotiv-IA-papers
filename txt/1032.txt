Ingeniare. Revista chilena de ingeniería, vol. 23 Nº 4, 2015, pp. 496-504

Control de movimiento robótico con detección cognitiva
y facial mediante Emotiv EEG
Robotic motion control with cognitive and facial detection via Emotiv EEG
Sebastián Monge Lay1  Diego Aracena Pizarro1
Recibido 1 de diciembre de 2014, aceptado 11 de mayo de 2015
Received: December 1, 2014   Accepted: May 11, 2015
RESUMEN
Este trabajo presenta el desarrollo de una interfaz cerebro-computador utilizando el dispositivo Emotiv
EEG, en donde se realizan reconocimientos de gestos faciales y pensamientos cognitivos, los que
serán traducidos en movimientos que serán ejecutados por un robot Lego Mindstorms, con la idea de
verificar si realmente Emotiv realiza un buen reconocimiento y puede ser utilizado para el movimiento
de dispositivos. El trabajo es motivado por otras investigaciones realizadas, en donde se aplicaron
distintas técnicas para el reconocimiento, pero en algunos casos la velocidad y en otros la precisión no
fueron las esperadas, por lo que en esta investigación utiliza otros métodos para evitar esos problemas.
Se realizan pruebas de reconocimiento con seis personas, arrojando en el reconocimiento cognitivo un
81% de precisión con dos acciones aprendidas y para el reconocimiento facial una precisión de 67%
con seis gestos utilizados. Posteriormente se realiza una prueba en el sistema implementado con una
arquitectura dirigida por eventos, junto con el dispositivo Emotiv y el robot, obteniendo una precisión
del 70%, evaluada según lo que el usuario quería hacer y lo que ejecutaba el robot, donde se muestra que
el dispositivo Emotiv es recomendable para su uso en el control de movimientos, ya que entrega buenos
resultados de reconocimiento y permite múltiples acciones en tiempo real.
Palabras clave: Emotiv, robot, EEG, electroencefalograma, señales mentales, gestos faciales, interfaces
cerebro-computador BCI.
ABSTRACT
This papers presents the development of a brain-computer interface using the Emotiv EEG device, where
a facial gestures and cognitive thought are detected, which will be translated into movements by a Lego
Mindstorms robot. This translation is made to verify if the Emotiv is able to perform a good recognition
and it can be used to control the movement of devices. This research work was motivated by previous
works, where different recognition techniques were applied, but in some cases the speed and in others
the precision of detection were not the expected. Therefore, in this research work uses others methods to
solve these problems. Recognition tests were performed with 6 persons, resulting on an 81% of precision
with cognitive recognition for two actions learned and a 67% of precision with facial recognition for six
gestures used. Lately, a test was performed in a system implemented under an event-driven architecture,
using the Emotiv device and the Lego robot, obtaining a precision on detection of 70%, evaluated by
comparison between the action desired by the user and the action effectively performed by the robot,
which shows that the Emotiv device is recommendable for its use, given the quality/price factor, because
it gives good recognition results and allows multiple actions in real time.
Keywords: Emotiv, robot, EEG, electroencephalogram, mental signal, facial gestures, brain-computer
interfaces, BCI.
1

Escuela Universitaria de Ingeniería Industrial, Informática y Sistemas. Universidad de Tarapacá. Casilla 6-D. Arica, Chile.
E-mail: smonge@live.cl; daracena@uta.cl

Monge y Aracena: Control de movimiento robótico con detección cognitiva y facial mediante Emotiv EEG

INTRODUCCIÓN
Hoy la innovación en interfaces humano-computador
está siendo muy frecuente en la industria, empezando
con la consola de juegos Wii® de propiedad de la
empresa Nintendo®, con su revolucionario mando
inalámbrico que lee movimientos gracias a su sensor
integrado de tres ejes [1], por otra parte la Kinect®
para Xbox 360® de Microsoft®, donde la interfaz
es tu propio cuerpo gracias a su cámara RGB y
su sensor de profundidad [2], que detectan los
movimientos, entre otros dispositivos en el mercado.
Estas interfaces utilizan la movilidad del cuerpo
para la detección, el cual no es el caso de Emotiv
EEG® de Emotiv®, que consiste en un casco
neuronal de alta resolución con 14 canales y un
giroscopio de dos ejes. El dispositivo es inalámbrico
y se comunica en radiofrecuencia a una entrada
USB del computador, que recibe las señales para
su procesamiento [3], lo que permite mediante un
ambiente desarrollado obtener las acciones cognitivas,
afectivas y expresivas generadas por el usuario y
transformarlas en alguna acción a realizar, como
puede ser el movimiento de algún dispositivo. En
la Figura 1 se puede apreciar el casco.

controlar el brazo, pero en vez de emplear su software
de reconocimiento, utiliza una implementación
propia, logrando reconocer más acciones mentales
que el propio Emotiv (que posee un máximo de
cuatro), pero con una precisión bastante baja (menos
del 40% de aciertos). Otro caso interesante es el
control de una silla motorizada con un brazo mecánico
incrustado, el que es controlado por medio de las
señales obtenidas por un electroencefalograma,
que fue construida para satisfacer las necesidades
de las personas con movilidad reducida [5], pero
su sistema era muy lento, debido a que se mostraba
una pantalla con varias opciones de movimiento, la
que se iluminaba por sectores cada cierto tiempo,
y se detectaba la señal mental que se generaba
cuando se iluminaba el sector que el usuario quería
ejecutar, lo que consumía mucho tiempo, y podría
ser riesgoso en el caso de utilizarlo en una silla de
ruedas eléctrica y encontrarse con un desnivel o
escaleras, y tener que esperar a que se ilumine la
opción de parar para lograr detenerla. Por último
y semejante a este trabajo es el control mental de
un robot Lego Mindstorms® por medio del sistema
Neurosky® [6] que es similar al Emotiv EEG, pero
cuenta con un único canal de recepción de señales
EEG y solo realiza detecciones de estados mentales
(calma, concentración, etcétera) y no de pensamientos
y gestos faciales como Emotiv.
Este trabajo se motiva en las investigaciones descritas
anteriormente, tratando de solventar las falencias
mencionadas, en búsqueda de un reconocimiento
rápido y preciso, por lo que se desarrolla una
interfaz de movimiento neuronal para el robot Lego
Mindstorms, que es utilizado como dispositivo de
movimiento, para medir su precisión mediante las
diferentes acciones captadas por el casco de Emotiv,
las que se traducirán en movimientos que serán
ejecutados por el robot, con el fin de determinar
qué tan cercano y efectivo es el reconocimiento,
centrándose en acciones faciales (pestañear, masticar,
etcétera) y cognitivas (pensamientos) para las pruebas.
METODOLOGÍA

Figura 1. Dispositivo Emotiv EEG.
Existen diversos trabajos en torno a las interfaces
cerebro-computador, como por ejemplo el control
de un brazo robótico mediante electroencefalografía
(EEG) [4], el que utiliza la tecnología Emotiv para

Para la selección del dispositivo EEG se consideraron
diversos factores como su accesibilidad, costo,
portabilidad y cantidad de canales, siendo la
mejor opción el dispositivo Emotiv EEG, que es
un sistema acorde a las necesidades del proyecto
y además es una decisión apoyada por el trabajo
497

Ingeniare. Revista chilena de ingeniería, vol. 23 Nº 4, 2015

descrito en [7], que realiza una comparación con
diversos dispositivos EEG, obteniendo la misma
conclusión, sobre todo lo que más se destaca de
Emotiv es su calidad/precio, además de ser un
dispositivo portable, en comparación con los
grandes sistemas de EEG que cuentan con muchos
canales y grandes equipos de recepción de la señal.
En el caso de la selección del robot Lego Mindstorms
es un robot armable, con tres servomotores, por lo que
es ideal su utilización para medir los movimientos
que serán generados por el casco, ya que su forma
depende de cómo uno lo arme, y en este caso se
optó por un diseño de dos ruedas con un motor
independiente por cada una, como se observa en
la Figura 2.

medio de un receptor USB que recibe las señales
y las despacha a su software de detección que
reconoce expresiones faciales, estados anímicos y
pensamientos cognitivos, estas además pueden ser
procesadas por medio del SDK, que está disponible
para diferentes plataformas de programación [3].
La distribución de los sensores del Emotiv EEG
como se muestra en la Figura 3 están posicionados
de tal manera de obtener beneficios óptimos para
la interacción hombre-máquina. Los sensores se
encuentran principalmente en la corteza frontal, por
lo que es útil para detectar gestos faciales (como
el parpadeo) y también determinar ciertos tipos de
ondas mientras el usuario se concentra en alguna
tarea. Los sensores en P3(CMS) y P4(DLR) son
los utilizados como referencia.

Figura 2. Robot Lego Mindstorms.
Posteriormente para las pruebas, se realizaron
mediciones de reconocimiento facial y cognitivo
(mental), en sujetos de prueba, para determinar los
niveles de precisión, y así determinar si realmente
Emotiv EEG se puede utilizar para realizar
movimientos de dispositivos en tiempo real y con
una alta precisión.
EMOTIV EEG
Emotiv EEG es un producto ofrecido por la
empresa Emotiv que consiste en un dispositivo que
realiza un electroencefalograma de alta resolución
con 14 canales ubicados y nombrados según el
sistema internacional 10-20 [8] más dos canales
de referencia, posee un giroscopio de dos ejes, es
inalámbrico y tiene una batería para 12 horas de
uso continuo, se comunica con el computador por
498

Figura 3. Distribución de sensores en Emotiv.
ARQUITECTURA DEL SISTEMA
Para la implementación del sistema se ha utilizado
C# como lenguaje de programación, el API de
Emotiv para trabajar con el Emotiv EEG, el
framework AForge.NET [9] para el desarrollo de
la aplicación que realiza la conexión con el Robot
Lego Mindstorms y un computador con sistema
operativo Windows 8.1.
En lo que respecta a hardware se ha utilizado un
computador con conexión bluetooth incorporada, que
actúa como intermediario (unidad de procesamiento),
con el dispositivo Emotiv EEG, el que se comunica
con el computador usando su propio sistema
inalámbrico con un adaptador USB y el robot

Monge y Aracena: Control de movimiento robótico con detección cognitiva y facial mediante Emotiv EEG

Lego Mindstorms NXT que utiliza bluetooth para
la comunicación [10]. En la Figura 4 se presenta
la arquitectura de hardware utilizado.

generado corresponde a la acción asociada y en ese
caso ejecutar el movimiento del robot.

Comunicación
Inalámbrica 2.4GHz

Ejecución de
acciones

Procesamiento de Señales

Comunicación
Bluetooth
Movimiento del
Robot

Figura 4. Arquitectura del sistema.
Respecto del software desarrollado, la Figura 5
muestra el diagrama de clases, donde se exponen
a grandes rasgos los métodos implementados para
el funcionamiento del sistema, destacando que el
API de Emotiv (para C#) denotado por la clase
“EmoEngine”, utiliza una arquitectura dirigida por
eventos, por lo que la clase principal de la aplicación
llamada “Robot” también lo implementa. Este
no es el caso del API para manejar el robot Lego
representado por la clase NXTBrick, que se crea
una instancia de él (en la clase Robot), y se llama a
sus funciones para establecer acciones en el robot.
En la Figura 6 se muestra el diagrama de secuencia
que inicia el proceso de generación de eventos cuando
se produce alguna señal en el Emotiv EEG, esta
función se debe ejecutar constantemente para que
analice en tiempo real la información otorgada por
el casco sin procesar, la que al realizar la detección
de alguna acción, genera eventos para que sean
manejados por sus suscriptores. Esta se genera un
proceso aparte, para no interrumpir al hilo principal.
En la Figura 7 se aprecia el diagrama de secuencia
que se encarga de capturar y manejar los eventos que
genera el dispositivo Emotiv, el que encapsula los
valores que se obtienen y genera un nuevo evento,
para que sea manejado por las cinco opciones de
movimiento que tendrá el robot (que serán descritas
más adelante), las que analizarán si el evento

Figura 5. Diagrama de clases.
DESARROLLO DEL SISTEMA
El sistema considera cinco acciones de movimiento
disponible, que son avanzar, ir a la izquierda, ir
a la derecha, retroceder y parar, estas se activan
cuando se genera un evento expresivo o cognitivo
que corresponda a la propiedad seleccionada para
cada acción.
Por ejemplo, si el usuario realiza un pestañeo y está
asociado a la acción avanzar, el robot encenderá sus
motores (en caso de que estén apagados) y avanzará
indefinidamente hasta que ocurra otra acción que le
haga cambiar de estado. En el caso del gesto mirar
a la derecha asociado a la acción de ir a la derecha,
no es necesario mantener el gesto para que el robot
realice esta acción, simplemente con ejecutarla una
499

Ingeniare. Revista chilena de ingeniería, vol. 23 Nº 4, 2015

vez basta, en otras palabras el usuario puede mirar
a la derecha y luego volver a mirar al frente y el
robot ira a la derecha indefinidamente hasta que se
ejecute otra acción.
En la Figura 8 se muestra el diagrama de flujo
sobre la secuencia a seguir entre eventos y acciones
diseñadas por medio de la arquitectura dirigida
por eventos.

Figura 6. Diagrama de secuencia de conexión y
procesamiento.

Figura 8. Diagrama de flujo de eventos.
En la Figura 9 se aprecia la interfaz del sistema,
en donde el panel de la izquierda está el área de
selección de acciones y velocidades del robot, y en
la derecha están las conexiones con los dispositivos
y la activación del sistema.

Figura 7. Diagrama de secuencia para manejar
evento.
Figura 9. Sistema de movimiento.
500

Monge y Aracena: Control de movimiento robótico con detección cognitiva y facial mediante Emotiv EEG

Cuando se genera un evento Emotiv, este entrega un
conjunto de señales correspondientes a las acciones
que ocurrieron en ese momento, como por ejemplo,
las acciones de pestañeo, guiño y masticar del área
expresiva (gestos faciales) y empujar, tirar y rotar
del área cognitiva (pensamiento). En la Figura 10
se muestran todas las acciones disponibles a elegir.

PRUEBAS Y RESULTADOS
Para la utilización de acciones cognitivas se requiere
un entrenamiento previo de ello. Se ha fijado un
total de dos acciones como máximo (disponible
hasta cuatro acciones) para el aprendizaje del
pensamiento, ya que con más de dos se requiere un
entrenamiento que toma bastante tiempo.
Se ha utilizado a seis sujetos de prueba de sexo
masculino y edades entre 24 y 26 años para la
realización del entrenamiento. Los resultados
mostrados por la Tabla 1 indican el tiempo de
entrenamiento y la evaluación de la efectividad
del reconocimiento (en porcentaje), respecto de lo
que realmente estaban pensando, entregando una
precisión de reconocimiento promedio de 81%.
Tabla 1. Tiempos de entrenamiento y porcentaje
de reconocimiento.
Sujeto
1
2
3
4
5
6

Figura 10. Listado de acciones disponibles.
Además de escoger la acción, se puede elegir
la potencia de movimiento del robot, teniendo
rangos desde 0 a 100. En la Figura 11 se muestra
esta configuración, existiendo la posibilidad de
establecer el valor mediante la barra de control
deslizante (track bar) o desde el cuadro numérico.

Figura 11. Panel de configuración para la acción
avanzar.

Tiempo de
entrenamiento
37 minutos
46 minutos
53 minutos
48 minutos
35 minutos
40 minutos

Porcentaje de
reconocimiento
80
85
70
75
85
90

En el caso de los gestos faciales, no se requiere
entrenamiento previo, ya que las señales son
genéricas. Se realizaron pruebas con distintos gestos,
en donde los sujetos realizaron 10 repeticiones
de cada acción, para posteriormente verificar los
niveles de detección. Estos fueron los resultados:
Tabla 2. Porcentaje de reconocimiento de gestos
faciales parte 1.
Sujeto

Pestañeo

1
2
3
4
5
6

100
90
90
100
90
100

Mirar
derecha
100
100
80
90
70
70

Mirar
Izquierda
100
100
80
90
60
70

En la Tabla 2 se puede observar que las tres primeras
acciones tuvieron muy buen nivel de reconocimiento,
pero en la Tabla 3 para las acciones de guiño
izquierdo y derecho existieron problemas. Esto se
501

Ingeniare. Revista chilena de ingeniería, vol. 23 Nº 4, 2015

Tabla 3. Porcentaje de reconocimiento de gestos
faciales parte 2.
Sujeto
1
2
3
4
5
6

Guiño
Derecho
20
20
0
10
30
40

Guiño
Izquierdo
30
20
0
20
20
10

Masticar
100
100
100
100
100
100

puede deber al nivel de habilidad de la persona para
realizar el guiño, la intensidad con que lo realiza, el
nivel de conexión del casco con el cuero cabelludo,
entre otros factores que podrían impedir una buena
detección. Por último la detección de masticar fue de
100% en todos los casos, por lo que es una acción
muy recomendable a usar. El promedio general de
detección de las acciones realizadas es de un 67%,
pero si descontamos del promedio las acciones de
guiño, se obtiene una precisión de 91%, un valor
bastante alto, lo que indica que esas acciones entregan
muy buenos resultados de reconocimiento.
Indagando en las señales en bruto captadas desde
las pruebas realizadas, la señal de guiño derecho
en algunos sujetos generó varios picos en la onda
del canal F8 como se aprecia en la Figura 12, sin
embargo su detección fue bastante pobre, por lo que
podría ser un problema de detección del algoritmo
del sistema Emotiv. Por otro lado, también se observó
que algunos sujetos no eran capaces de realizar
un guiño correctamente, por lo que también pudo
influir en su detección.

Finalmente, se realizó la prueba de control de
movimiento del robot utilizando para ello cinco
gestos faciales, que fueron asignados a cada acción
de movimiento disponible, los que se detallan en
la Tabla 4.
Tabla 4. Asignación de acciones en el sistema.
Acción
Avanzar
Derecha
Izquierda
Retroceder
Parar

Gesto
Pestañeo
Mirar derecha
Mirar izquierda
Sonreír
Masticar

Los resultados de la prueba indican que entre lo
que se desea realizar y lo que se realiza realmente,
está en un 70% de concordancia. Esto es debido a
varios factores como se mencionan anteriormente
incluyendo ahora la acción de mover la cabeza y
hablar que alteran las señales, ya que aún no existen
dispositivos EEG que no se vean afectados por los
movimientos.
En el link http://youtu.be/H6_m9hq6nkc, se puede
ver un video resultante de lo desarrollado en este
trabajo. Las Figuras 12, 13, 14 y 15 muestran
acciones de las pruebas realizadas con el robot.

Figura 13. Pestañear para avanzar.

Figura 12. Señal de guiño derecho.

502

Figura 14. Masticar para detener el robot en
movimiento.

Monge y Aracena: Control de movimiento robótico con detección cognitiva y facial mediante Emotiv EEG

de guiño haya sido provocada por dos motivos, una
donde el usuario no tiene la habilidad de realizar
un guiño correctamente y la otra donde el software
de Emotiv no es capaz de reconocerlo.

Figura 15. Mirar a la derecha para ir a la derecha
parte 1.

Figura 16. Mirar a la derecha para ir a la derecha
parte 2.
CONCLUSIONES
Inicialmente se puede decir que el SDK de Emotiv
posee un buen diseño arquitectónico (orientado a
eventos), el que permite capturar la información
que realmente se desee utilizar, mediante la
suscripción a los eventos correspondientes, como
por ejemplo conocer el estado de conexión del
dispositivo Emotiv EEG o los eventos cognitivos
generados, de los que en vez de preguntar de forma
explícita e iterativa por la información, se espera
a que el publicador genere un evento para que
subsecuentemente se ejecuten las funciones suscritas,
manteniendo un buen orden y claridad en el ámbito
del código, por lo que hace más legible y escalable la
aplicación.
El SDK junto con el dispositivo Emotiv EEG en el
caso de acciones expresivas, tiene un buen grado
de detección para la mayoría de sus opciones
(pestañear, masticar, mirar a los lados, etcétera),
obteniendo una precisión promedio de 67%,
mejorable sustancialmente si se descuenta del
promedio la acción de guiño que no fue detectada
correctamente, obteniendo una alta precisión del
91%. Posiblemente, la pobre detección de la acción

En el caso del área cognitiva, si bien está limitado
hasta cuatro acciones como máximo, ya desde el
entrenamiento para la tercera acción se complica
bastante, requiriendo un tiempo considerable (más
de 1 hora) de entrenamiento que fatiga mentalmente
al usuario, por lo que se recomienda utilizar solo dos
acciones, ya que su periodo de entrenamiento que
ronda por los 43 minutos y su precisión promedio de
81% están en un rango aceptable y que seguramente
con un mayor entrenamiento se obtendrán mejores
resultados.
Finalmente, destacar que el dispositivo Emotiv EEG
es una buena alternativa a usar en comparación de
los sistemas tradicionales de EEG, que cuentan
con muchos más canales que Emotiv, generando
complicaciones en el posicionamiento correcto
de los sensores, además que no son portables ni
inalámbricos, y su costo es sumamente elevado,
por lo que se concluye que Emotiv es un buen
dispositivo EEG respecto de su calidad/precio,
ofreciendo varias opciones de reconocimiento
con una buena precisión, en comparación con los
trabajos descritos anteriormente donde la precisión
fue baja, y ofreciendo resultados en tiempo real,
comparándolo con el trabajo descrito en [5] que su
reconocimiento requería de tiempo que puede ser
valioso para el usuario, por lo que en conclusión,
Emotiv EEG puede ser utilizado como interfaz para
el control de dispositivos.
RECOMENDACIONES
Los 16 sensores de Emotiv están compuestos de
una base con metal y un fieltro, los que deben
ser humedecidos con agua salinizada para su uso.
Posteriormente, el manual indica que para su
almacenamiento, se deben guardar los sensores en su
caja de hidratación, la cual incluye una almohadilla
que cubre todos los sensores (fieltros), y esta debe
ser humedecida con líquido salino.
Debido a esto, con el tiempo se han oxidado los
sensores, lo que puede provocar ruidos en las señales
y ocasionar problemas en la detección, por lo que se
503

Ingeniare. Revista chilena de ingeniería, vol. 23 Nº 4, 2015

recomienda sacar el fieltro de la base, y almacenarlo
de forma separada, para evitar la oxidación de este.
En la Figura 17 se puede observar cómo han quedado
los sensores a causa del almacenamiento húmedo
provocado por la utilización del líquido salino luego
de múltiples usos.

REFERENCIAS
[1]

[2]

[3]
[4]

[5]
Figura 17. Sensores de Emotiv oxidados.
TRABAJO A FUTURO
El desarrollo de este proyecto sigue activo, por lo
que se espera implementar el sistema por medio de
interfaces para las clases, de esta forma será posible
realizar cambios de dispositivos de una manera más
rápidamente, en el caso del sistema de captura de
EEG, que podría pasar de Emotiv a NeuroSky o del
dispositivo motor pasando de Lego Mindstorms a
una silla eléctrica u otros.
También se está trabajando en un sistema de
reconocimiento de patrones, como complemento
al sistema de reconocimiento que trae por defecto
el API de Emotiv, otorgando distintos mecanismos
de detección que se adecuen mejor al usuario.

[6]

[7]

[8]

AGRADECIMIENTOS
[9]
Agradecer en parte al financiamiento aportado por el
proyecto de pregrado 8711-14 denominado “Sistema
de Control Mediante Señales Neurológicas para
Gobernar Movimientos” Universidad de Tarapacá,
y a todas las pacientes personas que entregaron
parte de su tiempo para realizar las pruebas con
el dispositivo.
504

[10]

H. Wisniowski. “Analog Devices And Nintendo
Collaboration Drives Video Game Innovation
With iMEMS Motion Signal Processing
Technology”. Norwood MA, United States.
2006.
Microsoft. “Project Natal”. 2009. Date
of visit: June 10, 2014. URL: http://blog.
seattlepi.com/digitaljoystick/2009/06/01/
e3-2009-microsoft-at-e3-several-metric-tonsof-press-releaseapalloza/
Emotiv. “EEG Specifications”. 2012. Date
of visit: June 12, 2014. URL: http://emotiv.
com/upload/manual/EEGSpecifications.pdf
J. O’Connor. “Real-time Control of a Robot
Arm Using an Inexpensive System for
Electroencephalography Aided by Artificial
Intelligence”. Computer Science Honors
Papers. Nº 3. 2013.
M. Palankar, K. De Laurentis and R. Dubey.
“Using biological approaches for the control
of a 9-DoF wheelchair-mounted robotic
arm system: Initial experiments”. IEEE
International Conference on Robotics and
Biomimetics, pp. 1704-1709. February, 2009.
A. Vourvopoulos and F. Liarokapis. “Braincontrolled NXT Robot: Tele-operating a
robot through brain electrical activity”. Third
International Conference on Games and Virtual
Worlds for Serious Applications, pp. 140-143.
2011.
J. Esteban, J. Cárdenas y A. Peña. “Sistema
para Rehabilitación del Síndrome del Miembro
Fantasma utilizando Interfaz CerebroComputador y Realidad Aumentada”.
Revista Ibérica de Sistemas y Tecnologías de
Información. Nº 11, pp. 93-106. Junio 2013.
American Electroencephalographic Society.
“Guideline thirteen: Guidelines for standard
electrode position nomenclature”. Journal of
Clinical Neurophysiology, pp. 25:111-113.
1994.
AForge.NET. “AForge.NET”. Date of visit:
June 14, 2014. URL: http://www.aforgenet.
com/
The LEGO Group. “LEGO® MINDSTORMS®
NXT Executable File Specification”. 2009.
Date of visit: June 14, 2014. URL: http://
www.lego.com/es-ar/mindstorms/downloads/
nxt/nxt-sdk

