Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

Minding the (Transatlantic) Gap: An Internet-Enabled
Acoustic Brain-Computer Music Interface
Tim Mullen

Richard Warp

Adam Jansch

Dept. of Cognitive Science and
Swartz Center for Computational
Neuroscience, UC San Diego, USA
tmullen@ucsd.edu

Composer
1735 Martin Luther King Way
Berkeley, CA, USA
richwarp@gmail.com

Dept. of Music
University of Hudderfield
Huddersfield, UK
adam@adamjansch.co.uk

ABSTRACT

brain to manipulate an external actuator [14]. In Solo Performer
Lucier’s amplified alpha (8-12.5 Hz) brainwaves were played
through loudspeakers coupled to a battery of percussive
instruments, allowing him to generate resonant acoustic events
by modulating his alpha rhythm. Lucier’s pioneering work was
followed by a number of artists and throughout the 1960’s and
1970’s experimentation with brainwave sonification flourished
(see [9] for a review). However, this was followed by over a
decade of relative silence.
Within the last decade, due in part to successes in the BCI
field, there has been a resurgence of interest in the use of EEGbased BCI technology in musical composition leading Miranda
and Brouse to coin the term Brain-Computer Music Interface
(BCMI) to refer to systems that use a BCI for musical
expression [9,10]. Some BCMI researchers have focused
primarily on active control of a musical interface using standard
BCI tools; for example, Mick Grierson's adaptation of a P300
speller, which allows a user to construct a sequence of musical
notes by attending to various symbols on a display [5]. Others
have focused on neurofeedback applications and passive
cognitive state detection/sonification [6,15]. Still others have
explored collaborative sonification of the mental state of
multiple individuals simultaneously. For instance, Steve Mann,
James Fung, Ariel Garten and Chris Aimone’s
Regen/DECONcert series had dozens of participants don
wearable EEG hardware and alter a synthesized music
soundscape via changes in their collective alpha activity [8].
Importantly, most of these and other BCMI systems have
incorporated local control of a digital and/or synthesized music
interface. There have been comparatively few attempts to create
BCMI systems that mechanically control acoustic instruments
using EEG. As we shall later discuss, the use of visible,
acoustic instrument ensembles, with their somewhat
anthropomorphic, unpredictable and thus essentially ‘human’
method of sound production, introduces new aesthetic
opportunities and challenges. Secondly, although a number of
artists have explored interactive music creation over the
Internet (as reviewed in [10]), comparatively fewer Internetenabled BCMI installations/performances have been developed.
One exception is Andrew Brouse’s InterHarmonium project
[3]. As with any other Internet-enabled interactive media
system, including the possibility for distributed communication
and interaction in a BCMI may significantly expand the range
of possibilities for collaborative musical expression and
audience participation.

The use of non-invasive electroencephalography (EEG) in the
experimental arts is not a novel concept. Since 1965, EEG has
been used in a large number of, sometimes highly sophisticated,
systems for musical and artistic expression. However, since the
advent of the synthesizer, most such systems have utilized
digital and/or synthesized media in sonifying the EEG signals.
There have been relatively few attempts to create interfaces for
musical expression that allow one to mechanically manipulate
acoustic instruments by modulating one’s mental state.
Secondly, few such systems afford a distributed performance
medium, with data transfer and audience participation
occurring over the Internet. The use of acoustic instruments and
Internet-enabled communication expands the realm of
possibilities for musical expression in Brain-Computer Music
Interfaces (BCMI), while also introducing additional
challenges. In this paper we report and examine a first
demonstration (Music for Online Performer) of a novel system
for Internet-enabled manipulation of robotic acoustic
instruments, with feedback, using a non-invasive EEG-based
BCI and low-cost, commercially available robotics hardware.

Keywords
EEG, Brain-Computer Music Interface, Internet, Arduino.

1. INTRODUCTION
Electroencephalography, first applied to humans by Hans
Berger in 1924, is the recording of summed electrical activity
of large populations of similarly oriented and locally
synchronous neurons, located primarily in the human
neocortex. Although the earliest effort to sonify EEG was
reported in a 1934 paper in Brain [1] Alvin Lucier’s 1965
Music for Solo Performer is widely considered the first EEGbased musical composition. Lucier was strongly motivated by
“the image of the immobile if not paralyzed human being who,
by merely changing states of visual attention, could
communicate with a configuration of electronic equipment” [7].
Interestingly, this was nearly a decade before the earliest
published attempts by Jacques Vidal and others to create what
we now call a brain-machine/computer interface (BMI/BCI),
which is a system that uses signals recorded directly from the
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. To copy
otherwise, to republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee.
NIME’11, 30 May–1 June 2011, Oslo, Norway.
Copyright remains with the author(s).

1.1 Music For Online Performer
On January 16, 2010 we premiered Music for Online Performer
as part of Adam Jansch and Richard Glover’s In Tones:
Organ/Radio/Television/Internet installation series. The name
and other subtle references to Lucier’s Solo Performer –
including the use of acoustic percussive instruments – were

469

Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

chosen due to our mutual respect for Lucier’s pioneering work.
Here electrical signals recorded from the brain of a participant
(T.M.) in San Diego, USA were used to manipulate, in near
real-time, acoustic instruments in front of a live audience at
Phipps Hall at the University of Huddersfield, UK. Using
freely-available LivestreamTM and SkypeTM technology, the
music was streamed back to the conductor/composer (R.W.) in
San Francisco and the “brainist” in San Diego, who used this
feedback (along with local visual feedback), combined with
compositional instructions delivered by the conductor, to
manipulate his brain rhythms and thereby inform the ongoing
composition. In addition, a live Internet audience watched
audio-video feeds from all three locations and was in constant
communication with the conductor via a Livestream chatroom,
allowing them to indirectly influence the composition.
The installation was structured around the concept of a
quartet: four instruments being manipulated by four
fundamental neuronal frequency bands estimated from four
neural signals recorded from the brain of the solitary performer.
The installation was also comprised of four participating
parties, distributed around the world but connected via the
Internet: the brainist, the composer/conductor, the physical
audience (Phipps Hall), and the virtual (Internet) audience.

G9&;';IE*M6[W*N6BP*

T(&)#=:(**
S;I@&)*L((HU&$V*

A'IE9@$=8'I*

A7B*
6C&=&)*
D)E(9;'F*

\<*

-**

"#.*

/0#/123*

`*

/123#40*

^*

.#/123*

]*

_*

B@H;8*L((HU&$V*M);S(P*

!"a*##"

G&'H#
C&II*
D)E(9*

!"#$%&''()*++,*

567*

>&?J>6K*A'E(9L&$(*MNOP*

567*
>&?J>6K*
Q*567*

'89:&);<&=8'*

78:C8I(9J*
78'H@$E89*
M6ZWN6BP*

>&?@;'8*
!"

!"

!"

B9H@;'8*
G8&9H*

!"

2. TECHNICAL DESIGN
The design schematic for Online Performer is outlined in
Figure 1. The brainist is seated in a room in front of two
displays, a visual neurofeedback display and a compositional
instructions display. Stereo auditory feedback is provided via
speakers.

R;S(IE9(&:*

2.1 Data Acquisition

`*

^*

$X:U&)*

$%;:(I* EX:C&';*

_*

]*

$())8*

A'E(9'(E*&@H;('$(*
MY89)HP*

K%XI;$&)*&@H;('$(*MNOP*

64-channel EEG (Biosemi, Inc) is recorded from the brainist at
a sampling rate of 256 Hz. The data is imported into Matlab®
(Mathworks, Inc) in 2-second segments using the open-source
ERICA/Datariver environment [4]. Due to a hardware issue
involving Arduino memory buffer maintenance, data
controlling the musical instruments could not be updated faster
than 5 instructions/sec. Thus, we fixed the time interval
between data segments to 200 ms, although this could
theoretically be decreased by at least a factor of 10 or more.

B@H;8JS;H(8*IE9(&:*
MR;S(IE9(&:W*6VXC(P*

Figure 1. Installation flowchart for Music for Online
Performer. Globes represent Internet transmission.
choice was informed by a large quantity of published literature
relating power modulation in these bands, near the four selected
brain areas, to several mental tasks such as motor imagery,
mental calculation, and relaxation. Specifically, it is known that
motor imagery (imagination of body part movement) leads to a
decrease in mu and beta power, termed event-related
desynchronization (ERD), in the region of sensorimotor cortex
corresponding to the body part being imaged with a
concomitant increase in power (event-related synchronization
(ERS)) in distal regions of sensorimotor cortex. Relaxation is
known to result in alpha ERS in visual cortex, while visual
imagery or task engagement/focus leads to alpha ERD.
Engagement in tasks with high working memory demands, such
as mental calculation, is associated with increases in frontal
midline theta power [12,13].
The four bandpower estimates are then fed back to the user
via a bar graph display. Such real-time neurofeedback is known
to be a powerful tool in improving the ability of an individual
to modulate his/her neuronal rhythms and is considered an
integral component of a closed-loop BCI [14]. The same
bandpower estimates are also simultaneously packaged and
transmitted to a computer at the performance site (Phipps Hall,
University of Huddersfield, United Kingdom) using Open
Sound Control (OSC).

2.2 EEG Features
Each 2-second data segment is separated into 64 maximally
independent time series (independent components or “ICs”) by
projection through a spatial filter previously learned on training
data by Independent Component Analysis [2,11]. Here, the
training data was a 30-minute long continuous EEG time series
recorded from the brainist performing a series of mental
exercises, similar to those used to control the music BCI
(relaxation, left hand motor imagery, right hand motor imagery,
mental calculation). Four of these components are selected
based on prior analysis of the spatial topography of the
components across the scalp. In our implementation, we
selected four components each with spatial filter weights
resembling the projection of a single equivalent-current dipole
(e.g., a patch of locally synchronous neurons constituting an
EEG “source”) located near one of frontal midline cortex
(FMC), visual cortex (VC), or left or right sensorimotor cortex
(lSMC, rSMC).
The power spectral density for each selected IC is then
obtained using the Burg method (with an eighth-order
autoregressive model) and a bandpower quantity computed by
integration over one of four frequency bands. In our
implementation, we estimated bandpower for the FMC, VC,
lSMC, and rSMC ICs using the respective bands 4-8 Hz (theta),
8-12.5 Hz (alpha), 10-12.5 Hz (mu), 12.5-30 Hz (beta). This

2.3 Acoustic Instrument Control
At the performance site, OSC packets are unpacked and
imported into Max/MSP, where the bandpower values are
rescaled, converted into servomotor angular rotation values,

470

Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

produces a continuous “sweeping” or oscillating timbre whose
volume can be varied by modulating the rotational velocity of
the servo; increasing the rotational velocity causes the
drumstick to brush the cymbal at a higher rate, increasing the
resonance of the cymbal and thus the perceived volume.
The frequency-instrument mappings were selected so as to
map the more controllable frequencies (respectively, alpha, mu,
beta) to the more acoustically salient instruments in the
ensemble. Additionally, the mappings were intended to loosely
reflect the acoustic qualities of the individual neural
frequencies. For instance, the rhythmic sweeping sound of the
cymbal was evocative of low-frequency “droning” of a 3-7 Hz
theta rhythm while the rapid beating and sharp attack of the
woodblock was evocative of the high-frequency beta rhythm.

2.4 Audience Participation and Feedback
In our installation, a live audience in Phipps Hall observed the
performance first-hand. Simultaneously, live audio and video
(from all three geographic locations) was recorded and
streamed over the Internet using freely available software (here,
Skype and Livestream) to a virtual global audience. Here we
had a public Livestream channel/chatroom setup, which
audience members could log in to and communicate with each
other and the conductor while watching the live performance.
A branch of the audio stream was transmitted to a
composer/conductor in another location (here, San Francisco,
USA). The conductor had a Max/MSP control interface, which
was linked via OSC to the brainist’s compositional
instructions/notes display, implemented in Matlab. Based on a
predetermined, loosely structured, compositional score and the
influences of the audience, the conductor could direct the
brainist to individually modulate different instruments (e.g.,
increase the cello pitch by increasing alpha bandpower through
relaxation).
A third branch of the audio stream was fed back to the
brainist who could use this, along with visual neurofeedback, to
help control his neuronal rhythms. This also allowed the
brainist to experience the full musical ensemble, making the
BCI-instrument interaction less abstract and affording an
element of direct improvisational control in the ongoing
evolution of the composition.

Figure 2. Instruments used in Music for Online Performer
and transmitted to an Arduino board over an RS232 (serial)
interface using the Maxuino patch developed by Chris
Coleman1. The Arduino board (we used the Arduino
Duemilanove with the ATmega168 microcontroller) uses a
mixture of analog and digital pulse-width modulation (PWM)
sequences to control four servomotors, each of which
mechanically manipulates a separate musical instrument
thereby acoustically sonifying the respective bandpower. The
four instruments chosen, with respective frequency band / brain
anatomy mappings were cello (alpha, VC), chimes/bells, (mu,
lSMC), woodblock (beta, rSMC), and cymbal (theta, FMC).
The instruments were chosen for their percussive quality (with
a nod towards Lucier’s own choice of percussive instruments in
Solo Performer) as well as based on our ability to effectively
manipulate the instrument using a simple rotational servomotor.
The mechanical devices actuating the instruments (shown in
Figure 2) were designed as follows.
The cello, using standard A3/D3/G2/C2 tuning, was played
via a cello bow attached to a mechanical ‘arm’ which
connected to a rotational servomotor the angle of which was
smoothly varied between 45 and –45 degrees by a 4 Hz
oscillator. This produced a “tremolo” effect. The specific note
evoked by the tremolo was determined via the brainist’s alpha
power modulation. Alpha power was scaled to the range [0 90]
degrees and added as an offset to the servomotor angle. This
changed the mean angle the bow made with the cello neck
producing a bowed tremolo over a different subset of strings.
The chime array was actuated by a 9V DC fan whose speed
varied inversely proportionate to mu power. The chimes (an
array of 20 washer discs ranging in size and weight) were
distributed from heaviest to lightest (front to rear) such that
increases in fan speed (due to mu ERD) would resonate the
heavier chimes resulting in an overall higher pitch effect.
The woodblock was actuated by a double ball-headed
drumstick attached at its midpoint to a servo with a 180 degree
angular range and positioned over the woodblock. Similar to
mu, beta power was inversely mapped to rotation speed such
that beta ERD (as occurs in motor imagery) would lead to
increased percussive tempo.
The cymbal was actuated by a standard drumstick attached to
a 360 degree full-rotation servo via a piece of string and
positioned over an upturned cymbal. The angular velocity of
the servo was varied proportionately to theta power. This
1

3. DISCUSSION
Music for Online Performer was a novel venture in several
regards. Perhaps the most important novel element was our use
of acoustic media, with instruments actuated by low-cost
Arduino robotics hardware. This stands in contrast to the
majority of BCMIs that have used digital/synthesized audio as
their primary media. The use of acoustic instruments introduces
an additional element of uncertainty in performances, which we
believe is important for compositional expressiveness. Nuances
of the performer’s modulation of his or her neural state may
result in unpredictable behavior of the instruments, due to the
nature of their physical construction. How far one attempts to
mentally compensate for this unpredictability is a measure of
one’s willingness to “let go” of a perfect rendition and leave
elements to chance.
Secondly, performances and installations combining BCMI
technology and synthesized music can be somewhat abstract
and acousmatic in nature. Even when the performer is visible,
he or she is often immobile and the mechanism of sound
production is unclear. This form of musical expression may
alienate some audiences, as there is no immediate physicality to
the sounds they are hearing. Using acoustic instruments allows
the audience to engage with a method of sound production
familiar to them and then move on to trying to
understand how these instruments are being controlled.

http://www.maxuino.org/

471

Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

creating live acoustic music halfway around the world,
influencing the neurobiological networks – and thereby the
perceptions, emotions, and intentions – of others worldwide,
and ultimately returning, via the directives of the audience and
the human conductor, to again influence the source: a solo
performer sitting in a room; alone, yet intimately connected to
the world at large.

Aside from the novelty of controlling musical instruments
4000 miles away using one’s thoughts, Online Performer was
also in many ways a social experiment. By allowing audience
members from around the globe to be brought together in a
virtual space where they could communicate with each other
throughout the performance, and influence the ongoing
composition through their live interactions with the composer,
we sought to highlight new kinds of social environments for
musical performance. By encouraging audience participation in
the physical musical production we effectively extended the
virtual space back into the real and tangible, which, as Marshall
McLuhan discusses in his 1994 book Understanding Media:
The Extensions of Man, is the opposite of what usually happens
with technology.

5. ACKNOWLEDGMENTS
R.W. is supported in part through Subito, the quick
advancement grant program of the SF Bay Area Chapter of the
American Composers Forum. Many thanks to Chris Coleman
who consulted with R.W. on the Maxuino interface. T.M.
thanks Yijun Wang and Nima Bigdely-Shamlo for their help
with configuring the EEG system. Finally, a big thanks to all
those who participated as audience members and helped create
the interactive experience.

4. CONCLUSIONS AND THE FUTURE
In this paper we reported the live demonstration of a novel
Internet-enabled acoustic brain-computer music interface
system. To our knowledge, this is the first BCMI that has
attempted to mechanically control acoustic instruments over the
Internet using non-invasive EEG and low-cost, off-the-shelf
Arduino robotics hardware, accessible to most artists and do-ityourself hobbyists. Although we used medical-grade EEG
equipment, affordable, high-quality EEG hardware is now
becoming ubiquitous with a number of companies offering dry
(gel-free) electrode systems (BrainProducts, Emotiv, Quasar,
g.Tec, Nouzz, Neurosky, etc)
Although EEG is not a novel element in the experimental
arts, it is only recently, with the advent of low-cost wearable
EEG
hardware, exponentially
increasing
computing
capabilities, and powerful new signal-processing algorithms
from the expanding neuroscience and BCI fields, that we are
seeing a renewed interest in and expansion of the applications
of EEG technology in the arts. As our knowledge of human
cognitive neuroscience increases and low-cost EEG technology
advances and becomes ubiquitous, we will see a new
generation of artists, technologists, and musicians with a
passion for artistically representing and expressing the subtle
nuances and inner workings of the human mind via the use of
brain-machine interfaces. At the same time, there will be a rise
in the number of for-profit companies aimed at this generation
of DIY bio-artists. Currently, one such company – InteraXon –
has gained worldwide recognition for its development of BCIenabled artistic performance pieces, including lighting up the
CN Tower, Ottawa Parliament Buildings, and Niagara Falls at
the 2009 Winter Olympics using wearable EEG (Neurosky’s
MindSetTM) with brainwaves streamed from Vancouver.
As BCI technology develops, we may one day be able to
remove the boundary of sensorimotor input/output and directly
communicate our intentions, emotions, and desires to machines
and human beings in our surrounding environment as well as
across the globe. The effect will be extension of the
neurobiological networks underlying thought and body schema
representation and expression into much larger, externalized
networks encompassing multiple other conscious and
nonconscious agents.
In producing Music for Online Performer we found a
beautiful poetry in the ubiquity and interplay of multi-scaled
internalized and externalized networks and loops. On some
levels of description, micro- and macroscopic neurobiological
networks in the brain of the performer were rapidly transmitting
information, translating the conductor’s instructions into
cognitive thought processes which manifested as detected
modulations in neural activity influencing his local feedback
display and thereby again his neural processes. Simultaneously,
on other levels of description, this same neural information was
being routed through megascopic globe-spanning networks,

6. REFERENCES
[1] Adrian, E.D. and Matthews, B.H.C. The Berger Rhythm:
potential changes from the occipital lobes in man. Brain
57, 4 (1934), 355.
[2] Bell, A.J. and Sejnowski, T.J. An informationmaximization approach to blind separation and blind
deconvolution. Neural comp. 7, 6 (1995), 1129–1159.
[3] Brouse, A. The Interharmonium: an investigation into
networked musical applications and brainwaves. M.A
Thesis. (2001).
[4] Delorme, A., Mullen, T., Kothe, C., et al. EEGLAB, SIFT,
NFT, BCILAB, and ERICA: New tools for advanced
EEG/MEG processing. Computational Intelligence and
Neuroscience, In Press, (2011).
[5] Grierson, M. Composing With Brainwaves: Minimal Trial
P300 Recognition As An Indicator of Subjective
Preference for the Control of a Musical Instrument. ICMC,
(2008).
[6] Hinterberger, T. and Baier, G. Parametric orchestral
sonification of EEG in real time. Multimedia, IEEE 12, 2
(2005), 70–79.
[7] Lucier, A. Interview: Everything Is Real. TAG Publishing,
2010.
[8] Mann, S., Fung, J., Garten, A. DECONcert: Bathing in the
light, sound, and waters of the musical brainbaths. ICMC,
(2007) Copenhagen.
[9] Miranda, E.R. and Brouse, A. Interfacing the Brain
Directly with Musical Systems: On Developing Systems
for Making Music with Brain Signals. Leonardo 38, 4
(2005), 331-336.
[10] Miranda, E.R. and Wanderley, M. New Digital Musical
Instruments: Control And Interaction Beyond the
Keyboard. A-R Editions, 2006.
[11] Makeig, S., Bell, T., Jung, T., Sejnowski, T. Independent
component analysis of electroencephalographic data. NIPS
8, (1996), 145–151.
[12] Pfurtscheller, G. and Lopes da Silva, F.H. Event-related
EEG/MEG synchronization and desynchronization: basic
principles. Clin. Neurophysiology 110,11(1999), 1842-57.
[13] Pfurtscheller, G. Functional topography during
sensorimotor activation studied with event-related
desynchronization mapping. Clinical Neurophysiology 6, 1
(1989), 75-84.
[14] Wolpaw, J.R., Birbaumer, N., McFarland, D.J.,
Pfurtscheller, G., and Vaughan, T.M. Brain-computer
interfaces for communication and control. Clinical
Neurophysiology 113, 6 (2002), 767–791.
[15] Wu, D., Li, C.-Y., and Yao, D.-Z. Scale-free music of the
brain. PloS one 4, 6 (2009), e5915.

472

