This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access

Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identifier 10.1109/ACCESS.2020.DOI

Towards Estimation of Emotions from
Eye Pupillometry with Low-cost Devices
SIDRA RAFIQUE1 , NADIA KANWAL1,2 (Senior Member, IEEE), IRFAN KARAMAT3 ,
MAMOONA N. ASGHAR2,4 , MARTIN FLEURY5
1

Lahore College for Women University, Lahore, Pakistan. (sidra.rafique@lcwu.edu.pk, nadia.kanwal@lcwu.edu.pk)
Software Research Institute, Athlone Institute of Technology, Ireland (nkanwal@ait.ie, masghar@ait.ie)
Ophthalmology Unit 2, Mayo Hospital, Lahore, Pakistan (drirfankaramat@gmail.com )
4
The Islamia University of Bahawalpur, Punjab, Pakistan. 63100 (mamona.asghar@iub.edu.pk)
5
University of Suffolk, Ipswich, UK (fleury.martin55@gmail.com)
2
3

Corresponding author: S.Rafique (e-mail:sidra.rafique@lcwu.edu.pk)

ABSTRACT Emotional care is important for some patients and their caregivers. Within a clinical or home
care situation, technology can be employed to remotely monitor the emotional response of such people. This
paper considers pupillometry as a non-invasive way of classifying an individual’s emotions. Standardized
audio signals were used to emotionally stimulate the test subjects. Eye pupil images of up to 32 subjects
of different genders were captured as video images by low-cost, infrared, Raspberry Pi board cameras.
By processing of the images, a dataset of pupil diameters according to gender and age characteristics was
established. Appropriate statistical tests for inference of the emotional state were applied to that dataset to
establish the subjects’ emotional states in response to the audio stimuli. Results showed agreement between
the test subjects’ opinions of their emotional state and the classification of emotions according to the range
of pupil diameters found using the described method.
INDEX TERMS Emotion classification, Image segmentation, Pi camera, Pupillometry

I. INTRODUCTION

Humans commonly express their emotions using facial, gestural, verbal, and written communication. However, all of
these are methods can be controlled and, therefore, can hide
the true feelings of the subject. Hence, the focus of research
has been diverted to extracting emotions from involuntary
responses, such as heart rate variability (HRV), skin temperature (SKT), electroencephalography (EEG) and, herein,
from eye pupil responses. Even if involuntary responses are
utilized, the hardware needed to identify such responses may
inhibit a user’s free movements when expressing emotions
[1], which clearly impedes that hardware’s use in a clinical
setting, which could include end-of-life care. Systems based
on involuntary responses are nonlinear systems, and various
methods have been investigated for estimating parameters
that depend upon such nonlinear data [2]. Accordingly, in
the presented study based on pupillometry, calculated pupil
diameters and other statistical features have been used as
parameters.
However, pupillometry, as described in this study, represents a non-invasive method of monitoring patients and
their caregivers, so that their emotional state can be judged

remotely. Thus, this study demonstrates automated classification of emotional states to aid remotely observing clinicians.
Over the past few decades, many researchers, especially
psychologists, have explored the reasons behind changes in
pupil diameter and have found that those changes could be an
index of cognitive function. The pupil within the human eye
is the opening of the iris that permits light to enter the eye
and reach the retina, allowing humans to see. The diameter
of the pupil is controlled by two sets of smooth muscles in
the iris. Constrictor muscles decrease its diameter, which is
called pupil constriction, while dilator muscles increase the
diameter, which is termed pupil dilation [3]. An example of
different dilated pupils is shown in Fig. 1. The constriction
and dilation of a 3 pupil are entirely controlled by motor
sensors linked to the brain, meaning that they operate autonomously and, hence, cannot be consciously controlled by
humans [3].
Broadly, the pupil dilates when people are more attentive,
emotional, have a greater cognitive load [4], or are feeling
aroused. Humans themselves are commonly unaware of these
changes happening to their eyes. Pupils dilate and constrict
continuously when humans are thinking hard, feeling pain
1

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

Section III. After that, in Section IV, pupil diameters are
analysed to determine the required output, while section V
contains a detailed discussion of the results. Concluding
remarks include future research directions. Section VII.
FIGURE 1: Selected frames of pupils from different subjects.
II. CONTEXT
A. PUPILLOMETRY

or happiness, taking drugs, watching advertisements and for
many more reasons, see [5], [6], [7] and [8] respectively.
Pupillometry is the study of such changes in pupil diameter
as a function of different neurological activities in a human
brain. In medical terms, a pupil diameter can vary from 1.5
mm to 9 mm and reacts to stimulation in approximately 200
ms. A normal pupil is approximately 3 mm in size during
standard light conditions [9].
In this study, to examine the emotional responses of test
participants, audio stimuli were applied. Naturally, within a
clinical setting, emotional responses would normally come
from elsewhere, but for the purpose of this study’s test,
they represented convenient and standardized stimuli. Audio/video stimuli given to a person of any gender, whatever
their age and the activity level of their brain, may change
their thinking ability. Moreover, such stimuli may cause
many different physiological changes in a subject’s mood and
body. Given that, a test subject experiences many emotional
responses. In the current study, six emotions were considered for the identification of human emotional behaviour by
means of the eye’s behaviour, i.e., happiness, sadness, anger,
irritation, disgust, and fear.
The two primary research contributions of this current
study are as follows:
• Identification of a range of different emotions based on
pupil diameter;
• Extraction of pupil diameter by means of a simple
camera so that the diameter can be used for medical,
or other, needs.
More specifically, with respect to the first contribution mentioned above, this study comprises a proof of concept that
capturing eye pupil responses using a tiny Raspberry Pi board
camera is feasible. Such a camera could easily be fixed in
goggles, for example, in a future wireless sensor network
application. In addition, in terms of implementation, the
study contains a demonstration of video/image processing,
allowing accurate pupil-diameter calculation. In terms of
practical methodology, the study contains a comparison of
voluntary and involuntary responses of a subject, in this
case using an audio stimulus; identification of the underlying
data characteristics required for statistical analysis of pupil
responses; and application of parametric and non-parametric
statistical tests to perform statistical validity inference.
The remainder of the study is organised as follows.
Previous research and findings are discussed in Section II.
Data collection, video images of eye pupils, and extraction
of features, i.e. , pupil diameters, are then described in

Pupillometry involves measurement of a pupil’s diameter and
can be applied in security applications [10], traffic safety
[11], a clinical setting [12], psycho-physiology [13], and psychology [14]. Here, we examined recognition of a human’s
emotional state by means of pupillometry. In that respect,
this study is most directly related to research on psychopathy
[15], as well as helping visually and mentally impaired patients. However, unlike that research, which employs costly
eye trackers or other such mechanisms [16], [17], the current
research was designed to obtain quality results, while at the
same time operating with simple cameras at a low-cost. In
fact, the Raspberry Pi board can be extended via a wireless
sensor network [18], which in turn might form part of an
Internet of Multimedia Things (IoMT).
B. APPLICATIONS OF PUPILLOMETRY

Initial research findings have indicated that pupil dilation
is a measure of emotion, cognitive load [19] and memory
recognition [20]. Greater arousal is indicated by increased
pupil dilation, which in turn indicates decisions and choices
are being made by the subject, see [21] and [22]. Researchers
have investigated the size of human pupils, comparing pupil
dilation to constriction [23]. A subject’s pupils tend to dilate
more during recognition of relatively older memories [23].
In other words, pupil dilation is greater when a subject is
presented with older or more well-established memory traces
rather than newer stimuli. Indeed, a prominent difference
in pupil behaviour was observed in many early studies, for
example, as part of gender-based comparisons [24]. In the
same era, pupil sensitivity was identified as different from
one person to another during various tense situations, such
as during muscle tension when lifting weights or in response
to the threat of a gunshot [25]. More recently, an important
application of pupillometry has been the detection of drug
use [6].
C. PUPILLOMETRY FOR RECOGNITION OF EMOTIONS

A strong trend in pupillometry research and eye behaviour
in general has been emotion recognition, which has been enhanced by the ability to classify more categories of emotion
and by the use of different analytic techniques. A summary
of such investigations is provided in Table 1.
An important application of the investigations of Table 1 is
to provide a communication medium for those who otherwise
experience difficulties in expressing their feelings, especially
during clinical screening and therapy sessions. For example,
one such investigation [37] analysed experimental results to

2

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

TABLE 1: Related research on emotion recognition by pupillometry.
Ref.
[26]

Year
2000

Study topic
Emotion classification using pupil data.

[27]

2003

[28]

2004

[29]

2006

[30]

2008

Pupil size variation during and after
emotional stimuli.
Application of feature extraction methods to classify emotions more accurately.
Analysis of involuntary behavior and
mirroring of pupil size during the sadness emotion.
Effect of skin conductance and heart rate
on pupil changes

[31]

2010

[32]

2013

[33]

2019

[34]

2020

[35]

2020

[36]

2020

Stimuli
Audios (14 positive, 14
neutral and 14 negative)
Audios (10 positive, 10
neutral and 10 negative)
5 videos and mathematical
questions

Elicited emotions
Positive, neutral and negative emotion
Affective processing

Analytical techniques
Average pupil diameter
and rmANOVA
rmANOVA

Sadness,
anger,
fear,
surprise, frustration and
amusement
Neutral, happiness, sadness and anger

Min., max., mean and variance

96 pictures

Pleasant, neutral and unpleasant

Correlation between eye pupil diameter,
Facial Expressions (FE), and self assessment.
Gender differences in response to basic
emotional states.
Estimation of positive emotions through
eye fixation and gaze.
Classify emotions into four classes using eye pupil diameter and machine
learning classifiers

Task based

Positive and negative

Multivariate
analysis
(ANOVA,
hierarchical
regression)
Average percentage

21 Audios

Pupil diameter average
percentage
Manual analysis

emotions are classified and find the
relationship between emotional stimuli
and attentional stimuli at the same time
when given to subjects resulted in opposition of emotional effect.
Emotions are classified using face expressions and then validated using
pupillometry

Audio and pictures

Basic positive, basic negative. and no emotion
Joy, love, inspiration,
serenity
Elicited
changes
in
high/low arousal and
positive/negative valance
of emotions
positive, negative, and
neutral

angry, disgusted, fearful,
happy, sad, and surprised
for both facial expressions
and pupillary expressions

-

Pictures

4 videos (one for each
emotion)
4 videos (one for each
emotion)

instructions based

provide an interface for health care. An exploratory experiment was conducted for the development of a screening
program for people with such disabilities. The interactive
computer-based screening program was comprised of sets of
stimuli designed to elicit emotional responses from subjects.

rmANOVA

-

rmANOVA

employed using a low-cost sensor board, which includes
wireless communication capability. A demonstration of the
feasibility of using the low-cost board is now given by way
of experiments on subjects in total 32 (27 for 12 audio clips
and 32 for 2 audio clips).

D. NON-PUPILLOMETRY TECHNIQUES

III. EXPERIMENTAL PROCEDURE

Human emotions were classified using EEG signals (within
the gamma band frequency) in [38] and [39]. In fact, multiple
different sensors have been utilized in research areas for emotion elicitation using EEG as described in [40], [41], [42] and
[43] also using other techniques, electrocardiography (ECG),
galvanic skin response, HRV, respiration rate (RR), SKT,
electromyogram (EMG), electrooculography, FE, Speech,
and gesture analysis (GA) [44].
Many eye trackers have also been developed [45], [46]
[47] to provide assisted living. Eye-tracking applications are
also relevant to multiple other domains, such as psychology,
engineering, advertising, and computer science [48].
Recent literature has shown that the human eye pupil
has a certain ‘language’, which can be translated into different emotions. The translation of those emotions using
pupillometry has taken place for several applications using
multiple analytic techniques, but the translation mechanism
is not completely reliable. In addition, a good number of
those techniques require expensive and customized sensors,
whereas in this study, passive sensing by visual means is

The experimental procedure comprised the acquisition/collection of data and its subsequent data processing,
which consisted of feature extraction from the captured
video followed by statistical analysis of the entire dataset,
as described below.
A. EXPERIMENTAL SETUP

The tests described in this study were constrained by the
need to operate on typical video sensor devices, as are
used in wireless sensor networks feeding into an Internet-ofThings (IoT) [49] (or more precisely, an IoMT) [50]). For
the purposes of the calibration, test subjects were carefully
positioned. The entire experimental setup is shown in Fig. 2.
The experimental setup was built to carefully record the
pupil response of each subject. Subject heads were positioned
at an angle of 93 degrees and at a distance of 17 cm from a
camera. Note that 17 cm was a comfortable position in the
test scenario. Since the prototype can eventually be mounted
on goggles or glasses to make the participant comfortable,
the separation distance will become irrelevant. A 5 mega
3

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

pixel (MP) camera with a wide-angle fisheye lens (A) was
used in combination with a Raspberry Pi-3 board (B) [51]
for capturing videos of subjects’ pupils. The pupil images
were captured in a room with open windows in normal day
light. Furthermore, emotional responses were captured for
audio stimuli, and therefore, subjects were not affected by
the lighting when responding to the stimuli.
Note in Fig. 2 that an IR camera was chosen. An IR camera
generally leads to a clearer image of an eye pupil, either in
comparison to the lower resolution of an RGB 5 MP camera
or the higher resolution of an RGB 16 MP camera, whatever
the illumination, as illustrated in Fig. 3. Using a camera with
an IR sensor in the experiments made the colour of the iris
irrelevant, as different coloured irises appeared light grey.
Therefore, this made it easier to identify pupils and calculate
their diameters (Note that this research utilizes cost-effective
equipment. Consequently, cameras with higher than 16 MP
resolution would violate this requirement.)
Recently, IR digital cameras have become more widely
available, and their usage for pupillometry has been established [52]. Aside from medical usage, where portability is
important, they also seem suitable for IoMT deployment. On
the other hand, a camera with 15-20 MPs resolution (infrared
or otherwise) might be used for this purpose but may prove
difficult to handle for the given type of experimentation.
Last, note that in part B of Fig. 2, alongside the central IR
camera connected to the main board by a ribbon cable, two
circular objects can be identified. These are, in fact, IR LED
flashlights. Their main purpose is to shine a light on a subject
to convey a clear image in any lighting condition.
After taking videos with the IR camera, the recorded
videos were then fed into the data processing module, where
image processing methods were employed to accurately calculate pupil diameter in every video frame. Since we had
raw pupil readings, pre-processing was performed to remove
various defects, including eye blink, missing values and
saccades. Linear interpolation and baseline correction were
also performed to obtain smooth pupil responses for analysis.

A

93o

A

38cm

17cm

B

B

FIGURE 2: Experimental setup for data acquisition, where
(A) is an Infra-Red (IR) camera and (B) is a Raspberry Pi-3
board.

5MP Camera Image

16MP Camera Image

Infrared Camera Image

FIGURE 3: Visual differences between normal RGB images
for pupillometry of 5 MP and 16 MP camera resolutions
under dark and bright light, respectively, compared to an IR
camera image.

Within an IoMT, nodes are commonly based upon Raspberry Pi [51] boards and CMOS circuitry with restricted
computing and communication, and consequently, are not
powerful enough for complex computations. In fact, these
nodes, for ease of placement, may be battery-powered, which
is an additional constraint upon complex computations. In
addition, if feature extraction, noise removal and subsequent
data processing (see Section III) were to occur remotely,
then the response may well need to be in near real-time to
allow clinicians to respond to some form of emotional upset,
or even crisis. However, some deep-learning techniques are
not without their disadvantages [53] for real-time responses,
such as a need for parallel computing, possibly on graphical
processing units (GPUs), especially during training, and a
need for large amounts of labelled data. On the other hand, it
is important to note that deep learning models (based on Convolutional Neural Networks) can also be trained using limited
ground-truth data (through transfer learning, data augmentation, and other recent regularization approaches), and, in
those circumstances, can make inferences very quickly in
hardware-constrained execution environments.
In fact, this study aimed to develop an automated statistical mechanism suitable for IoT to estimate emotions by
understanding the average pupil size variation, this being
the primary motivating factor behind the research. We hypothesized that different subjects’ pupils respond similarly,
that is, they constrict or dilate during an emotional response.
Various second and third order statistical tests were tried
out. Thus, during data processing to identify normality, a
Kolmogorov-Smirnov test was applied; for moments the kurtosis and skewness were found, a check on the homogeneity
of variance was followed by repeated measures analysis of
variance (that is via a rmANOVA test); Kruskal-Wallis and
Friedman tests were applied to infer whether the results
were statistically valid. Last, to infer emotional behaviour
based on gender and age, a Wilcoxon rank test was used.
Within an IoT, it is possible that pre-processing using imageprocessing techniques on video frames might take place
locally, while once features were extracted, images with
reduced data could be sent over a wireless network. The
authors have previously investigated [54] lightweight encryption for privacy protection of video communicated over IoMT
networks. Ultimately, that can take place once the feasibility
of pupillometry and the scale and structure of data processing

4

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

are established.
B. DATA ACQUISITION
a: Subjects

Subjects were instructed to avoid unnecessary eye and head
movement during the data collection phase. Some subjects
had difficulty with eye fixation (a lack of maintaining focus
on the camera) and teary eyes. Therefore, data for three subjects out of twenty-four was removed due to those limitations.
The entire data collection session lasted approximately 10 to
14 minutes per subject.
Thus, twenty-seven mentally and physically healthy subjects initially took part in this experiment (later increased to
32 for experiments on the fear and disgust audio stimuli),
consisting of eleven males (mean age = 25.64 years, with
Standard Deviation (SD) = 6.87) and sixteen females (mean
age = 25.20 years, SD = 7.07) with a minimum of 12 and
a maximum of 39 years of age (The overall mean age = 25
years, with an SD = 5.9.) All subjects voluntarily participated
in this research, among which 22.2% had either myopia
(Near-sightedness) or hyperopia (Far-sightedness) of up to
0.75 m and, were therefore using glasses. However, the rest
of the subjects had normal vision. As the experiments did not
involve any video stimulus, vision problems did not affect
data characteristics.
b: Stimuli

Fourteen audio stimuli were used, including twelve sounds
taken from the International Affective Digitized Sounds1
(IADS) [55]. Selected sounds were chosen based on the mean
values of pleasure and arousal related to their respective
emotion categories, with high pleasure and high arousal depicting the happiness emotion, low pleasure and low arousal
depicting the sadness emotion, and low pleasure and high
arousal representing the emotion of anger, irritation, disgust
and fear. Importantly, irritation and disgust were categorized
as secondary emotions of anger [56], and horror was categorized as a secondary emotion of fear [57], [58].
The categories used in this research were happiness, sadness, anger and irritation, whereas emotional sounds for
disgust and fear were created in-house and validated by
the process of Content Validity Index (CVI) [59]. CVI is
a process of taking more than 10 experts’ opinions on a
relevant rating scale and then checking that data for their
validity [60].
In total, fourteen stimuli were used, with a total of six
different emotion categories, as shown in Table 2. The stimuli
were input via ear pieces to both ears at a constant volume
level of 60 dB in order to hear sound clearly, without any
distortion or interruption.
c: Video Recording

Rating proforma to rank all the audio stimuli were also
given out at this stage. A cue was given to each subject

TABLE 2: Audio stimulation given to subjects.
Stimulus Number
(226, 365, 813)∗
(292, 293, 296)∗
(278, 420, 422)∗
(252, 115, 116)∗
(000)+
(222)+

Stimulus Duration
6 s each
6 s each
6 s each
6 s each
12 s
47 s

Explicitly Defined Emotion
Happiness
Sadness
Anger
Irritation
Disgust
Fear

* From IADS.
+ Validated using CVI.

before starting to play each audio stimuli. Each subject, after
listening to an audio recording, ranked that particular sound
on the given rating proforma. The next sound was played only
when a subject said they were ready to listen. An average
time of 6 s was taken by all subjects to rank each sound. This
time also allowed subjects to move their eyes and head to
prevent fatigue. The lighting of the room was kept constant
for all subjects.
During this process, videos of the subjects were captured
and compressed with an H.264/Advanced Video Coding
(AVC) standardized codec [61]. H.264/AVC as a codec is
a good compromise between the greater compression efficiency of the more recent High Efficiency Video Coding
(HEVC) standard [62] and the need to restrict the computational overhead and latency so that it is within reach of
constrained IoMT devices [63]. The H.264/AVC compressed
bitstream was encapsulated in MP4 containers to form video
files, using the well-known FFmpeg software tool. The MP4
container offers minimal overhead, while being widely compatible with a variety of media players. The relative lack of
header overhead also makes then suitable for transport over
lower bandwidth wireless sensor networks.
C. IMAGE PROCESSING
1) Feature extraction

Video data were captured using a Raspberry Pi camera V2,
having a resolution of 1080 × 1920 pixels/frame and utilizing
the maximum frame rate of 25 frames/s (fps). All captured
videos were stored separately for each subject’s eyes per
stimulus, awaiting extraction of the subjects’ pupils from the
eye images. The latter was accomplished using MATLAB’s
image processing toolbox, which subsequently served to
extract a pupil’s diameter.
Notice that, though a video of both eyes was captured
using the IR camera, only the left eye’s pupil diameter was
utilized for this research. In fact, the diameters of both eyes
were measured, but it was found and confirmed that there
was no difference between the two diameters. Therefore, to
avoid replication of the results, only one eye was chosen.
Furthermore, the left eye was chosen (see Fig. 1) because
the selection of that side’s eye had already been established
in the literature, see [37].

1 https://csea.phhp.ufl.edu/media/iadsmessage.html

5

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

2) Steps in image processing

Algorithm 1 summarizes the steps taken during image processing.
Algorithm 1: Steps in the Image Processing
1.While i != end-of-video
2. input video frames = f[i]
3. FGray[i]=rgb2gray(f[i])
4. threshold = 85
5. binary_FGray = binarizeImage(FGray , threshold)
6. binary_FGray=interpolate(binary_FGray)
7. circle=findCircle(binary_FGray)
8. diameter=calculate_circle_diameter(circle)
9. deliver/save for analysis
10. End(while)
In more detail, processing proceeded as follows, according
to Fig. 4’s numbering.
1) Initially, for a particular stimulation, RGB images from
all frames were processed. The colour contrast was
enhanced by 1
2) RGB to grey scale conversion was performed to maintain the luminance but to remove hue and saturation
from each coloured image.
3) The image was converted into binary or monochrome
form (black and white) to aid identification of the circular region containing the pupil. The iris surrounding
the pixel was reduced to black because the conversion
threshold was set close to black. In fact, for each subject, from the initial frames for that subject, five pixels
were selected whose intensity value was less than 20,
which was already known to approximately represent
a pupil’s area. The arithmetic mean of these five pixels
was used as a threshold to aid in segmenting the pupil
in the image. Therefore, currently the threshold value
is relative to each participant, though future work will
investigate how to completely automate this part of the
processing. After conversion to monochrome format,
the image was then inverted so that white became black
and black became white.
4) If the inverted image still contained some black and
white ’spots’, including reflections within a pupil, then
those spots were filled with their background tone
if their size was less than 20 pixels, a value found
heuristically.
5) The roundness of shapes identified within the inverted
image was identified by means of a roundness function. If the roundness value was near to one, then a
shape was declared to be a circle. Otherwise, it was
designated to be a random shape. At this stage, the
boundaries of circles were identified. From part five of
Fig. 4, the value of the roundness is marked against the
pupil shape identified by MATLAB. In the example,
the value is 0.94. Any shape with roundness values

close to one is identified by the roundness function as
a circle2 .
6) To determine the diameter of a circle, the built-in
MATLAB function regionprops was applied. In
general, this function helps in measuring the diameter, area, centroid, bounding-box, perimeter and many
other properties of any shape. However, for this experiment, it was the diameter that was used. An example of
a diameter measurement calculated through MATLAB
is provided in part 6 Fig. 4. All of the properties found
with this function resulted in measurements given as in
units of pixels.
Last, the calculated diameter in pixels was converted into the
desired unit of measurement, millimetres (mm), to perform
further processing. In fact, one pixel is equal to 0.26458333
mm and numerically 20 × 0.26458333 = 5.29 mm.
3) Computational overhead

Theoretical time complexity of the algorithm is O(n) for a
video per session. Segmentation of the pupil is facilitated
by binarisation of the coloured video frame and therefore,
complete segmentation and diameter calculation step takes
O(width × height) which is linear with respect to number
of pixels of a video frame.
Moreover, Fig. 5 shows the time required to process each
video frame to locate an eye pupil and then calculate its
diameter. As mentioned previously, in Section III.C part 1,
the frame rate was 25 fps. Hence, each frame was captured in
0.04s while pupil segmentation took 0.08s, which includes
noise removal, greyscale conversion, and binarisation of the
image. Last, the diameter calculation step took 0.02s to
mathematically calculate a pupil circle’s diameter.
4) Noise Removal

When capturing a video of a human eye, it is natural to
acquire some video frames with a closed eye due to blinking
or When capturing a video of a human eye, it is natural to
acquire some video frames with a closed eye due to blinking
or rapid movement of an eye between fixation points (saccade). In the current study, this limitation was considered to
be noise. For accurate response measurement, the data were
therefore adjusted before analysis. Thus, pupil size values for
eye blinks and saccades were removed by means of methods
described in [64]:
•

Blink Extraction: Blinking is the state of eye where
the pupil diameter cannot be measured correctly, as
shown in Fig. 6. When blinks were detected by the pupil
extraction algorithm, the measured pupil size value was
higher or lower prior to the eye blink. To remove such
noisy data, a linear interpolation was performed. Subsequently, the last measured pupil size before the eye blink
was averaged with the first measured pupil size after the

2 https://www.mathworks.com/help/images/identifying-roundobjects.html

6

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

Original Image

Diameter Calculation

Grayscale Image

1

Feature
Extraction

6
Circle Identification

5

2
Binary Image

3

4

Interpolation

FIGURE 4: Visual representation of the algorithm to calculate a Pupil’s diameter, with numbering according to the list in
Section III-C1.

FIGURE 5: Processing time for the algorithm.

FIGURE 6: Frames with eye blinks showing incomplete
visibility for diameter measurement.

•

blink to allow removal of the blink size values from the
data.
Saccade Extraction: Saccades are the quick and fast eye

•

movements when a person’s focus changes quickly from
one object to another, resulting in high jumps in the
values of pupil size, as shown in Fig. 7. Such values
were also removed, using the same linear interpolation
method as for blink extraction, by averaging the first and
last values of pupil saccades.
Baseline Correction: Minor fluctuations in pupil size
affect statistical results. To reduce this type of noise,
baseline correction was needed to identify mean noise.
Thus, subtracted baseline correction was applied to remove identified noise in the data, resulting in enhanced
statistical power, as was also performed in [65].

7

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

FIGURE 7: Outliers identified from the results for saccades.

IV. RESULTS AND ANALYSIS

In this section, we used statistical tests and assessment proforma techniques to arrive at an emotional content classification for involuntary and voluntary findings, respectively.
Subject gender and age were also the subject of analysis, with
results presented herein.
A. PARAMETRIC VS NON-PARAMETRIC STATISTICAL
ANALYSIS

In this study, the analysis was performed with a null hypothesis approach by applying appropriate statistical tests using
the RStudio software tool [66]. Statistical analysis assumes a
statistical distribution of data characteristic before applying a
particular test. For normally distributed data, parametric tests
are recommended, and if data is not normally distributed,
then non-parametric tests better approximate the acceptance
or rejection of the null hypothesis.
A complete statistical analysis was performed, as indicated
in Fig. 8. Note that some tests mentioned below are omitted
from Fig. 8 for simplicity of understanding. After checking
for normality, moment tests were performed followed by a
homogeneity of variance test for the entire dataset. For checking the normal distribution of data, Kolgomorov Smirnov
(KS), Cramer Von Mises (CVM), and Anderson Darling
(AD) tests were chosen [67]. Then, moments of data were
checked using the data’s skewness and kurtosis measures
[67].
Therefore, when data were symmetrical or moderately
skewed and did not have any outliers, parametric tests were
used. If data were highly skewed and did exhibit outliers,
non-parametric tests were applied, depending on the size
of the data set. If the size of the dataset were large, then
in any case, parametric tests can also be helpful in identifying more meaningful statistical results. The homogeneity

of variance is an assumption when applying an Analysis of
Variance (ANOVA) test for parametric data, which also includes a paired Student’s t-test for pairwise analysis. For nonparametric tests, Friedman and Kruskal-Wallis tests were
applied for data analysis. Gender- and age-based analysis was
also performed using the Wilcoxon test.
Subjects’ reactions to the emotional content were determined in two different forms. One form was involuntary,
when an involuntary organ (an eye pupil) was involved, to
identify or classify content. The second form was voluntary,
in which each subject wrote down their perceived thoughts
with respect to each stimulus, as shown in Fig. 9.
B. INVOLUNTARY BEHAVIOR ANALYSIS

Pupillometric data for the abovementioned audio stimuli (Table 2) were analysed to better estimate the predicted emotion.
Initially, applied normality tests showed that data were not
normally distributed, as their p value < 0.0001. Normality
was also checked using skewness and kurtosis tests, which
revealed that the data were moderately skewed but eventually
became non-normal. This was reflected in the kurtosis results
for subsets of the data for the disgust emotion only. In this
case, the statistical indicators for the data were less than
would be expected for a normal distribution. In addition,
the irritation and fear emotion kurtosis values showed that
there were more outliers, and happiness, sadness, and anger
emotion moment tests presented moderately skewed and
symmetrical data, as shown in Table 3.
Furthermore, to identify the existence of homogeneity of
variance, two tests were performed: the Fligner-Killeen (FK)
and Levene tests, which resulted in p < 0.0001. These p
values indicated that the data do not uphold an assumption of
equal variance for each emotion. As a result, non-parametric
tests were applied, as shown in Table 4.

8

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

Data
Yes

No

Normality
Distribution
n>3

* If(skewness= -0.5to+0.5) then
"Data are Normal" else
If(skewness=-1to-0.5 OR 0.5to1)
then "Data are Moderately Skewed"
else If(skewness < -1 OR skewness
>1) then "Data are Highly Skewed"

Kolmogorov
Smirnov

Yes
* If(kurtosis= 3) then "Data are
Symmetrical", else If (kurtosis <3)
then "Data has lack of Outliers", else
If(kurtosis >3) then "Data has more
Outliers"

Cramer Von
Mises

No

*Moment Tests
(Skewness,
Kurtosis)
(For Large
Dataset)

(For Small
Dataset)

No

Yes

No

Homogeneity of
Variance Test

For Not Normal Data

For Normal Data

Barlett test

Levene Test/
Fligner-Kileen

Non-Parametric Tests

Parametric Tests

ANOVA

Friedman test

Paired T Test

Kruskal Wallis
Test

FIGURE 8: Statistical analysis model.

Explicitly Defined
Emotional Content

Involuntary Behavior

Eye
Pupil

Voluntary Behavior

Content
Classification

Survey

FIGURE 9: Basic types of behavioural analysis.

A Kruskal-Wallis (KS) test with Holm p-adjustment
method was performed for all emotions. Notably, for the
happiness emotion, the sounds’ null hypothesis was rejected

because p < 0.05. The pupil responses of each subject for
all three sounds were not the same. That is, , there were
differences among the sounds. The Student’s t-test as a post
9

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

TABLE 3: Normality and moments test results.
Emotions
Happiness
Sadness
Anger
Irritation
Disgust
Fear

KS Test (p-value)
***
***
***
***
***
***

Normality Tests
AD Test (p-value)
***
***
***
***
***
***

CVM Test (p-value)
7.37x10−10
7.37x10−10
7.37x10−10
7.37x10−10
7.37x10−10
7.37x10−10

Skewness
0.49
0.36
0.15
0.50
-0.12
0.17

Kurtosis
3.06
3.04
2.66
3.14
2.41
3.20

Moments Test
Remarks
Symmetrical/ Maesokurtic
Symmetrical/ Maesokurtic
Symmetrical/ Maesokurtic
Moderately Skewed/ Leptokurtic
Moderately Skewed/ Platykurtic
Moderately Skewed/ Leptokurtic

*** ’Highly Significant Data’.

TABLE 4: Homogeneity of variance and non-parametric test results.
Emotion

Homogeneity of Variance Tests
FK test
FK
Levene
Levene
(Chitest
Test
Test
squared)
(p(F(Prvalue)
Value)
value)

Critical
value

Chi-sq.
p-value

Posthoc test
(Studentt)

Kruskal-Wallis Test
MSD
Groups

Happiness

4734.2

***

70.783

***

71.78

2.22x10−16

2.39

199.9

226a ,365a , 813b

Sadness

5163.7

***

72.907

***

109.87

0

2.394

199.6

292a ,293b , 296c

Anger

3930.2

***

56.277

***

64.95

7.88x10−15

2.394

199.9

278a ,422a , 420b

Irritation

3587.7

***

52.346

***

17.919

0.0001

2.394

200.30

115a ,116b , 252b

Disgust
(within
subjects
comparison)

814.33

***

49.076

***

1616.27

0

3.897

95.637

S013a ,S018a ,
S031b ,S005c ,
S019c ,S025c ,
S007d ,S024d ,
S029f ,S030f

Fear (within
subjects
comparison)

14931

***

652.42

***

40119.54

0

3.88

625.02

S009a ,S010a , S013b ,S014b , S021b ,S025b ,
S030b ,S031b , S001c ,S032c , S005d ,S022d ,
S026d ,S028d , S012e ,S023e , S027e ,S029e ,
S008f ,S035f , S017g ,S034g , S016h ,S033h

S027a ,S028a , S014b ,S020b ,
S006c ,S008c , S010c ,S012c ,
S026c ,S032c , S033c ,S035c ,
S001e ,S034e , S016f ,S022f ,

*** ’Highly Significant Data’.

hoc test was applied to identify similarity and dissimilarity
of sounds with each other. Results showed that audio clips
no. 226 and 365 belonged to the same group, and clip no.
813 turned out to be significant among the groups. Hence, all
subjects had the same pupil responses for two groups and
different pupil responses for the third group of emotional
stimuli (no. 813) compared to clips no. 226 and 365.
In case of the sadness emotion category, the null hypothesis was also rejected as p = 0, and the Student’s post hoc ttest revealed that the pupil responses of all subjects for audio
clip no. 292, 293, and 296 was different. Hence, this category
of emotion did not find any degree of agreement between
subjects’ pupil behaviour. However, the Kruskal-Wallis test
for the anger emotion exhibited p < 0.05, but the post hoc
test group (278, 420, 422) comparison showed that among
all three categories of the anger emotion sound, no. 278 and
422 belonged to the same group and 420 was in the group for
which the subjects’ pupil response had less agreement with
the predefined label of the emotion category. Similarly, the
irritation emotion had p < 0.05, and post hoc comparison of
groups showed that clips no. 116 and 252 were not significant
compared to clip no. 115.
The disgust and fear emotions had only one sound for
each category. Therefore, a within-subjects comparison was
performed for analysis rather than a within groups analysis.
The number of subjects was also increased to 32 to obtain

more promising results. Results of the Kruskal-Wallis analysis showed that p = 0 and for the Student’s t-test = 3.9 for
the within subject comparison analysis for both the disgust
and fear emotions. Non-significant results were shown in the
disgust emotion category for some subjects, e.g., subject no.
S013, S018, S027, and S028. Similarly, for the fear emotion,
some subjects, S009, S010, S013, S014, S021, S025, and
S030, resulted in non-significant results evident from Table 4.
Preferably, a non-parametric test was applied for the analysis of data but, due to the large dataset and to strengthen
the present research, a parametric test was also applied using
a repeated measures ANOVA (rmANOVA) analysis. For
happiness, sadness, anger and irritation emotions, rmANOVA
showed a p > 0.05, suggesting that the pupil’s behaviour
of each subject for these emotions was non-significantly
different. Conversely, a within subject comparison of the
disgust and fear emotions had p < 0.0001. This was
followed by a pairwise Student’s t-test (as a post hoc analysis) to identify subjects whose pupil behaviour was nonsignificantly different, as shown in Table 5. Moreover, to
identify pupil response for males and females, a Wilcoxon
test for independent groups was performed. The ratio of
females to males participating in this study was 16:11 for
happiness, sadness, anger, and irritation emotions, and the
ratio was 22:10 for the disgust and fear emotions. Each emotion for this test was analysed for both males and females,

10

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

TABLE 5: Parametric test results.
Repeated Measure Anova Test
Emotions

Sum
of
Square

Mean
Square

NumDf

DenDF

F-Value

Pr-value

Pairwise Student-t test (Post hoc test)

Happiness

0.0752

0.0376

2

78

0.195

0.8232

-

Sadness

0.2000

0.1000

2

78

0.444

0.642

-

Anger

0.0731

0.0365

2

78

0.159

0.826

-

Irritation

0.0187

0.008

2

78

0.041

0.959

-

Disgust (within subjects comparison)

6224.6

200.79

31

1643

2873

***

S001a ,S034a ,
S005b ,S006b ,
S012b ,S014b ,
S019b ,S020b ,
S026b ,S031b ,
S032b ,S033b ,
S024c ,S009d ,
S015d ,S013e ,
S028e ,S016f , S022f ,S029f , S030f

S008b ,S010b ,
S023b ,S025b ,
S035b ,S007c ,
S018e ,S027e ,

Fear (within subjects comparison)

273255

8814.7

31

43524

20537

***

S001a ,S032a ,
S005b ,S019b ,
S026b ,S028b ,
S008c ,S035c ,
S012e ,S023e ,
S024e ,S027e ,
S030f ,S014g ,
S021g ,S025g ,
S033h ,S017i , S034i

S020b ,S022b ,
S009d ,S010d ,
S029e ,S013f ,
S031g ,S016h ,

*** ’Highly Significant Data’.

with the significance being p < 0.05. Results revealed that for
the emotional stimuli of happy, sad, and irritation, there was
a difference in the mean pupil response between male and
female subjects. On the other hand, the mean pupil response
was almost the same for both genders in cases of angry,
disgust, and fear emotional stimuli, as shown in Fig. 10.
Thus, males and females have different behaviours towards
each emotional response, confirming observations in earlier
investigations (refer to Section II.C). Similarly, age-specific
analysis was also performed with three levels of age for the
available subjects. Each participant age level was mapped
onto pupil responses using boxplots with a jitter effect for
pupil diameter values, as shown in Fig. 11.

irritation emotional stimuli because all these emotions had
p > 0.05. Since there was only one group within the disgust
and fear emotions, a Kruskal-Wallis test within subjects comparison was performed, showing p > 0.05. Importantly, most
subjects had given their feedback according to the predefined
emotions, as shown in Table 7.
V. DISCUSSION

The code designed in this study could help to automatically
identify eye pupils, as well as to calculate the pupil diameter
of each detected pupil. The purpose of designing this system
is to make processing more robust, as discussed next.
A. INVOLUNTARY CLASSIFICATION OF EMOTIONS

C. VOLUNTARY BEHAVIOR ANALYSIS

Voluntary behaviour consists of a subjective assessment to
rank predefined emotional categories for each sound. Each
subject was given a time of almost 6 s for each question. In
total, approximately 84 s were utilized to complete the rating
scale proforma. The rating scale ranged from 1 to 3, applied
to a total of fourteen sounds with pre-categorized emotions.
For example, sound number 226 was pre-categorized as the
happiness emotion by just listening to the sound, which may
differ from person to person. Subjects rated 1 for happiness
if they felt the sound denoted happiness, 2 for neutral and 3
for unhappiness as the felt emotion. A similar rating scheme
was followed for all other sounds, and all subjects’ responses
from the rating proforma are shown in Table 6.
Analysing the statistics of subjects’ rating data, it was
established that the Shapiro-Wilk normality test for all emotions was at a less than significance level, i.e., < 0.05.
Therefore, data were not considered normal. Thus, a nonparametric Friedman test was applied due to the different
data types. Analysis concluded that all subjects had the same
perceptions with respect to the happiness, sadness, anger and

Various testing methods were avoided in this study, including
those involving long video clips/films [68] or those involving
hard mental activity on the part of the subjects [69] [70]. The
films have to be quite lengthy to create a sufficient emotional
impact on a human mind, and tests involving mental activity
increase the chances of more noisy data, leading to a less
reliable system. These decisions were confirmed in [71].
We analysed all data by means of ten different statistical
tests, including tests that, as far as we are aware, have not
been used for this purpose before. Some statistical tests
were performed in [72]. However, the authors of that study
[72] reported contradicting results because, it seems, they
worked only with positive and negative emotions, whereas
we worked on six different emotions.
Parametric statistical tests for involuntary organ data have
a strong degree of agreement for happiness, sadness, anger
and irritation emotions, see Table 8. Contrary to this, nonparametric tests classified clips no. 226 and 365 as the
explicitly defined happiness emotion, clips no. 278 and 422
as the anger emotion and clips nos. 116 and 252 as the
irritation emotion. This is the reason why involuntary organ
data has to pass through several types of statistical tests
11

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

FIGURE 10: Gender based analysis: Significance level of each emotion for males and females.

rather than only one. Considering both types of statistical
tests, we correctly classified six audio stimuli as shown in
Table 8. Based on these, two newly added stimuli were used,
and parametric tests exhibited a strong degree of agreement
from subjects of 90.6% and 84% for the disgust and fear
emotions, respectively. Similarly, in non-parametric tests,
values of 81% and 75% occurred for the disgust and fear
emotions, respectively. These results could be helpful for future research and further development of a robust system for
emotion classification. Pupil behaviour was also affected by
gender. Analysis showed that both genders exhibited similar
behaviour for angry, disgust and fear emotions, and subjects
with ages greater than and equal to 31 years presented reduced pupil dilation and more pupil constriction compared to
the other two groups.
B. VOLUNTARY CLASSIFICATION OF EMOTIONS

We employed voluntary behaviour analysis in which all
subjects filled surveys proforma for all audio clips. Findings of voluntary behaviour analysis revealed that explicitly
definition of the emotion category may lead to bias in the
categorization. This can easily be observed in audio clips no.
292, 293 and 296, which were categorized as the sadness
emotion in contradiction with the involuntary organ results.
The voluntary organs results also led us to conclude that
the majority of subjects felt happiness for the happiness

emotion clips and perceived other emotions correctly, as
indicated on the rating proforma for all other audio clips,
i.e., for the anger, irritation, disgust and fear emotions. As
the voluntary organ results were scaled data, distinct from the
pupil data, different statistical testing was applied to verify all
results. The same type of scale-based data analysis was also
performed in [72] but with a Positive and Negative Affect
Schedule – Expanded (PANASX) model.
C. ROUNDUP OF STATISTICAL FINDINGS

To summarize the findings of this study with respect to the
statistical analysis, Fig. 12 presents estimated ranges of a
human’s pupil for the given stimuli. Fig. 12 indicates similarity between subject responses according to one or other of
the types of test, i.e., the parametric or non-parametric tests.
However, though the ranges of constriction or dilation may
be consistent, within those ranges, no degree of agreement
was found between individual subjects in the expression of
any particular emotion. That is, the ranges coincided but
not the precise value of constriction or dilation of pupil
diameter. Notice also that one way to use ranges to estimate
the emotion, used by the authors, was to take the median of
each range.
Although the investigations cannot be termed conclusive,
statistically significant patterns were identified. This may
have been because of overlaps in the pupil constriction or

12

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

FIGURE 11: Age based analysis: Eye pupil behaviour estimation in three participant age groups (less than and equal to 21
years, from 22 years to 30 years, and greater than and equal to 31 years).

feels happy or when age is less than or equal to 30 years. For
other emotions, such as fear, disgust and anger, the pupils
grow but not to their maximum extent, regardless of gender.
The authors believe that the restraint to dilation could well
be due to the influence of another emotional response, which
may be curiosity. However, identification of that restraint is
beyond the scope of the current study, though it is under
investigation by the authors.
D. OVERALL COMPARISON

FIGURE 12: Pupil constriction and dilation ranges for different emotions.

dilation ranges for particular emotions that in [72] analysis
was confined to positive and negative emotions. However, the
method of selecting an average from the subjects to indicate
an emotion, as previously mentioned, may overcome the
restriction in pupil diameter ranges. It can also be concluded
that pupils constrict to their maximum level when a person
feels irritated and when age is greater than or equal to 31
years. Similarly, the maximum dilation occurs when he/she

A comparison was made between the proposed methodology of pupillometry for emotion recognition/classification
and papers, which, in this field, constitute state-of-theart methodology. In the comparison of Table 9, additional
methodological features have been included relative to the
earlier Table 1. Such analysis reveals that previous systems
focused on high-cost devices, mostly eye trackers, biosensors
to get EEG signals, SKT, Blood Volume Pulse (BVP) and
many other techniques to attain improved accuracy. However,
the current study was aimed at developing a cost-effective
system method. Instead, data were analysed by means of
ten statistical tests, which helped to achieve more accurate
results. In addition, previous research only classified general
categories of emotions, whereas this research sought to identify six basic emotions.
13

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

TABLE 6: Subjective assessment proforma for voluntary behaviour analysis.
Sr.
No.

Audio
Sounds

1
2
3
4
5
6
7
8
9
10
11
12
13
14

226
365
813
292
293
296
278
420
422
115
116
252
000
222

Explicitly Defined Emotion Label
Sadness

Happiness
X
X
X

Anger

Irritation

Disgust

Fear

X
X
X
X
X
X
X
X
X
X
X

Voluntary Behavior for Labelled
Sounds Agreement Level
Not Agree(3)
Agree (1) Neutral(2)
18
8
1
6
7
14
6
6
15
4
3
20
5
2
20
18
6
3
7
4
16
8
8
11
6
12
9
7
2
18
3
0
24
8
0
19
2
4
26
2
5
25

TABLE 7: Ratings proforma data analysis.
Emotions

Happiness
Sadness
Anger
Irritation
Disgust
Fear

Normality Test (p-value)
Shapiro-Wilk Test
Degrees(p-Value)
of-Freedom
(Df)
2.591x10−11
2
2.082x10−13
2
−10
4.24x10
2
−14
6.832x10
2
−09
1.93x10
4.873x10−09
-

TABLE 8: Emotion classifications of stimuli based on statistical results.
Stimulus ID
226
365
813
292
293
296
278
422
420
116
252
115
000
222

Type of statistical Tests
Parametric test
Non-parametric test
Happiness
Happiness
Happiness
Happiness
Happiness
Sadness
Sadness
Sadness
Angry
Anger
Angry
Anger
Anger
Irritation
Irritation
Irritation
Irritation
Irritation
Disgust
Disgust
Fear
Fear

VI. LIMITATIONS OF THE STUDY

Consistent with the needs of this scientific community, the
device used in this study is cost effective. It has never before
been used to carry out such promising research, in part
because of the technical limitation that the camera will heat
up if used for a long duration. Consequently, unlike with
other, more costly sensors/devices, we needed to periodically
stop the elicitation process for a few minutes to allow the
sensor to cool down.
Second, it is common practice to perform data normalization to remove irregularities from the data; however, we tried
to avoid this to preserve the ability to exploit hidden correlations within the data. Furthermore, this study focused on a

Degreesof-Freedom
(Df)
26
26
26
26
31
31

Statistical Analysis
Chi-Square
p-value

34.42
34.91
32.619
29.965
-

0.1248
0.1136
0.1734
0.2691
0.466
0.466

statistical analysis of the dilation and constriction of human
eye pupils in response to audio stimuli related to different
emotions; therefore, a large sample size was required for
statistically valid inference. Although the tests performed in
this study are also suitable for small data sets, increasing the
sample size will make the results more reliable.
Another limitation of this study is the light reactivity of the
pupil, which is a limitation that is common in all pupillometry
studies, as discussed in a systematic literature review of
pupillometry [81]. The data recorded for this study were
captured in a room with a constant light source; however, as
proposed, a possible IoMT system would need to address this
limitation.
Finally, for statistical evaluation or machine learning based
emotion classification using pupillometry, accurate measurement of the pupil diameter is crucial; however, the data from
previous studies have not been shared for research purposes,
and the lack of any reference data for comparison is a major
challenge in establishing standards. Therefore, one aim of
this study is to share the acquired data with the research
community.
VII. CONCLUSION

We investigated whether recognition of pupil dilation and
constriction, captured using a cost-effective camera, leads
to robust and meaningful results in terms of categorizing
human emotions. If this technological solution works, it can
be employed as part of a wireless sensor network, which in
turn can feed into an IoMT, where statistical processing takes
place.

14

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

TABLE 9: Comparison of methodological features with proposed research.
Subjects

Stimuli

Elicited emotions

Device used

Statistical tests

2000

Affect
Recognition
Technique
Eye Pupil

30

12 Audios

2003

Eye Pupil

30

30 Audios

Applied Sciences Laboratories
(ASL) series 4000 eye tracker
ASL series 4000 eye tracker

[29]

2006

Eye Pupil

15

20 Images

[30]

2008

Eye Pupil

27

96 Pictures

Positive, neutral and negative
emotion
Positive, neutral and negative
emotion
Neutral, happiness, sadness and
anger
Pleasant, neutral and unpleasant

[73]

2010

10

10 images

Amusement, Contentment, Disgust, Fear, No emotion and Sadness

PROCOMP Infiniti System

[74]

2010

EMG,
BVP,
SKT,
Skin
Conductance and
Respiration Rate
Facial
Expression,
Body gesture and
speech

Parametric test
(rmANOVA)
Parametric Test
(rmANOVA)
Parametric Test
(rmANOVA)
Parametric test
(ANOVA)
Mean and Standard Deviation

10

scenario
situations

Digital Video camera

max, mean, skewness, kurtosis,
range and amplitude

[32]

2013

Eye Pupil

30

21 Audios

2016

Hand Gesture

-

-

Basic positive, basic negative and
no emotion
-

Tobii TX300 eye tracker

[75]
[33]

2019

Eye Pupil

10

4 Videos

[76]

2019

ECG

22

6 videos

[77]

2020

EEG

19

[78]

2020

EEG

[80]

2020

Our
Method

2020

Ref.

Year

[26]
[27]

Tests performed
on
averaging
data? (Yes/No)
Yes

Accuracy

Yes

81.1

yes

94.6

Yes

-

Yes

92

Anger, Despair,
Interest, Pleasure,
Sadness,
Irritation,
Joy
and Pride
manual

Yes

48.3, 67.1, 57.1
respectively

Yes

68.5

mean

-

97.4

Joy and inspiration

Soft Kinetic depth camera
(DS325) and 3D IR camera
Tobii TX300 eye tracker

Manual

No

-

joy, sadness,pleasure,anger,fear
and neutral

Spiker-Shield Heartand Brainsensor

Yes

70.09

45 Images

Early detection of happy, scared,
sad and calm (valance-arousal)

EMOTIV

Yes

-

32

40 Videos

Positive, Negative and Neutral

Standard Electrode positioning
System [79] and Bio amplifier

Yes

85.9

HRV

25

5 videos

Happy, sad and Neutral

bracelet
(Algoband
smart
F8,Desay Electronics)

standard
deviation, mean
root mean square,
max
One-way
repeated measure
ANOVA
t-Test and F-1
Score(Accuracy
and Mean)
Mean, Median,
entropy

Yes

84

Eye Pupil

Up to 32

14 Audios

Happiness, Sadness, Angry, Irritation, Disgust and Fear

IR Camera

Parametric and
non-parametric
tests

No

78.5

ASL infrared eye tracker, model
504
ASL eye tracker, model 504

95.9

Currently, the image-processing toolbox in MATLAB was
used to from a robust algorithm specifically designed to identify these eye activities. Pupil diameters from the camera’s
captured data were used to determine the pupil diameter,
which in turn allowed identification of any pupil dilation or
constriction. Fourteen audio stimuli of emotions were used
for the categorization of six basic emotions, i.e., happiness,
sadness, anger, irritation, disgust and fear.

basic sensor nodes can be deployed in a flexible manner
through a wireless sensor network. Beyond that, a deeplearning system based on input from both pupil diameters
and pupil images is under development by the authors, providing methods for overcoming the possible disadvantages
of deep-learning in real-time environments mentioned in
Section III-A can be applied.

This study also focused on identifying suitable statistical tests for this purpose, and based this study, ten such
tests were shortlisted. The low-cost system correctly classified 226, 365, 278, 422, 116 and 252 audio clips taken
from IADS. Newly created audio clips no. 000 and 222
were verified from experts using CVI. Parametric and nonparametric tests for the response to these clips, representing
disgust and fear emotional stimulus, respectively, gained
agreement of opinion from 26 of 32 subjects and 22 of 32
subjects, respectively. Gender and age-based analysis was
also performed, which confirmed the varying behaviour for
different emotional content with respect to gender and age.
Subjective assessment also acted as a voluntary organ, and
its statistical analyses showed a strong degree of agreement
between subjects’ opinions and the already explicitly defined
emotion category for each stimulus.

REFERENCES

Human emotion recognition has recently become popular
for its application in work on the Human-Computer Interface
(HCI) and for its applications in psychological studies. Thus,
this study also has a role in those fields, especially if the
whole can be made part of an automated system that delivers
results via an IoMT. It will also have the advantage that the

[1] A. Dzedzickis, A. Kaklauskas, and V. Bucinskas, “Human emotion recognition: Review of sensors and methods,” Sensors, vol. 20, no. 3, p. 592,
2020.
[2] S. H. Kim, H. J. Yang, N. A. T. Nguyen, R. M. Mehmood, and S. W.
Lee, “Parameter estimation using unscented kalman filter on the graybox model for dynamic eeg system modeling,” IEEE Transactions on
Instrumentation and Measurement, vol. 69, no. 9, pp. 6175–6185, 2020.
[3] J. Beatty, B. Lucero-Wagoner et al., “The pupillary system,” in Handbook
of psychophysiology. Cambridge Univ Press, 2000, pp. 142–162.
[4] V. Peysakhovich, F. Dehais, and M. Causse, “Pupil diameter as a measure
of cognitive load during auditory-visual interference in a simple piloting
task,” Procedia Manufacturing, vol. 3, pp. 5199–5205, 2015.
[5] J. Anderson, “Acquisition of cognitive skill,” Psychological Review,
vol. 89, no. 4, pp. 369–406, 1982.
[6] J. Richman, K. McAndrew, D. Decker, and S. Mullaney, “An evaluation of
pupil size standards used by police officers for detecting drug impairment,”
Optometry-J. of the Amer. Optometric Assoc., vol. 75, no. 3, pp. 175–182,
2004.
[7] C. R. Chapman, S. Oka, D. H. Bradshaw, R. C. Jacobson, and G. W.
Donaldson, “Phasic pupil dilation response to noxious stimulation in
normal volunteers: relationship to brain evoked potentials and pain report,”
Psychophysiology, vol. 36, no. 1, pp. 44–52, 1999.
[8] D. Arch, “Pupil dilation measures in consumer research: Applications and
limitations,” Advances in Consumer Research, vol. 6, no. 1, pp. 166–168,
1979.
[9] S. Sirois and J. Brisson, “Pupillometry,” Wiley Interdisciplinary Reviews:
Cognitive Science, vol. 5, no. 6, pp. 679–692, 2014.
15

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

[10] R. Mora-Martinez and E. Suaste-Gomez, “Biometric identification system based on pupillary hippus: a preliminary study,” in 2018 15th International Conference on Electrical Engineering, Computing Science
and Automatic Control (CCE).
Mexico City: IEEE, Sep. 2018,
doi:10.1109/ICEEE.2018.8533878.
[11] X. Yang and J. Kim, “Pupillary response and EMG predict upcoming
responses to collision avoidance warning,” in Int. Conf. on Appl. Human
Factors and Ergonomics, 2019, pp. 135–143.
[12] D. Olson and M. Fishel, “The use of automated pupillometry in critical
care,” Critical Care Nursing Clinics, vol. 28, no. 1, pp. 101–107, 2016.
[13] M. Eckstein, B. Guerra-Carrillo, A. Singley, and S. Bunge, “Beyond eye
gaze: What else can eyetracking reveal about cognition and cognitive
development?” Developmental Cognitive Neuroscience, vol. 25, pp. 69–
91, 2017.
[14] S. J. Webb, E. Neuhaus, and S. Faja, “Face perception and learning
in autism spectrum disorders,” The Quarterly Journal of Experimental
Psychology, vol. 70, no. 5, pp. 970–986, 2017.
[15] D. Burley, N. Gray, and R. Snowden, “Emotional modulation of the pupil
response in psychopathy,” Personality Disorders: Theory, Research, and
Treatment, vol. 10, no. 4, pp. 365–375, 2019.
[16] S. O’Brien, “Eye-tracking and translation memory matches,” Perspectives:
Studies in Translatology, vol. 14, no. 3, pp. 185–205, 2007.
[17] S. Thiyagarajan, S. Rosli, M. Amin, and S. Wibirama, “Tracking the eyemind relationship of positive emotion using eye tracking technique,” J. of
Advanced Manufacturing Technol., vol. 13, no. 2, pp. 93–100, 2019.
[18] I. L. Olokodana, S. P. Mohanty, E. Kougianos, and O. O. Olokodana,
“Real-time automatic seizure detection using ordinary kriging method in
an edge-iomt computing paradigm,” SN Computer Science, vol. 1, no. 5,
pp. 1–15, 2020.
[19] E. Hess, “Pupillometrics. a method of studying mental, emotional, and
sensory processes.” Handbook of Psychophysiology, pp. 491–531, 1972.
[20] S. Goldinger and M. Papesh, “Pupil dilation reflects the creation and
retrieval of memories,” Current Directions in Psychological Sci., vol. 21,
no. 2, pp. 90–95, 2012.
[21] P. Murphy, J. Vandekerckhove, and S. Nieuwenhuis, “Pupil-linked arousal
determines variability in perceptual decision making,” PLOS Comput.
Biol., vol. 10, no. 9, p. e1003854, 2014.
[22] J. de Gee, T. Knapen, and T. Donner, “Decision-related pupil dilation
reflects upcoming choice and individual bias,” Proc. of the Nat. Academy
of Sci., vol. 111, no. 5, pp. E618–E625, 2014.
[23] S. Otero, B. Weekes, and S. Hutton, “Pupil size changes during recognition
memory,” Psychophysiology, vol. 48, no. 10, pp. 1346–1353, 2011.
[24] T. Simms, “Pupillary response of male and female subjects to pupillary
difference in male and female picture stimuli,” Attention, Perception, &
Psychophysics, vol. 2, no. 11, pp. 553–555, 1967.
[25] J. Nunnally, P. Knott, A. Duchnowski, and R. Parker, “Pupillary response as a general measure of activation,” Attention, Perception, &
Psychophysics, vol. 2, no. 4, pp. 149–155, 1967.
[26] T. Partala, M. Jokiniemi, and V. Surakka, “Pupillary responses to emotionally provocative stimuli,” in Symp. on Eye-Tracking Research & Applications. Association for Computing Machinery, 2000, pp. 123–129.
[27] T. Partala and V. Surakka, “Pupil size variation as an indication of affective
processing,” Int. J. of Human-Comput. Stud., vol. 59, no. 1, pp. 185–198,
2003.
[28] C. Lisetti and F. Nasoz, “Using noninvasive wearable computers to recognize human emotion,” EURASIP J. on Advances in Signal Process., vol.
2004, no. 11, 2004, article no. 929414.
[29] N. Harrison, T. Singer, P. Rotshtein, R. Dolan, and H. Critchley, “Pupillary
contagion: Central mechanisms engaged in sadness processing,” Social
Cognitive and Affective Neuroscience, vol. 1, no. 1, pp. 5–17, 2006.
[30] M. M. Bradley, L. Miccoli, M. A. Escrig, and P. J. Lang, “The pupil as a
measure of emotional arousal and autonomic activation,” Psychophysiology, vol. 45, no. 4, pp. 602–607, 2008.
[31] L. Valverde, E. de Lera, and C. Fernàndez, “Inferencing emotions through
the triangulation of pupil size data, facial heuristics and self-assessment
techniques,” in Second Int. Conf. on Mobile, Hybrid, and On-Line Learning, 2010, pp. 147–150.
[32] A. Babiker, I. Faye, and A. Malik, “Non-conscious behavior in emotion
recognition: gender effect,” in IEEE 9th Int. Colloquium on Signal Process.
and its Applicat., 2013, pp. 258–262.
[33] N. Baharom, N. Jayabalan, M. Amin, and S. Wibirama, “Positive emotion
recognition through eye tracking technology,” Journal of Advanced Manufacturing Technology (JAMT), vol. 13, no. 2 (1), pp. 143–152, 2019.

[34] L. J. Zheng, J. Mountstephens, and J. Teo, “Four-class emotion classification in virtual reality using pupillometry,” Journal of Big Data, vol. 7,
no. 1, pp. 1–9, 2020.
[35] S. Nakakoga, H. Higashi, J. Muramatsu, S. Nakauchi, and T. Minami,
“Asymmetrical characteristics of emotional responses to pictures and
sounds: Evidence from pupillometry,” PloS one, vol. 15, no. 4, p.
e0230775, 2020.
[36] J. Nie, Y. Hu, Y. Wang, S. Xia, and X. Jiang, “SPIDERS: Low-cost wireless
glasses for continuous in-situ bio-signal acquisition and emotion recognition,” in 2020 IEEE/ACM Fifth International Conference on Internet-ofThings Design and Implementation (IoTDI). IEEE, Apr. 2020, pp. 27–39.
[37] D. Al-Omar, A. Al-Wabil, and M. Fawzi, “Using pupil size variation
during visual emotional stimulation in measuring affective states of non
communicative individuals,” in International Conference on Universal
Access in Human-Computer Interaction. Springer, 2013, pp. 253–258.
[38] V. Bajaj and R. Pachori, “Human emotion classification from EEG signals
using multiwavelet transform,” in Medical Biometrics, IEEE Int. Conf. on
Medical Biometrics, 2014, pp. 125–130.
[39] M. Li and B.-L. Lu, “Emotion classification based on gamma-band EEG,”
in Annu. Int. Conf. of the IEEE Eng. in Medicine and Biology Society,
2009, pp. 1223–1226.
[40] P. Sawangjai, S. Hompoonsup, P. Leelaarporn, S. Kongwudhikunakorn,
and T. Wilaiprasitporn, “Consumer grade eeg measuring sensors as research tools: A review,” IEEE Sensors Journal, vol. 20, no. 8, pp. 3996–
4024, 2019, doi: 10.1109/JSEN.2019.2962874.
[41] A. Haag, S. Goronzy, P. Schaich, and J. Williams, “Emotion recognition
using bio-sensors: First steps towards an automatic system,” in Affective
Dialogue Systems, Tutorial and Research Workshops. Springer, 2004,
pp. 36–48.
[42] F. Nasoz, K. Alvarez, C. Lisetti, and N. Finkelstein, “Emotion recognition
from physiological signals for presence technologies,” Int. J. of Cognition,
Technol., and Work, vol. 61, no. 1, pp. 1–32, 2003.
[43] P. Lakhan, N. Banluesombatkul, V. Changniam, R. Dhithijaiyratn,
P. Leelaarporn, E. Boonchieng, S. Hompoonsup, and T. Wilaiprasitporn, “Consumer grade brain sensing for emotion recognition,”
IEEE Sensors Journal, vol. 19, no. 21, pp. 9896–9907, 2019, doi:
10.1109/JSEN.2019.2928781.
[44] M. Egger, M. Ley, and S. Hanke, “Emotion recognition from physiological
signal analysis: A review,” Electronic Notes in Theoretical Computer
Science, vol. 343, pp. 35–55, 2019.
[45] I. Grubisic, I. Grbesa, T. Lipic, K. Skala, O. Zrinscak, R. Ivekovic, and
Z. Vatavuk, “Natural eye gaze computer interaction for medical oculography diagnosis: Current status and future prospects,” in 37th IEEE Int. Conv.
Inform. and Commun. Technol., Electron. and Microelectronics, 2014, pp.
421–425.
[46] J.-Y. Kim, S.-N. Min, M. Subramaniyam, and Y.-J. Cho, “Legibility
difference between e-books and paper books by using an eye tracker,”
Ergonomics, vol. 57, no. 7, pp. 1102–1108, 2014.
[47] S. Alkhashrami, H. Alghamdi, and A. Al-Wabil, “Human factors in the
design of arabic-language interfaces in assistive technologies for learning
difficulties,” in International Conference on Human-Computer Interaction. Springer, 2014, pp. 362–369.
[48] A. T. Duchowski, “A breadth-first survey of eye-tracking applications,”
Behavior Research Methods, vol. 34, no. 4, pp. 455–470, 2002.
[49] A. Al-Fuqaha, M. Guizani, M. Mohammad, M. Aledhari, and M. Ayyash,
“Internet of Things: A survey on enabling technologies, protocols, and
applications,” IEEE Commun. Surv. Tutorials, vol. 17, pp. 2347–2376,
2015.
[50] S. A. Alvi, B. Afzal, G. A. Shah, L. Atzori, and W. Mahmood, “Internet of
multimedia things: Vision and challenges,” Ad Hoc Networks, vol. 33, pp.
87–111, 2015, doi:10.1016/j.adhoc.2015.04.006.
[51] “Raspberry Pi user guide.” Wiley & Sons: Chichester, UK, 2016, pp.
1–19.
[52] F.Martínez-Ricarte, A.Castro, M.A.Poca, J.Sahuquillo, L.Expósito,
M.Arribas, and J.Aparicio, “Infrared pupillometry. basic principles and
their application in the non-invasive monitoring of neurocritical patients,”
Neurología, vol. 28, no. 1, pp. 41–51, 2013.
[53] A. Voulodimos, N. Doulamis, A. Doulamis, and E. Protopapadakis, “Deep
learning for computer vision: A brief review,” Computational Intell. and
Neuroscience, vol. 2018, pp. 1–13, 2018.
[54] A. Shifa, M. Asgha, S. Noor, N. Gohar, and M. Fleury, “Lightweight cipher
for H.264 videos in the Internet of Multimedia Things with encryption
space ratio diagnostics,” Sensors, vol. 19, no. 5, pp. 1228–1238, 2019.

16

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

[55] M. Bradley and P. Lang, “The international affective digitized sounds
(IADS-2): Affective ratings of sounds and instruction manual,” IDAS,
Tech. Rep., 2007, tech. Rep. B-3.
[56] W. Parrott, “Emotions in social psychology:volume overview,” in Essential
readings. Psychology Press, 2001, pp. 1–19.
[57] R. Yuvaraj, M. Murugappan, N. Ibrahim, M. Omar, K. Sundaraj, K. Mohamad, R. Palaniappan, E. Mesquita, and M. Satiyan, “On the analysis
of EEG power, frequency and asymmetry in Parkinson’s disease during
emotion processing,” Behavioral and Brain Functions, vol. 10, no. 1, 2014,
article no. 12.
[58] J. Russell, “A circumplex model of affect,” J. of Personality and Social
Psychology, vol. 39, no. 6, pp. 1161–1178, 1980.
[59] D. Polit and C. Beck, “The Content Validity Index: Are you sure you
know what’s being reported? Critique and recommendations,” Research
in Nursing & Health, vol. 29, no. 5, pp. 489–497, 2006.
[60] D. Polit, C. Beck, and S. Owen, “Is the CVI an acceptable indicator of
content validity? Appraisal and recommendations,” Research in Nursing
& Health, vol. 30, no. 4, pp. 459–467, 2007.
[61] T. Wiegand, G. J. Sullivan, G. Bjontegaard, and A. Luthra, “Overview of
the h. 264/avc video coding standard,” IEEE Transactions on circuits and
systems for video technology, vol. 13, no. 7, pp. 560–576, 2003.
[62] G. J. Sullivan, J.-R. Ohm, W.-J. Han, and T. Wiegand, “Overview of the
High Efficiency Video Coding (HEVC) standard,” IEEE Transactions on
circuits and systems for video technology, vol. 22, no. 12, pp. 1649–1668,
2012.
[63] R. Pereira and E. Pereira, “Video streaming: H.264 and the Internet of
Things,” in IEEE 29th Int. Conf. on Advanced Information Networking
and Applicat. Workshops, 2015, pp. 711–714.
[64] M. Kret and E. Sjak-Shie, “Preprocessing pupil size data: Guidelines and
code,” Behavior Research Methods, vol. 51, no. 3, pp. 1336–1342, 2019.
[65] S. Mathôt, J. Fabius, E. Van Heusden, and S. Van der Stigchel, “Safe
and sensible preprocessing and baseline correction of pupil-size data,”
Behavior Research Methods, vol. 50, no. 1, pp. 94–106, 2018.
[66] J. Verzani, Getting started with RStudio. O’Reilly: Sebastopol, CA, USA,
2011.
[67] B. Yap and C. Sim, “Comparisons of various types of normality tests,” J.
of Statistical Computation and Simulation, vol. 81, no. 12, pp. 2141–2155,
2011.
[68] J. A. Coan and J. J. B. Allen, “Emotion Elicitation Using Films,” in
Handbook of emotion elicitation and assessment, ser. Series in affective
science. Oxford ; New York: Oxford University Press, 2007, pp. 9–28.
[69] C. Balkenius, C. Fawcett, T. Falck-Ytter, G. Gredebäck, and B. Johansson,
“Pupillary correlates of emotion and cognition: A computational model,”
in Int. IEEE/EMBS Conf. on Neural Eng., 2019, pp. 903–907.
[70] E. Hess and J. Polt, “Pupil size in relation to mental activity during simple
problem-solving,” Science, vol. 143, no. 3611, pp. 1190–1192, 1964.
[71] M. Gilzenrat, S. Nieuwenhuis, M. Jepma, and J. Cohen, “Pupil diameter
tracks changes in control state predicted by the adaptive gain theory of locus coeruleus function,” Cognitive, Affective, & Behavioral Neuroscience,
vol. 10, no. 2, pp. 252–269, 2010.
[72] A. Babiker, I. Faye, and A. Malik, “Differentiation of pupillary signals
using statistical and functional analysis,” in 5th Int. Conf. on Intell. and
Advanced Syst., 2014, doi: 10.1109/ICIAS.2014.6869498.
[73] C. Maaoui and A. Pruski, “Emotion recognition through physiological
signals for human-machine communication,” Cutting Edge Robotics, vol.
2010, pp. 317–332, 2010.
[74] L. Kessous, G. Castellano, and G. Caridakis, “Multimodal emotion recognition in speech-based interaction using facial expression, body gesture
and acoustic analysis,” Journal on Multimodal User Interfaces, vol. 3, no.
1-2, pp. 33–48, 2010.
[75] P. Molchanov, X. Yang, S. Gupta, K. Kim, S. Tyree, and J. Kautz, “Online
detection and classification of dynamic hand gestures with recurrent 3d
convolutional neural network,” in Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), June 2016, pp. 4207–
4215.
[76] T. Dissanayake, Y. Rajapaksha, R. Ragel, and I. Nawinne, “An ensemble
learning approach for electrocardiogram sensor based human emotion
recognition,” Sensors, vol. 19, no. 20, p. 4495, 2019.
[77] R. M. Mehmood, H. J. Yang, and S. H. Kim, “Children emotion regulation:
Development of neural marker by investigating human brain signals,”
IEEE Transactions on Instrumentation and Measurement, vol. 70, pp. 1–
11, 2020, doi: 10.1109/TIM.2020.3011817.
[78] M. A. Asghar, M. J. Khan, M. Rizwan, R. M. Mehmood, and S.-H. Kim,
“An innovative multi-model neural network approach for feature selection

in emotion recognition using deep feature clustering,” Sensors, vol. 20,
no. 13, p. 3765, 2020.
[79] R. W. Homan, J. Herman, and P. Purdy, “Cerebral location of international
10–20 system electrode placement,” Electroencephalography and clinical
neurophysiology, vol. 66, no. 4, pp. 376–382, 1987.
[80] L. Shu, Y. Yu, W. Chen, H. Hua, Q. Li, J. Jin, and X. Xu, “Wearable
emotion recognition using heart rate data from a smart bracelet,” Sensors,
vol. 20, no. 3, p. 718, 2020.
[81] S. S. Phillips, C. M. Mueller, R. G. Nogueira, and Y. M. Khalifa, “A Systematic Review Assessing the Current State of Automated Pupillometry
in the NeuroICU,” Neurocritical Care, vol. 31, no. 1, pp. 142–161, Aug.
2019.

SIDRA RAFIQUE is a Ph.D Scholar in Computer
Science Department at Lahore College for Women
University (LCWU), Lahore, Pakistan. She has
obtained her MS and BS degrees in Computer Science from Government College University (GCU),
Lahore, Pakistan, in 2012 and 2010 respectively.
She has completed her Intermediate in Computer
science from LCWU, in 2006. She is also working in LCWU as a Lecturer at Computer Science
Department since 2014. Her research is in the field
of Image Processing, Emotion Identifications and classification using Deep
Learning, Computer Vision, IoT and its applications.

NADIA KANWAL received her M.Sc. and
Ph.D.degrees in computer science from the University of Essex, Essex, U.K., in 2009 and 2013,
respectively. She is currently a Marie SklodowskaCurie Career-Fit Postdoctoral Fellow hosted by
Software Research Institute, Athlone Institute of
Technology (AIT), Ireland. The primary objective of this fellowship is to propose technological solutions for privacy protection of humans in
surveillance videos as per GDPR guidelines. She
is also applying deep learning methods to improve the performance of vision
algorithms for detection and matching tasks which can help to develop robust
solutions for different vision-related applications. She remained associated
with Lahore College for Women University, Pakistan for more than 17 years,
where she was working as Associate Professor of Computer Science. Her
research interests include machine learning, image & video processing, medical imaging, encryption, steganography, hashing and privacy preservation
in video surveillance. She is senior member IEEE and has been actively
involved in reviewing for reputed conferences and journals.

IRFAN KARAMAT graduated (MBBS) from King
Edward Medical University (KEMU) in 2007-8.
He did his four years’ fellowship (FCPS) in General Ophthalmology from college of Physicians
and Surgeons of Pakistan (CPSP) in 2014. Royal
College of Surgeons (Edinburgh) awarded him the
degree of MRCS in 2015. Currently he is doing his
fellowship in Vitreo-Retina (VR) from King Edward Medical University (KEMU), Lahore, Pakistan. He is also part of editorial board in Punjab
Institute of Preventive Ophthalmology (PIPO)/KEMU.

17

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3048311, IEEE Access
Rafique et al.: Estimating Emotions from Eye Pupillometry with Low-cost Devices

MAMOONA N. ASGHAR received the PhD
degree with the School of Computer Science
and Electronic Engineering, University of Essex,
Colchester, UK in 2013. Currently, she is working
as Marie Sklodowska-Curie (MSC) Career-Fit Research Fellow in the Software Research Institute,
Athlone Institute of Technology (AIT), Ireland
since June 2018. As an MSC Principal Investigator
(PI), her research targets the proposals and implementation of technological solutions for General
Data Protection Regulation (GDPR) compliant Surveillance systems. She is
also a regular faculty member in the Department of Computer Science and
Information Technology (DCS & IT), The Islamia University of Bahawalpur,
Punjab, Pakistan and currently on postdoc leave. She has more than 14 years
of teaching and R&D experience. She has published several ISI indexed
journal articles along with numerous International conference papers. She is
also actively involved in reviewing for renowned journals and conferences.
Her research interests include security aspects of multimedia (image, audio
and video), compression, visual privacy, encryption, steganography, secure
transmission in future networks, Internet of Multimedia Things, video quality metrics, and key management schemes.

MARTIN FLEURY holds a degree in Modern History (Oxford University, UK) and a
Maths/Physics-based degree from the Open University, Milton Keynes, UK. He obtained an MSc
in Astrophysics from QMW College, University of
London, UK in 1990 and an MSc from the University of South-West England, Bristol in Parallel
Computing Systems in 1991. He gained a PhD in
Parallel Image-Processing Systems from the University of Essex, Colchester, UK. He worked as a
Senior Lecturer at the University of Essex, after which he became a Visiting
Fellow. Currently he is associated with the School of Engineering, Arts, Science, Technology and Engineering (EAST), University of Suffolk, Ipswich,
UK. He is also a free-lance consultant. Martin has authored or co-authored
around two hundred and ninety-five articles and book chapters on topics such
as document and image compression algorithms, performance prediction
of parallel systems, software engineering, re-configurable hardware and
vision systems. His current research interests are video communication over
wireless networks. He has published or edited books on high performance
computing for image processing and peer-to-peer streaming.

18

VOLUME 4, 2020

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/

