Analyzing the Recognition of Color Exposure and
Imagined Color from EEG Signals
1st Alejandro A. Torres-Garcı́a

2nd Marta Molinas

Department of Engineering Cybernetics
Norwegian University of Science and Technology (NTNU)
Trondheim, Norway
alejandro.a.t.garcia@ntnu.no

Department of Engineering Cybernetics
Norwegian University of Science and Technology (NTNU)
Trondheim, Norway
marta.molinas@ntnu.no

Abstract—The most widely used visual brain-computer interfaces (BCIs) are based on flickering stimulators. It remains
unclear whether a passive visual stimulator could be used. Colors
are an integral part of everyday life. We analyzed the potential
use of both imagined colors and color exposure to control BCIs.
We present here a comprehensive study of the feasibility of using
the recognition of both exposed and imagined colors from the
EEG signals recorded in a previous study. This constitutes the
first step towards the development of a color-based BCI. We
processed and analyzed the EEG signals from seven subjects
recorded whilst they were exposed to three colors (red, green and
blue) and whilst they imagined the same colors. The outcomes
obtained suggest that the simultaneous use of these two tasks
would facilitate the effective recognition of both imagined and
displayed colors for the control of a BCI. An analysis of each
task separately revealed that color exposure provided more
information than color imagination for discriminating between
the three colors.
Index Terms—classification, imagined colors recognition, exposure color recognition

I. I NTRODUCTION
EEG-based brain-computer interfaces (BCIs) are designed
to provide individuals with a non-muscular channel for transmitting messages and commands to the external world [1].
Such interfaces were initially designed for use by the disabled,
but have since been developed for use by the able-bodied too.
The mechanism used for interaction or for generating messages or commands for the BCI is called the neuroparadigm.
Motor imaging is the most widely used paradigm for BCI
control. However, this approach suffers from being intuitively
difficult to achieve. Several studies have investigated alternative approaches based on the use of visual stimuli to control
BCIs.
The most common visual paradigms for BCI control
(SSVEP and P300) are based on an external flickering stimulator. We aimed to overcome the need for a flickering stimulator
by using only EEG responses to primary colors. A color-based
recognition system would be easier to use too, because passive
stimulators, such as signs indicating the way out and traffic
lights, are an integral part of everyday life. The integration
of a BCI of this kind into the environment required for
The European Research Consortium for Informatics and Mathematics
(ERCIM). 2004, Route des Lucioles. BP93. 06902 Sophia Antipolis, France.

home automation would make this system useful for elderly
and disabled people. It would provide these individuals with
greater autonomy by allowing them to control lights, the
virtual keyboard of the computer and music players. It would
also make it possible for them to answer the telephone and to
ask for assistance in emergencies.
II. R ELATED WORK
Few studies have focused on the automatic recognition of
either displayed or imagined colors from brain signals [2]–
[4]. Liu et al. [4] analyzed fNIRS signals, for which time
responses remain much less useful for practical applications
than those of EEG signals. Yu et al. [3], discussed the use of
discrimination between five imagined colors (red, green, blue,
yellow and white) recorded for 10 subjects with an Emotiv
Epoc headset (14 channels and a sampling frequency of 128
Hz). They claimed that applying a method based on eventrelated potential (ERP) values (selecting N2, P3 and N4) and
artificial neural networks (ANN), yielded a mean accuracy
for all subjects of about 65% if channel T7 or F4 was used
separately. However, they recorded only ten epochs for each
color. Another previous study [2] described the recording of
a dataset for three exposed and imagined colors (red, green
and blue), and the use of a method based on event-related
spectral perturbations (ERSP) and support vector machine
(SVM). More epochs (60 epochs per color) were recorded
in this study than in the study by Yu [3] and accuracies of
84-97% for exposed colors and 64-76% for imagined colors
were achieved, but the three averaged ERSP matrices were
calculated from the global information. This meant that the
method made use of the data used for training at the testing
stage, resulting in an overestimation of method performance.
Finally, this method had the drawback of being dependent on
a visually supervised stage for artifact removal, decreasing its
suitability for practical applications.
The question of the level of performance that can be
achieved with the identification of colors from EEG signals,
with both good data management and automatic artifact removal, therefore remains unanswered. This comprehensive
study using a dataset described elsewhere [2], aimed to provide
more evidence concerning the feasibility of using the recog-

nition of exposed and imagined colors from EEG signals as a
first step towards the development of a color-based BCI.
We performed three sets of experiments. In the first, we
assessed the feasibility of simultaneously recognizing all the
exposed and imagined colors. In the second, we investigated
whether a machine learning algorithm could suggest differences or similarities between exposed and imagined colors. In
the third, we separately analyzed the feasibility of recognizing
the three imagined colors and the three exposed colors. This
last experiment was designed to determine which approach
would be most suitable for practical applications in BCI
control.
III. DATASET
In this work, we processed and analyzed the dataset
recorded in a previous study [2], consisting of the EEG signals
recorded during both exposure to colors and the imagination
of colors. The study participants were exposed to and asked to
imagine the colors red, green, and blue. Signals were obtained
from the seven subjects by recording on a four-channel g.Tec
MOBIlab device with a sampling frequency of 256 Hz. In
accordance with the 10-20 international system, the channel
names were P1, P2, O1, and O2. The recording protocol
consisted of the random presentation of any exposed or
imagined color for 3 seconds per epoch. After the presentation
of each epoch of imagined or exposed color, a gray screen was
displayed for 3 seconds. Subjects were asked to keep their
eyes closed during the imagination of colors. At the end of
the recording period, the dataset for each subject consisted of
60 epochs for each exposed and imagined color. At the end
of the study, we had six classes available and 360 epochs per
subject.
IV. M ETHOD
The proposed method consists of the following stages:
preprocessing, feature extraction and classification. Figure 1
illustrates the method stages and provides a global overview
of the specific techniques analyzed. The preprocessing and
feature extraction stages calculate feature vectors for each
epoch of EEG signals (from all channels) recorded during the
imagination or exposure of colors. The arrangement of the
dataset depended on the aims of the experiment concerned.
These feature vectors were then used in the classification stage,
for the assessment of performance, using four classifiers after
the application of 10-fold cross-validation. Specific details are
provided below for each stage of the method.
Preprocessing

-artifact removal (ICA+Hurst exponent)
-downsampling to 128 Hz
-lowpass filtering at 30 HZ

Classification
-Random forest
-SVM
Naive Bayes
-KNN

A. Preprocessing
At this stage, an automatic method (described in [5]) was
used to remove artifacts. This method was based on independent component analysis and assessment of the components
obtained with the Hurst exponent. These method discards
components with Hurst exponents in the range 0.58-0.69
because such components are more likely to be related to
eye blinks and heartbeats [5]. The remaining components are
then mixed again to reconstruct the EEG signals. All epochs
with no useful components are automatically removed from
the dataset. It should be stressed that, in this work, we set
the number of independent components to be calculated as
equal to the number of available electrodes. Later on, the
signals were downsampled to 128 Hz. They were then lowpass filtered at 30 Hz, to eliminate most of the artifact-related
activity.
B. Feature extraction
Given the relevance of considering both the non-stationary
nature of EEG signals and temporal changes in the features
(see [6]), we computed a set of temporal and frequency
features from each epoch of color exposure and imagination.
For the temporal features, we calculated, for each epoch and
channel, the mean, median, standard deviation, variance, maximum, minimum, sum, difference and sum of the maximum
and minimum, kurtosis, skewness, entropy, zero-crossing rate
and autoregressive coefficients (6th order).
By contrast, the frequency features were computed after
the application of discrete wavelet transform (DWT), with
a biorthogonal 2.2 as the mother wavelet and four levels of
decomposition. We chose this number of levels to capture the
activity of brain rhythms in different ways. Table I shows the
decomposition levels and their corresponding frequency ranges
and brain rhythms. Following the low-pass filtering of signals
at 30 Hz, the D1 coefficients were rejected for subsequent
analyses.
TABLE I
D ESCRIPTION OF THE LEVELS OF DWT DECOMPOSITION , THEIR
FREQUENCY RANGES AND RELATED BRAIN RHYTHMS

Level
D1
D2
D3
D4
A4

Freq. range
32-64
16-32
8-16
4-8
0-4

Brain rhythm
gamma
beta (16-30 Hz.), and gamma (30-32 Hz.)
alpha (8-12 Hz.), and beta (12-16 Hz.)
theta
delta

Feature Extraction
-temporal features
-frequential features

Identi�ied
color

red
green
blue

Fig. 1. Method used for the classification of EEG signals recorded during
exposure to colors and the imagination of colors

For each remaining wavelet decomposition level (D2-D4
and A4), we calculated the teager (TWE), instantaneous
(IWE), relative (RWE) [7] and hierarchical (HWE) energies,
along with the following statistical values: the mean, median,
standard deviation, variance, ratio of the means in adjacent
sub-bands, maximum, minimum, sum, difference and sum of
the maximum and minimum, kurtosis, and skewness. Finally,
we concatenated all the features from each epoch in the
following order P1, P2, O1, and O2.

TWE and HWE are much less well known in BCI research
than RWE, as they were first applied to automatic speech
recognition (ASR) [8], making it possible to capture differences not considered by IWE. IWE captures the mean amplitude of the signal, whereas TWE helps to reduce the effect
of additive noise by considering the neighboring values of
an element of the signal before computing the corresponding
energy. Finally, HWE [9] was proposed for the analysis of
the central energy of a given analysis window, using the same
number of coefficients in every band. Below, we show the
corresponding formulas for each j-th decomposition level.
IW Ej is computed as follows,


Nj
X
1
(wj (r))2  ,
(1)
IW Ej = log10 
Nj r=1
where wj are the wavelet coefficients at the j-th level of
decomposition , and Nj is the number of samples at the j-th
level of decomposition.
T W Ej can be computed as follows,

Nj −1
X
1
|(wj (r))2 − wj (r − 1) ∗ wj (r + 1)| ,
T W Ej = log10 
Nj r=1
(2)


where wj are the wavelet coefficients at the j-th level of
decomposition, and Nj is the number of samples at the j-th
level of decomposition.
HWE can be computed as follows,


(Nj +NJ )/2
X
1
HW Ej = log10 
(wj (r))2  ,
(3)
NJ
r=(Nj −NJ )/2

where wj are the wavelet coefficients at this level, Nj is the
number of samples at this level, and NJ is the number of
samples at the last level of decomposition.
C. Classification
Classification covers any context in which a decision or
forecast is made on the basis of available historical data (see
section III in our case). Specifically, a supervised classification
problem has a database of the form [10]:
D = {hx1 , y1 i , hx2 , y2 i , · · · , hxm , ym i} = hX, Y i

(4)

where the values xi ∈ X are typically multidimensional
vectors (here, instances or epochs) of the form: xi =
{z1 , z2 , · · · , zn } the elements of which can take continues
or discrete values. These components are called attributes (or
features). In our experiment, these values are the frequential
and temporal features computed for each epoch. The goal is
to infer a function (or relationship) f mapping the elements
of X to Y, f : X → Y .
The values of Y are contained in a finite set of classes
C = {C1 , ..., Ck } that characterize the given data (here 2,
3 or 6 classes, depending on the experiment analyzed). The
models learned from training data are then evaluated with
a different set of tests to determine whether they can be
generalized to new cases [11]. Finally, cross-fold validation

is performed, in which the original data are partitioned into
k subsamples. The model is then trained on k-1 subsamples,
the remaining subsample being used to test the model. Each
subsample is used separately to test the model and an estimated
generalization error is then calculated.
Despite the deep learning’s boom in many machine learning
applications, most of these algorithms usually do not get
outstanding performances when small datasets are available
such as it commonly happens in BCIs [12]. Accordingly, we
trained and tested the following four classifiers: support vector
machine (SVM), random forests (RFs), naive Bayes (NB) and
K-nearest neighbors (KNN). Each of these classifiers differ
in nature. Random forests are a fast ensemble of decision
trees, Naive Bayes is based on Bayes theorem, SVM is based
on the kernel trick to find the hyperplane maximizing the
separation between classes and KNN is based on the majority
class in a set of k neighbors. Specifically, we used 50 trees
for RF and k=3 for KNN, the remaining hyperparameters
were obtained with the default values of the Weka toolbox
[13]. Subsequent experiments were performed on different
arrangements of the dataset, and performance was assessed by
10-fold cross-validation. The specific details of each classifier
are presented below.
1) SVM: SVM uses a discriminating hyperplane to identify
classes. The selected hyperplane is that which maximizes the
margins for the SVM classifier, that is, the distance between
the closest training points, with the aim of increasing generalization capabilities [14], [15]. SVM uses a C regularization
parameter that can accommodate outliers and is robust to
errors in the training set, which are common for EEG signals.
We used a linear SVM, implying the use of linear decision
boundaries for classification. However, non-linear decision
boundaries can be created with the kernel trick. This involves
the implicit assignment of data to another space (usually of
a higher dimension) by a kernel function. Some of the most
widely used alternative kernels are the radial base, Gaussian,
polynomial and sigmoid functions. In multiclass problems,
such as some of the experiments in this work, SVMs are
applied by the so-called ”OVR” (one versus the rest) strategy,
which involves separating each class from all the others [16].
Such an approach is suitable for use in the context of the
recognition of exposed and imagined colors. SVMs have
many advantages, including good generalization properties,
and insensitivity to over-training and dimensionality [14], [15].
2) NB: NB is based on the determined probabilities of the
data. New objects can be assigned to classes with various
degrees of probability [11]. NB assumes that the attributes are
independent, (i. e., that the effect of an attribute on a specific
class does not depend on the values of the remaining attributes
(variables or features)).
NB is based on Bayes Theorem:
P (h|D) =

P (D|h) · P (h)
,
P (D)

(5)

where P (h) is the probability a priori of the hypothesis h,
P (D) is the probability a priori of the training data D,

P (h|D) is the probability of h given D, and P (D|h) is the
probability of D given h. In general, the most likely hypothesis
for the given training data is calculated to obtain the most
likely hypothesis a posteriori hM AP , which is computed as
follows,
hM AP =

arg max P (h|D)
h∈H
(h)
max P (D|h)·P
P (D)
h∈H

=

arg

=

arg max P (D|h) · P (h)

(6)

V. E XPERIMENT AND RESULTS

h∈H

A. Recognition of all the exposed and imagined colors

If we assume that P (hi ) = P (hj ), then further simplification
is possible, by selecting the hypothesis with the maximum
likelihood, that is:
hM L = arg max P (D|hi )

The key to this classifier is the definition of the nearest
N
neighbors xN
1 , . . . , xk to a given instance (example or epoch)
xj . This is achieved by measuring the distance between the
object to be classified and all the elements in the dataset. The
k instances with the smaller distances (generally Euclidean
distances) to the instance concerned are considered to be its
nearest neighbors. Finally, the number of neighbors k selected
is typically odd rather than even (K=3 in this work) to avoid
draws in votes for the output class [10].

(7)

hi ∈H

NB can then be combined with a decision rule for data
classification. A common rule used for this purpose is selection
of the most likely hypothesis for an unlabeled vector, via MAP
or ML methods.
3) Random forests (RF): RFs are combinations of predictor
trees such that each tree depends on the values of a random
vector sampled independently and with the same distribution
for all trees in the forest. Each tree casts a single vote for the
most popular class for a given instance x. The output of RF is
then obtained by a sort of ”majority voting” system. The trees
are constructed with algorithm 1. The generalization error over
forests almost certainly converges to a limit as the number of
trees in the forest increases [17].
Algorithm 1 Random Forest
Require: IDT (a decision tree), T (the number of iterations),
S (the training set), µ (the size of the subsample), N (the
number of attributes at each node)
Ensure: Mt ; t = 1, . . . , T
for t ← 1 to T do
St ← a sample with µ instances from S allowing
replacement
Build the classifier Mt using IDT (N ) on St .
end for
In algorithm 1, IDT represents a decision tree with the
following modifications: the decision tree is not pruned, and
at each node, instead of selecting the best division from all
the attributes, the inducer randomly samples N attributes and
selects the best division from them. Finally, the key advantages
of the random forest approach are: its speed and the ease with
which it can handle a large number of input attributes [18].
4) K nearest neighbors (KNN): KNN is based on the
comparison of the object to be classified with all objects stored
in the dataset. The class of the unknown object xj is assigned
by analyzing the known classes of the k examples (neighbors)
most similar to the object in the dataset. The object is assigned
to the majority class among the neighbors [10].

In this experiment, we investigated whether a machine learning algorithm could discriminate between all the exposed and
imagined colors. In this case, there were six available classes
(red, green and blue, along with the epochs recorded during
the imagination of these colors). Table II shows the accuracy
(expressed as a percentage) and standard deviations (SDs)
for all four classifiers after 10-fold cross-validation for all
subjects. For all classifiers, the mean accuracy for all subjects
and for each subject individually was above expectations based
on chance alone for the six classes (16%). This suggests
that there was a difference between each of the exposed and
imagined colors.
TABLE II
P ERCENT ACCURACY AND SD S OBTAINED FOR THE CLASSIFIERS FOR THE
RECOGNITION OF ALL THE IMAGINED AND EXPOSED COLORS (6 CLASSES )
subj
S1
S2
S3
S4
S5
S6
S7
Avg

SVM
acc
std
27.64
8.17
30.72
7.72
33.89
9.15
25.83
6.29
29.44
6.44
30
5.97
33.61
7.91
30.16
7.38

RF
acc
30.12
27.65
37.78
27.78
30
27.22
32.78
30.48

std
7.8
3.54
5.27
3.93
4.1
7.5
9.15
5.9

NB
acc
26.23
20.96
37.78
26.39
28.61
28.61
29.44
28.29

std
5.31
6.15
8.2
6.84
6.81
4.91
8.2
6.63

KNN
acc
std
27.4
6.89
26.84
5.77
33.06
5.92
20.56
5.43
25.56
8.57
27.22
5.83
32.22
6.31
27.55
6.39

The imagined colors were recorded with the eyes closed.
In the next experiment, we eliminated the impact of the main
rhythm related to this activity by removing the alpha band,
which has been reported to be the most relevant difference
between open and closed eyes [19]. We applied a stop-band
filter (4-12 Hz) to the six classes before the feature extraction
stage. The outcomes can be seen in Table III. Despite the
decrease in mean performance for the classifiers, performance
remained above expectations based on chance alone for all six
classes.
B. Recognition of exposed and imagined colors
In this experiment, we investigated whether a machine learning algorithm could use differences or similarities between
imagined and actual colors to distinguish between them (i.e,
differences result in high levels of accuracy and similarities
results in low levels of accuracy). In this case, we ran a
binary classification scheme. For each subject, all the epochs

TABLE III
P ERCENT ACCURACY AND SD S OBTAINED FOR THE CLASSIFIERS FOR THE
RECOGNITION OF ALL THE IMAGINED AND EXPOSED COLORS (6 CLASSES
WITH REMOVAL OF THE ALPHA BAND )
subj
S1
S2
S3
S4
S5
S6
S7
Avg

SVM
acc
std
23.44
8.93
27.1
5.09
28.89
7.2
22.22
2.62
32.22
8.9
20.83
5.44
28.06
7.11
26.11
6.47

RF
acc
28.18
24.33
30.83
20.56
25.83
19.72
26.94
25.2

std
5.94
10.39
8.53
6.7
8.98
5.31
7.64
7.64

NB
acc
22.35
19.79
36.39
22.78
21.39
18.61
24.44
23.68

std
5.9
7.15
8.43
7.03
5.56
6.15
7.61
6.83

KNN
acc
std
20.13
3.52
20.42
5.71
28.33
5.68
16.67
5.71
25
6.93
22.5
6.47
23.33
7.2
22.34
5.89

recorded during either exposure to colors or the imagination
of colors were managed as single classes. Table IV shows
the percent accuracy and standard deviations for all four
classifiers after 10-fold cross-validation for all subjects. For all
four classifiers, the mean accuracy for all subjects was above
chance expectations for two classes, suggesting a difference
between exposed and imagined colors.
TABLE IV
P ERCENT ACCURACY AND STANDARD DEVIATIONS OBTAINED FOR THE
CLASSIFIERS FOR THE RECOGNITION OF EXPOSED AND IMAGINED COLORS
subj
S1
S2
S3
S4
S5
S6
S7
Avg

SVM
acc
std
87.11
6.71
82.66
6.06
97.22
1.85
75
7.52
75.83
5.56
88.06
4.35
96.39
2.64
86.04
4.96

RF
acc
86.55
85.52
98.33
76.39
77.5
89.72
98.06
87.44

std
7.9
7.57
2.68
9
5.92
3.72
2.64
5.63

NB
acc
74.56
64.79
98.33
59.17
72.5
78.61
96.39
77.76

std
8.04
7.1
2.68
5.72
5.15
6.68
2.94
5.47

KNN
acc
std
77.9
8.74
78.81
7.49
96.94
3.33
67.5
7.18
76.39
6.84
85.28
5.56
97.22
2.93
82.86
6.01

We assessed the impact of the eyes being closed during the
imagination tasks on the performances obtained in Table IV,
by applying a stop-band filter (4-12 Hz) to the signals before
the feature extraction stage. Table V shows the performance
achieved when this filter was applied to both classes. A better
performance was achieved when the alpha rhythm was retained, but all mean performances in Table V were nevertheless
above chance expectations for two classes.

TABLE V
P ERCENT ACC AND SD S GOT FOR THE CLASSIFIERS FOR THE
RECOGNITION OF EXPOSED AND IMAGINED COLORS ( WITH REMOVAL OF
THE ALPHA BAND )
subj
S1
S2
S3
S4
S5
S6
S7
Avg

RF
acc
81.28
77.36
97.78
71.11
78.89
77.5
93.33
82.46

std
5.35
6.81
2.55
6.31
6.7
4.98
4.18
5.27

NB
acc
71.48
56.97
96.11
56.67
59.44
64.72
80.28
69.38

std
7.95
5.88
1.94
3.75
7.77
6.15
6.47
5.7

KNN
acc
std
71.75
9.45
62.56
10.62
97.78
1.17
57.78
8.36
67.78
6.83
66.67
6.28
81.67
5.27
72.28
6.85

the other classifiers. Accuracy for SVM was highest for S5,
and accuracy for NB was highest for S3.
TABLE VI
P ERCENT ACCURACY AND STANDARD DEVIATIONS OBTAINED FOR THE
CLASSIFIERS FOR RECOGNITION OF THE THREE EXPOSED COLORS

subj
S1
S2
S3
S4
S5
S6
S7
Avg

SVM
acc
std
35.36
9.7
32.45
14.4
38.33
8.5
36.11
15.3
41.11
15.1
28.33
6.1
41.11
9.5
36.11
11.2

RF
acc
35.52
38.5
45.56
33.33
43.33
27.78
39.44
37.64

std
11.1
10.1
8.2
9.1
7.8
12
9.2
9.6

NB
acc
32.58
32.35
44.44
42.22
36.67
36.11
31.11
36.5

std
6.9
11.5
9.1
18.7
14.9
14.2
10.9
12.3

KNN
acc
std
32.52
10
26.21
7.3
32.78
9.2
33.33
9.8
31.11
12.1
34.44
9
32.78
12.7
31.88
10

By contrast, Table VII shows the percent accuracy and
standard deviations for the analysis of the three imagined
colors. The highest mean accuracy was achieved with SVM
(35.9%). However, the highest absolute value for accuracy
(39.44%) was that for S3 and S4 using RF. Regardless of
the classifier used, accuracy was close to chance expectations
for three classes (33%). Furthermore, taking into account the
standard deviation for the best subjects, their performances
were no better than would have been expected by chance.
These results suggest that it is more difficult to control devices
using only imagined colors.
TABLE VII
P ERCENT ACCURACY AND SD S OBTAINED FOR THE CLASSIFIERS FOR
RECOGNITION OF THE THREE IMAGINED COLORS

C. Separate recognition of exposed and imagined colors
In these experiments, we separately analyzed the epochs
for the three classes recorded during exposure to colors and
the imagination of colors, to assess the feasibility of color
recognition in both situations. For exposed colors, Table VI
shows the percent accuracy and standard deviations for all
the classifiers and for each subject analyzed. The highest
mean accuracy was obtained with random forest (37.64%). For
some subjects, accuracy was considerably higher than chance
expectations for three classes (33%). For example, S3 and S5
achieved accuracies of 45.56% and 43.33, respectively. These
individual performances were also consistent with those for

SVM
acc
std
82.13
4.13
78.18
6.25
97.5
2.05
65.28
7.88
78.61
5.86
75
3.93
92.5
4.91
81.31
5

subj
S1
S2
S3
S4
S5
S6
S7
Avg

SVM
acc
std
31.11
8.8
36.86
10.2
36.11
9.9
33.89
12.4
37.78
13.3
37.78
10.4
37.78
9.7
35.9
10.7

RF
acc
34.44
29.71
39.44
39.44
30
30.56
33.33
33.85

std
11.7
10.9
15.4
9.2
12.6
10.2
7.9
11.1

NB
acc
33.89
35.82
33.89
29.44
37.22
33.89
36.67
34.4

std
14.9
10.5
11.8
9.5
7.9
8.5
9.9
10.4

KNN
acc
std
30.56
11.5
30.16
10.8
31.67
13.1
34.44
6.3
37.78
6.8
35
11.7
37.22
7.9
33.83
9.7

We also assessed the impact of removing the alpha band
from the epochs of the three imagined colors before the feature

extraction stage. This activity was expected to be common
to these three classes. Table VIII shows the performances
obtained with this filtering. These results suggest that this brain
rhythm does not have a major impact on the performance of
the classifiers.
TABLE VIII
P ERCENT ACCURACY AND SD S OBTAINED FOR THE CLASSIFIERS FOR THE
RECOGNITION OF THE THREE IMAGINED COLORS ( WITH REMOVAL OF
ALPHA RHYTHMS )
subj
S1
S2
S3
S4
S5
S6
S7
Avg

SVM
acc
std
35
15.3
43.53
10.2
25.56
6.5
33.33
9.4
27.78
10.1
32.78
17.9
30.56
4.7
32.65
10.6

RF
acc
36.11
36.76
31.67
33.89
36.11
33.89
26.67
33.59

NB
std
8.8
13.8
10.5
13.0
11.5
10.0
6.3
10.5

acc
37.78
35.82
38.89
30.56
34.44
32.78
32.78
34.72

std
17.5
16.4
10.5
8.0
9.0
7.6
8.5
11.1

KNN
acc
std
28.33
7.6
36.21
13.3
32.22
13.8
25.56
10.9
35.56
12.3
36.67
10.5
25
9.2
31.36
11.1

VI. C ONCLUSIONS
We present here a comprehensive study of the feasibility
of recognizing both exposed and imagined colors from EEG
signals. We analyzed a dataset obtained by exposing subjects
to three colors (red, green and blue) and asking them to
imagine these same three colors. We designed a scheme
including an automatic artifact removal method, the extraction
of both temporal and frequency features, and the assessment
of a set of four classifiers of different natures. Besides,
we had a special care of the use of the available data to
avoid overestimated outcomes. We also performed additional
validations to ensure that the outcomes obtained were not
related to the main difference between exposure to colors and
the imagination of colors (eyes open and closed, respectively),
by removing the alpha band before the feature extraction stage
in the experiments concerned.
The outcomes got for all exposed and imagined colors
simultaneously suggested that the recognition of these six
classes was feasible. This performance was maintained even
if alpha rhythms were removed from all the classes. We then
designed a binary experiment assessing the ability to distinguish between exposed and imagined colors. All the classifiers
had percent accuracy values above chance expectations (even
after the removal of alpha rhythms). This suggests a difference
between the EEG signals recorded during exposure to colors
and those recorded during the imagination of colors. This may
explain the good performance in the first experiment. Finally,
we analyzed the separate recognition of imagined and exposed
colors. The outcomes suggest that exposure to colors is more
suitable than the imagination of colors for the development of
color-based BCIs, consistent with the outcomes reported in a
previous study [2] (despite their overestimation). However, the
performance was subject-dependent for color exposure. These
outcomes could potentially be improved by applying a feature
selection stage to analyze fewer features in each epoch.
Despite the promising outcomes got here, particularly for
discrimination between exposure to colors and the imagina-

tion of colors, further studies are required to improve our
understanding of this neuroparadigm and its potential use for
controlling BCIs. We will focus, in particular, on increasing
the numbers of subjects and epochs to determine whether the
performance of this method could be improved or, at least,
maintained. Furthermore, the recording of this new dataset
will keep in mind to get a suitable amount of instances for
testing deep learning algorithms too.
ACKNOWLEDGMENTS
We would like to thank Prof. S. Rasheed for providing us
with his dataset. We also thank the ERCIM for supporting this
research with a fellowship.
R EFERENCES
[1] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M.
Vaughan, “Brain–computer interfaces for communication and control,”
Clinical neurophysiology, vol. 113, no. 6, pp. 767–791, 2002.
[2] S. Rasheed, Recognition of primary colours in electroencephalograph
signals using support vector machines. PhD thesis, Università degli
Studi di Milano, 2011.
[3] J.-H. Yu and K.-B. Sim, “Classification of color imagination using
Emotiv EPOC and event-related potential in electroencephalogram,”
Optik, vol. 127, no. 20, pp. 9711–9718, 2016.
[4] X. Liu and K.-S. Hong, “Detection of primary RGB colors projected on
a screen using fNIRS,” Journal of Innovative Optical Health Sciences,
vol. 10, no. 03, p. 1750006, 2017.
[5] S. Vorobyov and A. Cichocki, “Blind noise reduction for multisensory
signals using ICA and subspace filtering, with application to EEG
analysis,” Biological Cybernetics, vol. 86, no. 4, pp. 293–303, 2002.
[6] F. Lotte, M. Congedo, A. Lcuyer, F. Lamarche, and B. Arnald, “A review
of classication algorithms for EEG-based brain-computer interfaces,”
Journal of Neural Engineering, vol. 4, pp. r1–r13, 2007.
[7] L. Guo, D. Rivero, J. Seoane, and A. Pazos, “Classification of EEG
signals using relative wavelet energy and artificial neural networks,”
in Proceedings of the first ACM/SIGEVO Summit on Genetic and
Evolutionary Computation, pp. 177–184, ACM, 2009.
[8] E. Didiot, I. Illina, D. Fohr, and O. Mella, “A wavelet-based parameterization for speech/music discrimination,” Computer Speech & Language,
vol. 24, no. 2, pp. 341–357, 2010.
[9] D. Kryze, L. Rigazio, T. Applebaum, and J.-C. Junqua, “A new noiserobust subband front-end and its comparison to PLP,” in Workshop on
Automatic Speech Recognition and Understanding (ASRU, IEEE, 1999.
[10] D. Michie, D. Spiegelhalter, and C. Taylor, Machine Learning, Neural
and Statistical Classification. Prentice Hall, 1994.
[11] R. Jensen and Q. Shen, Computational intelligence and feature selection:
rough and fuzzy approaches. IEEE Press, 2008.
[12] F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rakotomamonjy, and F. Yger, “A review of classification algorithms for EEGbased brain–computer interfaces: a 10 year update,” Journal of neural
engineering, vol. 15, no. 3, p. 031005, 2018.
[13] I. H. Witten, E. Frank, M. A. Hall, and C. J. Pal, Data Mining: Practical
machine learning tools and techniques. Morgan Kaufmann, 2016.
[14] C. Burges, “A tutorial on support vector machines for pattern recognition,” Data mining and knowledge discovery, vol. 2, no. 2, pp. 121–167,
1998.
[15] K. Bennett and C. Campbell, “Support vector machines: hype or hallelujah?,” ACM SIGKDD Explorations Newsletter, vol. 2, no. 2, pp. 1–13,
2000.
[16] A. Schlögl, F. Lee, H. Bischof, and G. Pfurtscheller, “Characterization
of four-class motor imagery EEG data for the BCI-competition 2005,”
Journal of neural engineering, vol. 2, p. L14, 2005.
[17] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp. 5–
32, 2001.
[18] L. Rokach, Pattern Classification Using Ensemble Methods. World
Scientific, 2009.
[19] R. J. Barry, A. R. Clarke, S. J. Johnstone, C. A. Magee, and J. A.
Rushby, “EEG differences between eyes-closed and eyes-open resting
conditions,” Clinical Neurophysiology, vol. 118, no. 12, pp. 2765–2773,
2007.

