Robot Navigation using Brain-Computer Interfaces
Athanasios Vourvopoulos

Fotis Liarokapis

Interactive Worlds Applied
Research Group (iWARG)
Coventry University
Coventry, UK
vourvopa@uni.coventry.ac.uk

Interactive Worlds Applied
Research Group (iWARG)
Coventry University
Coventry, UK
F.Liarokapis@coventry.ac.uk

Abstract— This paper identifies the user’s adaptation on
brain-controlled systems and the ability to control braingenerated events in a closed neuro-feedback loop. To
accomplish that, a working system has been developed
based on off-the-shelf components for controlling a robot in
both the real and virtual world. Using commercial BrainComputer Interfaces (BCIs) the overall cost, set up time
and complexity can be reduced. The system is divided in
two prototypes based on the headset type used. The first
prototype is based on the Neurosky headset and it has been
tested with 54 participants. The second prototype is based
on the Emotiv headset including more sensors and
accuracy. Initial evaluation results indicate that robot
navigation through commercial BCIs can be effective and
natural.
Keywords – brain-computer interfaces, human-robot
interaction, serious games, virtual worlds.

I.

INTRODUCTION

Brain-computer
interfaces
(BCIs)
are
communication devices which enable users to send
commands to a computing device using brain activity
only [1]. This technology is a rapidly growing field of
research and an interdisciplinary endeavor. Research into
BCIs involves knowledge of disciplines such as
neuroscience, computer science, engineering and clinical
rehabilitation. Brain-controlled robots and serious games
can be used for a wide range of applications from
modern computer games [2], prosthetics and control
systems [3] through to medical diagnostics [4]. Although
research on the field started during the 1970’s only the
last few years it became possible to introduce BCIs
mostly through commercial headsets for computer games
and other simulations.
There are three categories of BCIs: invasive,
partially-invasive and non-invasive. With invasive BCIs
the signals are recorded from electrodes implanted
surgically over the brain cortex, into the grey matter of
the brain during neurosurgery. Partially-invasive BCIs
are implanted inside the skull but rest outside the brain
rather than within the grey matter. Non-invasive BCIs
operate by recording the brain activity from the scalp
with EEG sensors attached to the head on an electrode
cap or headset without being surgically implanted.
Electroencephalography (EEG) is the most studied
non-invasive interface, mainly due to its portability, ease

of use and low set-up cost [5]. The raw EEG is usually
described in terms of frequency ranges: Gamma (γ)
greater than 30 Hz, Beta (β) 13-30 Hz, Alpha (α) 8-12
Hz, Theta (θ) 4-8 Hz, and Delta (δ) less than 4 Hz [6].
Delta (δ) waves with the lowest frequencies of all the
brainwaves are most apparent in deep sleep states, where
conscious brain activity is minimal. Theta (θ) waves
appear in a relaxed state and during light sleep and
meditation. Alpha (α) waves are typically associated with
meditation and relaxation, more so than any other waves.
Beta (β) waves are connected to alertness and focus.
Gamma (γ) waves can be stimulated by meditating while
focusing on a specific object.
Early research on the area began from UCLA in the
1970’s introducing the term ‘Brain-Computer Interface’
[7], [8]. It was mainly focused on neuro-prosthetics
which aimed at restoring damaged hearing, sight and
movement. Over the past decades, researchers developed
systems that carry out movements of robotic arms
handled by monkeys getting biofeedback. This allowed
to map patterns of the neural activity of the brain and to
develop novel algorithms. Around 20 years later,
researchers at University of California, Berkeley,
decoded neuronal firings to reproduce images seen by
cats. They decoded signals from the targeted brain cells
of the cats’ brain, and reconstructed images of what the
cats saw [9].
In the 1980s, researchers at Johns Hopkins
University found a mathematical relationship between
the electrical responses of single motor-cortex neurons in
rhesus macaque monkeys as well as the direction that
monkeys moved their arms [10]. Later experiments using
rhesus monkeys researchers reproduced reaching and
grasping movements in a robotic arm [11]. By mid 90’s
invasive BCI’s have started applied into humans. One
example of invasive BCI is the direct brain implant to
treat non-congenital blindness. Sixty-eight electrodes
were implanted into the patient’s visual cortex and
succeeded in producing phosphenes, the sensation of
seeing light without light actually entering the eye,
through cameras that were mounted on glasses to send
signals to the implant [12]. In 1998, researchers at Emory
University in Atlanta installed a brain implant into a
human that produced signals of high enough quality to
simulate movement [13]. More recently, researchers
succeeded in building a BCI that reproduced owl
monkey movements while the monkey operated a

joystick or reached for food [14]. Moreover, in 2004,
researchers from Washington University in St Louis
enabled a teenage boy to play Space Invaders using his
Electrocorticography (ECoG) implant, a partially
invasive BCI device [15].
The past few years’ various ways in controlling
robotic platforms (mainly electrical wheelchairs or
robotic arms) for people suffering from a diverse range
of impairments were investigated [16], [17]. Patients
suffering from ‘locked-in syndrome’, spinal cord injury
or damaged regions of the brain responsible for the body
movement have used BCIs focusing on motor
neuroprosthetics in order to rehabilitate or assist their
interaction with the outside world [18]. With the launch
of commercial headsets, as an alternative gaming
controller, BCIs appeared in the computer gaming
domain. This allowed researchers to start developing
various novel applications with relatively low cost noninvasive EEG equipment and software development kits
(SDKs). This technology boosted the BCI’s in games
oriented research with main target medical applications
and brain rehabilitation through the use of serious games
and virtual worlds. Furthermore, gaming technology has
been assisted by virtual and augmented reality systems,
making hybrid BCI systems for enhancing the user
experience, study and improvement of brain-computer
interaction [2].
This research focuses on how a robot operated
through brainwaves can overcome the kinetic constraints
of the user. It investigates ways in extracting valuable
information from user’s brain activity by interacting with
both real world objects and virtual world environments.
The experimental prototype uses the basic movement
operations of a Lego Mindstroms NXT Robot. There are
two versions of this prototype, taking readings from the
users' brain electrical activity in real-time performance.
The first version is made by using a single dry sensor
headset from Neurosky using the attention levels of the
user. The second version is using a 14 wet sensor headset
from Emotiv taking readings not only from EEG signals
but also from facial expressions, eye movement and head
tilt enabling the user to fully control the robot with the
required user training. Initial evaluation results indicate
that robot navigation through commercial BCIs can be
effective and natural.
The rest of the paper is structured as follows.
Section II provides a brief overview of similar systems
and section III presents an overview of our system.
Sections IV and V present the two experimental
prototypes that were implemented. Finally, section VI
illustrates initial evaluation results and section VII
presents conclusions and future work.
II.

BACKGROUND

Non-invasive BCI research methods in serious
games development are usually oriented in the medical
domain rather than entertainment. An early study
developed an internet game linked to a BCI. The system
translated real-time brain activities from prefrontal

cortex (PFC) or hippocampaus (CA1) of a rat into
external device control commands and used them to
drive an internet game called RaviDuel [19]. Another
BCI based 3D game measured user’s attention level to
control a virtual hand’s movement, making use of 3D
animation techniques. This system has been developed
for training those who suffering from Attention Deficit
Hyperactivity Disorder (ADHD) [20]. In another study,
researchers focused on the design and implementation of
a game capable of moving an avatar from a tennis game
using only brain activity [21]. This can assist people with
diseases involving movement difficulties for controlling
keyboard and mouse of a computer. To achieve this, the
mu (μ) rhythm of brain activity has been used [22].
An EEG pattern recognition system has been
designed to adapt a serious game by comparing
recognition rates for experimental purposes without any
traditional controllers. Their proposed Support Vector
Machine (SVM) algorithm for classification is compared
with other algorithms, improving the recognition rate
[23]. A Quantitative and Qualitative Study in Self-Paced
Brain-Computer Interaction with Virtual Worlds showed
that, without training, roughly half of the subjects were
able to control the application by using real foot
movements and a quarter were able to control it by using
imagined foot movements. The application consisted of
an interaction with a virtual world inspired by the “Star
Wars” movie. Participants were asked to control the
take-off and height of a virtual spaceship using their
motor-related brain activity [24].
III.

SYSTEM OVERVIEW

The basic hardware components of the project
include two EEG headsets: the Neurosky (Mindset and
Mindwave), and the Emotiv Epoc EEG headset.
Additionally, a LEGO Mindstorms NXT robot, a desktop
computer, an Ultra-Mobile PC (UMPC) and a Netbook
PC were used. The software components include a
computer simulation with a 3D reconstruction of the
NXT robot, a JAVA application for the physical robot
and a client/server program that establishes connection
with both the physical robot and the simulation.
The Neurosky headsets have been used to extract the
Attention and Meditation levels of the user. The headset
is calculating the Raw EEG signals to produce the
“eSense Meters” [25] based on an algorithm patented
from Neurosky. The patterns of the electrical activity are
analyzed with the help of specialized algorithms (feature
extraction and classification) by converting the EEG
signals into control commands.
The Neurosky headset is using a single dry sensor
attached to the forehead outside the cerebral cortex in the
frontal lobe of the brain being responsible for the
attention level and short-term memory tasks. The Emotiv
EPOC Headset is using 14 saline sensors being able not
only to detect brain signals but also user’s facial
expressions, eye movement and head position through a
2 axis gyroscope. The various facial expressions referred
to as “Expressiv” by Emotiv, levels of engagement,

frustration, meditation, and excitement as “Affectiv”;
and training and detection of certain cognitive neuroactivities such as “push”, “pull”, “rotate”, and “lift” as
“Cognitiv” [26].
By measuring these brainwaves, these headsets can
infer which mental state the user is. Providing all the
brain activity details on a computer, the user is able to
receive a closed neuro-feedback loop. This is a type of
biofeedback that can be useful for the analysis and
development of tools that can help people with severe
brain damage, for brain rehabilitation or enable them to
control a computer program or robotic device.
For the robot, the Lego Mindstorms NXT kit was
used with a Java Virtual Machine (JVM) installed. This
tiny JVM for ARM processors (called LeJOS) was
ported to the NXT robot in 2006. The main advantage of
LeJOS NXJ is that it provides the extensibility and
adaptability of the JAVA language [27]. The robot
version that was used includes a 32-bit AT91SAM7S256
(ARM7TDMI) main microprocessor at 48 MHz (256 KB
flash memory, 64 KB RAM). It includes five different
types of sensors such as: servo motors, touch, light,
sound and ultrasonic.

Figure 1 Overview of the system

The touch sensor has been implemented in the
prototype to allow collision detection with the real
environment. The light sensor is also used as a height
detector on this model [28]. An abstract way that this
prototype works with the headsets is illustrated on Figure
1, were the user is visually stimulated; the raw data is
calculated on the headsets chip and then sent to the
dedicated computer. Afterwards, a client/server program
instructs the robot to move based on the user brainactivity (or muscular movement) both on real and virtual
world. The robot terminates the connection when it hits
an obstacle or is stopped by the user.
IV.

FIRST PROTOTYPE (NEUROSKY)

Two types of Neurosky headsets were used for this
research (see Figure 2). The main difference between
them is that the “Mindset” (left) is a complete headset
with speakers and microphone transmitting data on
bluetooth while the “Mindwave” (right) comes without a
headset and transmits data using radio frequency.

Figure 2 Neurosky Mindset (left), Mindwave (right)

Both headsets use the same ThinkGear
Communications Driver (TGCD) library from
Neurosky’s Development Tools. After establishing
connection with the headset, both the physical and virtual
robot connects on the client/server application to send
instructions through sockets. The first step in
communicating is to establish a connection (which has
an initiator and a receiver). The receiver waits for a
connection from the initiator.
The initiator is the dedicated PC running the
client/server program and the receiver is the NXT Robot
including both the physical and the computer simulation.
The application running in the robot waits for Bluetooth
connection; when the PC program establishes connection
with the headset, it initiates a connection with the robot
and the simulation to send instructions in the
client/server architecture. When the connection has been
established, both ends of the connection can ‘open’ input
and output streams and read/write data. The computer
sends integers (in the range 0 to 100) to the robot
depending on the attention levels of the user. Based on
those inputs the robot performs actions by sending
feedback back to the computer. In this case, if the
bumper sensor has been pressed the session is terminated
in both ends.
V.

SECOND PROTOTYPE (EMOTIV)

Based on the first prototype, a more advanced
version has been created, with the robot performing basic
maneuvering (moving forwards, backwards, turn left and
turn right) using the Emotiv Epoc headset. The basic idea
is to use the Cognitive functions (brainwaves) to move
the robot forwards/backwards, and the Expressive
functions to steer the robot left/right when the user blinks
accordingly. The Emotiv Epoc Headset is a neuro-signal
acquisition and processing wireless neuro-headset with
14 wet sensors (and 2 reference sensors) being able to
detect brain signals and user’s facial expressions,
offering optimal positioning for accurate spatial
resolution [4]. An integrated gyroscope generates
positional information for cursor and camera controls,
connected wirelessly through a USB dongle and comes
with a lithium battery providing 12 hours of continuous
use. The sixteen (14 plus 2 reference) sensors are placed
on the international 10-20 system [11], an internationally
recognized method which describes the electrode

placement on the scalp for EEG tests or experiments as
illustrated in Figure 3.

the end using the Emotiv headset. An example
screenshot of the game is illustrated in Figure 4.

Figure 4 The robot navigating through the maze
Figure 3 International 10-20 system placement and number
designation

The system is fully operational and it relies on a
combination of Cognitive and Facial/Muscular functions
[29]. In terms of the implementation, the Emotiv
Development Kit was used connecting the Emotiv Epoc
headset to the Emotiv control panel to create and train a
new user profile. The Cognitive functions (brainwaves)
are used to move the robot forwards/backwards, and the
Expressive functions to steer the robot left/right when the
user blinks accordingly. Unlike the Neurosky Mindset
(which uses only one dry sensor and does not require
user training), Emotiv needs a unique user profile to be
trained to map users’ brain-patters. In a training session
no more than 1 hour, user’s skills increased
approximately up to 45% for the forward & backward
moves and around 10% for left & right.
Training the profile requires practice and
familiarization, especially when the user needs to train
more than two actions as it is easy to get distracted from
outside stimuli and ‘confuse’ the training process of the
users real ‘intentions’. To take control of the events the
EmoKey application was used connected with Emotiv
Control Panel to generate keyboard events for each
identified and trained thought. After that the EmoKey
application transferrs these events to a key-reading
client/server application with each event being treated as
‘thought’, passing it through a socket connection to the
physical robot and the computer simulation.
Moreover, the 3D environment has been designed
using the ‘Unity 3D’ game engine with a 3D
reconstruction of the robot, navigating through a simple
maze. Unity 3D is an integrated development
environment for computer game design and runs on
Microsoft Windows and Mac OS X [30]. For this
simulation both C# and Javascript programming
languages were used. On the computer simulation, the
user navigates the robot through the maze and reaches

It is worth-mentioning, that for this prototype an
Ultra Mobile PC (UMPC) was used attached to the robot
for movement independency as shown on Figure 12. This
UMPC carries an Intel Core Solo U1500 processor at
1.33 GHz, 1GB DDR2 RAM, 32GB Solid state drive,
running Windows.

Figure 5 NXT robot with UMPC attached

The Emotiv version seems to have the appropriate
attributes needed for a robust multimodal BCI-Robot
system by making use of all the available user inputs.
VI.

INITIAL EVALUATION

To identify how fast the users adapt on braingenerated events, an initial evaluation has been
conducted for three days in a national technology
exhibition centre. Neurosky headset was used for this
evaluation, as it is easier to set-up and does not require
any user training. The testing was hosted at “The Gadget
Show Live 2011” at the NEC, Birmingham and 54
participants tested the prototype. The Gadget Show Live
is UK's ultimate consumer electronics and gadgets event
with hundreds of exhibitors from all over the world and
thousands of visitors from all over the UK. This was a

unique opportunity to gather data and feedback for
further analysis and improvements of the application.
After each testing session, users completed a
questionnaire, collecting initial feedback regarding the
overall navigation experience with the Neurosky
“Mindset” and the robot.
Qualitative Evaluation
For this part, participants had to complete a quick
and easy task. That was to move the robot forwards,
accelerate up to a point and finally try to decelerate and
stop it before the end of a small track. All participants
used for the first time their brainwaves to interact with
the robot. However, even if the Neurosky headset does
not require any training, for some users it was quite a
unique experience and hard to adapt in taking control
straight away. Some of themfound it a strange experience
to concentrate and tele-operate a robot with only their
brain power. Allowing them a few minutes to familiarise
participants started to get control and adapt to the
prototype system. The majority of them reported that
they have been amazed from the fast response (latency
around 1 sec) of the robot as they tried to move it
forwards and then stop it.
A common problem that was reported was external
distractions which produced unwanted results from the
lack of concentration. Some participants found it difficult
to stay focused to their task with the external stimuli
generated from the people standing around. Another
issue that was recorded was the initial excitement of the
participants when they saw the robot moving, boosting
their attention level as a result to make the robot
movement unstable and not being able to stop it. A few
participants reported that it was easy to navigate at first,
but as soon as you ‘feel success and thinking about it, the
robot starts to move again’. That made it difficult for
some to stay focused and relax in order to stop the robot
and complete the task. The task showed that through the
continuing neuro-feedback from the system, participants
started to adapt and increase their control levels
relatively fast (within 5 minutes).

Figure 6 demonstrates that the participant ages
where almost equally distributed for each age group and
as a result the results are more representative.

A.

Figure 6 Age group

Since the demonstration took part in the Future Tech
Zone of the expedition, men and young children was the
vast majority of the participants. As a result the feedback
gathered was mainly from males (see Figure 7).

Figure 7 Gender differences

Interesting concept
Could be very useful
Lots of potential
Can increase people’s
concentration level
Can
help
improve
disabled people’s lives
Can help for brain
rehabilitation
by
triggering the motor
cortex of the brain
The feel of controlling it
with my mind was
awesome
Table 1

Being able to move
left/right
Need some indication
on the PC as to the
level of thought
Needed
less
distractions
Difficult to keep the
robot stationary
Too many outside
stimuli
Hard to remain calm

Positive and negative comments that have been reported
from the users

B.

Quantitative Evaluation
This section presents the distribution of the
participant responses based on their experience with the
robot. Each chart has a descriptive label and the answers
are based on a 7-point scale. Standard deviation (σ
sigma) was used to measure the variation from the
average user responses:

eq. 1
From the square root of the Variance:
eq. 2

Averaging the squared differences from the Mean:

eq. 3

Mean
(average)
Variance
Standard
Deviation

Chart 3
N=54
4.2

Chart 4
N=54
4.1

Chart 5
N=54
4.1

Chart 6
N=52
4.2

Chart 7
N=53
4.5

1.4

1.6

1.5

1.6

3.1

1.2

1.2

1.2

1.2

1.7

Table 2

Average and standard deviations

A plot of a normal distribution (or bell curve) has a
width of 1 standard deviation. Analyzing the statistical
charts below, it is possible to identify a normal
distribution on the users’ choices with standard deviation
of 1.2 as shown on Table 2.
Figure 9 Application responsiveness to actions that have been initiated

Figure 9 illustrates the responsiveness of the robot in
relation to the actions that the participants wanted to
perform. The majority of them answered between 3 and
5, an average of 4.2 and again a standard deviation of 1.2
(see Table 2).

Figure 8 Ability to control events

Figure 8 illustrates the number of participants that
were almost (but not completely) able to control the
events that have been generated from their brainwaves
(to the robot) with an average of 4.2 and standard
deviation 1.2 (see Table 2).
Figure 10 User interactions with the robot

Figure 10 illustrates the effectiveness of the humanrobot interaction. It demonstrates how the participants
felt when interacting with the robot to perform certain
tasks (i.e. move forward and finish the track). The
deviation of this chart is the same with Figure 8 and
Figure 9 at 1.2 with almost the same average of 4.1 (see
Table 2).

Figure 11 How natural was the mechanism which controlled the robot

Figure 11 illustrates how natural was the mechanism
which controlled the robot. Results showed almost
identical distribution with the previous Figures and a
deviation of 1.2 (see Table 2).

generated events in a closed neuro-feedback loop. Initial
evaluation results indicate that participants successfully
managed to interact with the system on a basic level
despite the difficulties of the outside stimuli and the short
period of time they had for familiarisation of the BCI
technology. Participants managed to adapt in a limited
amount of time (a few minutes) in a closed loop
interaction with the robot without any previous
experience of a similar device and BCI equipment
making this research one the few if not the only one that
tested a commercial BCI headset in a wide and diverse
user group. This may have a profound effect on future
user interaction with machines.
Moreover, this research initiates a new generation of
serious non-clinical applications for brain rehabilitation
with the help of robotic systems or through virtual
worlds and serious games. Initial results confirm that an
extended user testing is necessary (from different user
groups) operating a more robust system and a more
detailed comparison of the existing commercial BCI
headsets. Our target is to develop a more user-friendly
BCI system requiring minimal training and mental load
with fewer distractions and outside stimuli, helping the
users to efficiently control the system. This kind of
research can transform the way we live enhancing human
potential through modern technology.
Future work will include the combination of
different readings with the use of more sophisticated
non-invasive BCI devices equipped with more electrodes
and sensors. The development of a virtual environment
for brain-user interaction will allow for extensive
monitoring and better testing. Furthermore, a large
sample of users will be tested to qualitatively measure
the effectiveness of the system. Finally, finding ways for
additional physiological data extraction, including body
posture and facial expression, determine that way user’s
intentions. This opens up new opportunities and
challenges in brain-computer interaction and pervasive
computing. Identifying and extracting features from
brainwaves (EEG signals) and multimodal systems in a
closed feedback loop through serious games.
ACKNOWLEDGEMENTS

Figure 12 How compelling was the sense of moving the robot

Finally, Figure 12 illustrates how compelling was
the participants sense of moving the robot through space.
The deviation increased to 1.7 with an average of 4.5
(see Table 2).

The authors would like to thank the Interactive
Worlds Applied Research Group (iWARG) members for
their support and inspiration. Videos that illustrate the
operation of the system can be found at:
http://www.youtube.com/user/vourvopa.
REFERENCES

VII.

CONCLUSIONS AND FUTURE WORK

This paper presented an affordable human-robot
interaction system with commercial and non-invasive
BCI headsets using off-the-shelf components for robotic
tele-operation. Two prototypes have been experimental
tested to discover how easy it is to control brain-

[1]

Wolpaw, J.R, Birbaumer, N., et al. “Brain–computer
interfaces for communication and control”, Clinical
neurophysiology: official journal of the International
Federation of Clinical Neurophysiology, 133(6): 767791, 2002.

[2]

[3]

[4]

[5]

[6]
[7]

[8]
[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

Lecuyer, A., Lotte, F., et al. “Brain-Computer Interfaces,
Virtual Reality, and Videogames”, IEEE Computer,
IEEE Computer Society, 41(10): 66-72, 2008.
Loudin, J.D., Simanovskii, D.M., et al. “Optoelectronic
retinal prosthesis: system design and performance”,
Journal of Neural Engineering, IOP Publishing, 4, 72-84,
2007.
Levine, S.P., Huggins, J.E., et al. “A direct brain
interface based on event-related potentials”, IEEE
Transactions on Rehabilitation Engineering, IEEE
Computer Society, 8(2):180-185, 2000.
Niedermeyer,
E.,
Lopes
da
Silva,
F.H.
“Electroencephalography: Basic Principles, Clinical
Applications, and Related Fields”, Lippincott Williams
& Wilkins, 140, 2004.
Sanei, S., Chambers, J.A. “EEG Signal Processing”,
Wiley, July, 2007.
Vidal,
J.J.
“Toward
direct
brain-computer
communication”, Annual review of biophysics and
bioengineering, Vol. 2, 157-80, 1973.
Vidal, J.J. “Real-Time Detection of Brain Events in
EEG”, IEEE Proceedings, 65(5): 633-641, 1977.
Stanley, G.B., Li, F.F., Dan, Y. “Reconstruction of
natural scenes from ensemble responses in the lateral
geniculate nucleus”, Journal of Neuroscience, 19(18):
8036-8042, 1999.
Georgopoulos, A.P., Lurito, J.T., et. al. “Mental rotation
of the neuronal population vector”, Science, 243(4888):
234-246, 1989.
Carmena, J.M., Lebedev, M.A., et al. “Learning to
control a brain-machine interface for reaching and
grasping by primates”, PLoS biology, 1(2): E42, 2003.
Vision Quest, A Half Century of Artificial-Sight
Research has Succeded. And Now This Blind Man Can
See,
Wired
Magazine,
Available
at:
[http://www.wired.com/wired/archive/10.09/vision.html,
Accessed at: 10/03/2012,
Kennedy, P.R., Bakay R.A. “Restoration of neural output
from a paralysed patient by a direct brain connection”,
Neuroreport, 9(8): 1707-1711, 1998.
Wessberg, J., Stambaugh, C.R., et al. “Real-time
prediction of hand trajectory by ensembles of cortical
neurons in primates”, Nature, 408, 361-365, 2000.
Fitzpatrick, T. “Teenager moves video icons just by
imagination”, Washington University in St Louis,
Available
at:
[http://news.wustl.edu/news/Pages/7800.aspx], Accessed
at: 10/03/2012.
McFarland, D.J., Wolpaw, J.R. “Brain-Computer
Interface Operation of Robotic and Prosthetic Devices”,
Computer, IEEE Computer Society, 41(10): 52-56, 2008.
Philips, J., del R. Millan, J. “Adaptive Shared Control of
a Brain-Actuated Simulated Wheelchair”, Proc. of the
10th Int’l Conference on Rehabilitation Robotics, IEEE
Computer Society, 408-414, 2008.
Kennedy, P.R., Bakay, R.A. “Restoration of neural
output from a paralyzed patient by a direct brain
connection”, Neuroreport 9(8): 1707-1711, 1998.
Lee, U., Han, Han, S.H., et al. “Development of a
Neuron Based Internet Game Driven by a BrainComputer Interface System”, Proc. of the 2006 Int’l
Conference on Hybrid Information Technology, IEEE
Computer Society, Vol. 02, 600-604, 2006.
Jiang, L., Guan, C., Zhang, Wang, C., Jiang, B., “Brain
computer interface based 3D game for attention training

[21]

[22]

[23]

[24]

[25]

[26]

[27]
[28]

[29]

[30]

and rehabilitation”, Proc. of the 6th IEEE Conference on
Industrial Electronics and Applications (ICIEA), IEEE
Computer Society, Beijing, 124-127, 2011.
Lopetegui, E., Zapirain, B.G., Mendez, A. “Tennis
computer game with brain control using EEG signals”,
Proc. of the 16th Int’l Conference on Computer Games
(CGAMES), 228-234, 2011.
Mattiocco, M., Babiloni, F. et al. “Neuroelectrical source
imaging of mu rhythm control for BCI applications”,
Proc. of the 28th Annual Int’l Conference of the IEEE on
Engineering in Medicine and Biology Society, IEEE
Computer Society, 980-983, 2006.
Ahn, J.S., Lee, W.H., Using EEG pattern analysis for
implementation of game interface. Proc. of the 15th
International Symposium on Consumer Electronics
(ISCE), IEEE Computer Society, Singapore, 348-351,
2011.
Lotte, F., Renard, Y., Lécuyer, A. “Self-Paced BrainComputer Interaction with Virtual Worlds: A
Quantitative and Qualitative Study - Out of the Lab”,
2008.Proc. of the 4th Int’l Brain Computer Interface
Workshop and Training Course, 2008.
NeuroSky’s eSense™ Meters and Detection of Mental
State,
Available
at:
[http://company.neurosky.com/files/neurosky_esense_wh
itepaper.pdf], Accessed at: 10/03/2012.
Emotiv EPOC Software Development Kit, Available at:
[http://www.emotiv.com/store/hardware/299/], Accessed
at: 10/03/2012.
LeJOS, Java for Lego Mindstorms, Available at:
[http://lejos.sourceforge.net/], Accessed at: 10/03/2012.
Lego
Mindstorm
NXT,
Available
at:
[http://www.legomindstormsnxt.co.uk/lego-nxt.html],
Accessed at: 10/03/2012.
Vourvopoulos, A., Liarokapis, F. “Brain-controlled NXT
Robot - Tele-operating a robot through brain electrical
activity”, Proc. of the 3rd Int’l Conference in Games and
Virtual Worlds for Serious Applications, IEEE Computer
Society, Athens, Greece, 4-6 May, 140-143, 2011.
Unity
3D
Game
Engine,
Available
at:
[http://www.unity3d.com/], Accessed at: 10/03/2012.

