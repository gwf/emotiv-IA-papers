Classification of Primitive Shapes Using Brain-Computer Interfaces
Ehsan Tarkesh Esfahani, V. Sundararajan
Department of Mechanical Engineering, University of California Riverside
Riverside, CA

Abstract. Brain-computer interfaces (BCI) are recent developments in alternative technologies of
user interaction. The purpose of this paper is to explore the potential of BCIs as user interfaces
for CAD systems. The paper describes experiments and algorithms that use the BCI to
distinguish between primitive shapes that are imagined by a user. Users wear an
electroencephalogram (EEG) headset and imagine the shape of a cube, sphere, cylinder, pyramid
or a cone. The EEG headset collects brain activity from 14 locations on the scalp. The data is
analyzed with independent component analysis (ICA) and the Hilbert-Huang Transform (HHT).
The features of interest are the marginal spectra of different frequency bands (theta, alpha, beta
and gamma bands) calculated from the Hilbert spectrum of each independent component. The
Mann-Whitney U-test is then applied to rank the EEG electrode channels by relevance in five
pair-wise classifications. The features from the highest ranking independent components form
the final feature vector which is then used to train a linear discriminant classifier. Results show
that this classifier can discriminate between the five basic primitive objects with an average
accuracy of about 44.6% (compared to naïve classification rate of 20%) over ten subjects
(accuracy range of 36-54%). The accuracy classification changes to 39.9% when both visual and
verbal cues are used. The repeatibility of the features extraction and classification was checked
by conducting the experiment on 10 different days with the same participants. This shows that
the BCI holds promise in creating geometric shapes in CAD systems and could be used as a
novel means of user interaction.
Keywords— EEG, Brain Computer interface, visual imagery, user interfaces

http://dx.doi.org/10.1016/j.cad.2011.04.008

1. INTRODUCTION
The traditional mouse and keyboard dominate the interfaces for computer-aided design (CAD)
systems. However, the emergence of technologies such as pen-based systems, haptic devices and
speech recognition software has provided alternative means of interaction [1-3]. These
alternatives seek to reduce the number of steps to activate commands for creating or editing
CAD models and to obtain faster feedback from users [4].
A more recent commercial technology is the brain-computer interface. New developments in
brain computer interfaces (BCI) have made it possible to use human thoughts in virtual
environments [5-7]. BCIs create a novel communication channel from the brain to an output
device bypassing conventional motor output pathways of nerves and muscles. The goal of a BCI
system is to detect and relate patterns in brain signals to the subject’s thoughts and intentions.
Currently, noninvasive BCIs are mostly based on recording electroencephalography (EEG)
signals by placing electrodes on the scalp. Several recent prototypes already enable users to
navigate in virtual scenes, manipulate virtual objects or play games just by means of their
cerebral activity [8, 9]. Some recent studies have also investigated EEG signals in problem
solving and creative design process [10-13].
In this paper, we explore the application of brain computer interfaces in CAD environments.
The overall objective of such interfaces is to use the designer’s brain activity to create and edit
CAD geometry. For a BCI system to succeed as a CAD interface, it must at least allow the basic
interactions that a mouse-and-keyboard system (the traditional CAD interface) does. A typical
CAD system allows users 1) to create geometrical shapes, 2) to edit shapes by resizing or by
operations such as Booleans, sweeps and extrusions amd 3) to move shapes by rotations and
translations. In addition, a CAD system provides extensive viewing capabilities such as zooms

http://dx.doi.org/10.1016/j.cad.2011.04.008

and rotations. In order to operate a CAD system with BCI, the following issues should be
addressed:


Geometry representation: Before designing a 3D geometrical model of an object, a user
has a mental representation of the shape. Is it possible to capture and use this visual
imagery to construct a 3D CAD model? Can the BCI capture the shape and its attributes
such as dimensions and proportions accurately enough to generate CAD shapes?



Geometrical editing: Can the BCI be used to edit the shapes that have been created? For
example, can it be used to perform Boolean operations, sweeps, or lofts accurately?



Object manipulation: To edit a design, it is important to move, scale and rotate an object
in a desired direction. Can BCIs be used to precisely locate and orient objects?



Error corrections: Can we get feedback from users’ thoughts to correct errors in the
model generated by the BCI interface? For example, can we perform an “undo”
command by getting emotional or other forms of cognitive feedback?



Training period: How much training data is needed per subject to train different BCI
command? Does the training have to start de novo for each subject or can we establish
baseline classifiers for the various operations that can then be fine-tuned to each user by a
customization procedure?

Some of these issues such as moving or rotating object via brain signals have been well studied
by other research groups and are discussed in the next section. The aim of this paper is to
investigate the possibility of using BCI for geometry representation. The objectives of the
current study are as follow: (i) to investigate the feasibility of using BCI to distinguish between
five primitive shapes (cubes, cylinders, spheres, cones and pyramids) imagined by the designer
and (ii) to check the stability of the results over time.

http://dx.doi.org/10.1016/j.cad.2011.04.008

In order to achieve these objectives, we have performed two sets of experimental studies. The
first experiment is a pilot study for the classification of visual object imagery while the second
experiment addresses the robustness of classification. In each experiment, the subject performs a
series of visual object imagery tasks after receiving a visual or text cue. The recorded signals are
then analyzed by Independent Component Analysis and Hilbert-Huang Transformation (for
feature generation) and linear discriminant classifier (for Classification).
In section 2, we briefly review the current research on mental imagery and, specifically, visual
imagery. Since the analysis methods are the same in both experimental studies, we first describe
the analytic methods used for converting brain activity to classified objects in Section 3. Section
4 and 5 discusses each of the experiments and their results. Finally, we summarize outcomes of
the study in Section 6.

2. Mental Imagery
Mental imagery is defined as “an experience that resembles perceptual experience, but which
occurs in the absence of the appropriate stimuli for the relevant perception” [14]. Mental imagery
arises when perceptual information is recalled from memory or previous perceptual input [15].
For every type of perception, there is a corresponding type of imagery. Visual mental imagery
(‘seeing with the mind’s eye’), auditory imagery and kinesthetic imagery - commonly called
‘motor imagery’- are the main categories of mental imagery that can be considered in braincomputer interfaces [16].
Motor imagery has been the primary focus of most BCIs [17-24]. Here signals are obtained
during imagined motor responses. For example, Brunner et al. [17] classifies imagined
movements of the right and left limbs. Kubler et al. [18] use BCI to capture desired motor
movements for patients that suffer from paralysis. Lemm [19] describe a system to classify
http://dx.doi.org/10.1016/j.cad.2011.04.008

imaginary hand movements using the -rhythm (8-13 Hz) EEG signals. It has been shown that
using EEG based BCI, it is possible to control the 2D motion of a cursor [20-22] and rotate an
imaginary object along various axes [23, 24].
Visual imagery can be classified into two subcategories: object-based imagery (e.g. of shapes
and colors) and spatial imagery (such as location and spatial relations) [25]. During visual mental
imagery, neural representations of a visual entity are re-activated endogenously from long-term
memory and maintained in visuo-spatial working memory to be inspected and transformed [25,
26].
Visual mental imagery consists of two main stages: 1) image generation 2) image maintenance
[15]. It has been estimated that the average duration of generated image is about 250 ms [27].
Thereafter, another mental mechanism (image maintenance) is involved in keeping the internal
representation of generated image. The neural processes underlying each stage are still unclear.
However, fMRI studies have shown that the middle-inferior temporal region of the brain,
especially of the left hemisphere, is involved in image generation [28]. Another study by Mellet
et al. [29] reported activity in right occipital cortex during generation-maintenance of the mental
image. Cornoldi et al. [30] identified six fundamental characteristics which affect the vividness
of mental image or, in other words, the maintenance of mental images. These characteristics are:
specificity, richness of detail, color, saliency, shape and contour and context. Although the
individual contributions of each characteristic vary; shape and contours are shown to be the best
predictor of vividness [31].
Visual imagery has been mostly studied with brain imaging techniques such as positron emission
tomography (PET) or functional magnetic resonance imaging (fMRI) rather than EEG analysis,
because EEG signals have a poor spatial resolution that makes it difficult to detect detailed visual

http://dx.doi.org/10.1016/j.cad.2011.04.008

imagery. However, even though exact geometry may be difficult to detect, it may be possible to
determine the features of the geometry such as roundness, sharpness, symmetry and curvature
from the EEG signals [32]. Since the selected objects in this study cubes, cylinders and spheres,
differ in these and other features, the hypothesis of this paper is that these primitive geometries
can be distinguished from each other using EEG signals when the user imagines these
geometries.

3. Materials and Methods
The recording device and the block diagram of EEG processing are shown in Figure 1.

Figure 1 Process of developing BCI for constructing 3D objects from human thoughts

EEG signals were recorded using the emotiv neuroheadset [8] at 14 channels (plus CMS/DRL
references, P3/P4 locations). The channel names based on the international 10-20 locations are:
AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4 (see Figure 1). The recorded brain
signals include artifacts such as muscle movement, eye movement, etc. The goal of the
preprocessing step is to remove the artifacts from the EEG signals. The feature generation and
selection blocks in Figure 1 transform the preprocessed signals into a feature vector. The
generated feature vector should have statistically significant differences for different classes of
imagined objects. The classification block uses the feature vector to classify an unknown event

http://dx.doi.org/10.1016/j.cad.2011.04.008

based on a set of observed events (training data). The details of each of these blocks are
explained in next subsections.

3.1 EEG Acquisition and Preprocessing
EEG signals were recorded from 14 locations at sampling rate of 2048 Hz through a C-R highpass hardware filter (0.16Hz cutoff), pre-amplified and low-pass filtered at 83Hz cutoff and
preprocessed using two notch filters at 50 Hz and 60 Hz to remove the main artifacts. The signals
were down-sampled further to 128 Hz.
The experiments were conducted as a series of trials. Each trial had two parts. In the first part,
the stimulus was presented to the participant; in the second part, participants imagined the object
(Details of the experiments are presented in the next section). Figure 2 shows the signal recorded
in two trials at channel O2.

Figure 2 EEG signals recorded at channel O2 and time course for two trials

The first 10% of the signals recorded during the mental task was chopped and the rest was used
for processing. The EEG signals recorded between the trials were used as baseline. Let E i , Bi
and X i be the chopped data, baseline and the combination of two during the ith trail (Figure 2).
For preprocessing, we used the combined signal X i whereas for feature extraction, we analyzed

E i and Bi separately.
http://dx.doi.org/10.1016/j.cad.2011.04.008

The recorded EEG data at each channel is the difference between the potential from a source to
the

channel

location

and

the

reference

electrodes.

The

recorded

EEG

signals

{X  x1 (t ), x2 (t ),... x14 (t )} can be assumed to be a linear combination of ‘n’ unknown and

statistically independent sources {S  s1 (t ), s2 (t ),... sn (t )} (Figure 3). In other words, X=WS where
‘W’ is a weighting matrix. Since we record the signals at 14 channels, we assume that there are
14 independent sources too (n=14).

Figure 3 A) Blind source separation of EEG signals through ICA B) Brain map and power spectral of an IC
associated with blink artifact used as a template

In the preprocessing step, we use ICA decomposition to find the independent components of the
recorded data ( IC X ) such that independent component had the minimum mutual information
[33]. In other words:

IC X  W 1 X where { IC X  ic1 (t}, ic 2 (t},...ic n (t} }

(1)

Independent components ( IC X ) represent synchronous activity in underlying cerebral and noncerebral sources (e.g., potentials induced by eye or muscle movement) [33]. To implement ICA
decomposition, we use the logistic infomax ICA algorithm [34, 35] with natural gradient and
extended ICA extensions implemented in EEGLAB by Delorme and Makeig [36]. After
performing ICA decomposition, independent component ‘ ic i (t} ’ associated with artifacts like
eye blink or muscle movement are removed using brain maps and power spectral density method
[33, 37]. Figure 3b shows the brain map and power spectra of a blink–related component where
http://dx.doi.org/10.1016/j.cad.2011.04.008

there is a strong far-frontal projection typical of eye artifacts. The process of artifact removal is
also shown in Figure 4.

3.2 Feature Extraction
After removing the artifacts from the EEG signals, we perform another ICA on the artifact free
signal X̂ which results in 14 independent components { IC Xˆ  ic1 Xˆ (t ), ic 2 Xˆ (t ),...ic14 Xˆ (t ) }. The
new components represent non-artifact sources with minimal mutual information. Then, we
divide each component into two segments: the EEG component IC E and the base line
component IC B . These components are now used to extract features.
Several features have been used in the literature for classifying EEG data in BCI for different
mental tasks. Some of these features are: band powers (BP) [38], power spectral density (PSD)
values [6, 17, and 39], autoregressive (AR) and adaptive autoregressive (AAR) parameters [40,
41], time frequency features [42] and inverse model-based features [43, 44]. EEG signals are
nonlinear and non-stationary i.e. they may rapidly vary over time and especially over sessions.
To deal with this characteristic of EEG signals, we have selected the Hilbert-Huang transform
(HHT) over classical time–frequency analysis methods [45].
HHT adaptively tracks the evolution of the time–frequency in the original signal and provides
detailed information at arbitrary time–frequency scales. HHT is computed in two steps:
1) empirical mode decomposition (EMD) and 2) Hilbert spectral analysis.
HHT uses the EMD to decompose a signal into a finite set of intrinsic mode functions (IMFs),
and then uses the Hilbert transform of the IMFs to obtain instantaneous frequency and amplitude
data. Using the EMD method, a time series signal x(t ) is represented as a sum of n IMFs u i (t )

http://dx.doi.org/10.1016/j.cad.2011.04.008

and a residue r. The IMFs are sorted in descending order of frequency - u1 (t ) associated with the
locally highest frequency and u n (t ) with the lowest frequency.
Having obtained the IMFs using EMD method, we apply the Hilbert transform to each IMF
component. Instantaneous amplitude  i (t ) , phase  i (t ) and frequency wi (t ) can be expressed
as equation 2-5.

 i (t )  ui2 (t )  H {ui (t )}2
 H {u i (t )} 

 u i (t ) 

(2)

 i (t )  arctan

(3)

d i (t )
dt

(4)

 i (t ) 

Where {u i (t )}is the Hilbert transform of the IMF. The frequency-time distribution of the
amplitude over different IMFs is designated as the Hilbert Spectrum (, t ) . Ultimately the
marginal spectrum is computed as Equation 5:
T

h( )   H ( , t )dt
0

(5)

Using the Hilbert marginal spectrum, we calculate the power of five frequency bands (Delta1-4
Hz, Theta 4-8 Hz, Alpha 8-12 Hz, Beta 12-30 Hz, Gamma 30-64 Hz) for each independent
component of the EEG signals. This results in 70 features per trial (5 power bands for each of
the 14 independent components). Finally, since the total energy of the recorded data can change
in time, we normalize the features of each trial with respect to the features of baseline of the
same trial. Figure 4 shows the steps involved in preprocessing and feature generation.

http://dx.doi.org/10.1016/j.cad.2011.04.008

Figure 4 Block diagram for Artifact removal and feature generation

3.3 Feature Selection
EEG processing requires a large number of features because (i) EEG signals are nonstationary;
thus features must be computed in a time-varying manner, and (ii) the number of EEG channels
is large (14 channels which produce total number 70 features). To evaluate which of the features
provides the most useful information about the mental task, we used the Mann-WhitneyWilcoxon (MWW) test [46]. We rank the features of the training data by MWW test in separate
binary evaluations (each class vs. the other classes). We thus get a set of top features for a
particular class. Top features for overall classification are selected by using a voting method
among all sets of ranked features.

3.4. Classification
Linear discriminator analysis (LDA) is used to evaluate the classification score for any possible
class using the following procedure.
http://dx.doi.org/10.1016/j.cad.2011.04.008

Suppose that the number of classes is C and that for each class, the number of training samples
c e
is E. For each of these training samples, we extract F features. Let f i be the i th feature of the

e th example in the training set of class c. The sample estimate of the mean feature vector per

class is given by:
c

fi 

1 E c e
 fi
E e 1

(6)

The sample estimate of the covariance matrix of class ‘c’ is:
c

E

cov ij   ( c f i e  c f i )( c f je  c f j )

(7)

e 1

Then the ‘ cov ij ’ of all classes are averaged to calculate an estimate of the common covariance
matrix ‘ c cov ij ’. Finally the weight associated to each of the features is calculated as:
F

c

w j   cov ij1.c fi 1  j  F ; cw0 
i 1

1 F c c
 w j . fi
2 i 1

(8)

For each testing data, a score of classifying as class ‘c’ is calculated by using equation (9).
F

c

score  c w0   c wi .c f i 0  c  C
i 1

(9)

The output of the classification stage for each data set is the class with the highest
corresponding score calculated through equation (9).

4. Experimental Study 1: Classifiability
The first experimental study consisted of 10 subjects (7 male and 3 female) between the ages of
18 and 36. All the subjects had a background in engineering but no experience with brain
computer interfaces. The experiment was run in three sessions. Each session lasted 15-20
minutes. There was a break of 5 minutes between sessions 1 and 2, and a break of 15-25 minutes

http://dx.doi.org/10.1016/j.cad.2011.04.008

between sessions 2 and 3. Subjects were instructed to notify the experimenter if they experienced
fatigue or needed a break at any time during the experiment. The experimental studies were
approved by the institutional review board (IRB) of the University of California, Riverside.
In the first session, an image of one of the primitive objects (cube, cylinder, sphere, cone and
pyramid) was displayed in a random sequence on the screen for 2 seconds. The images were
presented in isometric view at the same location on the screen; they had the same overall size
and color. After this period, the screen went blank and the participant was instructed to imagine
the same object on the blank screen at the same location and orientation. The subject was given 5
seconds for the imagery during which EEG data was recorded. At the end of each trial, a
message appeared on the screen asking the subject to get ready for the next trial. The interval
between each trial randomly varied between 2 and 5 seconds. Each session consisted of 10 trials
per object.
In the second session, instead of using the image of an object, a word (e.g. “cube”) appeared as a
verbal cue. The reason for this change of type of cue is as follows: Using only visual image cues
followed by visual imagery, it would be difficult to tell if the EEG signals are a result of the
imagery or just a remnant of visual perception. However, using a different type of cue such as
the name of the object instead of its image can help gain more confidence in the algorithms. If
the classifier trained on imagery prompted by visual image cues can perform well on the imagery
prompted by verbal cues, then we can be more confident that the classifier is indeed capturing
visual imagery.
The cues in the third session were the same as first one. However, in this session, each subject
performed object imagery of ten simple and ten complex objects shown in Figure 5.

http://dx.doi.org/10.1016/j.cad.2011.04.008

Figure 5 Simple and complex objects used in the third session as visual cue

Simple objects are categorized into two sets: 1) five primitive shapes which were used in the first
two sessions and we will refer to them as S1-set, 2) five new shapes which were only presented
in session 3 (S2-set). The second sets of simple objects were either incomplete versions of the
first set (e.g. truncated cone instead of a standard cone) or the same objects with more edges (e.g.
hexagonal pyramid instead of a triangular pyramid). The last set of recorded data was complex
objects, which were considered as a combination of two or more primitive shapes (Figure 5).

4. 2 Results and Discussion - Classification of Primitive Shapes
We rank the features of the training data for the S1-set of shapes by MWW test in five binary
evaluations (each imagined geometry vs. the other classes). We thus obtain five rankings for
features, each of which represents the most important features of its corresponding class. The
top 12 or 18 features (depending on the subject’s performance) for the overall classification are
selected by using a voting method among the five set of ranked features. The MWW test

http://dx.doi.org/10.1016/j.cad.2011.04.008

ranked beta and gamma activity at channel location (AF4, FC6, P8 and O2) and theta band at
channels O1 and O2 to be among the top features.
Figure 6 shows the mean activity of the brain in five frequency bands over 30 trials for subject 5
for each of the five classes. In accordance with the results of the MWW test, Figure 6 shows
that the right hemisphere is more active during mental imagery-maintenance. These results are
also consistent with the findings of Mellet et al. [29].

Figure 6 Band based map of brain activity for subject 1 during visual imagery of different objects

We perform a subject-based classification within and between different sessions of the same
stimulus (image) or different stimuli (text vs. image). In all evaluations, we use a LDA multiclass classifier. The classification result for each subject along with information of training and
testing sets are given in Table 1.
Table 1 shows that if 80% of the recorded data is used for training, the average accuracy among
all the subject is about 44.6% (chance accuracy is 20%) which is more than double the expected
accuracy of a naïve classifier. The third row of Table 1 shows the robustness of the feature

http://dx.doi.org/10.1016/j.cad.2011.04.008

extraction and classification where training and testing data are recorded in two different
sessions with a 30~50 min time gap. Note that the accuracy of the classifier does not decrease
significantly from the first evaluation where the same number of training data had been used.
To test the robustness of the classification to the stimulus type, both image and text cues are
used. The last row in Table 1 shows that the classifier trained on the image stimulus data
performs just as well on the text stimulus. The average accuracy of the classifier in this case is
about 40% (ranging from 26.5 to 56.7%) which is slightly lower than previous conditions.
Table 1-Subject based Classification rate for three different conditions

Data set information

S2

S3

S4

S5

S6

S7

S8

S9

S10

38.6 40.9 38 34.8 34 44.97 27.9 38.5 29.4 34
50.5 43.1 51.6 48 37.1 54.1 35.8 45.8 36
34.5 40

34 32.5 30

34.9 38.7 36.5 34.7 32

44

42.5 27.5 36 29.5 31
38

26

Accuracy (%)

Training: 50% of sess. 1&3 (50 trials)-image
Test:50% of sess. 1&3 (50 trials)-image
Training: 80% of sess. 1&3 (80 trials)-image
Test:20% of sess. 1&3 (20 trials)-image
Training: sess.1 (50 trials)-image
Test: sess. 3 (50 trials)-image
Training:50% of all sess.(75 trials)-image& txt
Test: 50% of all sess.(75 trials)-image & txt
Training: All sess.1&3 (100 trials)-image
Test: All sess. 2 (50 trials)-text

S1

34 29.3 30

38.7 40 41.2 44 33.4 56.7 26.5 44

32 42.5

The confusion matrix for 60% data training and 40% testing is shown in Table 2. The confusion
matrix shown in Table 2 is averaged over all 10 subjects. Therefore, it shows the overall
confusion between classes. The bold numbers in the main diagonal show the rate that the true
class is correctly predicted by the classifier which is between 30.6 to 37.1%. Also Table 2
shows the misclassification rate between any two classes. For example, cube is often
misclassified as sphere (18.3%) and pyramid is mostly misclassified as cylinder (20.4%).
The misclassification could result from insufficient or inappropriate information in the features.
It could also arise from the similarity between certain objects causing the features of different

http://dx.doi.org/10.1016/j.cad.2011.04.008

objects to be very close to each other. To test for the latter reason, we determine the number of
times a misclassified object occurs as the second choice in a classifier’s output.

True Class

Table 2 Classification results of object imaginary detection averaged over all subjects

Classification Result %
Cube Pyramid Cylinder Sphere Cone
Cube
16.2
15
18.3
13.7
36.7
Pyramid 16.6
20.4
13.5
15.9
33.6
Cylinder 16.2
11.5
15
19.6
37.6
Sphere
16.4
14.6
23.2
15.3
30.6
Cone
24.1
12.1
8.8
17.9
37.1

Table 3 shows this result. To read this table, assume that the true class of the object is a “cube”.
The first row of Table 3 shows the number of times the classifier misclassifies the cube as some
other object but the true class, viz. cube, is ranked as the second choice. For example, when the
classifier for the cube incorrectly classifies the cube as a sphere, 8.3% of the time the cube
occurs as the second choice for this classifier. Since the cube classifier classifies a cube as a
sphere 18.3% of the time (see the first row of Table 2), the proportion of times that the cube
occurs as a second choice when the cube classifier outputs a sphere is 8.3/18.3 = 45.4%. It
should be noted that this result is more or less symmetric. In other words, the classifier for the
sphere also often confuses the cube with the sphere (see the first column of the fourth row of
Table 3). A similar confusion can be seen between the cone and the pyramid.
Table 3 Classification results of object imaginary detection (Top 2 class) averaged over all subjects

Classification Result %
Classification rate
(Top 2 classes)
Cube
46.7
0.8 (4.9%)
0.8 (5.3%) 8.3 (45.4%) 0.4 (2.9%)
Pyramid 3.3 (19.9%)
52.3
4 (19.6%) 5.9 (43.7%) 5.4 (34%)
Cylinder 1.1 (6.8%) 1.9 (16.5%)
50.3
2.5 (16.7%) 7.1(36.2%)
Sphere 6.5 (39.6%) 5.4 (37%) 7.1 (30.6%)
53.3
3.8 (24.8%)
Cone
51
3.8 (15.8%) 5.4 (44.6%) 1.1 (12.5%) 3.6 (20.1%)
-

True Class

Cube

Pyramid

Cylinder

Sphere

Cone

One possible explanation is that both the cone and pyramid have angled segments making them
different from the other objects. The cube and sphere also have the highest ambiguity possibly

http://dx.doi.org/10.1016/j.cad.2011.04.008

because of multiple axes of symmetry. However, it should be noted that this “logical” confusion
between similar objects is not entirely consistent. For example, Table 3 shows that the cylinder
and cone are frequently confused, possibly because of curved segments. Similarly, Table 2
shows the cone classifier frequently outputs the cube.
These results points out the possible relationship between the features of the geometry (such as
roundedness, sharpness, symmetry and curvature) and the extracted features of EEG signal. For
further investigation, we used the classifier trained on 5 primitive shapes to classify the other 15
geometries (S2-set and complex objects). The classification results are shown in Table 4. The
columns represent the true class while the rows correspond to the classification results for each
of the primitive shape classifiers on the complex objects.
Table 4Classification result of complex objects with a 5-way classifier for subject 2

Obj #

1

2

3

4

5

6

7

8

9

10

11

12

13

14

15

20

0

0

10

10

20

20

20

0

40

0

0

70

20

20

20

40

10

30

30

10

10

10

30

20

30

40

0

10

30

30

20

30

30

10

10

30

0

10

20

10

40

10

20

40

10

10

50

10

40

40

10

10

50

0

50

20

10

40

0

20

30

10

20

10

10

30

60

10

20

10

0

10

0

10

Classification Result %

Testing
Set

The underlined numbers in Table 4 indicate the most frequent classification output for each
training data. As an example, the truncated cone (object 1) is classified most frequently as
cylinder (30%). The truncated cone shares more common geometrical features (for example, no
sharp edges, round segment) with the cone and cylinder than with other primitive shapes. This
may explain why the truncated cone is classified mostly as a cylinder and not a sphere. Also a

http://dx.doi.org/10.1016/j.cad.2011.04.008

hexagonal pyramid (object 2) is geometrically more similar to a pyramid (both have angled
segments, sharp edges and similar axis of symmetry). This geometrical similarity can be the
reason that the hexagonal pyramid is more classified as the triangular pyramid.
However, as the geometrical complexity of the object increases (objects 7-15), there is no
strong relationship between geometrical features and the most common classification result.
Objects 11 and 13 are two examples of complex objects. Each of these objects are combination
of three boxes. The classification outcome shows object 11 is classified as sphere in 50% and
never classified as a cube. Object 13, on other hand, is classified 70% as cube and 10% as
sphere. It should be noted that we have defined complex objects as a combination of two or
more primitive shapes. Therefore, we can divide their geometrical features into two categories.
The first category consists of the local features which are related to the primitive shapes
forming the complex objects (e.g. roundness of sphere and sharp edges of cube). The second
category consists of the global features of the primitive shapes such as symmetry. In the given
example, objects 11 and 13 are created from the same primitive shapes (same local features) but
their global features are different. So the output of the classifier may depend on the dominant
feature which, in this case, seems to be the global features. However, object 14, which is similar
to object 13, is more frequently classified as a sphere. Further experimentation is necessary to
investigate the role of local and global features in object imagery.

5. Experimental Study 2: Robustness
In the first experiment, the robustness of the classification to the stimulus type was tested by
using both image and text cues in two different sessions where the sessions were separated by
20-30 minutes. The classification results show when a classifier is trained with EEG data
recorded with image as stimulus, it still can classify the EEG data associated with another
http://dx.doi.org/10.1016/j.cad.2011.04.008

stimulus with an average accuracy of 39.9% (range of 26.5-56.7%). The goal of the second
experiment is to evaluate the robustness of classification on EEG data recorded over a longer
periods.
For this purpose, we repeated the experimental procedure for sessions 1 and 2 (section 4.2) for
two subjects over 20 days. We conducted the experiment every other day for twenty days, with
two sessions per day and 50 trials per session. There was a 20-30 minute gap between the
sessions. This resulted in 100 trials per day per subject. We analyzed the data using two
approaches: 1) By classifying the data on a daily basis 2) By training a classifier on the data
obtained on the first day and using this classifier to classify the data recorded on the other days.
For the first approach, we chose 80% of the data recorded in each day to classify the 20%
remaining data of the same day. The top 12 Hilbert Spectral features were chosen by using the
MWW method. The average classification results of subject 4 over 10 recording days are shown
in Table 5.

True Class

Table 5 Classification results of object imaginary recorded over 10 days

Classification Result %
Cube Pyramid Cylinder Sphere Cone
3.8
18.1
29.4
18.3
Cube
30.4
14.3
16.1
16.9
Pyramid 16.8
35.9
19.8
12.9
15.9
Cylinder 12.3
39.1
32.4
15.1
13.4
10.5
Sphere
28.6
14.8
20.2
16.4
13.2
Cone
35.4

The overall classification rate for subject 1 and 2 are 37.5 and 33.9% respectively which is very
close to the result of the first experiment shown in Table (34.9% for subject1 and 34.7% for
subject 4). This result shows the repeatability of the procedure when the testing and training
data set are recorded in the same day.
To check the robustness of the features over time, we used the data recorded on the first day to
train the classifier and then evaluate its performance on the data recorded on the other days.
http://dx.doi.org/10.1016/j.cad.2011.04.008

Based on results shown in Figure 7, when training and testing data are not recorded on the same
day, the accuracy of the classifier ranges from 20% (chance) to 35%. This accuracy was
previously close to 40% as discussed in section 4.2.

Figure 7 Classification rate of data recorded 10 different days when classifier is trained on data of the first day

This is mostly due to the non-stationary properties of EEG data which changes the dynamics of
the signal over different days. In the analysis process, we normalized the features of each trial
with respect to the features of its baseline. This normalization was used to reduce the effect of
boredom and fatigue which arise with time in each session. However, to reduce the time effect
between different recording days, it seems that another normalization is necessary.

6. Conclusions
This paper evaluates the feasibility of using the brain computer interface as an interaction
device for computer aided design software using visual imagery for geometry representation.
Among the six characteristics of visual imagery listed by Cornoldi et al. [30] (specificity,
richness of detail, color, saliency, shape and contour & context), all but the shape were fixed.
Subjects imagined one of the five primitive objects – a cube, sphere, cylinder, pyramid and
cone. The brain activity, collected by the EEG headset, was analyzed using independent
components analysis and the Hilbert Huang Transform (HHT). The results show an average
http://dx.doi.org/10.1016/j.cad.2011.04.008

classification accuracy of 44.6% over ten subjects (ranging from 36 to 56.4%) in a 5-way
classification. Marginal spectra of different frequency bands calculated from Hilbert spectrum
of each independent component of EEG data were used as the features of a linear discriminant
classifier.
Conducting the same experiment over a longer period (20 days) resulted in an average
classification rate of 35.7% over two subjects when the classifiers are trained and tested on a
daily basis. Comparing this results to the results of Table 2 with the same conditions (4 th row),
shows the repeatability of the proposed method. However, due to the non-stationary dynamics
of the signals, the method does not perform effectively when training and testing data are
recorded in different days.
Additionally, the research methodology of using computer images to elicit imagined objects is
validated by using text cues as another modality. The fact that classifier can maintain the
classification accuracy over different modalities supports the finding of Suppes et al. [47] that
show invariance of brain-wave representation of visual image and their name.
This paper shows that the brain computer interface can be used to distinguish between primitive
shapes and thus has the potential for use in CAD systems. However, as the complexity of shape
increases, generation and maintenance of visual objects becomes a more difficult task because
visual memory cannot hold more than few objects simultaneously [31]. This means that
classification of visual imagery may be limited to very simple and primitive shapes and may not
provide the CAD system the whole capability of object generation. However the classification
results of complex objects (Table 4) shows a possible correlation between the features of the
geometry (such as roundedness, sharpness, symmetry and curvature) and the extracted features
of EEG signal. Being able to capture some of the main geometrical features of complex objects

http://dx.doi.org/10.1016/j.cad.2011.04.008

will provide the CAD system with some overall information about the object that user intends to
create or modify. The correlation between EEG features and geometrical features seems to be
very promising for application of BCI in CAD systems and needs further investigation.
It is crucial to notice that we only have 10 trials per object in each session. This does not
provide enough evidence to generalize the findings. Increasing the number of trials causes
fatigue in subjects. Therefore for future work, it is recommended to reduce the number of
objects and increase the number of trials. This will provide the necessary tools to investigate the
correlation of EEG features and geometrical features of an object,
Furthermore, to be successful, the BCI would have to allow users the rich variety of interactions
that traditional interfaces offer. Future research will focus on determining the features relevant
to resizing, positioning and manipulating objects.
It is also important to check the classification result in a real-time system. In the current study,
the time required to process and classify an artifact free EEG signal is about 340 ms on a
machine with 2.10 GHz AMD Dual-Core CPU. The artifact removal process used in this study
is not real-time. Implementing a real-time artifact removal process will enable the real-time
application as a future work.
Another improvement to the techniques described here is the use of other machine learning
techniques. In a comparison between linear and nonlinear classifier, Muller et al. [48] showed
that kernel-based learning machines such as support vector machine increase the performance of
EEG classification. Furthermore, the classification algorithm can be improved by recording data
from more locations on the scalp.
However, none of these improvements will make the system totally reliable. Therefore, it might
be essential to obtain real-time feedback from the user, possibly through the use of emotional

http://dx.doi.org/10.1016/j.cad.2011.04.008

responses of the user. For example, Ko et al. proposed an emotion recognition system based on
the EEG relative power value and a Bayesian network [39]. Frustration, meditation, excitement,
boredom are some of the emotional states that can be classified [49-51]. In our previous work
[52], we have demonstrated that we can detect the satisfaction n human-machine interaction.
Detecting the emotional state and user’s satisfaction can be used in CAD systems to correct for
errors and to strengthen proper classifications. For example, if the BCI system misinterprets a
user’s imagined object and consequently, the user shows hints of frustration, the BCI can sense
this emotion to modify the output till the user’s emotional response becomes more neutral.
Further experimental studies and software development in this direction will lead to novel ways
of obtaining user’s feedback about the system’s performance.

References
[1]

C. Tian, M. Masry, and H. Lipson, "Physical sketching: Reconstruction and analysis of
3D objects from freehand sketches," Computer Aided Design, 41(3): 147-158, 2009.

[2]

S. Payandeh, J. Dill, and J. Zhang, "Using haptic feedback as an aid in the design of
passive mechanisms," Computer Aided Design, 39(6): 528-538, 2007.

[3]

X.Y. Kou, S.K. Xue, and S.T. Tan, "Knowledge-guided inference for voice-enabled
CAD," Computer Aided Design, 42(6): 545-557, 2010.

[4]

F.M. Amirouche, Principles of computer aided design and manufacturing, 2nd ed.:
Prentice Hall, 2004.

[5]

A. Lecuyer, F. Lotte, R.B. Reilly, R. Leeb, M. Hirose, and M. Slater, "Brain-computer
interfaces, virtual reality, and videogames," Computer, 41(10): 66-72, 2008.

[6]

R. Krepki, B. Blankertz, G. Curio, and K.R. Muller, "The Berlin Brain-Computer
Interface (BBCI) - towards a new communication channel for online control in gaming
applications," Multimedia Tools Applications, 33(1): 73-90, 2007.

[7]

G. Pfurtscheller, R. Leeb, C. Keinrath, D. Friedman, C. Neuper, C. Guger, and M. Slater,
"Walking from thought," Brain Research, 1071(1): 145 -152, 2006.

[8]

http://www.emotiv.com.

[9]

http://www.neurosky.com.

http://dx.doi.org/10.1016/j.cad.2011.04.008

[10] Z. Kosmadoudi, R.C.W Sung, Y. Liu, T. Lim, J. Ritchie “Evaluation user interfaces for
engineering tasks with biometric logging.” Proceedings of the TMCE 2010, Ancona,
Italy.
[11] T.A. Nguyen, Y. Zeng “Analysis of design activities using EEG signals.” Proceedings of
the ASME 2010 International Design Engineering Technical Conferences & Computers
and Information in Engineering Conference IDETC/CIE 2010, Montreal, Canada
[12] K. Alexiou, T. Zamenopoulos, J.H. Johnson, S.J. Gilbert “Exploring the neurological
basis of design cognition using brain imaging: some preliminary results.” Design Studies.
30(6): 623-647, 2009.
[13] H.M. Göker “The Effects of Experience on Human Problem Solving.” Design Studies.
18(4): 405-426, 1997.
[14] N.J.T. Thomas, “Mental Imagery," in Stanford encyclopedia of philosophy.
[15] SM. Kosslyn, "Mental Imagery," in An Invitation to Cognitive Science: Visual Cognition,
2nd ed, S. M. Kosslyn and D. N. Osherson, Eds.: MIT Press, 1995, pp. 267-296.
[16] S.T. Moulton and S.M. Kosslyn, "Imagining predictions: mental imagery as mental
emulation," Philosophical transactions of the royal society B, 364(1521): 1273-80, 2009.
[17] C. Brunner, M. Naeem, R. Leeb, B. Graimann, and G. Pfurtscheller, "Spatial filtering and
selection of optimized components in four class motor imagery EEG data using
independent components analysis," Pattern Recognition Letter, 28(8): 957-964, 2007.
[18] A. Kubler, B. Kotchoubey, J. Kaiser, JR. Wolpaw, and N. Birbaumer, "Brain-computer
communication: Unlocking the locked in" Psychological Bulletin, 127(3): 358-375, 2001.
[19] S. Lemm, C. Schafer, and G. Curio, "BCI competition 2003-data set III: probabilistic
modeling of sensorimotor &mu; rhythms for classification of imaginary hand
movements," IEEE Transactions on Bio-Medical Engineering, 51(6): 1077-1080, 2004.
[20] J.R. Millan and J. Mourino, "Asynchronous BCI and local neural classifiers: an overview
of the adaptive brain interface project," IEEE Transactions on Neural Systems and
Rehabilitation Engineering, 11(2): 159-161, 2003.
[21] G.E. Fabiani, D. J. McFarland, J.R. Wolpaw, and G. Pfurtscheller, "Conversion of EEG
activity into cursor movement by a brain-computer interface (BCI)," IEEE Transactions
on Neural Systems and Rehabilitation Engineering, 12(3): 331-338, 2004.
[22] L.J. Trejo, R. Rosipal, and B. Matthews, "Brain-computer interfaces for 1-D and 2-D
cursor control: designs using volitional control of the EEG spectrum or steady-state
visual evoked potentials," IEEE Transactions on Neural Systems and Rehabilitation
Engineering, 14(2): 225-229, 2006.
[23] A.R. Nikolaev, "Investigation of the stages of the mental rotation of complex figures with
the intracortical interaction mapping technique," Neuroscience and Behavioral
Physiology, 25(3): 228-233, 1995.
[24] J. Martinovic, T. Gruber, and M.M. Mueller, "Induced gamma band responses predict
recognition delays during object identification," Journal of Cognitive Neuroscience,
19(6): 921-934, 2007.

http://dx.doi.org/10.1016/j.cad.2011.04.008

[25] M. Kozhevnikov, S.M. Kosslyn, and J.M. Shephard, "Spatial versus object visualizers: A
new characterization of visual cognitive style," Memory & Cognition, 33(4): 710-726,
2005.
[26] M.J. Farah, K.M. Hammond, D.N. Levine, and R. Calvanio, "Visual and spatial mental
imagery: dissociable systems of representation," Cognitive Psychology, 20(4): 439-462,
1988.
[27] S.M. Kosslyn, Image and brain: MIT Press, 1994.
[28] M. Desposito, J.A. Detre, G.K. Aguirre, M. Stallcup, D.C. Alsop, L.J. Tippet, and M.J.
Farah, "A functional MRI study of mental image generation," Neuropsychologia, 35(5):
725-730, 1997.
[29] E. Mellet, N. Tzourio, M. Denis, and B. Mazoyer, "A positron emission tomography
study of visual and mental spatial exploration," Journal of Cognitive Neuroscience, 7(4):
433-445, 1995.
[30] C. Cornoldi, R. De Beni, A. Cavedon, G. Mazzoni, F. Giusberti, and F. Marucci, "How
can a vivid image be described? Characteristics influencing vividness judgments and
relationship between vividness and memory," Journal of Mental Imagery, 16(3&4):89108, 1992.
[31] D. Pearson, R. De Beni, and C. Cornoldi, "The generation, maintenance, and
transformation of visuo-spatial mental image," in Imagery, Language and visuo-spatial
thinking, M. Denis, R. H. Logie, C. cornoldi, M. De Vega, and J. engelkamp, Eds.:
Psychology Press Ltd, 2001, pp. 1-23.
[32] C. Koch and T. Poggio, "Predicting the visual world: silence is golden," Nature
Neuroscience, 2(1): 9-10, 1999.
[33] J. Onton and S. Makeig, "Information-based modeling of event-related brain dynamics,"
in Event-related dynamics of brain oscillations. vol. 159: Elsevier Science Inc., 2006, pp.
99-120.
[34] A.J. Bell and T.J. Sejnowski, "An information maximization approach to blind separation
and blind deconvolution," Neural Computation, 7(6): 1129-1159, 1995.
[35] S. Amari, A. Cichocki, and H.H. Yang, "A new learning algorithm for blind signal
separation," in Advances in neural information processing systems, D. Touretzky, M.
Mozer, and M. Hasselmo, Eds.: MIT Press, 1996, pp. 757-763.
[36] A. Delorme and S. Makeig, "EEGLAB: an open source toolbox for analysis of singletrial EEG dynamics," Journal of Neuroscience Methods, 134(1): 9-21, 2004.
[37] T.P. Jung, S. Makeig, C. Humphries, T.W. Lee, M.J. McKeown, V. Iragui, and T.J.
Sejnowski, "Removing electroencephalographic artifacts by blind source separation.,"
Psychophysiology, 37(2): 163-178, 2000.
[38] G. Pfurtscheller, C. Neuper, D. Flotzinger, and M. Pregenzer, "EEG-based discrimination
between imagination of right and left hand movement," Electroencephalography and
Clinical Neurophysiology, 103(6): 642-651, 1997.

http://dx.doi.org/10.1016/j.cad.2011.04.008

[39] K.E. Ko, H.C. Yang, and K.B. Sim, "Emotion recognition using EEG Signals with
relative power values and bayesian network," International Journal of control
Automation and Systems, 7(5):865-870, 2009.
[40] X. Baoguo, S. Aiguo, "EEG recognition based on wavelet transform and AR parameter
model," Journal of Data Acquisition and Processing, 23(5):580-583, 2008.
[41] G. Pfurtscheller, C. Neuper, A. Schlogl, and K. Lugger, "Separability of EEG signals
recorded during right and left motor imagery using adaptive autoregressive parameters,"
IEEE Transactions on Rehabilitation Engineering, 6(3):316-325, 1998.
[42] T. Wang, H. Deng, and B. He, "Classifying EEG-based motor imagery tasks by means of
time-frequency synthesized spatial patterns," Clinical Neurophysilogy, 115(12):27442753, 2004.
[43] S. Chiappa and S. Bengio, "HMM and IOHMM modeling of EEG rhythms for
asynchronous BCI systems," in European Symposium on Artificial Neural Networks
(ESANN), 2004.
[44] F. Lotte, M. Congedo, A. Lecuyer, F. Lamarche, and B. Arnaldi, "A review of
classification algorithms for EEG-based brain-computer interfaces," Journal of Neural
Engineering, 4(2):R1-R13, 2007.
[45] N. Huang, N.O. Attoh-Okine, The Hilbert-Huang transform in engineering, Taylor &
Francis, 2005.
[46] G.W. Corder and D.I. Foreman, Nonparametric statistics for non-statisticians: John
Wiley and Sons, 2009.
[47] P. Suppes, B. Han, J. Epelboim, and Z.L. Lu, "Invariance of brain-wave representations
of simple visual images and their names," Proceedings of National Academy of Science,
96(25):14658-14663, 1999.
[48] K.R. Müller, C.W. Anderson, and G.E. Birch, "Linear and nonlinear methods for brain
computer interfaces," IEEE Transactions on Neural Systems and Rehabilitation
Engineering, 11(2):165-169, 2003.
[49] P.C. Petrantonakis and L.J. Hadjileontiadis, "Emotion recognition From EEG using
higher order crossings," IEEE Transactions on Information Technology in Biomedicine,
14(2):186-197, 2010.
[50] T. Pun, T.I. Alecu, G. Chanel, J. Kronegg, and S. Voloshynovskiy, "Brain-computer
interaction research at the computer vision and multimedia laboratory, University of
Geneva," IEEE Transactions on Rehabilitation Engineering, 14(2): 210-213, 2006.
[51] M. Li and B.L. Lu, "Emotion classification based on gamma-band EEG," in Proc. Annual
Int. Conf. of the IEEE Engineering in Medicine and Biology Society EMBC 2009, 2009,
pp. 1223-1226.
[52] E.T. Esfahani and V. Sundararajan, "Using brain computer interface to detect human
satisfaction in human-robot interaction," International Journal of Humanoid Robotics,
8(1): 87-101, 2011.

http://dx.doi.org/10.1016/j.cad.2011.04.008

