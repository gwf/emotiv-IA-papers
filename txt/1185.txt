information
Article

Multi-Modal Emotion Aware System Based on Fusion
of Speech and Brain Information
Rania M. Ghoniem 1,2, *, Abeer D. Algarni 2 and Khaled Shaalan 3
1
2
3

*

Department of Computer, Mansoura University, Mansoura 35516, Egypt
Department of Information Technology, College of Computer and Information Sciences,
Princess Nourah bint Abdulrahman University, Riyadh 84428, Saudi Arabia
Faculty of Engineering & IT, The British University in Dubai, Dubai 345015, United Arab Emirates
Correspondence: RMGhoniem@pnu.edu.sa

Received: 28 May 2019; Accepted: 27 June 2019; Published: 11 July 2019




Abstract: In multi-modal emotion aware frameworks, it is essential to estimate the emotional features
then fuse them to different degrees. This basically follows either a feature-level or decision-level
strategy. In all likelihood, while features from several modalities may enhance the classification
performance, they might exhibit high dimensionality and make the learning process complex for the
most used machine learning algorithms. To overcome issues of feature extraction and multi-modal
fusion, hybrid fuzzy-evolutionary computation methodologies are employed to demonstrate
ultra-strong capability of learning features and dimensionality reduction. This paper proposes
a novel multi-modal emotion aware system by fusing speech with EEG modalities. Firstly, a mixing
feature set of speaker-dependent and independent characteristics is estimated from speech signal.
Further, EEG is utilized as inner channel complementing speech for more authoritative recognition,
by extracting multiple features belonging to time, frequency, and time–frequency. For classifying
unimodal data of either speech or EEG, a hybrid fuzzy c-means-genetic algorithm-neural network
model is proposed, where its fitness function finds the optimal fuzzy cluster number reducing the
classification error. To fuse speech with EEG information, a separate classifier is used for each
modality, then output is computed by integrating their posterior probabilities. Results show the
superiority of the proposed model, where the overall performance in terms of accuracy average
rates is 98.06%, and 97.28%, and 98.53% for EEG, speech, and multi-modal recognition, respectively.
The proposed model is also applied to two public databases for speech and EEG, namely: SAVEE and
MAHNOB, which achieve accuracies of 98.21% and 98.26%, respectively.
Keywords: multi-modal emotion aware systems; speech processing; EEG signal processing; hybrid
classification models

1. Introduction
In human–computer interaction (HCI), comprehending and discriminating emotions turned into
a principal issue to construct intelligent systems that could perform purposed actions. Emotions
can be discriminated utilizing distinct forms of sole modalities, like facial expression, short phrases,
speech, video, EEG signals, and long/short texts. These modalities vary over the computer applications,
e.g., the well-known modality in computer games is video.
Recent studies unveil several merits of employing physiological signals for recognizing
emotions [1]. For instance, the electroencephalogram (EEG) signals have been shown to be a robust sole
modality [2,3]. The control of these bio-signals is managed by our central nervous system; thus, it cannot
be affected intentionally, while actors can pretend with emotion on their faces deliberately. Further,
physiological signals are emitted constantly and since sensors are directly attached to subject’s body,

Information 2019, 10, 239; doi:10.3390/info10070239

www.mdpi.com/journal/information

Information 2019, 10, 239

2 of 39

they are not out of reach. Moreover, physiological information could also be utilized as supplementary
to emotional information gained from facial expressions or speech to optimize the recognition rates.
In this regard, several multi-modal emotion recognition approaches have achieved significant
contributions [4–6]. Though there are distinctive input modalities for recognizing emotions, the most
widely recognized are bimodal inputs that combine both speech and video. The two modalities have
been selected frequently by the researchers because of being captured in a noninvasive way and are
expressive than the other modalities. Nevertheless, face expressions might be falsified by not returning
accurate information regarding the person’s internal state.
From between the modalities that tend to be in the literature, speech is an intuitive measure for
computers to comprehend human emotion, whereas EEG [7] is an internal measurement from the brain
that makes an intriguing alternative for recognizing multi-modal emotion. Up until now, there are
no proposals that have endeavored to consider speech and EEG simultaneously for recognizing
spontaneous emotion. This has motivated us to propose a new multi-modal emotion aware system
that fuses speech and EEG modalities for discriminating emotional state of the subject.
In the multi-modal fusion, input information from many modalities, like audio, EEG, video,
and electrocardiogram (ECG) can be fused together coherently [8]. More than one modality cannot be
combined in a context-free way; a context-dependent model has to be utilized. Information fusion is
classified into three progressive stages: (I) Early fusion, (II) intermediate fusion, and (III) late fusion.
As for early fusion, integration of information is implemented at signal-/feature-level, while in late fusion,
information at a semantic level is to be fused. The foundation in combining multi-modal information is
the modalities number [2,3], information derivation synchronization, and fusion procedure, besides
finding the proper fusion level of information. However, it is not constantly fundamental that diverse
modalities give complimentary information through the fusion stage; therefore, it is essential to
comprehend each modality’s contributions with respect to accomplishment of distinct tasks.
One of the important problems faced in the multi-modal emotion recognition is the fusion of
features belonging to different modalities. By reviewing the literature on the multi-modal emotion
analysis, it has been observed that the majority of works have concentrated on the concatenation of
feature vectors obtained from different modalities. However, this does not take into consideration the
conflicting information that may be carried by the unimodal modalities. On the other hand, little work
has addressed this issue through new feature-fusion methods to enhance the multi-modal fusion
mechanism [9–12].
In [9], the authors proposed a feature fusion strategy that proceeds from the unimodal to bimodal
data vectors and later, the bimodal to trimodal data vectors. In [10], a context-aware audio–video system
is proposed. Accordingly, a contextual audio–video switching approach is presented to switch between
visual-only, audio-only, and audio–visual clues. The proposed approach integrates the convolutional
neural network (CNN) and the long-short-term memory (LSTM) network. Moreover, a multi-modal
hybrid deep neural network architecture is presented in [11] for audio–visual mask estimation. In [12],
an image-text emotion analysis model is proposed. To utilize the internal correlation between the
image and textual features, an intermediate fusion-based multimodal model was proposed. Eventually,
a late fusion scheme was applied to combine the models of sentiment prediction.
One of the drawbacks of feature-level emotion fusion is that it might exhibit high dimensionality
and poor performance because of redundancy and inefficiency. For overcoming the high dimensionality
issue, hybrid intelligent models can introduce several choices for unorthodox handling of complex
problems, which carry uncertainty, vagueness, and high dimensionality of data. They can exploit
a priori knowledge and the raw data to introduce innovative solutions. In this regard, hybridization is
a crucial phase in many domains of human activity [13,14].
This paper suggests a new hybrid emotion recognition model based on the fuzzy clustering,
genetic search-based optimization. The proposed model selects and trains the neural network (NN)
with the optimal fuzzy clusters representing each modality, without any prior knowledge about
the fuzzy clusters number. Unlike the previous works on multimodal emotion fusion, the fusion

Information 2019, 10, 239

3 of 39

of two modalities is implemented on the decision-level instead of the feature-level fusion that may
carry conflicting information. For comparison purposes, the proposed hybrid model is likewise
compared to another developed hybrid c-means-genetic algorithm-neural network model, namely,
FCM − GA − NN f ixed model that relies on defining a fixed fuzzy clusters number.
2. Related Work
This study suggests a novel multi-modal emotion aware system by fusing together speech and
EEG modalities. Consequently, the literature review section is progressive. So, the emotion recognition
topic of interest is divided into three parts speech emotion recognition, EEG-based emotion recognition,
and multi-modal emotion recognition.
2.1. Literature Review on Speech Emotion Recognition
The related work on speech emotions classification reveals three crucial aspects. Firstly, preparing
the emotional database is critical to validate the system performance. Secondly, selecting features
properly for speech characterization. Eventually, design of accurate classification model. For emotional
speech databases, acoustic analysis has been utilized to recognize emotions using three kinds of
databases: Natural spontaneous emotions, acted emotions, and elicited emotions.
Most emotional speech databases depend on inviting professional actors for expressing
pre-determined sentences related to the purposed emotion. Though, in some databases like the
Danish Emotional Speech (DES) database [15], semi-professional actors are invited for avoiding
exaggeration during expressing emotions. As for spontaneous speech, databases may be collected
from interaction with robots, or call center data.
The performance of the speech emotion recognition system depends on the features extracted from
the speech signal. A challenging issue in recognizing speech emotions is extracting speech features that
efficiently describe the speech emotional content and, simultaneously, do not rely on the speakers or the
lexical contents. The majority of published works on speech emotion classification have concentrated
on analyzing speech spectral information and prosodic features. Some new parameters are utilized for
recognizing speech emotion, like the Fourier parameters [16]. Although numerous acoustic parameters
have been found to carry emotional content, little success was proved to define a feature set that
performs invariably over diverse conditions [17].
Therefore, the majority of works use mixing feature set, which comprises several types of
features involving more emotional information [18]. This is implemented by dividing the signal into
k frames/segments containing n samples per frame/segment, which causes a high dimensionality
problem. As a result, the computational cost and over-fitting likelihood of the speech classifier are
increased. Thus, feature selection approaches are imperative to minimize feature redundancy and
speed up the learning process of speech emotion recognition.
Table 1 [19–22] summarizes some works on speech emotion recognition. By reviewing these works,
it is obvious that the most used classifiers are artificial neural networks, Gaussian mixture model,
and multiple different one-level standard methodologies. Satisfactory outcomes are obtained using
these standard classifiers. Nevertheless, the improvements in their performances are usually restricted.
Thus, the fuzzy genetic search-based optimizations and fusion of speech classifiers could constitute
a new step toward robust emotion classification [23,24]. Therefore, we hypothesize that the proposed
hybrid soft computing model can overcome the existing limitations of speech emotion recognition.

Information 2019, 10, 239

4 of 39

Table 1. Recent Research on speech emotion recognition.
Reference

Publication Year

Corpus

Berlin EmoDB
[19]

2019
IEMOCAP

[20]

2018

Chinese speech database
from Chinese academy of
sciences (CASIA)

Speech Analysis

Feature Selection

The local feature learning
blocks (LFLB) and the LSTM
are used to learn the local
and global features from the
raw signals and log-mel
spectrograms.

-

Speaker-dependent features,
and speaker-independent
features.

A correlation analysis
and Fisher-based
method.

Classifier

1D & 2D CNN LSTM
networks.

Extreme learning
machine (ELM)

2015

LDC Emotional Prosody
Speech and Transcripts
Polish Emotional Speech
Database

Two prosodic features and
four paralinguistic features of
the pitch and spectral energy
balance.

None

Support vector
machines (SVM)

2015

LDC
FAU Aibo

89.16% and 52.14% for
speaker-dependent and
speaker-independent
results, respectively.
89.6%

88.32%
90%

BES
[22]

95.33% and 95.89% for
speaker-dependent and
speaker-independent
results, respectively.

94.9%

BES
[21]

Classification Accuracy

82.1%
Spectral and prosodic
features.

None

Ranking SVMs

52.4%
39.4%

Information 2019, 10, 239

5 of 39

2.2. Literature Review on EEG-Based Emotion Recognition
The initial stage of EEG-based emotion classification is to take accurately labeled EEG signals
induced by picture, video clips, or music. Subsequent to presentation of visual stimuli to a subject,
multi-channel signals of EEG are recorded, and afterwards, the signal labeling is done based on subject
ratings. The previous work on EEG emotional features extraction [25–29] has revealed that there
exist several valuable features of time, frequency, and time–frequency that have been evidenced to
be efficient in differentiating emotions. Furthermore, there is no standard feature set that has been
agreed as the most appropriate for EEG emotion classification. This causes a high dimensionality
issue in EEGs, because not all features would contain significant information concerning emotions.
The redundant and irrelevant features maximize the feature space, making the detection of patterns
more difficult, and maximizing the over-fitting risks.
However, the majority of classifiers that have been proposed in the literature for EEG emotion
classification are based on the conventional classification algorithms such as ANN, SVM, and k-nearest
neighbor. Some of these approaches are demonstrated in Table 2. Few works have presented hybrid
methods of evolutionary computer algorithms and classification methods [25], where the objective of
these works is to deal with the high dimensionality issue of EEG emotion recognition. Therefore, in this
work, we propose a novel fuzzy c-means-genetic algorithm-neural network (FCM-GA-NN) model for
EEG emotion recognition. The meta-heuristic can provide the optimal initial centroids for the FCM
algorithm, which will be trained to the NN as optimized solutions for emotion recognition. In this
regard, the proposed algorithm has been experimented on our collected database and another publicly
available database, namely MAHNOB. The comparative results are demonstrated in the experimental
results section.

Information 2019, 10, 239

6 of 39

Table 2. Recent research on electroencephalogram (EEG)-based emotion recognition.
Reference

[26]

[27]

[28]

[29]

Corpus

DEAP database

DEAP database

DEAP database

Collected database

Publication Year

2018

2018

Feature Extraction

Feature Selection

Multivariate
synchrosqueezing transform.

Non-negative matrix
factorization and
independent
component analysis.

Liquid State Machines (LSM)

2016

Empirical mode
decomposition and sample
entropy.

2016

Features of time (i.e., Latency
to Amplitude Ration, Peak to
Peak Signal Value, etc.),
frequency (i.e., Power
Spectral Density, and Band
Power), and wavelet domain.

Classifier

Emotion Classes

Classification
Accuracy

Artificial Neural
Network (ANN)

Arousal/valence states
(high arousal-high
valence, high arousal-low
valence, low arousal-low
valence, low arousal -high
valence).

82.03% and 82.11%
for valence and
arousal state
recognition.

Decision Trees

Valence, arousal as well as
liking classes.

84.63%, 88.54%, and
87.03% for valence,
arousal, and liking
classes, respectively.

None

SVM

Arousal/valence states
(high arousal-high
valence, high arousal-low
valence, low arousal-low
valence, low arousal-high
valence).

94.98% for
binary-class tasks,
and 93.20% for the
multi-class task.

None

Three different
classifiers (ANN,
k-nearest neighbor,
and SVM)

Happiness, sadness, love,
and, anger

78.11% for ANN
classifier.

-

Information 2019, 10, 239

7 of 39

2.3. Literature Review on Multi-Modal Emotion Recognition
Recently, a little work has investigated multiple modalities to recognize emotions [9–12].
Many studies have fused facial expression together with physiological signals [30]. Table 3 [30–33]
presents some of the surveyed studies on the multi-modal emotion fusion, including the corpus,
modalities of fusion, feature extractors, fusion approach, classifier, and classification accuracy.
As indicated in the literature and to the best of our knowledge, there are no works reported on
fusing speech with EEG for recognizing emotions. Furthermore, the resulting accuracies are still
below expectations.
It is also observed that the large number of multi-modal emotion fusion works is based on the
feature-level fusion that concatenates the features obtained from the signals of multiple modalities
before feeding them to the classifier. This may exceed the risks of conflicting information that may
be carried by the sole modalities. On the other hand, a little work has been implemented on the
decision-level fusion, which attempts to manipulate each sole modality separately, and integrates later
the results from their classifiers for making the final recognition.
In the two cases of multi-modal emotion fusion, the high dimensionality or the redundant
features resulting from the processing of each modality may make the learning process complex for
the most used machine learning algorithms like the NN classifier, which will be optimized in this
work. The training step of NN classifier is a crucial procedure. A high computational time is needed if
NN is trained by high-dimensional data. It presupposes network architecture of a huge input layer,
which significantly maximizes the weights number, usually causing an infeasible training.
This issue could be solved by minimizing the input space dimensionality to a manageable size,
then a network is trained on fewer dimensions. Clustering is a strictly necessary solution that has been
exploited to make the dimensionality reduction of an NN training data through organizing group of
objects or patterns into clusters. Accordingly, objects within the cluster itself reveal common attributes
and others within different clusters reveal dissimilarity. k-means [34] and FCM [35] are the most
employed clustering algorithms to train NN. As for FCM, a soft partitioning is executed as a pattern
and is esteemed as a member of all clusters, but distinct membership degrees are set for distinct clusters.
Despite that, the two algorithms are centroid-based, so they previously assume a fixed cluster number,
and are fully sensitive for centroid initialization [36].
On the contrary, several problems do not have knowledge on the clusters number a priori.
Numerous studies suggested solutions for these problems through running the algorithm repeatedly
along with diverse fixed centroid value k and with diverse initializations. Nevertheless, this might not
be doable with the big datasets. Moreover, running the algorithm through using a limited centroid
number might be inefficient because one solution relies on a limited initialization set. This is referred
to as “clusters number dependency” issue [37].
For solving this issue, evolutionary approaches reveal alternative optimization approaches
utilizing stochastic principles for evolving clustering solutions. They also are based upon probabilistic
rules for returning the near-optimal solution among the global search space. A few evolutionary
methods have been suggested for optimizing clusters number in data partitioning issues [38,39]. In [38],
an algorithm of artificial bee colony was executed to mimic an intelligent foraging conduct of honey
bee swarms. In [40], a k-means was optimized through a GA, which esteems the impact of isolated
points. Several studies also suggested a number of approaches for NN optimization by GA [41–44].
To overcome the high dimensionality issue, this paper suggests a new multi-class NN model,
which is optimized by hybridization of the FCM and GA. For each sole modality, the GA selects the
optimal centroids for the FCM algorithm. Then, the NN is automatically trained with the optimized
solutions from each modality, which reduce the classification error without any prior knowledge about
the fuzzy clusters number. For comparison purposes, we developed another hybrid model of FCM,
GA, and NN, which is trained using a fixed number of fuzzy clusters. The fusion is then implemented
on the decision level.

Information 2019, 10, 239

8 of 39

Table 3. Research on multi-modal emotion recognition utilizing multiple modalities.
Reference

[30]

[31]

[32]

Publication Year

2019

2019

2017

Corpus

RML, Enterface05,
and BAUM-1s

Private database,
namely, Big Data and
a publicly available
database of Enterface05.

MAHNOB-HCI
database

Fused Modalities

Speech and image
features

Feature Extraction

2D convolutional neural
network for audio
signals, and 3D
convolutional neural
network for image
features.

Speech and video

For speech feature
extraction,
Mel-spectrogram is
obtained.
For video feature
extraction, a number of
representative frames
are selected from
a video segment.

Facial expressions
and EEG

As for facial expressions,
the appearance features
are estimated from each
frame block then the
expression percentage
feature is computed.
The EEG features are
calculated using the
Welch’s Averaged
Periodogram.

Fusion Approach

Deep belief network

Classifier

SVM

Classification Accuracy
85.69% for Enterface05
dataset in case of
multi-modal
classification based
upon six discrete
emotions.
91.3% and 91.8%,
respectively, for binary
arousal-valence model,
in case of multi-modal
classification using
Enterface05 dataset.

Extreme Learning
Machines (ELM)

The CNN is separately
fed with speech and
video features,
respectively.

An accuracy of 99.9%
and 86.4% for the favor
of ELM fusion using the
Big Data and
Enterface05,
respectively.

Feature-level fusion by
concatenating all
features within a single
vector, in addition to
decision-level fusion by
processing each
modality in a separate
way, and amalgamating
results from their special
classifiers in the
recognition stage.

For decision-level
fusion, LWF and EWF
are used.
For feature-level fusion,
the paper used several
statistical fusion
methods like Canonical
Correlation Analysis
(CCA) and Multiple
Feature Concatenation
(MFC).

For decision-level
fusion, LWF achieved
recognition rates of
66.28% and 63.22%,
for valence and arousal
classes, respectively.
For feature-level fusion,
MFC achieved
recognition rates of
57.47% and 58.62%,
for valence and arousal
classes, respectively.

Information 2019, 10, 239

9 of 39

Table 3. Cont.
Reference

[33]

Publication Year

2015

Corpus

Private database, which
were collected from
university students.

Fused Modalities

Feature Extraction

Fusion Approach

EEG images along
with speech
signals

For EEG images,
the features were
extracted using the
threshold, the Sobel
edge detection,
and some statistical
measures (e.g., mean,
variance, standard
deviation, etc.).
While the intensity,
the RMS Energy, and the
pitch were used for
speech signal feature
extraction.

None (the study
investigated correlation
of EEG images as well
as Speech signals)

Classifier

None

Classification Accuracy

The significance
accuracy of correlation
coefficient was about
95% for the favor of said
emotional status.

Information 2019, 10, x FOR PEER REVIEW

3. Proposed Methodology
Information 2019, 10, 239

10 of 39

10 of 39

The proposed system architecture is as depicted in Figure 1, which comprised five steps: (i)
Multi-modal
data acquisition,
3. Proposed
Methodology(ii) pre-processing, (iii) feature extraction, (iv) classification using the
proposed hybrid
c-means-genetic
algorithm-neural
network
model
(CM-GA-NN)
model, and
The fuzzy
proposed
system architecture
is as depicted in Figure
1, which
comprised
five steps:
(v) fusion (i)
onMulti-modal
the decision-level.
Speech
and EEG signals
are acquired
subjects simultaneously.
data acquisition,
(ii) pre-processing,
(iii) feature
extraction,from
(iv) classification
using
proposed
c-means-genetic
algorithm-neural
model (CM-GA-NN)
model,
Recorded the
signals
arehybrid
then fuzzy
pre-processed
to eliminate
noisenetwork
of external
interferences.
Next to preand (v) fusion on the decision-level. Speech and EEG signals are acquired from subjects simultaneously.
processing, some speaker-dependent and -independent features are estimated from speech signals.
Recorded signals are then pre-processed to eliminate noise of external interferences. Next to
For EEG signals,
features
estimated from
three domains:
Time,
frequency,
and time–frequency.
pre-processing,
someare
speaker-dependent
and -independent
features
are estimated
from speech
signals.
For classifying
unimodal
data (speech
andfrom
EEG),
the
proposed
FCM-GA-NN
model is used.
For EEG
signals, features
are estimated
three
domains:
Time,hybrid
frequency,
and time–frequency.
For
classifying
unimodal
data
(speech
and
EEG),
the
proposed
hybrid
FCM-GA-NN
model
is
used.FCM-GATo fuse speech and EEG information, two algorithms are experimented, where a separate
To fuse speech and EEG information, two algorithms are experimented, where a separate FCM-GA-NN
NN classifier is used for each modality, then the output is computed by integrating posterior
classifier is used for each modality, then the output is computed by integrating posterior probabilities
probabilities
of modalities.
sole modalities.
of sole

1. Architecture
the multi-modal
multi-modal emotion
awareaware
system.system.
FigureFigure
1. Architecture
ofofthe
emotion

3.1. Multimodal Data Acquisition

3.1. Multimodal
Data
There
areAcquisition
a number of multi-modal emotional databases that comprise either speech or facial
expressions with different types of physiological signals. However, to the best of our knowledge,

There are a number of multi-modal emotional databases that comprise either speech or facial
there are no available databases that integrate both speech and EEG information. The details about
expressions
with different
typescould
of physiological
signals.
However,
the best
knowledge,
multi-modal
data collection
be found in [45]. In
the literature,
there areto
different
typesofof our
stimuli
there are no
databases
integrate
both emotions.
speech and
information.
Thewere
details about
thatavailable
have been utilized
by thethat
researchers
to induce
In [7],EEG
different
genres of music
chosen
as
stimuli
for
inducing
EEG-based
emotions.
In
[46],
a
hypermedia
system,
namely
MetaTutor,
multi-modal data collection could be found in [45]. In the literature, there are different types of
was utilized as stimuli for students learning about a complicated science topic.
stimuli that have been utilized by the researchers to induce emotions. In [7], different genres of
To collect the multi-modal data in the current study, the subjects were 36 girls from the College of
music were
chosen
stimuli Sciences,
for inducing
emotions. In
[46], a(PNU)
hypermedia
Computer
andas
Information
PrincessEEG-based
Nourah Bint Abdulrahman
University
for girls, system,
namely MetaTutor,
was utilized
as stimuli
about
complicated
science
topic.
KSA, who voluntarily
participated.
Theirfor
agesstudents
range waslearning
between 18
and 20ayears.
The subjects
are
all
Arabic
native
speakers.
The
study
with
its
methodology
was
implemented
under
institutional
To collect the multi-modal data in the current study, the subjects were 36 girls from the College
ethical review of the University. The steps of the research ethics approach that we adopted are applied
of Computer and Information Sciences, Princess Nourah Bint Abdulrahman University (PNU) for
as follows:
girls, KSA, who voluntarily participated. Their ages range was between 18 and 20 years. The
subjects are all Arabic native speakers. The study with its methodology was implemented under
institutional ethical review of the University. The steps of the research ethics approach that we
adopted are applied as follows:

Information 2019, 10, 239

11 of 39

Information
2019, 10,
x FOR
PEER REVIEW
11 of 39
•
Before
the
experiment,
students were informed of the experiment purpose, and each completed

a consent form subsequent to an introduction about the steps of simulation.
• Information
subjects
employed
only
for the information
aim of thewill
current
•
Identities
of subjectsacquired
were keptfrom
anonymous
andwas
confidential,
where
the personal
research.
not be
ever associated or disclosed with any answer.
• The accuracy
suitability
health only
andformedical
information
used in current
•
Information
acquired and
from subjects
was of
employed
the aim of
the current research.
researchand
were
confirmed
by experts.
•
The accuracy
suitability
of health
and medical information used in current research were
experts. 35 music tracks, which were employed as external stimuli for inducing
Theconfirmed
materialsby
involved

emotions. The English music utilized in this research involved 5 genres: Namely, electronic, metal,
The materials involved 35 music tracks, which were employed as external stimuli for inducing
rock,emotions.
rap, andThe
hip-hop.
Theseutilized
genresin are
considered
to cover
a range
that
produces
discernable
English music
this research
involved
5 genres:
Namely,
electronic,
metal,
rock,
emotions.
Each
participant
listened
to
7
songs
from
each
genre
used
(one
by
one);
each
of which
rap, and hip-hop. These genres are considered to cover a range that produces discernable emotions.
was Each
followed
by 15
s of silence
in from
order
to genre
allowused
for (one
emotion
labeling.
In this
context,
participant
listened
to 7 songs
each
by one);
each of which
was
followedthe
by total
number
tracksinthat
participant
listened
35 (5 genres
× number
7 songs,of each
which
15 s ofofsilence
orderevery
to allow
for emotion
labeling.toInwas
this context,
the total
tracksof
that
every participant
listened toclasses),
was 35 (5
× 7 songs,ineach
of which
represents a type of emotion
represents
a type of emotion
asgenres
demonstrated
Table
4.
classes),
as demonstrated
in Tablewas
4. employed to avoid inducing anxiety and boredom in a
Only 60
s of every song
Only
60
s
of
every
song
was
employed
to avoid
inducing
anxietytheir
and boredom
in astatus
participant's
participant's brain, as depicted in Figure 2. The
subjects
annotated
emotional
by the end
brain, as depicted in Figure 2. The subjects annotated their emotional status by the end of each music
of each music track picking one of the seven available emotional keywords: Fear, surprise, happy,
track picking one of the seven available emotional keywords: Fear, surprise, happy, disgust, neutral,
disgust, neutral, anxiety, and sadness [7,45]. To avoid the participant’s subjectivity and
anxiety, and sadness [7,45]. To avoid the participant’s subjectivity and exaggeration while expressing
exaggeration while expressing emotions, the experiment was also noticed by an expert of
emotions, the experiment was also noticed by an expert of psychology who judged through listening
psychology
who judged
through
listening
to the
the final
participant’s
and
the final
to the participant’s
emotional
speech
and did
labeling ofemotional
emotions tospeech
exclude
the did
spoofed
labeling
of
emotions
to
exclude
the
spoofed
labeled
emotion.
According
to
the
keyword
used
by the
labeled emotion. According to the keyword used by the subject and the evaluation of the expert,
subject
and the speech/EEG
evaluation recording
of the expert,
the approved
speech/EEG
mapped to its
the approved
was mapped
to its representative
class.recording
Otherwise, was
the unapproved
representative
class. Otherwise, the unapproved one was excluded.
one was excluded.

Figure 2. Structure of audio tracks that were played for participants: 5 s of silence, then 60 s from every
Figure
2. Structure of audio tracks that were played for participants: 5 s of silence, then 60 s from
track, and in between 15 s for performing emotion labelling.

every track, and in between 15 s for performing emotion labelling.

While the participants were listening to the music tracks through headphones, the speech
While
the
participants
were listening
to the music
tracks
through
the speech and
and
EEG
signals
were acquired
simultaneously.
The speech
signals
wereheadphones,
acquired spontaneously
EEGusing
signals
were acquired
simultaneously.
The speech
were C660N”.
acquiredThe
spontaneously
a microphone,
namely:
“SHURE dynamic
cardioidsignals
microphone
distance fromusing
microphone tonamely:
speaker was
kept atdynamic
3 m. The speech
signals
were sampled
at 16 KHz.
Arabic from
a microphone,
"SHURE
cardioid
microphone
C660N".
The For
distance
speech
signal
acquisition,
each
subject
had
to
describe
the
feeling
induced
while
listening
to
the
audio
microphone to speaker was kept at 3 m. The speech signals were sampled at 16 KHz. For
Arabic



 

i.e., signal
“ @Yg. éJ.acquisition,
JºÓ AK @/I am feeling
depression”.
speech
each deep
subject
had to describe the feeling induced while listening to the
Likewise,
the‫ﺃﻧﺎ‬EEG
recordings
weredepression”.
taken using the Emotiv-EPOC System. The device involves
audio i.e.,
“ً ‫ﻣﻜﺘﺌﺒﺔ ﺟﺪﺍ‬
/I am
feeling deep
14
electrodes
together
with
2
reference
channels,
whichthe
present
accurate spatial
resolution.
The inner
Likewise, the EEG recordings were taken using
Emotiv-EPOC
System.
The device
involves
14 electrodes together with 2 reference channels, which present accurate spatial resolution. The
inner sampling rate of the device was 2048 Hz prior to filtering, and its output sampling equaled
128-samples/second. The electrode placement was chosen as 10/20. This placement is commonly
used in studies for EEG-based emotions recognition using visual and audio stimuli. This system
relies on relationships of diverse electrode positions existing on the scalp as well as the main

Information 2019, 10, 239

12 of 39

sampling rate of the device was 2048 Hz prior to filtering, and its output sampling equaled
128-samples/second. The electrode placement was chosen as 10/20. This placement is commonly used
in studies for EEG-based emotions recognition using visual and audio stimuli. This system relies on
relationships of diverse electrode positions existing on the scalp as well as the main cerebral cortex
Information
x FOR
REVIEW
12 of 39
side 2019,
[38]. 10,
Figure
3 PEER
depicts
the 16 electrodes, “AF3, F7, F3, FC5, T7, CMS, P7, O1, O2, P8, DRL, T8, FC6,
F4, F8, and AF4”, which were embedded to the record of the EEG signals. In the locations of P3/P4,
locations
of P3/P4,
channels
of average
reference
(CMS/DRL)
were situated.
Thebox
MATLAB
the channels
of the
average
reference
(CMS/DRL)
were situated.
The MATLAB
and Puzzle
synapseand
were
forwere
recording
signals
Emotiv EPOC
headset.
Puzzle
boxutilized
synapse
utilized
forof
recording
signals
of Emotiv EPOC headset.

Figure
3. Emotiv-EPOC
headset
with16-channel
16-channel placement;
placement; 14-channels
intended
to detect
headset
with
14-channelswere
were
intended
to detect
Figure
3. Emotiv-EPOC
signals of human brain, along with 2 reference channels situated adjacent to the ears. Channel locations

signals of human brain, along with 2 reference channels situated adjacent to the ears. Channel
were based upon the 10/20 system of electrodes placement.
locations were based upon the 10/20 system of electrodes placement.

The data collection was implemented during the analysis [7], by using only the parts of speech

The
collection
was
implemented
during
the analysis
by using
only
theofparts
of speech
and data
EEG data
that the
subjects
were listening
to music
through,[7],
leaving
out the
parts
annotation.
and EEG
data
that1260
the subjects
were listening
music
leaving
out the
parts
of annotation.
In this
regard,
speech samples,
as well asto1260
EEGthrough,
signals, were
acquired
from
students.
Thus,
theregard,
total number
samplessamples,
within ourasmulti-modal
emotional
2520.
The duration
each
In this
1260ofspeech
well as 1260
EEG database
signals, was
were
acquired
fromofstudents.
speech/EEG
signal
was
60
s.
Characteristics
of
the
corpus
and
its
statistics
are
demonstrated
in
Table
Thus, the total number of samples within our multi-modal emotional database was 2520.4,The
where
are sorted by
arousal
and60
valence
dimensions [46],
with theand
representation
of are
duration
ofemotions
each speech/EEG
signal
was
s. Characteristics
of along
the corpus
its statistics
their
classes
in
the
corpus.
demonstrated in Table 4, where emotions are sorted by arousal and valence dimensions [46], along
with the representation of their classes in the corpus.

Information 2019, 10, 239

13 of 39

Table 4. Corpus statistics, where emotions are sorted by arousal and valence, along with the representation of their classes in the corpus.
Valence

Emotion
Dimension

Arousal

High arousal

Positive
valence

Num. of music
tracks inducing
emotion

Samples number
in database

Negative
valence

Num. of music
tracks inducing
emotion

Samples number
in database

Class

Num. of music
tracks inducing
emotion

Samples number
in database

Happy

5

5 music tracks ×
36 participants
= 180 samples

Fear

5

5 music tracks ×
36 participants
= 180 samples

Neutral

5

5 music tracks ×
36 participants
= 180 samples

Anxiety

5

180

Surprise

5

180

Disgust

5

180

Sadness

5

180

Low arousal

Total

Others

180 samples representing positive valence-high
arousal emotion classes of each single modality
× 2 modalities (speech and EEG) = 360

540 samples representing negative valence-high
arousal emotion classes of each single modality
× 2 modalities = 1080
180 samples representing negative valence-low
arousal emotions of each single modality
× 2 modalities = 360

360 samples for each single modality
× 2 modalities = 720

Information 2019, 10, 239

14 of 39

3.2. Speech Signal Pre-Processing
To identify diverse speech multi-styles and emotion states, silent features had to be isolated.
As the recorded signals are of distinct sampling frequency rates, we down-sampled all signals to
8 KHz. Then, signals were sub-divided into non-overlapping segments (frames) of 32 ms (256 samples).
This frame length gave the best performance in this paper. According to [47], the unvoiced parts are
eliminated from signals based upon existing energy within the frames and the frames with lower
energy are eliminated before the feature extraction takes place. The speech segment (of 256 samples)
was classified as voiced, unvoiced, or silence by computing its discrete wavelet transform (DWT) at
scales m = 21 through m = 25 and computing the signal energy over each scale. Then, a decision
scheme based on scale-energy was employed for detecting voiced speech segments.
As the unvoiced speech is of a high frequency nature, its DWT energy was high at scale m = 21 .
Thus, this method determines the scale at which the DWT of signal reaches the highest energy.
The segment is considered unvoiced if its DWT reaches its highest energy at scale m = 21 . Otherwise,
it may be a silence or voiced segment relying on its energy over the higher scales. Accordingly, the DWT
energy over the scale m = 23 of each segment, which was not classified as unvoiced, is compared
to a predefined threshold. Thus, the segments that exceed this threshold are classified as voiced.
Otherwise, they are classified as silence.
According to [24], the median of segment energies over the scale m = 23 gives a good criterion for
discriminating the silence speech signals from the voiced ones. Thus, the threshold used in this regard
was the median of segment energy distribution at the silence and voiced speech segments computed
over scale m = 23 . The other voiced frames were then concatenated and the glottal waveforms were
obtained using inverse filtering as well as linear predictive analysis approach. The resulting speech and
glottal waveforms were filtered utilizing a 1st-order pre-emphasis filter [24] expressed by Equation (1).
The parameters of such filter were set as in [48].
H (S) = 1 − a ∗ S−1 0.9 ≤ a ≤ 1.0

(1)

where S → the speech signal and a → takes a value of 0.9375.
3.3. EEG Signal Pre-Processing
EEG signals comprise multiple extrinsic and intrinsic artifacts that obscure the waves of brain.
Extrinsic artifacts (i.e., wiring noise of the EEG sensor) have different frequencies that may interfere
with the brain waves. Eliminating these artifacts requires filtering frequencies that are out of EEG
signals scope. Accordingly, we employed a band-pass filter with 64 Hz and 0.5 Hz of the maximum
cutoff frequency and the minimum cutoff, respectively [49]. Likewise, the notch filter that filters out
the signal narrow frequencies band [50], was utilized for noise isolation from the ambient electrodes'
wire resulting from the signal of power line interference (i.e., 60 Hz).
To isolate intrinsic artifacts, the independent component analysis algorithm is employed to
determine artifactual elements (i.e., blinking, eye movement) found in EEG recordings. This algorithm
eliminates intrinsic artifacts from signal without loss through determination of the artifactual EEG
elements and subtraction of the elements, which are associated with intrinsic artifacts for obtaining
a cleaner EEG signal. It has also been frequently utilized in EEG clinical studies for detecting and
eliminating intrinsic artifacts [51].
3.4. Speech Feature Extraction
Recently, most of the speech emotion recognition systems shown in the literature are trained
only using a specific emotional database of particular persons. This may cause drawbacks like the
lake of generality. As a result, the emotion recognition accuracy of any person who does not belong
to this database will be low. Thus, eliminating the speech individual difference is the primary step
to enhance the speech emotion recognition accuracies and the system universality. In this stage,

Information 2019, 10, 239

15 of 39

feature extraction is implemented after dividing the speech signals into a sequence of non-overlapped
sub-frames/segments, each of which has a length of 256 samples. Then, the speaker-dependent and
speaker-independent features [20,21,52] are computed from each frame as follows. The selected features
have been shown in the literature to carry an important emotional content [20,21,52].
(a)

(b)

Speaker-dependent features: Usually involve valuable personal emotional information [52],
as demonstrated in Table 5. They comprise fundamental frequency, fundamental frequency
four-bit value [20], Mel-frequency cepstral coefficients (MFCC), etc.
Speaker-independent features: These features are used to suppress the speaker’s personal
characteristics. In [20], the speaker-independent features from emotional speech involve the
fundamental frequency average change ratio, etc.

3.5. EEG Feature Extraction
In this paper, we use a mixing set of EEG features that have been proven in the literature to be highly
effective in emotion recognition [26,29]. These features belong to time, frequency, and time–frequency.
The EEG signal is blocked into windows/frames. From the literature, the efficient window size ranges
from 3 to 12 s, when classifying the individuals’ mental state utilizing EEG signals [53]. Two methods
of windowing i.e., fixed windowing and sliding windowing, were tested to choose the superior method
in reference to classification accuracy. In this paper, we estimate 23 measures of time, frequency,
and time–frequency presented in Table 6, from 14 EEG channels.

Information 2019, 10, 239

16 of 39

Table 5. The estimated speaker-dependent and speaker-independent features.
Speech Features

Characteristics
Prosodic Features

Speaker-dependent

Speaker-independent

Speech Quality Features

Spectrum Features

Fundamental
frequency-related

Energy-related

Time length
correlation-related

Fundamental frequency

Short-time maximum
amplitude

Short-time average
crossing zero ratio

Breath sound

12-order MFCC

Fundamental frequency
maximum

Short-time average energy

Speech speed

Throat sound

SEDC for 12 frequency
bands (equally-spaced)

Fundamental frequency
four-bit value

Short-time average
amplitude

Maximum and average values of first,
two as well as three formant frequencies

Linear Predictor
Coefficients (LPC)

Fundamental frequency
average change rate

The Short-Time-Energy
average rate

Average variation of 1st, 2nd, and 3rd
formant frequency rate

1st-order difference MFCC

Standard deviation of
fundamental frequency

The amplitude of
short-time energy

Standard deviation of 1st, 2nd, and 3rd
formant frequency rate

2nd-order difference
MFCC

Change rate of four-bit
point frequency

Partial time ratio

Every sub-point value of 1st, 2nd,
and 3rd formant frequencies change ratio

Information 2019, 10, 239

17 of 39

Table 6. Time, frequency, and time–frequency domains features, extracted from EEG signals in this work.
Domain of Analysis

Time domain

EEG Feature

Explanation

Equation

Cumulative maximum

Highest amplitude of channel M until sample R

MaxRM = max(EEG1:R,M )

Cumulative minimum

Lowest amplitude of channel M until sample R

Mean

Amplitude average absolute value over the
various EEG channels

Median

Signal median over the various channels

Standard deviation

EEG signals deviations over the various channels
in every window

MinRM = min(EEG1:R,M )
PK
EEGRM
MeanM = R=1
K
K+1
MedianM = sort(EEG)
,M
2
s
K
P
1
SDM =
EEGRM 2
K − 1 R=1

Variance

EEG signal amplitude variance over the
various channels

VM =

K
P
1
EEGRM 2
K − 1 R=1

Kurtosis

Reveals the EEG signal peak sharpness

Smallest window components

Lowest amplitude over the various channels

1P
4
R (EEGRM − MeanM )
K
KurM = 
2
1P
2
R (EEGRM − MeanM )
K
SM = minEEGRM

Moving median using a window size of n

Signal median with channel M and a widow of
n -samples size



MovR,M = Median EEGR:R+n−1,M

Maximum-to-minimum difference

Difference among highest and lowest EEG signal
amplitude over the various channels

Max − MinM = maxEEGRM − minEEGRM

Peak

Highest amplitude of EEG signal over the various
channels in time domain

PM = maxEEGRM

R

R

R

Information 2019, 10, 239

18 of 39

Table 6. Cont.
Domain of Analysis

EEG Feature

Explanation

Equation

Peak to Peak

Time among EEG signal’s peaks over the
various windows

PTPM = PLM − arg max

Peak location

Location of highest EEG amplitude over channels

PLM = arg max

EEG signal’s Norm 2 divided by the square root of
samples number over the various EEG channels

s

Root-mean-square level

Root-sum-of-squares level

Frequency domain

Peak-magnitude-to-root-mean-square ratio

EEG signal’s Norm over the distinct channels in
every window
Highest amplitude of EEG signal divided by
the QM

R,M,LPM RM

RRM

PK

R=1 EEGRM

QM =

K
s

RLM =

2

K
P

R=1

PMM = s

|EEGRM |2

EEG:,M ∞
PK

R=1 |EEGRM |

2

K

Time-frequency
domain

Total zero crossing number

Points number where the EEG amplitude
sign changes

ZCM = |{R|EEGRM = 0}|

Alpha mean power

EEG signal power Pow in channel M in an interval
of [[8H, 15H ]]

αM = Pow(EEG:,M , F ∈ [8Hz, 15Hz])

Beta mean power

EEG signal power in Beta interval

βM = Pow(EEG:,M , F ∈ [16Hz, 31Hz])

Delta mean power

EEG signal power in Delta interval

δM = Pow(EEG:,M , F ∈ [0Hz, 4Hz])

Theta mean power

EEG signal power in Theta interval

θM = Pow(EEG:,M , F ∈ [4Hz, 7Hz])

Median frequency

Signal power half of channel M which is
distributed over the frequencies lower than MFM .

Pow(EEG:,M , F ∈ [0Hz, MFM ]) =
Pow(EEG:,M , F ∈ [MFM , 64Hz])

Spectrogram

The spectrogram (short-time Fourier transform),
is computed through multiplying the time signal
by a sliding time-window, referred as (M) .
The time-dimension is added by window location
and one outputs time-varying frequency analysis.
o refers to time location and K is the discrete
frequencies number.

SPR =

PM−1
R=0



j2πMR
EEG(M)W (M − o)exp − K ,

where0 ≤ M ≤ (K − 1)

Information 2019, 10, 239

19 of 39

3.6. The Classifier
3.6.1. The Basic Classifier
NN is a parallel processor distributed massively [41]. It involves single input layer, besides
one output layer, as well as one hidden layer. Such a hidden layer is like a connection in between
input/output layers through multiple weights, nodes, biases, in addition to activation functions.
The operational procedure of NN is denoted by the subsequent equations:
Wj =

k
X

ω jt Xt

(2)

t=1



Yj = φ Wj + Aj

(3)

where j indicates a neuron in the network, k is input parameters number, Xt (t = 1, 2, . . . , k) represents
the t-th input parameter, in addition to ω jt (t = 1, 2, . . . , k) , denotes respective synaptic weight. Initially,
every synaptic weight ω jt multiplies the input sample Xt , which conforms to it. Values of weight are
P
put in a summator , getting an output, indicated as W j . Afterwards, W j is applied to the activation
function φ for producing an output signal Y j , and A j denotes the bias.
This study deals with a multi-class classification problem where N classes of emotions are to be
recognized. According to the literature [54], the multi-class neural network (NN) classification can be
implemented using: (1) Either a single neural network model with K outputs or (2) a multi-NN model
with hierarchical structure. In this regard, there are two methods that can be employed to model the
pattern classes, one-against-all (OAA) approach, and one-against-one (OAO) approach [54–56].
In this paper, we present a multi-class back propagation neural network for emotion classification.
We adopt the OAA approach scheme, which operates on a system of M = N binary neural networks,
NN j , j = 1, . . . , N , where each single network, NN j , holds one output node P j that has an output
function F j being modulated to output F j (x) = 1or0 to determine whether or not the input sample x
belongs to class j . Accordingly, each neural network NN j is trained using the same dataset but with
different class labels.
For training the jth neural network NN j , the training set ΩT is subdivided into two sets,
j

j

j

j

ΩT = ΩT ∪ ΩT , where ΩT includes all the class j samples, which obtain the label 1, and ΩT includes
all the samples belonging to all of the rest classes, which obtain the label 0. The decision function
considers the activation function output at every neural network NN j , and outputs the label of the
class that confronts to neural network NN j , which obtains the highest output value by the activation
function of the Pj output node:
 
F(x, y1 , . . . , yM ) = arg max y j .

(4)

j=1,...,M

In this study, we constructed an OAA system of 7 binary NNs with 20 hidden nodes for each of
which. Selecting the appropriate parameters for the multi-class back-propagation neural network was
based upon minimizing the root-mean-square-error (RMSE) between target and the predicted output.
The parameters giving the best accuracy are demonstrated in Section 4.
3.6.2. Proposed Classifier Design
The proposed hybrid FCM-GA-NN model is depicted in Figure 4. The chromosome fitness (feature
selection outcome) is evaluated based on the optimal fuzzy clusters that reduce the NN classification
error with no prior knowledge about the number of fuzzy clusters. Thereafter, chromosomes are
encoded utilizing the resulting fuzzy memberships. Since the cluster number u is not determined,
the population may have chromosomes with uniform or different lengths. Therefore, a modified
single-point crossover algorithm is developed. Accordingly, the mutation algorithm changes every

Information 2019, 10, 239

20 of 39

dimension of a solution by a probability PT . Then, the fuzzy memberships are computed, and the
20 of 39
clustering indices of the object are taken based on the maximum fuzzy membership values respecting
different fuzzy
clusters.
In this regard,
some
empty
classesInmay
found when
two parents
of
membership
values
respecting
different
fuzzy
clusters.
thisberegard,
some crossing
empty classes
may be
different
lengths,
or
if
a
class
number
is
of
big
size.
In
case
of
an
empty
j-th
class,
the
center
u
will
be
found when crossing
two parents
n
o of different lengths, or if a class number is of big size. Inj case of
eliminated
from
. Update
the new memberships
with the
u
,
u
,
.
.
.
.
.
.
,
u
2
J
1
an empty j-th class, the center 𝑢
will beofeliminated
from {𝑢 , 𝑢 and
, . . . .centroids
. . , 𝑢 }. Update
ofremaining
the new
centroids
is
accordingly
implemented.
In
the
last
generation,
the
algorithm
generates
a
non-dominated
memberships and centroids with the remaining centroids is accordingly implemented. In the last
set of solutions
number
changes
in reference to population
size. Such
solutions
regarded
generation,
the whose
algorithm
generates
a non-dominated
set of solutions
whose
numberare
changes
in
as
equal
in
relation
to
the
fitness
values
obtained
by
fitness
function.
The
final
clustering
outcome
is
reference to population size. Such solutions are regarded as equal in relation to the fitness values
determined
based
upon
a
clustering
ensemble
method.
Algorithm
A1
reveals
the
pseudo-code
of
the
obtained by fitness function. The final clustering outcome is determined based upon a clustering
whole proposed
model
Appendix
A. the pseudo-code of the whole proposed model.
ensemble
method.
Algorithm
1 reveals
Information 2019, 10, x FOR PEER REVIEW

Figure
recognizing human
human emotions.
emotions.
Figure 4.
4. Flowchart of the proposed hybrid FCM-GA-NN model for recognizing
The steps of the proposed algorithm are as follow:
The
(a) Fuzzy-clustering of each modality’s emotional dataset
(a)
The purpose
algorithm
goes
to separation
of resembling
objects,objects,
i.e., feature
vectors
The
purposeofofa aclustering
clustering
algorithm
goes
to separation
of resembling
i.e., feature
into
shared
cluster
and
unalike
objects
into
distinct
clusters;
thus,
the
within-cluster
objects
similarity
vectors into shared cluster and unalike objects into distinct clusters; thus, the within-cluster objects
measure ismeasure
large and
contrarily
for the
between-cluster
objects. In this
paper,
eachpaper,
modality’s
similarity
is is
large
and is small
contrarily
small
for the between-cluster
objects.
In this
each
feature set is
separated
traininginto
andtraining
testing sets.
the FCM algorithm
is executed
to
modality's
feature
set isinto
separated
and Consequently,
testing sets. Consequently,
the FCM
algorithm
cluster
the relevant
emotion
vectorsemotion
in each modality’s
set individually
categories,
is
executed
to cluster
the relevant
vectors in training
each modality's
traininginto
set several
individually
into
i.e., emotional
speech
as well
as emotional
classes.
The developed
clustering
is
several
categories,
i.e.,classes
emotional
speech
classes EEG
as well
as emotional
EEG classes.
Theprocedure
developed
demonstrated
in
Equations
(5)–(9)
of
Algorithm
A2,
clustering procedure is demonstrated in Equations (5)–(9) of Algorithm 2, where:
𝐾 → the number of emotional classesX
inueach modality’s dataset,
µm 𝑘=, 1, k = 1, 2, . . . , K
(5)
𝑥 → the emotion feature vector of the sample
i=1 i,k
𝜇 → the emotion feature vector’s membership grade belonging to cluster 𝑖 at time 𝑚 ,
XK
0<∑
𝜇 < 𝐾,
0<
µm < K, i = 1, 2, . . . , u
(6)
k=1 i,k
𝜀 → decides time complexity and precision for clustering. (in this paper, it was set as 10 ).
(b) Chromosome representation
After applying fuzzy clustering, a 𝑐 × 𝑏 matrix, , representing fuzzy cluster centers, can be
considered as a chromosome in GAs terminology. Every chromosome’s gene is indicated by an

Information 2019, 10, 239

21 of 39

m a
k=1 (µi,k ) · xk
,i
PK
m )a
(
µ
k =1
i,k

PK
Rm
i

=

"X

+1
µm
=
i,k

u



h=1

Kam+1 =

XK

= 1, . . . , u, 1 < a

2
m 2
||xk − Rm
i || /||xk − Rh ||

Xu h

k =1

i=1



1
a−1

a

(7)

#−1

+1
(µm
) · ||xk − Ri ||2
i,k

(8)
i

(9)

where:
K → the number of emotional classes in each modality’s dataset,
xk → the emotion feature vector of the sample k ,
µm
→ the emotion feature vector’s membership grade belonging to cluster i at time m, 0 <
ik
ε → decides time complexity and precision for clustering. (in this paper, it was set as

PK

m
k=1 µik
10−10 ).

< K,

(b) Chromosome representation
After applying fuzzy clustering, a c × b matrix, representing fuzzy cluster centers, can be
Information
2019,
x FOR PEER REVIEW
21 of 39
considered
as 10,
a chromosome
in GAs terminology. Every chromosome’s gene is indicated by an element
of U of Equation (10). To express the decision variables, a matrix U is transformed into a vector
element
𝑈K1
of, ..,Equation
To express the decision variables, a matrix 𝑈 is transformed into a
= [B11 , . .of
.,B
B1n , . . . , B(10).
Kn ] , which is encoded as a single chromosome whose length is changing
vector
= [𝐵to n, . (see
. . , 𝐵 Figure
, . . , 𝐵 5)., . . . , 𝐵 ] , which is encoded as a single chromosome whose length is
according
changing according to 𝑛 (see Figure 5).


B
B1n 
 B
11
𝐵12 B
𝐵13 .. .. .. 𝐵
 𝐵

B2n 
𝐵21 B
𝐵22 B
𝐵23 . . . 𝐵
 B
(10)
U𝑈==

(10)
 ::
:: 
::
::
.. . .


𝐵K2 B
𝐵K3 .. .. .. 𝐵
𝐵K1 B
B
BKn

Figure 5.
Chromosome representation.
representation.
Figure
5. Chromosome

(c) Fitness
(c)
Fitness evaluation
evaluation
Applying
the
attempts
to determine
optimal
number
of clusters,
that is,that
for cluster
Applying theFCM
FCMalgorithm
algorithm
attempts
to determine
optimal
number
of clusters,
is, for
validity.
The
WB
index
[57]
is
utilized
to
find
cluster
number
u
that
diminishes
the
intra-cluster
cluster validity. The WB index [57] is utilized to find cluster number 𝑢 that diminishes the intravariancevariance
acv(µ, R)𝑎𝑐𝑣(𝜇,
and increases
inter-cluster
variance. Onvariance.
this basis,On
thethis
feature
selection
result
cluster
𝑅) and the
increases
the inter-cluster
basis,
the feature
(the
fitness)
of
the
chromosome
is
assessed
by
finding
the
optimal
fuzzy
clusters
that
minimizes
the
selection result (the fitness) of the chromosome is assessed by finding the optimal fuzzy clusters
classification
error
prior error
knowledge
onprior
the clusters
number.
that
minimizes
the without
classification
without
knowledge
on the clusters number.
To
handle
that
multi-criteria
decision-making
issue,
we
used
a
standalone
weighted
fitness function
To handle that multi-criteria decision-making issue, we used
a standalone
weighted
fitness
combining
two
single
processes
into
one
objective.
According
to
Equation
(11),
two
predefined
weights
function combining two single processes into one objective. According to Equation (11),
two
Wu and Wc are
associated
with
optimal
number
of clusters
, respectively.
p and classification
predefined
weights
𝑊 and
𝑊 are
associated
with
optimalunumber
of clusters error
𝑢 and
classification

error , respectively.

h i
FitFCM−GA−NN = Wu up + Wc [ce]
𝐹𝑖𝑡
= 𝑊 [𝑢 ] + 𝑊 [𝑐𝑒]

(11)
(11)

The optimal
optimal cluster
The
cluster number
number u
𝑢p is
is obtained
obtained by:
by:

𝑢
= 𝑎𝑟𝑔
𝑚𝑎𝑥WB
𝑊𝐵==
up =
arg max
u

where:

𝑒𝑐𝑣(𝜇,
(µ, R𝑅)
)
ecv
𝑎𝑐𝑣(𝜇,
(µ, R𝑅)
)
acv

(12)
(12)

u X
K
X

acv(µ, R) = 𝒖 𝑲 (Ui )−1 ||xk − Ri ||2
𝒂𝒄𝒗(𝝁, 𝑹) = i=1 k=1(𝑼𝒊 ) 𝟏 ‖𝒙𝒌 − 𝑹𝒊 ‖𝟐

(13)
(13)

𝒊 𝟏𝒌 𝟏

𝟏
𝒆𝒄𝒗(𝝁, 𝑹) = 𝒖
𝑼𝟐

𝒖 𝟏

𝟏

𝒖

(𝑼𝒊 )
𝝀 𝟏𝒂 𝝀 𝟏

𝟏

∗
𝒌∈𝒗𝝀 ∪𝒗𝒂

(𝝁𝝀𝒌 ⋅ 𝝁𝒂𝒌 )
|𝒗𝝀 | + |𝒗𝒂 |

‖𝑹𝝀 − 𝑹𝒂 ‖𝟐

(14)

Information 2019, 10, 239

22 of 39


−1
u
u−1
 X (µ · µ ) 
1 X X
λk
ak 

 ||Rλ − Ra ||2
(Ui )−1 ∗ 
ecv(µ, R) = u

U2
|vλ | + |va | 
λ=1 a=λ+1

Ui =

K
X

(14)

k∈vλ ∪va

µ2ik /|vi |, i = 1, 2, . . . , u

(15)

k =1

vi = {k|Iik = 1}, Iik




 1, f µik = max µhk
1≤h≤u
= 

 0, otherwise

(16)

and U2u indicates a combination computation.
The classification error is computed using:
ce =

NE
NT

(17)

where NE ← the number of incorrectly classified samples. NT ← the total number of training instances.
(d) Binary tournament selection
Binary tournament selection technique [58] is utilized in this work to choose parents for creating
new generation. Accordingly, two individuals are picked haphazardly for playing a tournament.
During that, the winner is picked by ≺n , referred to as a crowded comparison operator. Such operator
relies on two attributes denoted as non-domination rank (Arank ) and crowding distance (Adist ). If A
and B are assumed to be two individuals, ≺n can be defined as follows. In applying this approach,
chromosomes of 40% top-ranking are picked for producing the child chromosomes through crossover
and mutation procedures.
A ≺n Bi f (Arank < Brank )or(Arank = Brank )and(Adis < B gist )

(18)

(e) Crossover procedure
In this framework, the length of every chromosome within the population is denoted by K × n .
The two parent chromosomes might be with equal or unequal lengths relying on the K values; therefore,
each cluster centroid may be indivisible while crossing two parents. Thus, a modified single-point
crossover algorithm is developed in this paper.
Assuming that R = {r1 , r2 , r3 , r4 } and M = {m1 , m2 , m3 , m4 } are two parent solutions having four
cluster centers, where each ri and mi represents a feature vector, D1 and D2 are two children created.
To perform uniform crossover, each two centers from parents are crossed with a probability of 0.5.
As depicted in Figure 6, the crossover is not implemented to gene 3, and values of r3 and m3 are copied
to gene 3 of children D1 and D2 , respectively. Values of genes 1, 2, and 4 have been updated to y1 , y2 ,
y4 and z1 , z2 , z4 on D1 and D2 , respectively. If the two parents are with different lengths (see Figure 6),
the centroids considered for crossover in the longer parent are randomly picked. Thus, r1 , r2 , r3 , r5
are chosen and crossed with m1 , m2 , m3 , m4 , respectively. Then, the omitted centers values (r4 ,r6 ) are
copied to an offspring. Algorithm A3 demonstrates the crossover procedure between parents. The new
centers’ values for two offspring are calculated using Equations (19)–(20) of Algorithm A3.
D1j =

[(1 + beta) ∗ Q1 + (1 − beta) ∗ Q2 ]
2

(19)

D2j =

[(1 − beta) ∗ Q1 + (1 + beta) ∗ Q2 ]
2

(20)

(f) Mutation procedure
In this procedure, a small probability of mutating PT is assigned to each gene, determined by
randomly generating a number (the gene is mutated if the generated number is less than PT , otherwise
not). According to this framework, a change of a gene within a chromosome will trigger a sequence of

procedure between parents. The new centers’ values for two offspring are calculated using
Equations (19)–(20) of Algorithm 3.
(f) Mutation procedure
In this procedure, a small probability of mutating 𝑃 is assigned to each gene, determined by
Information 2019, 10, 239
23 of 39
randomly
generating a number (the gene is mutated if the generated number is less than
𝑃 ,
otherwise not). According to this framework, a change of a gene within a chromosome will trigger a
sequence
of geneThus,
changes.
Thus,
the fuzzy memberships
of apoint
chromosome
point
be together
chosen to
gene changes.
the fuzzy
memberships
of a chromosome
will be chosen
to will
mutate
mutate
usingPTa. probability
𝑃 procedure
. The mutation
procedure in
is Algorithm
demonstrated
usingtogether
a probability
The mutation
is demonstrated
A4. in Algorithm 4.

Figure 6. Crossover procedure of two chromosomes in our framework.
Figure
6. Crossover procedure of two chromosomes in our framework.

(g) Obtaining final solution

(g)The
Obtaining
final solution
cluster validity indices are utilized by taking their ranking numbers, so we can choose
Thesolutions
cluster validity
indices
are
utilized
taking
their
ranking
numbers,
so weascan
choose
some
to integrate.
The
ranking
of by
every
cluster
validity
index
is illustrated
follows.
some
solutions
to
integrate.
The
ranking
of
every
cluster
validity
index
is
illustrated
as
follows.
Assuming the cluster validity indices number is V , a corresponding ranking vector of every solution is
Assuming
validity indices number is 𝑽 , a corresponding ranking vector of every
(s), rcluster
R(s) = (r1the
2 (s), . . . , rV (s)) , r j (s) ∈ {1, 2, . . . , n}, 1 ≤ j ≤ V . Besides that, the sum of cluster validity
Information
2019,
10,
x
FOR
PEER
23 is
of of
39
solution
is
𝑹(𝒔)
=
(𝒓
(𝒔),
𝒓REVIEW
. . . , 𝒓𝑽 (𝒔))Accordingly,
, 𝒓𝒋 (𝒔) ∈ {𝟏, 𝟐,
. . .sum
, 𝒏}, ranking
𝟏 ≤ 𝒋 ≤ number
𝑽 . Besides
that, solution
the sum
𝟏
𝟐
indices for every solution
s(𝒔),
is computed.
the
for every
is computed.
the asum
numberasfor
cluster
validity
every
solution
(s𝟏) for generating
computed
as rindices
. . . , n},
which is𝒔 integrated
to R𝑽Accordingly,
newranking
vector referred
{1, 2,
sum (s) ∈ for
∑
Therefore,
a
total
ranking
number
of
𝒔
is
𝑹
(𝒔)
=
𝒓
(𝒔)
.
The
smallest
value
of
𝑹𝒕 may be
0
𝒕
𝒋
𝒋
𝟏
every
𝒓𝒔𝒖𝒎
a
(s), . . . , rV +1as
(s))
R (s)solution
= (r1 (s),isr2 computed
. (𝒔) ∈ {𝟏, 𝟐, . . . , 𝒏}, which is integrated to 𝑹(𝒔) for generating
P
V +are
1 the best. Algorithm 5 shows the steps to
′ indices for a single solution
𝑽 + 𝟏vector
ifTherefore,
all cluster
validity
(
)
(
)
a total
ranking
s
is
R
s
=
r
s
.
The
smallest
value
of
R
may
be
V
+
1
new
referred
as 𝑹
(𝒔) = number
(𝒓𝟏 (𝒔), 𝒓of
(𝒔),
.
.
.
,
𝒓
(𝒔))
.
t
t
𝟐
𝑽 𝟏 j=1 j
getiffinal
clustering
solution.
Thea whole
processare
is illustrated
in FigureA57.shows
Manythe
clustering
ensemble
all cluster
validity
indices for
single solution
the best. Algorithm
steps to get
final
algorithms
have
been
compared
in
the
literature
[59].
As
the
meta-clustering
algorithm
showed
clustering solution. The whole process is illustrated in Figure 7. Many clustering ensemble algorithmsthe
highest
performance
of accuracy
rates (AVR), it algorithm
has been applied
in Step
13 of
have been
comparedininterms
the literature
[59].average
As the meta-clustering
showed the
highest
Algorithm
5.
performance in terms of accuracy average rates (AVR), it has been applied in Step 13 of Algorithm A5.

Figure7.
7. Clustering
Clustering ensemble
Figure
ensemblebased
basedfinal
finalsolution.
solution.

(h) Decision-level fusion

(h) Decision-level fusion
A decision-level fusion first processes every modality separately, and then integrates the outcomes
A decision-level fusion first processes every modality separately, and then integrates the
from their classifiers to reach the final decision. Motivated by [60], for every trial, assume that MxSpeech ,
outcomes
from their classifiers to reach the final decision. Motivated by [60], for every trial, assume
Mx 𝒙 and Mx0𝒙 ∈ [0, 1] denoting
the probability of classifier for class x ∈ [1, 2, 3] for speech, EEG,
that EEG
𝑴𝑺𝒑𝒆𝒆𝒄𝒉 , 𝑴𝑬𝑬𝑮 and 𝑴𝒙𝟎 ∈ [𝟎, 𝟏] denoting the probability of classifier for class 𝒙 ∈ [𝟏, 𝟐, 𝟑] for
and fusion, each in order. Subsequently, the class probabilities are obtained using
speech, EEG, and fusion, each in order. Subsequently, the class probabilities are obtained using
x
𝒙
βM𝒙xSpeech +
1 −−β)𝜷)𝑴
MxEEG
(21)
𝑴M𝒙 0==𝜷𝑴
+((𝟏
(21)
𝟎

𝑺𝒑𝒆𝒆𝒄𝒉

𝑬𝑬𝑮

where β is a modality weight. Subsequently, we tested two approaches to calculate β .

where 𝜷 is a modality weight. Subsequently, we tested two approaches to calculate 𝜷 .
Equal weights fusion method (EWF) that makes fusion on a decision-level, where each class
final probabilities are estimated using the class probability taken from every sole modality. That is,
𝑴𝒙𝟎 = 𝑴𝒙𝑺𝒑𝒆𝒆𝒄𝒉 + 𝑴𝒙𝑬𝑬𝑮 .

(22)

Information 2019, 10, 239

24 of 39

Equal weights fusion method (EWF) that makes fusion on a decision-level, where each class final
probabilities are estimated using the class probability taken from every sole modality. That is,
Mx0 = MxSpeech + MxEEG .

(22)

Learned weights fusion method (LWF) is a different method for the decision-level fusion,
where an optimal decision weight from each modality is numerically approximated. This is implemented
by changing from 0 to 1 and selecting the value that outputs the superior accuracy for the training set.
Then, an estimated weight is implemented to the sample, by Equation (22).
4. Experimental Results
This section discusses how the system is evaluated and compared to the state-of-the-art systems.
For testing the proposed FCM-GA-NN model performance, we not only conduct our experiments on
the collected multi-modal emotion database, but also on two different public unimodal datasets speech
and EEG, which are used in the literature for the purposes of comparisons.
4.1. Performance Evaluation Metrics
In this paper, the performance is assessed through four criteria: AVR, root mean squared error
(RMSE), percent deviation (PD), and correlation coefficient (ρ). Accuracy is referred to as a truly
classified samples ratio, T over the total samples number considered for classification, N . The RMSE is
employed for measuring the difference between both predicted and actual values. The correlation
coefficient reveals the degree of approximation between estimated and actual values. Calculating the
aforementioned indicators is as follows:
T
(23)
AVR =
N
r
1 XNum
(µx − µ y )2
(24)
RMSE =
i=1
Num
Num
1 X µx − µ y
× 100
Num
µx
i=1

PNum 
i=1 µ y − µY (µx − µX )

PD =

ρ= q

2 PNum
i=1 (µ y − µY )
i=1

PNum

(µx − µX )

(25)

.

(26)

2

Num ← the data points number.
µ y ← the output predicted by the model.
µx ← the sample actual value.
µY ← the mean of µ y .
µX ← the mean of µx .
4.2. Cross Validation (CV)
For each single modality, the accuracy of the hybrid classification algorithm is evaluated using
the nested cross validation [61,62]. In this method, two nested CV loops are used to test the classifier,
as shown in Figure 8. For the outer loop, the 1260 samples of each modality are split into seven
folds. This means that in each fold we will have 210 testing samples and 1050 training samples.
Every 210 samples are used as validation set (outer testing set), and the other 1050 samples are
combined as outer training set. This procedure is repeated for each fold. Then, each outer training set
is split into seven folds. Therefore, each fold will have 175 testing samples and 875 training samples.
A single set of 175 samples is accordingly used for validation (inner testing set), and the 875
samples are used as inner training set. This is implemented repeatedly for each fold. In this manner, the

samples. Every 210 samples are used as validation set (outer testing set), and the other 1050 samples
are combined as outer training set. This procedure is repeated for each fold. Then, each outer
training set is split into seven folds. Therefore, each fold will have 175 testing samples and 875
training samples.
A single set of 175 samples is accordingly used for validation (inner testing set), and the 875
Information 2019, 10, 239
25 of 39
samples are used as inner training set. This is implemented repeatedly for each fold. In this manner,
the inner CV is used for selecting the best parameters that reduces the 𝑅𝑀𝑆𝐸 of our algorithm. For
instance,
activation
functions
for parameters
the hidden that
andreduces
output the
layers,
theoflearning
rate, theFor
maximum
inner CV the
is used
for selecting
the best
RMSE
our algorithm.
instance,
generations,
mutation
the reproduction
ratio, the
etc.learning
The outer
CV
used for final
model
the activationthe
functions
for ratio,
the hidden
and output layers,
rate,
theismaximum
generations,
testing.
the mutation ratio, the reproduction ratio, etc. The outer CV is used for final model testing.

Figure 8.
8. Nested
Nested cross
cross validation.
Figure
validation.
Information 2019, 10, x FOR PEER REVIEW

of 39
4.3. First Experiment: Comparison of the Proposed FCM-GA-NN Model to the Developed FCM − GA − 25
NN
f ixed
4.3. First Experiment: Comparison of the Proposed FCM-GA-NN Model to the Developed
To
reveal
efficiencyofofthe
theproposed
proposed FCM-GA-NN
FCM-GA-NN model,
with
To𝐺𝐴
reveal
efficiency
model, ititisiscrucial
crucialtotocompare
compareit it
with
𝐹𝐶𝑀 −
−
𝑁𝑁thethe
another
developedmodel
modelthat
thatuses
usesFCM
FCMand
and GA
GA as
as hybrid
hybrid to
is is
another
developed
to train
train NN
NN[63].
[63].The
Thesecond
secondmodel
model
centroid-based
and
requirespreviously
previouslyaafixed
fixed cluster
cluster number.
number. Contrarily,
optimal
centroid-based
and
requires
Contrarily,our
ourmodel
modelfinds
finds
optimal
fuzzy
clusters
with
prior
knowledge
their
number.
Accordingly,
the
proposedmodel’s
model’serrors
errorsare
fuzzy
clusters
with
nono
prior
knowledge
onon
their
number.
Accordingly,
the
proposed
are
compared
with
the
second
hybrid
model.
Figure
9
depicts
the
bar
graph
illustrations
compared with the second hybrid model. Figure 9 depicts the bar graph illustrations representing the
representing
the and
AVR,correlation
𝑅𝑀𝑆𝐸 s, 𝑃𝐷𝑠
, and correlation
𝜌𝑠 among
both measured
AVR,
RMSE s, PDs,
coefficients
ρs amongcoefficients
both measured
and predicted
valuesand
from
predicted
values
from
the
two
models.
the two models.

(a)

(b)

(c)

(d)

Figure
9. The
results for the
proposed
and FCM −model,
GA − NN
Figure
9.overall
The evaluation
overall evaluation
results
for FCM-GA-NN
the proposedmodel,
FCM-GA-NN
and
f ixed
model,
on different
emotion
classes(speech
of each
single
𝐹𝐶𝑀 on
− 𝐺𝐴
− 𝑁𝑁 emotion
model,
different
classes
of each single
modality
and
EEG).modality (speech and
EEG).

Taking fear class as an instance, the AVR rates of the proposed FCM-GA-NN model are 98.85%
and 98.08% for EEG and speech modalities, respectively. By comparison, the AVRs using
𝐹𝐶𝑀 − 𝐺𝐴 − 𝑁𝑁
model are smaller than those of the proposed model, 95.66%, and 95.11% for

Information 2019, 10, 239

26 of 39

Taking fear class as an instance, the AVR rates of the proposed FCM-GA-NN model are 98.85%
and 98.08% for EEG and speech modalities, respectively. By comparison, the AVRs using FCM − GA −
NN f ixed model are smaller than those of the proposed model, 95.66%, and 95.11% for EEG and speech
Information 2019,
10, x FOR PEER
REVIEW
26 ofEEG
39
modalities,
respectively.
The
percent deviations (PDs ) are 21.42% and 21.24% for speech and
modalities, respectively, which are also higher than the values from the proposed model. Furthermore,
Table 7. The parameters that give the best accuracy in this work.
the proposed model exhibits ρ s of 0.9511 and 0.841, for EEG and speech modalities, respectively,
which are higher than those obtained
using the other model. The parametersValue
that give the best accuracy
Parameter
for the two models
are illustrated in Table 7.
Number of layers for each NN
3 (input, hidden, output)
Number of
hidden
nodes
in eachthat
NNgive the best accuracy
20 in this work.
Table
7. The
parameters
Activation functions forParameter
the hidden and output layers tansig-purelin
Value
Learning rule
Back-propagation
Number of layers for each NN
3 (input, hidden, output)
Learning
rateof hidden nodes in each NN
Number
20 0. 1
Momentum
constant
0. 7
Activation
functions for the hidden and output layers tansig-purelin
Learning rule

Units Learning
of population
rate
Momentum
constant
Maximum
generations
Units of population

Mutation
rate generations
Maximum
Mutation
Crossover
raterate

Crossover rate

Back-propagation
150
0. 1
0. 750
150
50 0.2
0.2 0.5
0.5

4.4. Second Experiment: Speech Emotion Recognition
4.4. Second Experiment: Speech Emotion Recognition
The second experiment depicted in Figure 10 was intended to investigate how robust the
The
second
experiment
depicted in NN
Figure
10 was
to emotion
investigate
how robustAccordingly,
the proposed
proposed
hybrid
fuzzy-evolutionary
model
is intended
for speech
recognition.
hybrid
fuzzy-evolutionary
NN
model
is
for
speech
emotion
recognition.
Accordingly,
two
two databases for speakers of two different languages were used to test the model. The firstdatabases
dataset
forisspeakers
two different
languages
weredataset
used toistest
the model.
The first[24].
dataset
collected
during
collectedofduring
this study.
The second
a public
one (SAVEE)
For is
each
dataset,
the
this
study. The
second
is afrom
public
(SAVEE)
[24].
For each
dataset,
emotional
features
emotional
features
aredataset
extracted
theone
speech
testing
samples.
Further,
the the
proposed
classifier
is
out. from the speech testing samples. Further, the proposed classifier is carried out.
arecarried
extracted

Figure
10.10.
Cross
validation
of speech
emotion
recognition
usingusing
the proposed
FCM-GA-NN
model.
Figure
Cross
validation
of speech
emotion
recognition
the proposed
FCM-GA-NN
model.

The comparative results for the proposed model over the two speech datasets are illustrated as
follows:

Information 2019, 10, 239

27 of 39

The comparative results for the proposed model over the two speech datasets are illustrated
Information
27 of 39
as
follows:2019, 10, x FOR PEER REVIEW
(a)
vividly
recognized
thatthat
using
the collected
dataset,
the proposed
model
(a) From
From Figure
Figure 9c,
9c,ititcan
canbebe
vividly
recognized
using
the collected
dataset,
the proposed
achieved
higher
AVRs
of
98.08%,
98.03%,
97.44%,
97.24%,
96.87%,
96.76%,
and
96.55%
for
fear,
model achieved higher AVRs of 98.08%, 98.03%, 97.44%, 97.24%, 96.87%, 96.76%, and 96.55%
neutral,
anxiety,
happy,
surprise,
disgust,
and
sadness
classes,
respectively.
Likewise,
it
can
for fear, neutral, anxiety, happy, surprise, disgust, and sadness classes, respectively. Likewise,be
observed
duringthat
training
thetraining
proposed
model
exhibits
lowerexhibits
RMSE of
9.6233,
9.9299,
10.8219,
it can be that
observed
during
the
proposed
model
lower
𝑅𝑀𝑆𝐸
of 9.6233,
10.9501,
11.5206,10.9501,
12.0312, 11.5206,
and 13.1127,
respectively,
for the
aforementioned
The model
9.9299, 10.8219,
12.0312,
and 13.1127,
respectively,
for theclasses.
aforementioned
also
gives
lower
PDsalso
of 16.99%,
17.15%,
18.19%,
18.35%,
19.13%,
19.32%,18.35%,
and 20.67%,
respectively,
classes.
The
model
gives lower
𝑃𝐷𝑠
of 16.99%,
17.15%,
18.19%,
19.13%,
19.32%,
for
samerespectively,
classes.
andthe
20.67%,
for the same classes.
(b) From Figure
(b)
Figure 11a,
11a, the
the proposed
proposedmodel
modelachieved
achievedhigher
higherAVRs
AVRsusing
usingthe
theSAVEE
SAVEEdataset,
dataset,98.98%,
98.98%,
happy,
fear,
98.96%, 98.93%,
98.93%, 98.11%,
98.11%,97.83%,
97.83%,97.42%,
97.42%,and
and97.21%
97.21%for
forsurprise,
surprise,anxiety,
anxiety,sadness,
sadness,
happy,
fear,
neutral, and
neutral,
and disgust
disgust classes,
classes, respectively.
respectively. The
The minimum
minimumand
andmaximum
maximum𝑃𝐷𝑠
PDsoccur
occuratatsurprise
surprise
class (15.31%)
(15.31%)and
anddisgust
disgustclass
class
(18.49%),
respectively.
highest
correlation
coefficient
𝜌
(18.49%),
respectively.
TheThe
highest
correlation
coefficient
ρ equals
equals 0.9801,
at class,
surprise
class,
lowest
value
0.733 at
and
notedclass.
at
0.9801,
which iswhich
foundisatfound
surprise
while
the while
lowestthe
value
is 0.733
andisnoted
disgust
disgust class.
By
it is
that the
the estimated
estimated measures
measuresgive
givesuperior
superiorresults
resultsover
overthe
thetwo
two
By comparison,
comparison, it
is observed
observed that
datasets.
setset
inin
combination
with
the
datasets. These
These results
results reflect
reflectthe
therobustness
robustnessofofthe
themixing
mixingspeech
speechfeature
feature
combination
with
proposed
hybrid
model.
On
the
other
hand,
there
are
low
noticeable
differences
with
respect
to
AVRs,
the proposed hybrid model. On the other hand, there are low noticeable differences with respect to
RMSE
and ρ, s,
obtained
on emotion
classesclasses
of the of
two
for thefor
favor
the of
SAVEE
AVRs, s,PDs
𝑅𝑀𝑆𝐸, s,𝑃𝐷𝑠
and
𝜌 s, obtained
on emotion
thedatasets,
two datasets,
the of
favor
the
dataset.
These differences
may be due
difference
in the stimuli
used
to induce
SAVEE dataset.
These differences
maytobethe
due
to the difference
in themode
stimuli
mode
used toemotions.
induce
Whereas
our
speech dataset
was dataset
acquired
using
music using
stimuli,
the SAVEE
was collected
emotions.
Whereas
our speech
was
acquired
music
stimuli,dataset
the SAVEE
dataset using
was
video
stimuli.
collected
using video stimuli.
Eventually,
results to
to the
the other
other published
publishedresults
resultsobtained
obtainedusing
usingthe
thepublic
public
Eventually, we
we compared
compared our results
database
of
SAVEE,
as
illustrated
in
Table
8
[24,64,65].
It
is
obvious
that
our
model
yields
superior
database of SAVEE, as illustrated
[24,64,65]. It is obvious that our model yields superior
results
the the
state-of-the-art
on speech
emotionemotion
recognition,
in terms ofinclassification
results which
whichoutperform
outperform
state-of-the-art
on speech
recognition,
terms of
classification
accuracy,
the totalwas
AVR
achieved
was 98.21%.
result supports
the
accuracy,
where
the totalwhere
AVR achieved
98.21%.
This result
supportsThis
the previously
published
previously
published
works
that have of
reported
the powerful
of the
hybrid classification
approaches
works
that have
reported
the powerful
the hybrid
classification
approaches
in optimizing
the speech
in optimizing
the speech
recognition
recognition
results
in general
[66,67]. results in general [66,67].

(a)

(b)

Figure11.
11.The
Theoverall
overallevaluation
evaluation results
results for the proposed
Figure
proposed FCM-GA-NN
FCM-GA-NNmodel
modelover
overtwo
twopublicly
publicly
available
databases:
(a)
SAVEE
database
for
speech
emotion
recognition,
and
(b)
MAHNOB
for
available databases: (a) SAVEE database for speech emotion recognition, and (b) MAHNOB forEEG
EEG
emotion recognition.
recognition.
emotion

Information 2019, 10, 239

28 of 39

Table 8. Comparison of the proposed model accuracy to the state-of-the-art accuracies obtained using SAVEE database.
Corpus

Reference

[24]

Year of
Publication

Feature Extraction

Feature Selection

2017

The study used 50 higher order
features (28 Bispectral feature +
22 Bicoherence), which were
combined with Inter-Speech
2010 features for improving the
recognition rate.

2018

2019

SAVEE

[64]

[65]

Proposed speech
emotion recognition
model

Classifier

Classification Accuracy

Feature selection included to
phases: Multi-cluster feature
selection, and proposed
hybrid method of
Biogeography-based
Optimization as well as
Particle Swarm Optimization.

SVM and ELM.

The speaker-independent
accuracies were 62.38% and
50.60%, for SVM and ELM,
respectively, whereas the
speaker-dependent
accuracies were 70.83%,
and 69.51%, respectively.

Feature set of 21 statistics

Principal Component
Analysis (PCA), Linear
Discriminant Analysis (LDA),
PCA + LDA.

Genetic algorithm- Brain
Emotional Learning model.

The speaker independent
accuracy was 44.18%,
when using PCA as
feature selector

The openSMILE toolbox was
used to extract 1582 features
from each speech sample

Proposed feature selection
method relies on the changes
in emotions according to
acoustic features.

SVM, k-nearest neighbor
(k-NN), and NN.

77.92%, 73.62%, and 57.06%
for SVM, k-NN, and NN,
respectively.

Mixing feature set of
speaker-dependent and
speaker-independent
characteristics.

Hybrid of FCM and GA.

Proposed hybrid
Optimized multi-class NN

98.21%

Information 2019, 10, x FOR PEER REVIEW
Information 2019, 10, 239

29 of 39
29 of 39

4.5. Third Experiment: EEG Emotion Recognition
4.5. Third Experiment: EEG Emotion Recognition
In terms of performance comparison purposes, we used two different datasets to check the
robustness
of of
theperformance
proposed model
regarding
EEG emotion
first dataset
In terms
comparison
purposes,
we used recognition.
two differentThe
datasets
to checkwas
the
collected
during
current research,
which contains
EEG signals
of only
Thecollected
second
robustness
of the proposed
model regarding
EEG emotion
recognition.
The14
firstchannels.
dataset was
dataset
is a public
one (MAHNOB)
[45],
which
comprises
EEG
signalsThe
of second
32 channels.
during current
research,
which contains
EEG
signals
of only 14
channels.
datasetMAHNOB
is a public
dataset
comprises[45],
userwhich
response
recordings
to a multimedia
content.
Video fragments
from online
one (MAHNOB)
comprises
EEG signals
of 32 channels.
MAHNOB
dataset comprises
user
sources,
last among
34.9 s andcontent.
117 s, using
content,
were sources,
chosen for
inducing
nine
responsewhich
recordings
to a multimedia
Videodifferent
fragments
from online
which
last among
emotions
thes,participants.
34.9 s andin117
using different content, were chosen for inducing nine emotions in the participants.
Next
video clip,
clip,the
thesubjects
subjectswere
wereasked
askedtoto
express
their
emotional
status
a keyword
Next to each video
express
their
emotional
status
by abykeyword
like
like
neutral,
amusement,
surprise,
happiness,
disgust,
sadness,
fear,
and anxiety.
For
each
neutral,
amusement,
surprise,
happiness,
anger,anger,
disgust,
sadness,
fear, and
anxiety.
For each
dataset,
dataset,
we estimated
a set of extracted
features
from
three domains,
frequency,
and time–
we estimated
a set of extracted
features from
three
domains,
frequency,
time, andtime,
time–frequency.
frequency.
the
proposed
model
is applied
classify
each
dataset emotions
into seven-class
Thereafter, Thereafter,
the proposed
model
is applied
to classify
eachtodataset
into
seven-class
of EEG
emotions
of
EEG
signals
as
depicted
in
Figure
12.
The
overall
comparative
results
for
the
proposed
signals as depicted in Figure 12. The overall comparative results for the proposed model over the two
model
over
two datasets
datasets
arethe
overviewed
as: are overviewed as:
Using
proposed
model
achieved
higher
AVRs
of 98.85%,
98.69%,
and
Using the
thecollected
collecteddataset,
dataset,thethe
proposed
model
achieved
higher
AVRs
of 98.85%,
98.69%,
98.57%,
respectively,
for
fear,
anxiety,
and
disgust
classes.
It
also
gives
lower
𝑅𝑀𝑆𝐸
s
of
8.7233,
and 98.57%, respectively, for fear, anxiety, and disgust classes. It also gives lower RMSE s of 8.7233,
8.9111,
8.9111, and
and 8.9555,
8.9555, respectively
respectively for
for these
these classes,
classes, as
as shown
shown in
in Figure
Figure 9d.
9d. The
The lowest
lowest and
and highest
highest 𝑃𝐷𝑠
PDs
found
fearclass
class(16.33%)
(16.33%)and
and
sadness
class
(19.72%),
respectively.
The greatest
𝜌 is 0.9511,
also
found at fear
sadness
class
(19.72%),
respectively.
The greatest
ρ is 0.9511,
also found
found
fear class.
at fearat
class.

Figure 12.
12. Cross validation of EEG emotion recognition using proposed model.
Figure

With respect
dataset,
thethe
proposed
model
givesgives
higherhigher
AVRs AVRs
of 98.96%,
98.91%,
With
respect totothe
theMAHNOB
MAHNOB
dataset,
proposed
model
of 98.96%,
and
98.55%,
respectively,
for
surprise,
neutral,
and
sadness
classes.
On
contrast,
it
gives
lower
RMSE
98.91%, and 98.55%, respectively, for surprise, neutral, and sadness classes. On contrast, it givess
of 8.0112,
8.5402,
8.9555,
respectively,
for the
aforesaid classes,
shown inclasses,
Figure 11b.
lower
𝑅𝑀𝑆𝐸
s of and
8.0112,
8.5402,
and 8.9555,
respectively,
for theasaforesaid
as shown in
In
terms
of
performance
comparisons
over
the
two
datasets,
the
overall
AVR
of
the
proposed
Figure 11b.
classifier
is
98.26%,
which
shows
low
noticeable
differences
on
the
emotion
classes
for
the
favor of
In terms of performance comparisons over the two datasets, the overall AVR of the proposed
MAHNOB
database.
This shows
can be interpreted
by the
differences
mode
employed
induce
classifier
is 98.26%,
which
low noticeable
differences
on in
thestimuli
emotion
classes
for thetofavor
of
emotions,
where
MAHNOB
database
uses
visual
stimuli.
From
these
results,
we
deduce
that
the
MAHNOB database. This can be interpreted by the differences in stimuli mode employed to induce
selected EEG
feature
set in combination
to the
proposed
FCM-GA-NN
is robust
for classifying
emotions,
where
MAHNOB
database uses
visual
stimuli.
From thesemodel
results,
we deduce
that the

Information 2019, 10, 239

30 of 39

emotions from EEG signals. We also conducted a comparison of the impact of fixed/sliding windowing
methods on AVRs over the two datasets. As demonstrated in Table 9, the comparison reveals that
fixed windowing returns better AVRs over the two compared datasets. The window size of 6 S was
found to give the best AVR. We also compared our results to the other state-of-the-art results obtained
using MAHNOB database, as shown in Table 10 [68–70]. The results indicate that the proposed model
outperforms the state-of-the-art concerning EEG emotion recognition.
Table 9. Comparison of using fixed windowing and sliding windowing on AVRs over the two
compared databases.
Dataset

Collected database
MAHNOB

AVR (%)
Fixed window

Sliding window

98.06
98.26

96.93
97.41

Table 10. Comparison of the proposed model accuracy to the state-of-the-art accuracies obtained using
MAHNOB database.
Corpus

Feature Extraction

Feature Selection

Classifier

Classification
Accuracy

2017

Features of time,
frequency, and
time–frequency.

Genetic Algorithm,
Ant Colony
Optimization, Particle
Swarm Optimization,
and Differential
Evolution.

Probabilistic
Neural Network

96.97 ± 1.893%

[69]

2018

Empirical Mode
Decomposition
(EMD) and the
Wavelet Transform.

Heuristic Black Hole
Algorithm

Multi-class SVM

92.56%

[70]

2018

EMD and the Bat
algorithm

Autonomous Bat
Algorithm

Multi-class SVM

95%

Hybrid of FCM and
GA.

Optimized
multi-class NN

98.06%

Reference

[68]

Year of
Publication

MAHNOB

Proposed
model

4.6. Fourth Experiment: Multi-Modal Emotion Recognition
The experiment is depicted in Figure 13. Figure 14 depicts comparisons of the multi-modal emotion
recognition results obtained using the two decision level algorithms i.e., EWF and LWF, when changing
the value of the NN weight. From the figure, LWF outperforms the EWF over all cases of changing
the learned NN weight. This recommends that predicting or approximating a weighting among the
modalities performs better than classifying the samples employing individual modality outputs just
Information
2019, 10, x FOR PEER REVIEW
31 of 39
as features.

Figure
Flowchart ofof
multi-modal
emotion
recognition
using speech
and EEG
information.
Figure
13.13. Flowchart
multi-modal
emotion
recognition
using
speech
and EEG
information.

Figure 13. Flowchart of multi-modal emotion recognition using speech and EEG
Information
2019, 10, 239
information.

31 of 39

(a)

(b)
Figure
14.Multi-modal
Multi-modal
emotion
recognition
results
usingand
(a)(b)
EWF
(b)the
LWF
versus
the
Figure 14.
emotion
recognition
results using
(a) EWF
LWFand
versus
learned
weights.
learned weights.
Although the results taken by speech modality are only lower than those obtained with EEG only,
this recommends
that
it is imperative
to fuse speech
andare
EEG
signals,
because
there obtained
are emotional
Although the
results
taken by speech
modality
only
lower
than those
withstates
EEG
that are
recognized
in a superior
from EEG to
than
from
speech.
Eventually,
the overall
performance
only,
this
recommends
that it way
is imperative
fuse
speech
and
EEG signals,
because
there are
of multi-modal
systeminisa98.53%,
which
higher
inthan
comparison
to performance
taken
emotional
statesemotion
that areaware
recognized
superior
way is
from
EEG
from speech.
Eventually,
the
by
our
superior
unimodal
system,
which
is
based
upon
EEG.
Thus,
the
decision-level
fusion
is
superior
overall performance of multi-modal emotion aware system is 98.53%, which is higher in
than the soleto
measurement.
comparison
performance taken by our superior unimodal system, which is based upon EEG.
conclusion,
consideration
of multiple
modalities
is useful during the time that some modality
Thus,Inthe
decision-level
fusion is superior
than
the sole measurement.
features
are
lost
or
unreliable.
This
might
happen,
for
instance,
the during
process of
is
In conclusion, consideration of multiple modalities iswhen
useful
thefeature
time detection
that some
critical duefeatures
to noisyare
environmental
factors, when
the signals
through transmission,
or, if the
modality
lost or unreliable.
This corrupting
might happen,
for instance,
when the process
of
system
is
incapable
of
recording
one
of
the
modalities.
Therefore,
the
emotion
recognition
system
must
feature detection is critical due to noisy environmental factors, when corrupting the signals through
be robust enough to manage these real-life naturalistic scenarios.
4.7. Computational Time Comparisons
The total run time (in seconds) is computed over our corpus for the three modalities, by testing
the two algorithms, i.e., proposed FCM-GA-NN and FCM − GA − NN f ixed . In terms of computational
run time, Figure 15 depicts the comparative performance analysis of the two classifiers, during the CV.
In comparison to the proposed FCM-GA-NN model, we can find that multi-modal emotion recognition
using FCM − GA − NN f ixed model reaches the highest total computational time of 3150 s. On contrary,
the total computational time achieved by the proposed FCM-GA-NN model is reduced by about 1512 s.

the two algorithms, i.e., proposed FCM-GA-NN and 𝐹𝐶𝑀 − 𝐺𝐴 − 𝑁𝑁
. In terms of
computational run time, Figure 15 depicts the comparative performance analysis of the two
classifiers, during the CV. In comparison to the proposed FCM-GA-NN model, we can find that
multi-modal emotion recognition using 𝐹𝐶𝑀 − 𝐺𝐴 − 𝑁𝑁
model reaches the highest total
computational
the
Information 2019, 10,time
239 of 3150 seconds. On contrary, the total computational time achieved by
32 of
39
proposed FCM-GA-NN model is reduced by about 1512 seconds.

Figure 15.
comparisons
of the
threethree
modalities
(speech,
EEG, multi-modal),
Figure
15. Computational
Computationaltime
time
comparisons
of the
modalities
(speech,
EEG, multiusing the two
algorithms
the proposed
FCM-GA-NN
and FCM
− GA − NNFCM-GA-NN
modal),
using
the i.e.,
two
algorithms
i.e., the
proposed
and
f ixed .
𝐹𝐶𝑀 − 𝐺𝐴 − 𝑁𝑁
.
5. Conclusions and Future Work

5. Conclusions
Future Work
This paperand
introduces
a novel multi-modal emotion recognition framework by fusing speech
and EEG information. The EEG signals were utilized as internal channel complementing of speech
This paper introduces a novel multi-modal emotion recognition framework by fusing speech
for more reliable emotions recognition. For classification of unimodal data (speech and EEG) and
and EEG information. The EEG signals were utilized as internal channel complementing of speech
multi-modal data, a proposed hybrid FCM-GA-NN soft computing model was introduced. The fitness
for more reliable emotions recognition. For classification of unimodal data (speech and EEG) and
function of the algorithm checks the optimal number of fuzzy clusters that reduces the classification
multi-modal data, a proposed hybrid FCM-GA-NN soft computing model was introduced. The
error. The proposed model was compared to another developed FCM-GA-NN model that relies on
fitness function of the algorithm checks the optimal number of fuzzy clusters that reduces the
determination of a fixed number for fuzzy clusters, and the fitness function of the algorithm checks the
classification error. The proposed model was compared to another developed FCM-GA-NN model
optimal chromosome that reduces the classification error. Accordingly, the proposed model’s errors
that relies on determination of a fixed number for fuzzy clusters, and the fitness function of the
were compared with the second model. The overall evaluation results of the two model, on emotion
algorithm checks the optimal chromosome that reduces the classification error. Accordingly, the
classes show the superiority of the proposed model. The algorithm was also implemented on two
proposed model’s errors were compared with the second model. The overall evaluation results of
public databases for speech and EEG signals, namely SAVEE and MAHNOB, respectively. The overall
the two model, on emotion classes show the superiority of the proposed model. The algorithm was
performance of the proposed model reached 98.26%, 98.21%, and (98.06% and 97.28%) respectively for
also implemented on two public databases for speech and EEG signals, namely SAVEE and
MAHNOB, SAVEE, and our dataset of its two modalities (EEG and speech, respectively). These results
MAHNOB, respectively. The overall performance of the proposed model reached 98.26%, 98.21%,
outperform the state-of-the-art results obtained using SAVEE and MAHNOB databases. Although the
and (98.06% and 97.28%) respectively for MAHNOB, SAVEE, and our dataset of its two modalities
results taken by speech modality only (97.28%) are lower than those obtained with EEG only (98.06%),
(EEG and speech, respectively). These results outperform the state-of-the-art results obtained using
when applied to our dataset, this recommends fusing speech and EEG signals, because there are
SAVEE and MAHNOB databases. Although the results taken by speech modality only (97.28%) are
emotional states that are better recognized by the EEG than by speech. Therefore, after executing
lower than those obtained with EEG only (98.06%), when applied to our dataset, this recommends
the automatic classification of every modality, the two modalities were fused on the decision-level.
fusing speech and EEG signals, because there are emotional states that are better recognized by the
The comparative results of the fusion methods demonstrate that LWF performs better than EWF.
EEG than by speech. Therefore, after executing the automatic classification of every modality, the
By fusing speech and EEG modalities, the total computational time achieved by the proposed model
two modalities were fused on the decision-level. The comparative results of the fusion methods
also was reduced by about 1512 seconds than the FCM − GA − NN
model (with fixed number
demonstrate that LWF performs better than EWF. By fusing speechf ixed
and EEG modalities, the total
of centroids), when applied on our collected database. For the future work, we intend to recognize
computational time achieved by the proposed model also was reduced by about 1512 seconds than
emotions through using a deep learning approach, which is optimized by the bio-inspired algorithms.
the 𝐹𝐶𝑀 − 𝐺𝐴 − 𝑁𝑁
model (with fixed number of centroids), when applied on our collected
Author Contributions: Conceptualization, R.M.G. and A.D.A.; formal analysis, R.M.G. and A.D.A.; method and
algorithms, R.M.G.; validation, R.M.G. and A.D.A.; visualization, R.M.G. and and A.D.A.; project management,
R.M.G. and A.D.A.; writing—original draft: R.M.G.; writing—review and editing; R.M.G. and K.S.; supervision
and paper revision, K.S.
Funding: This research received no external funding.
Conflicts of Interest: The authors declare no conflict of interest.

Appendix A Algorithms
The pseudo-code of algorithms used in this paper is shown in Appendix A.

Information 2019, 10, 239

33 of 39

Algorithm A1: The proposed FCM-GA-NN model for emotion classification.
 
Inputs: (A) Parent population of each single modality Mp (speech and EEG), indicated as P Mp , (p = 1, 2).
 
Output: Offspring population Pt+1 Mp where each offspring contains an optimal fuzzy cluster set that
minimizes the classification error.
1.
2.
3.
4.
5.

For each sin gle modality Mp (p = 1, 2) do
Generate fuzzy memberships of each training sample from Mp by one-step FCM clustering of
Algorithm A2.
Construct coded chromosome contain clusters centroids.
Evaluate the fitness by Equations (11)–(17).
 
 
Implement binary tournament selection on modality’s population P Mp to get the mating pool P0 Mp .

6.

For each p → 1 to psize

7.

 
Decode two chromosomes from P0 Mp as matrices Cp and Cp+1 using Equation (10).

8.
9.
10.

Compute fuzzy centroid sets Fp and Fp+1 for Cp and Cp+1 , respectively.
For k = 1 to K // k indicates the index of kth centroid of the chromosome
Apply crossover on Fp (k) and Fp+1 (k) according to Algorithm A3 to get F0 p (k) and F0p+1 (k) .

11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.

Apply mutation on offsprings F0 p (k) and F0p+1 (k) according to Algorithm A4 to get Fp (k) and Fp+1 (k)
that represent updated centroid sets Np and Np+1 .
End For
End For
// Assigning object to a cluster
For j = 1 to Z
Compute distances among objects and every cluster centroid in N j , then update fuzzy memberships of
U j by Equation (8) of Algorithm A2.
Calculate the clustering indices by the maximum fuzzy memberships with regard to different clusters.
IF empty cluster is found THEN
Remove this empty cluster and then Go To Step 16.
ELSE
Assign fitness by Equations (11)–(17).
END
Get the resulting i-th offspring chromosome with its updated U j and objectives.
End For
 
Apply Algorithm A5 to the resulting population Pt+1 Mp to compute the final solutions by
clustering ensemble.
Train the NN with solutions having the optimal cluster set that minimizes the classification error.
End For
00

00

Information 2019, 10, 239

34 of 39

Algorithm A2: The clustering algorithm.
1.
2.
3.
4.

Determine the cluster number u .
Set the initial value of µm
at time m = 0 satisfying:
i,k
Pu
m
(5)
i=1 µi,k = 1, k = 1, 2, . . . , K,
PK
0 < k = 1 µm
< K, i = 1, 2, . . . , u , where:
(6)
i,k

P
m


If K

k=1 µi,k = 0, empty cluster
P

K
m

 I f k=1 µ = K, f eaturevector belongs to i
i,k

5.
6.

Assume that Kam indicates the cluster index at time m and its initial value at m = 0 as 0.
Set the centroid of a cluster i at time m as Rm
then compute the u cluster centroids for the partition as:
i
PK
a
m
k=1 (µi,k ) · xk
, i = 1, . . . , u, 1 < a.
(7)
Rm
= PK
i
m a
k=1 (µi,k )

7.

Update membership degree for each emotion vector xk :
"
#
 1 −1
Pu 
2
2 a−1
m+1
m
m
µi,k =
(8)
h=1 ||xk − Ri || /||xk − Rh ||

8.
9.
10.
11.
12.
13.

Calculate the clustering index
i
PK Pu h m+1 a
2
k=1 i=1 (µi,k ) · ||xk − Ri || ,

Kam+1 =

Kam+1

IF
− Kam ≥ ε THEN
m = m+1
Go to Step 7
END

(9)

Information 2019, 10, 239

Algorithm A3: Crossover algorithm.
Inputs: Parents Fp and Fp+1 assigned as R , and M , respectively, where R = {r1 , r2 , r3 , r4 }, and
M = {m1 , m2 , m3 , m4 }
Output: Children D1 , and D2
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.
23.
24.
25.
26.
27.
28.
29.
30.
31.

For i = 1 to y
IF Random(0, 1) <= 0.5 THEN
Ci = ri &Oi = mi
CROSS(Ci , Oi )
Else
Ci = ri &Oi = mi
End IF
End For
CROSS(C, O)
// random number between falls between 0 and 1
g = Random(0, 1)
IF g <= 0.5 THEN
beta = (2g)1/(etak +1)
Else
beta = (1/(2 ∗ (1 − g)))1/(etak +1)
End IF
For j = 1 to x // x is the maximum dimension
IF C j < O j THEN
Q1 = C j &Q2 = O j
Else
Q1 = O j &Q2 = C j
End IF
[(1 + beta) ∗ Q1 + (1 − beta) ∗ Q2 ]
D1j =
(19)
2
[(1 − beta) ∗ Q1 + (1 + beta) ∗ Q2 ]
(20)
D2j =
2
End For
IF Random(0, 1) <= 0.5 THEN
C = D1 &O = D2
Else
C = D2 &O = D1
End IF
Return D1 , and D2

35 of 39

Information 2019, 10, 239

36 of 39

Algorithm A4: Mutation algorithm.
Inputs: Set of crossover-subjected chromosomes CR , CR = {cr1 , cr2 , cr3 , . . . , crK }
Output: mutated chromosomes
1.
2.
3.
4.
5.
6.

7.
8.
9.
10.

// k is the index of kth centroid of the chromosome
For k = 1to K
For j = 1 to J // j is the index of chromosome point
Generate a random number from 0 to : B = random(0, 1)
IF B ≤ PT
Generate z j random number f1 , f2 ,. . ., f j,z
from [0, 1] for the jth point of the centroid Ekk, j
Pz j
Replace Ekk, j by fk / k=1 fk
End IF
End For
End For

Algorithm A5: Final solution computation by clustering ensemble.
Inputs: Solutions set T = {s1 , s2 , . . . , sn } indicating fuzzy memberships.
Output: Final cluster label S∗ .
1.
2.
3.

For j = 1 to V // for each solution>
 
Calculate the ranking vectors R0 s j .
Calculate the aggregated ranks based upon the sum of each solution to get a vector:


(1)
(2)
(n)
e
Rt = R j , .R j , .., R j .

4.

End For
Sort T by the aggregated ranks (values of R ) in ascending order and obtain a new solutions set:
o
n
T0 = s01 , s02 , . . . , s0n .

5.

Assume an ensemble of Z size.

6.

n
o
Select the first Z solutions to get a subset of non-dominated solutions: Tnew = s01 , s02 , . . . , s0Z

7.

For j = 1 to Z // for each non-dominated solution s0j in Tnew

8.

Decode s0j by Equation (10).

9.

Assign every object k of s0j to a cluster i by the maximum of µm
, where 1 ≤ k ≤ K .
i,k

10.
11.
12.
13.
14.

Get a vector x j containing n cluster labels.
Considering the size of Tnew is Z , add the resulting vector x j to a matrix E of size Z × n .
End
Apply the clustering ensemble algorithm to the resulting matrix E, where each row represents one
clustering solution.
Return a vector S∗ comprising n cluster labels as the final output.

References
1.

2.
3.

Kolodyazhniy, V.; Kreibig, S.D.; Gross, J.J.; Roth, W.T.; Wilhelm, F.H. An affective computing approach to
physiological emotion specificity: Toward subject-independent and stimulus-independent classification of
film-induced emotions. Psychophysiology 2011, 48, 908–922. [CrossRef] [PubMed]
Liu, Y.-J.; Yu, M.; Zhao, G.; Song, J.; Ge, Y.; Shi, Y. Real-Time Movie-Induced Discrete Emotion Recognition
from EEG Signals. IEEE Trans. Affect. Comput. 2018, 9, 550–562. [CrossRef]
Menezes, M.L.R.; Samara, A.; Galway, L.; Sant’Anna, A.; Verikas, A.; Alonso-Fernandez, F.; Wang, H.; Bond, R.
Towards emotion recognition for virtual environments: an evaluation of EEG features on benchmark dataset.
Pers. Ubiquitous Comput. 2017, 21, 1003–1013. [CrossRef]

Information 2019, 10, 239

4.

5.
6.
7.
8.
9.
10.
11.

12.
13.

14.

15.
16.
17.

18.
19.
20.
21.
22.
23.
24.

25.

26.

37 of 39

Gharavian, D.; Bejani, M.; Sheikhan, M. Audio-visual emotion recognition using FCBF feature selection
method and particle swarm optimization for fuzzy ARTMAP neural networks. Multimed. Tools Appl. 2016,
76, 2331–2352. [CrossRef]
Li, Y.; He, Q.; Zhao, Y.; Yao, H. Multi-modal Emotion Recognition Based on Speech and Image. Adv. Multimed.
Inf. Process. – PCM 2017 Lecture Notes Comput. Sci. 2018, 844–853.
Rahdari, F.; Rashedi, E.; Eftekhari, M. A Multimodal Emotion Recognition System Using Facial Landmark
Analysis. Iran. J. Sci. Tech. Trans. Electr. Eng. 2018, 43, 171–189. [CrossRef]
Wan, P.; Wu, C.; Lin, Y.; Ma, X. Optimal Threshold Determination for Discriminating Driving Anger Intensity
Based on EEG Wavelet Features and ROC Curve Analysis. Information 2016, 7, 52. [CrossRef]
Poh, N.; Bengio, S. How do correlation and variance of base-experts affect fusion in biometric authentication
tasks? IEEE Trans. Signal Process. 2005, 53, 4384–4396. [CrossRef]
Majumder, N.; Hazarika, D.; Gelbukh, A.; Cambria, E.; Poria, S. Multimodal sentiment analysis using
hierarchical fusion with context modeling. Knowl.-Based Syst. 2018, 161, 124–133. [CrossRef]
Adeel, A.; Gogate, M.; Hussain, A. Contextual Audio-Visual Switching For Speech Enhancement in
Real-World Environments. Inf. Fusion 2019.
Gogate, M.; Adeel, A.; Marxer, R.; Barker, J.; Hussain, A. DNN Driven Speaker Independent Audio-Visual
Mask Estimation for Speech Separation. In Proceedings of the Interspeech 2018, Hyderabad, India,
2–6 September 2018.
Huang, F.; Zhang, X.; Zhao, Z.; Xu, J.; Li, Z. Image–text sentiment analysis via deep multimodal attentive
fusion. Knowl.-Based Syst. 2019, 167, 26–37. [CrossRef]
Ahmadi, E.; Jasemi, M.; Monplaisir, L.; Nabavi, M.A.; Mahmoodi, A.; Jam, P.A. New efficient hybrid
candlestick technical analysis model for stock market timing on the basis of the Support Vector Machine and
Heuristic Algorithms of Imperialist Competition and Genetic. Expert Syst. Appl. 2018, 94, 21–31. [CrossRef]
Melin, P.; Miramontes, I.; Prado-Arechiga, G. A hybrid model based on modular neural networks and
fuzzy systems for classification of blood pressure and hypertension risk diagnosis. Expert Syst. Appl. 2018,
107, 146–164. [CrossRef]
Engberg, I.; Hansen, A. Documentation of the Danish emotional speech database des 1996. Available online:
http://kom.aau.dk/~{}tb/speech/Emotions/des (accessed on 4 July 2019).
Wang, K.; An, N.; Li, B.N. Speech emotion recognition using Fourier parameters. IEEE Trans. Affect. Comput.
2015, 6, 69–75. [CrossRef]
Eyben, F.; Scherer, K.R.; Schuller, B.W.; Sundberg, J.; Andre, E.; Busso, C.; Devillers, L.Y.; Epps, J.; Laukka, P.;
Narayanan, S.S.; et al. The Geneva Minimalistic Acoustic Parameter Set (GeMAPS) for Voice Research and
Affective Computing. IEEE Trans. Affect. Comput. 2016, 7, 190–202. [CrossRef]
Tahon, M.; Devillers, L. Towards a Small Set of Robust Acoustic Features for Emotion Recognition: Challenges.
IEEE/ACM Transact. Audio Speech Lang. Process. 2016, 24, 16–28. [CrossRef]
Zhao, J.; Mao, X.; Chen, L. Speech emotion recognition using deep 1D & 2D CNN LSTM networks.
Biomed. Signal Process. Control 2019, 47, 312–323.
Liu, Z.-T.; Wu, M.; Cao, W.-H.; Mao, J.-W.; Xu, J.-P.; Tan, G.-Z. Speech emotion recognition based on feature
selection and extreme learning machine decision tree. Neurocomputing 2018, 273, 271–280. [CrossRef]
Alonso, J.B.; Cabrera, J.; Medina, M.; Travieso, C.M. New approach in quantification of emotional intensity
from the speech signal: Emotional temperature. Expert Syst. Appl. 2015, 42, 9554–9564. [CrossRef]
Cao, H.; Verma, R.; Nenkova, A. Speaker-sensitive emotion recognition via ranking: Studies on acted and
spontaneous speech. Comput. Speech Lang. 2015, 29, 186–202. [CrossRef] [PubMed]
D Griol, D.; Molina, J.M.; Callejas, Z. Combining speech-based and linguistic classifiers to recognize emotion
in user spoken utterances. Neurocomputing 2019, 326-327, 132–140. [CrossRef]
Yogesh, C.K.; Hariharan, M.; Ngadiran, R.; Adom, A.; Yaacob, S.; Polat, K. Hybrid BBO_PSO and higher
order spectral features for emotion and stress recognition from natural speech. Appl. Soft Comput. 2017,
56, 217–232.
Shon, D.; Im, K.; Park, J.-H.; Lim, D.-S.; Jang, B.; Kim, J.-M. Emotional Stress State Detection Using Genetic
Algorithm-Based Feature Selection on EEG Signals. Int. J. Environ. Res. Public Health 2018, 15, 2461.
[CrossRef] [PubMed]
Mert, A.; Akan, A. Emotion recognition based on time–frequency distribution of EEG signals using
multivariate synchrosqueezing transform. Digit. Signal Process. 2018, 81, 106–115. [CrossRef]

Information 2019, 10, 239

27.
28.
29.
30.
31.
32.
33.
34.

35.
36.
37.

38.
39.
40.

41.
42.

43.

44.
45.
46.
47.

48.

49.
50.

38 of 39

Zoubi, O.A.; Awad, M.; Kasabov, N.K. Anytime multipurpose emotion recognition from EEG data using
a Liquid State Machine based framework. Artif. Intell. Med. 2018, 86, 1–8. [CrossRef] [PubMed]
Zhang, Y.; Ji, X.; Zhang, S. An approach to EEG-based emotion recognition using combined feature extraction
method. Neurosc. Lett. 2016, 633, 152–157. [CrossRef] [PubMed]
Bhatti, A.M.; Majid, M.; Anwar, S.M.; Khan, B. Human emotion recognition and analysis in response to audio
music using brain signals. Comput. Human Behav. 2016, 65, 267–275. [CrossRef]
Ma, Y.; Hao, Y.; Chen, M.; Chen, J.; Lu, P.; Košir, A. Audio-visual emotion fusion (AVEF): A deep efficient
weighted approach. Inf. Fusion 2019, 46, 184–192. [CrossRef]
Hossain, M.S.; Muhammad, G. Emotion recognition using deep learning approach from audio–visual
emotional big data. Inf. Fusion 2019, 49, 69–78. [CrossRef]
Huang, Y.; Yang, J.; Liao, P.; Pan, J. Fusion of Facial Expressions and EEG for Multimodal Emotion Recognition.
Comput. Intell. Neurosci. 2017, 2017, 1–8. [CrossRef]
Abhang, P.A.; Gawali1, B.W. Correlation of EEG Images and Speech Signals for Emotion Analysis. Br. J.
Appl. Sci. Tech. 2015, 10, 1–13. [CrossRef]
MacQueen, J.B. Some methods for classification and analysis of multivariate observations. In Proceedings
of the Fifth Berkeley Symposium on Mathematical Statistics and Probability, Berkeley, CA, USA,
21 June–18 July 1965 and 27 December 1965–7 January 1966; pp. 281–297.
Bezdek, J. Corrections for “FCM: The fuzzy c-means clustering algorithm”. Comput. Geosci. 1985, 11, 660. [CrossRef]
Duda, R.O.; Hart, P.E.; Stork, D.G. Pattern Classification, 2nd ed.; Wiley: Hoboken, NJ, USA, 2001.
Ripon, K.; Tsang, C.-H.; Kwong, S. Multi-Objective Data Clustering using Variable-Length Real Jumping
Genes Genetic Algorithm and Local Search Method. In Proceedings of the 2006 IEEE International Joint
Conference on Neural Network, Vancouver, BC, Canada, 16–21 July 2006.
Karaboga, D.; Ozturk, C. A novel clustering approach: Artificial Bee Colony (ABC) algorithm. Appl. Soft
Comput. 2011, 11, 652–657. [CrossRef]
Zabihi, F.; Nasiri, B. A Novel History-driven Artificial Bee Colony Algorithm for Data Clustering. Appl. Soft
Comput. 2018, 71, 226–241. [CrossRef]
Islam, M.Z.; Estivill-Castro, V.; Rahman, M.A.; Bossomaier, T. Combining K-Means and a genetic algorithm
through a novel arrangement of genetic operators for high quality clustering. Expert Syst. Appl. 2018,
91, 402–417. [CrossRef]
Song, R.; Zhang, X.; Zhou, C.; Liu, J.; He, J. Predicting TEC in China based on the neural networks optimized
by genetic algorithm. Adv. Space Res. 2018, 62, 745–759. [CrossRef]
Krzywanski, J.; Fan, H.; Feng, Y.; Shaikh, A.R.; Fang, M.; Wang, Q. Genetic algorithms and neural networks
in optimization of sorbent enhanced H2 production in FB and CFB gasifiers. Energy Convers. Manag. 2018,
171, 1651–1661. [CrossRef]
Vakili, M.; Khosrojerdi, S.; Aghajannezhad, P.; Yahyaei, M. A hybrid artificial neural network-genetic algorithm
modeling approach for viscosity estimation of graphene nanoplatelets nanofluid using experimental data.
Int. Commun. Heat Mass Transf. 2017, 82, 40–48.
Sun, W.; Xu, Y. Financial security evaluation of the electric power industry in China based on a back
propagation neural network optimized by genetic algorithm. Energy 2016, 101, 366–379. [CrossRef]
Soleymani, M.; Lichtenauer, J.; Pun, T.; Pantic, M. A Multimodal Database for Affect Recognition and Implicit
Tagging. IEEE Trans. Affect. Comput. 2012, 3, 42–55. [CrossRef]
Harley, J.M.; Bouchet, F.; Hussain, M.S.; Azevedo, R.; Calvo, R. A multi-componential analysis of emotions during
complex learning with an intelligent multi-agent system. Comput. Human Behav. 2015, 48, 615–625. [CrossRef]
Ozdas, A.; Shiavi, R.; Silverman, S.; Silverman, M.; Wilkes, D. Investigation of Vocal Jitter and Glottal Flow
Spectrum as Possible Cues for Depression and Near-Term Suicidal Risk. IEEE Trans. Biomed. Eng. 2004,
51, 1530–1540. [CrossRef] [PubMed]
Muthusamy, H.; Polat, K.; Yaacob, S. Particle Swarm Optimization Based Feature Enhancement and Feature
Selection for Improved Emotion Recognition in Speech and Glottal Signals. PLoS ONE 2015, 10, e0120344.
[CrossRef] [PubMed]
Jebelli, H.; Hwang, S.; Lee, S. EEG Signal-Processing Framework to Obtain High-Quality Brain Waves from
an Off-the-Shelf Wearable EEG Device. J. Comput. Civil Eng. 2018, 32, 04017070. [CrossRef]
Ferree, T.C.; Luu, P.; Russell, G.S.; Tucker, D.M. Scalp electrode impedance, infection risk, and EEG data
quality. Clin. Neurophys. 2001, 112, 536–544. [CrossRef]

Information 2019, 10, 239

51.
52.
53.

54.
55.

56.

57.
58.
59.
60.
61.
62.

63.
64.
65.
66.

67.
68.
69.

70.

39 of 39

Delorme, A.; Makeig, S. EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including
independent component analysis. J. Neurosci. Methods 2004, 134, 9–21. [CrossRef] [PubMed]
Ayadi, M.E.; Kamel, M.S.; Karray, F. Survey on speech emotion recognition: Features, classification schemes,
and databases. Pattern Recogn. 2011, 44, 572–587. [CrossRef]
Candra, H.; Yuwono, M.; Chai, R.; Handojoseno, A.; Elamvazuthi, I.; Nguyen, H.T.; Su, S. Investigation
of window size in classification of EEG-emotion signal with wavelet entropy and support vector machine.
In Proceedings of the 37th Annual International Conference of the IEEE Engineering in Medicine and Biology
Society (EMBC), Milano, Italy, 25–29 August 2015.
Ou, G.; Murphey, Y.L. Multi-class pattern classification using neural networks. Pattern Recogn. 2007, 40, 4–18.
[CrossRef]
Yang, J.; Yang, X.; Zhang, J. A Parallel Multi-Class Classification Support Vector Machine Based on Sequential
Minimal Optimization. In Proceedings of the First International Multi-Symposiums on Computer and
Computational Sciences (IMSCCS06), Hangzhou, China, 20–24 June 2006.
Ghoniem, R.M.; Shaalan, K. FCSR - Fuzzy Continuous Speech Recognition Approach for Identifying
Laryngeal Pathologies Using New Weighted Spectrum Features. In Proceedings of the 2017 International
Conference on Advanced Intelligent Systems and Informatics (AISI), Cairo, Egypt, 9–11 September 2017;
pp. 384–395.
Tan, J.H. On Cluster Validity for Fuzzy Clustering. Master Thesis, Applied Mathematics Department,
Chung Yuan Christian University, Taoyuan, Taiwan, 2000.
Wikaisuksakul, S. A multi-objective genetic algorithm with fuzzy c-means for automatic data clustering.
Appl. Soft Comput. 2014, 24, 679–691. [CrossRef]
Strehl, A.; Ghosh, J. Cluster ensembles—a knowledge reuse framework for combining multiple partitions.
J. Mach. Learn. Res. 2002, 3, 583–617.
Koelstra, S.; Patras, I. Fusion of facial expressions and EEG for implicit affective tagging. Image Vision Comput.
2013, 31, 164–174. [CrossRef]
Dora, L.; Agrawal, S.; Panda, R.; Abraham, A. Nested cross-validation based adaptive sparse representation
algorithm and its application to pathological brain classification. Expert Syst. Appl. 2018, 114, 313–321. [CrossRef]
Oppedal, K.; Eftestøl, T.; Engan, K.; Beyer, M.K.; Aarsland, D. Classifying Dementia Using Local Binary
Patterns from Different Regions in Magnetic Resonance Images. Int. J. Biomed. Imaging 2015, 2015, 1–14.
[CrossRef] [PubMed]
Gao, X.; Lee, G.M. Moment-based rental prediction for bicycle-sharing transportation systems using a hybrid
genetic algorithm and machine learning. Comput. Ind. Eng. 2019, 128, 60–69. [CrossRef]
Liu, Z.-T.; Xie, Q.; Wu, M.; Cao, W.-H.; Mei, Y.; Mao, J.-W. Speech emotion recognition based on an improved
brain emotion learning model. Neurocomputing 2018, 309, 145–156. [CrossRef]
Özseven, T. A novel feature selection method for speech emotion recognition. Appl. Acoust. 2019, 146, 320–326.
[CrossRef]
Ghoniem, R.M. Deep Genetic Algorithm-Based Voice Pathology Diagnostic System. In Proceedings of the
Natural Language Processing and Information Systems Lecture Notes in Computer Science, Salford, UK,
26–28 June 2019; pp. 220–233.
Ghoniem, R.M.; Shaalan, K. A Novel Arabic Text-independent Speaker Verification System based on Fuzzy
Hidden Markov Model. Procedia Comput. Sci. 2017, 117, 274–286. [CrossRef]
Nakisa, B.; Rastgoo, M.N.; Tjondronegoro, D.; Chandran, V. Evolutionary computation algorithms for feature
selection of EEG-based emotion recognition using mobile sensors. Expert Syst. Appl. 2018, 93, 143–155. [CrossRef]
Munoz, R.; Olivares, R.; Taramasco, C.; Villarroel, R.; Soto, R.; Barcelos, T.S.; Merino, E.; Alonso-Sánchez, M.F.
Using Black Hole Algorithm to Improve EEG-Based Emotion Recognition. Comput. Intell. Neurosci. 2018,
2018, 1–21. [CrossRef]
Munoz, R.; Olivares, R.; Taramasco, C.; Villarroel, R.; Soto, R.; Alonso-Sánchez, M.F.; Merino, E.;
Albuquerque, V.H.C.D. A new EEG software that supports emotion recognition by using an autonomous
approach. Neural Comput. Appl. 2018. [CrossRef]
© 2019 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).

