Virtual Environment for Monitoring Emotional
Behaviour in Driving
Claude Frasson, Pierre Olivier Brosseau, Thi Hong Dung Tran
Université de Montréal
Département d’informatique et de recherche opérationnelle
2920 Chemin de la Tour, Montréal, H3T-1J4, Canada
{frasson,pierre-olivier.brosseau,mylife.tran}@iro.umontreal.ca

Abstract: Emotions are an important behaviour of humans and may arise in driving situations.
Uncontrolled emotions can lead to harmful effects. To control and reduce the negative impact
of emotions, we have built a virtual driving environment in which we can capture and analyse
emotions felt by the driver using EEG systems. By simulating specific emotional situations we
can provoke these emotions and detect their types and intensity according to the driver. Then,
in the environment, we generate corrective actions that are able to reduce the emotions. After a
training period, the driver is able to correct the emotions by himself.
Keywords: Emotions, Simulation, EEG, Driving, Emotional state

1

Introduction

Emotions play an important role in decision. Emotions can last from a few minutes to
several days (in this case they are called moods). What is more important is that they
place the driver into a cerebral state that will allow or disallow him/her to react
adequately to a cognitive or a decisive situation. Mental engagement is related to the
level of mental vigilance and alertness during the task. Sometimes engagement is
considered as the level of attention and motivation. The loss or diminution of
engagement is considered as a distraction [16]. Mental workload can be seen as the
mental effort and energy invested in terms of human information processing during a
particular task. If a driver is in a high mental workload he can ignore possible
dangers. While driving, these emotions can have very harmful effects on the road, or
even cause death. For instance, anger can lead to sudden driving reactions, often
involving collisions. Sadness or an excess of joy can lead to a loss of attention.
Generally, emotions that increase the reaction time in driving situations are the most
dangerous. Several questions arise. How do we measure or estimate the emotion of
the driver in certain situations? How can the driver act on his emotions to reduce their
intensity? How can we train the driver to react differently and control his emotions?
Different technologies can be used to assess emotions. We can use physiological
sensors that are able to evaluate seat position, facial recognition, voice recognition,
heart rate, blood pressure, sweating and the amount of pressure applied on the
adfa, p. 1, 2011.
© Springer-Verlag Berlin Heidelberg 2011

computer mouse. The galvanic skin conductivity is a good indication of emotional
change but its evaluation is not precise. The use of Electroencephalograms (EEG)
sensors is more precise and more recently used [17]. In fact, EEG signals are able to
detect emotions and cerebral states which, synchronized with the driving scene, can
highlight what happens in the brain and when. To reduce emotions, most of the
systems use a voice to interact with the driver. In the present paper we aim to assess
emotions felt by a driver in specific driving situations. For that, we have built a virtual
environment that is able to generate these emotions. Then, a virtual agent intervenes
to reduce the emotional impact so that the driver can return to a neutral emotion.
Following this introduction, we first comment first on previous works realized in this
domain. Then, we present the main components of our simulator, a virtual
environment that is able to generate emotions and an agent in charge of reducing
emotions. We describe the experiments realized, show the resulting emotions and the
measures obtained. Finally, we show how the system can reduce emotional reactions
and create an impact on reducing road accidents.

2

Previous work

Intuitively emotions play a role in driving, but even if they are not listed as a direct
factor in road accidents [4, 12], it is reported that 16 million drivers in the United
States have disabilities road rage [10]. What is the effect on driving when emotions
such as anger and excitement arise, since they increase the driver reaction time?
Nass, Jonsson et al. [1, 2] realized a study to determine whether a car equipped
with the ability to speak may influence the performance of its user. Participants of the
simulation were invited to converse with the voice of car. Results showed that when
the voice of the car met the voice of the participant (happy / sad / moderate) he had
less accidents, paid more attention to the road and was more involved in the
conversation with the voice of car. Jones and Jonsson [14] have presented a method to
identify five emotional states of the driver during simulations. They used neural
networks as classifiers, but they have not studied the impact of ambient noise.
Schuller et al. [3] also based their experiences on driving simulators recognizing four
emotions using support vector machines. However, these studies have shown that the
performance of emotion recognition depends largely on the ratio of noise that they
have also ignored.
Results obtained by Cai et al. [13] show that anger and excitement, in a scenario
involving several drivers, cause an increase in heart rate, breathing and skin
conductivity. More specifically, drivers who are not in the neutral state, cross more
the lines on the road, turn more on the wheel, and are changing lanes much faster
when they are angry or excited. We can conclude that the emotions of anger and
excitement negatively affect the control of the vehicle when driving as compared to
driving in a neutral state. And this control is directly connected to road safety.
Works undertaken by a team at The Institute Human-Machine Communication in
Munchen [18] confirm the influence of the affective state on driver performances.
Again, the study emphasized the importance of developing an intelligent system

inside the car. To achieve this, emotion plays a significant role in the comfort and
safety of driver’s performance. Facial expressions, voice, physical measurements,
driving parameters and contextual knowledge of the driver are important and reliable
methods for recognizing the emotions and state of the driver. A distraction detection
system is also under way and will assist the driver with a Lane Keeping System and a
Head Tracking System. Research on emotions detection is being funded by Toyota.
Their system, which is still in the prototype stage, can identify the emotional state of
the driver with a camera that stands 238 points on their face. The car can then make
suggestions to the driver, or simply adjust the music for relaxation. Everything is still
in the prototype stage, but Toyota says that their system could be available in their
next car generation.

3
3.1

The Emotional Car simulator
The environment

To generate and assess emotions in a driving situation we have built a Virtual
Environment able to simulate specific driving situations that could be a source of
emotions. The virtual environment takes the form of a game in which the player is a
driver who is submitted to a variety of realistic situations that everybody could
experience every day in traffic. Our environment is divided into 6 parts: the profile of
the user, the quiz (before and after simulation), the simulation, the emotion corrector
(an agent able to calm the user and reduce his emotions), and the result part. On the
right side of the interface we have integrated the measures which come in real time
from an EEG headset: Excitement, Engagement, Boredom, Meditation, and
Frustration. First of all, the user has to register and provide personal information
(profile) in the Virtual Environment, then he is submitted to the first quiz in which he
has to determine the perception of his own emotions. This quiz is invoked before and
after each scenario in the simulator. The simulator is the part intended to cause
emotional reactions. It is based on a video game where a user can experiment nine
different driving scenarios designed to generate emotions by using stimulating sounds
and mobile cars or trucks that suddenly arise in the traffic to disrupt the driving
behaviour of the user. The emotion corrector module is intended to reduce player’s
emotions. It is represented by a virtual emotional agent which is aware of user’s
emotion and will try to talk to him according to various scenarios, explaining the good
behavior to adopt in order to reduce his emotions.

3.2

Collecting the data

To collect the data we used the EPOC headset built by Emotiv. EPOC is a high
resolution, multi-channel, wireless neuroheadset which uses a set of 14 sensors plus 2
references to tune into electric signals produced by the brain to detect the user’s
thoughts, feelings and expressions in real time (Figure 1). Using the Affectiv Suite we
can monitor the player’s emotional states in real-time. This method was used to
measure the emotions throughout the whole simulation process. Emotions are rated
between 0 and 100%, where 100% is the value that represents the highest

power/probability for this emotion.

Figure 1. The experimental environment (EEG and simulation system)

3.3

The correcting agent

To correct the negative emotions generated in the scenarios we created an agent in
charge of neutralizing player’s emotions (Figure 2).

Figure 2. The correcting agent

Its soothing voice combined with its funny appearance are there to reassure the player
and tell him the proper behaviour to handle the scenario correctly and to reduce his
emotions.

3.4

The results

The last part of the virtual environment is an interface which shows the results. For
each user it is possible to select a given scenario and retrieve the emotions captured
by the EPOC headset during the simulation. Each pair of emotions can be hidden or
shown in the graphic using this interface. For instance, in Figure 3, on the left side of
the screen, we select the user (which is blurred out for privacy purposes), a scenario
and the emotions to display.

Figure 3. The variation of emotions while in traffic, x-axis is the time and y-axis is the
percentage of emotion.

The graph appears on the right and displays the progression of the selected emotions
over the course of scenario. It represents scenario #2, in which the player finds
himself stuck in the traffic. We can clearly see the slow increase in both frustration
and excitement.

4

Experimentation

From the EPOC we collect 3 main emotions: boredom, excitement and frustration.
Frustration is an emotion that occurs in situations where a person is blocked from
reaching a desired outcome. In general, whenever we reach one of our goals, we feel
pleased and whenever we are prevented from reaching our goals, we may succumb to
frustration and feel irritable, annoyed and angry. Typically, if the goal is important,
frustration and anger or loss of confidence will increase. Boredom creeps up on us
silently, we are lifeless, bored and have no interest in anything, due perhaps to a
build-up of disappointments, or just the opposite, due to an excess of stimuli that
leads to boredom, taking away our ability to be amazed or startled anymore when
things happen. Excitement is a state of having a great enthusiasm while calm is a state
of tranquility, free from excitement or passion.
The simulation module contains nine scenarios; each scenario is a different
situation likely to cause emotions. The first scenario is simply intended to help the
participant to become familiar with the car’s controls and the simulator. In the second
scenario, the participant is stuck in a traffic jam with a lot of noise. In the third
scenario, the participant drives near a school with a school bus waiting on the other
side of the road. The participant has to stop five meters before the bus and wait until
the stop signal is gone. In the fourth scenario the participant is driving straight up to
an intersection with a stop sign on his side, he has to wait for all other cars to pass. T

The fifth scenario is similar to the fourth, this time with a pedestrian crossing the
street. In the sixth scenario, the participant has to find a place in a public parking.
There is only one place left and before the participant can reach it, another car takes
it. The participant has to look around to find another place (Figure 4). In the seventh
scenario, the car is already on a highway at high speed and the brakes are no more
working. The participant has to stay calm, verifies if the brakes are working properly
and tries to stop the car. In the eighth scenario, a fire truck comes from behind and
starts its siren.

Figure 4. The parking lot (Scenario #6)

Figure 5. The Fire Truck (Scenario #8)

The participant has to move his car to the right and stay immobilised until the fire
truck is gone (Figure 5). In the last scenario, the participant is late for work. If the
participant takes the first turn right he will arrive at work on time but there is an
interdiction to turn right, he has to take the second right turn. The participant has to
respect the signalisation and turn where it is permitted, even if he is in a hurry.

5

Results

The subjects of this study were 30 college students from Quebec, 6 females and 24
males aged between 17 and 33. Amongst them, 24 had their driving license. In this
section, we present the common emotions generated during the scenarios. Participants
are excited when an event (pedestrian, siren, stop sign, parking, etc.) occurs. They
become very frustrated and excited when they realized that their brakes did not work
(41.2% frustrated and 70.6% excited). Participants got bored when they had to wait
for the pedestrian (23.5%) or when nothing happened (29.4% during the first
scenario). Participants became very frustrated when they caused a collision (70.6% in
scenario 6) or when they failed a scenario. The following figures show the influence
of a Fire Truck’siren and a brakes failure. Figure 6 shows the generation of
excitement when the Fire Truck started its siren. Figure 7 shows the generation of
frustration when the user noticed that the brakes failed. These data are for a single
user.

Figure 6: Excitement generated

Figure 7: Frustration (in red) generated.

We consider a generated emotion by observing the emotions that have increased their
value by at least 20% in the course of the scenario (Figure 6). A corrected emotion is
also defined by observing a decrease of at least 20% between the end of the scenario
and the end of the correcting agent phase (Figure 7).
Table 1. Average emotions generated in the simulator for all participants
Scenario/emotions

Excitement

Boredom

Frustration

1
2
3
4
5
6
7
8
9

35.3%
70.6%
70.6%
41.2%
52.9%
76.5%
41.2%
52.9%
52.9%

29.4%
11.8%
5.9%
11.8%
23.5%
11.8%
0.0%
0.0%
5.8%

11.7%
64.7%
47.1%
52.9%
41.2%
70.6%
70,6%
58.8%
64.7%

Strong emotions generated during the scenarios are corrected with the correcting
agent. The efficiency is the percentage of the amount of reduction. The correcting
agent worked with the best efficiency of 70.0% for the frustrated participants in
scenario 8 (Table 2), and 66.7% of excited participants in scenario 3.
Table 2. Efficiency of the correcting agent for all participants

Scenario/emotions

Excitement

Boredom

Frustration

1
2
3
4
5
6
7
8
9

66.7%
50.0%
66.7%
42.9%
66.7%
46.2%
57.1%
55.6%
44.4%

21.1%
8.6%
4.8%
10.3%
19.7%
7.5%
0.0%
0.0%
4.3%

50.0%
45.5%
62.5%
66.7%
57.1%
41.6%
58.3%
70.0%
36.4%

Figure 8 shows the influence of the correcting agent on excitation in this case.

Figure 8. The effect of the correcting agent on the high excitement of the user (while
the agent is talking).

6

Conclusion

Emotions affect the drivers’ behaviour. Strong emotions or negative emotions can
lead to aggressive reactions. Correcting these strong or negative emotions can have a
positive impact on the drivers’ safety. In our simulator the correcting agent has a
considerable impact to improve drivers’ actions. Excitement and frustration have
decreased after the driver followed the advices of the agent with an efficiency varying
from 36.4% to 70.0%. Driving is an everyday activity in which people rarely prevent
or stop these strong or negative emotions. The correcting agent improve the drivers’
emotional behaviour. Our emotional correcting agent has only corrected the emotions
of excitement and frustration, but it is also possible to create agents to correct other
negative or strong emotions such as boredom, which can cause distractions and
accidents on the road. Correcting this emotion is also an important step to improve
driving quality and is a good subject for further research. Instead of correcting these
emotions, preventing them is also a very interesting subject for further investigations.
Preventing strong or negative emotion will also prevent bad driving behaviours. By
training the emotional behaviour of the driver with successive use of our system it
would reduce the impact on the driver's emotional state. Experiments in virtual
environments have shown the improvement for the user in terms of emotional
reactions (flight simulations, phobia reduction) and we think that applying our system
repeatedly should have the same effect on drivers. A useful application of our system
could be installed in driving schools, combining training driving codes and adequate
emotional behaviour. In order to integrate portable technology, we could insert into
future cars a face reading system which is able to detect not only driver’s emotions
but also his weariness. This approach will contribute to the birth of intelligent cars
that will detect the capabilities and emotional conditions of the driver for a safer
environment.
Acknowledgments. We acknowledge the National Science and Engineering
Research Council (NSERC) for funding this work.

References
1. Jonsson, I.M., Nass, C., Harris, H., Takayama, L.: Matching In-Car Voice with Driver State: Impact on
Attitude and Driving Performance. In: Proceedings of the Third International Driving Symposium on
Human Factors in Driver Assessment, Training and Vehicle Design, pp. 173–181 (2005).
2. Nass, C., Jonsson, I.M., Harris, H., Reaves, B., Endo, J., Brave, S., Takayama, L.: Improving
Automotive Safety by Pairing Driver Emotion and Car Voice Emotion. In: Proc. CHI (2005).
3. Schuller, B., Lang, M., Rigoll, G.: Recognition of Spontaneous Emotions by speech within Automotive
Environment. In: Proc. 32. Deutsche Jahrestagung f¨ur Akustik (DAGA), Braunschweig, Germany, pp.
57–58 (2006).
4. Setiawan, P., Suhadi, S., Fingscheidt, T., Stan, S.: Robust Speech Recognition for Mobil Devices in Car
Noise. In: Proc. Interspeech, Lisbon, Portugal (2005).
5. Grimm, M., Kroschel, K., Narayanan, S.: Support vector regression for automatic recognition of
spontaneous emotions in speech. In: Proceedings of IEEE International Conference on Acoustics,
Speech, and Signal Proccessing (ICASSP) (Accepted for Publication) (2007).
6. Grimm, M., Kroschel, K., Harris, H., Nass, C., Schuller, B., Rigoll, G., Moosmayr, T.: On the Necessity
and Feasibility of Detecting a Driver’s Emotional State While Driving, Affective Computing and
Intelligent Interaction, Lecture Notes in Computer Science Volume 4738, 2007, pp 126-138 (2007).
7. Al-Shihabi, T., Mourant, R.R., 2003. Toward more realistic driving behavior models for autonomous
vehicles in driving simulators. Transportation Research Record, n 1843, 2003, p 41-49.
8. Lisetti, C.L. and Nasoz, F., 2004. Using noninvasive wearable computers to recognize Human Emotions
from Physiological Signals. EURASIP Journal on Applied Signal Processing 2004:11, 1672–1687.
9. Balling, O., Knight, M.R., Walters, B., Sannier, A., (2002). Collaborative Driving Simulation. SAE 2002
World Congress & Exhibition, March 2002, Detroit, MI, USA, Session: Vehicle Dynamics &
Simulation (Part A). Document Number: 2002-01-1222.
10. CNN news, 2006. CNN News Health Study: 16 million might have road rage disorder, June 5, 2006.
http://www.cnn.com/2006/HEALTH/06/05/road.rage.disease.ap/.
11. Cowie, R., Douglas-Cowie, E., Cox, C., 2005. Beyond emotion archetypes: Databases for emotion
modeling using neural networks. Neural Networks, v 18, n 4, May, 2005, Emotion and Brain, p 371-388
DSC 2007 North America – Iowa City – September 2007.
12. NHTSA, Traffic safety facts 2005. DOT HS 809 848, NHTSA annual report. Washington, USA.(2005).
13. Cai, H., Lin, Y., Mourant, R. R., 2007, Study on Driver Emotion in Driver-Vehicle-Environment
Systems Using Multiple Networked Driving Simulators. DSC 2007 North America – Iowa City –
September 2007.
14. Jones, C., Jonsson, I.M.: Automatic recognition of affective cues in the speech of car drivers to allow
appropriate responses. In: Proc. OZCHI (2005).
15. Schuller, B., Arsic, D., Wallhoff, F., Rigoll, G.: Emotion Recognition in the Noise Applying Large
Acoustic Feature Sets. In: Proc. Speech Prosody, Dresden, Germany (2006).
16. Stevens, R., Galloway, T., Berka, C.: EEG-Related Changes in Cognitive Workload, Engagement and
Distraction as Students Acquire Problem Solving Skills. In: Conati, C., McCoy, K., Paliouras, G. (eds.)
User Modeling 2007, vol. 4511. pp. 187-196. Springer Berlin / Heidelberg (2007).
17. Chaouachi, M., Jraidi, I., Frasson, C.: Modeling Mental Workload Using EEG Features for Intelligent
Systems. User Modeling and User-Adapted Interaction, Girona, Spain, 50-61(2011).
18. Florian Eyben, Martin Wöllmer, Tony Poitschke, Björn Schuller, Christoph Blaschke, Berthold Färber,
and Nhu Nguyen-Thien. Emotion on the Road—Necessity, Acceptance, and Feasibility of Affective
Computing in the Car. Advances in Human-Computer Interaction Volume 2010 (2010) ), 263593,17p.

