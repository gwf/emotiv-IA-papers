sensors
Article

Characterizing Focused Attention and Working
Memory Using EEG
Zainab Mohamed 1 , Mohamed El Halaby 2 , Tamer Said 1 , Doaa Shawky 3, *
and Ashraf Badawi 1
1
2
3

*

Center for Learning Technologies, University of Science and Technology, Zewail City, Giza 12578, Egypt;
zrajab@zewailcity.edu.eg (Z.M.); tsaid@zewailcity.edu.eg (T.S.); abadawi@zewailcity.edu.eg (A.B.)
Mathematics Department, Faculty of Science, Cairo University, Giza 12613, Egypt; halaby@sci.cu.edu.eg
Engineering Mathematics Department, Faculty of Engineering, Cairo University, Giza 12613, Egypt
Correspondence: doaashawky@staff.cu.edu.eg

Received: 8 October 2018; Accepted: 30 October 2018; Published: 2 November 2018




Abstract: Detecting the cognitive profiles of learners is an important step towards personalized and
adaptive learning. Electroencephalograms (EEG) have been used to detect the subject’s emotional
and cognitive states. In this paper, an approach for detecting two cognitive skills, focused attention
and working memory, using EEG signals is proposed. The proposed approach consists of the
following main steps: first, subjects undergo a scientifically-validated cognitive assessment test that
stimulates and measures their full cognitive profile while putting on a 14-channel wearable EEG
headset. Second, the scores of focused attention and working memory are extracted and encoded
for a classification problem. Third, the collected EEG data are analyzed and a total of 280 timeand frequency-domain features are extracted. Fourth, several classifiers were trained to correctly
classify and predict three levels (low, average, and high) of the two cognitive skills. The classification
accuracies that were obtained on 86 subjects were 84% and 81% for the focused attention and working
memory, respectively. In comparison with similar approaches, the obtained results indicate the
generalizability and suitability of the proposed approach for the detection of these two skills. Thus,
the presented approach can be used as a step towards adaptive learning where real-time adaptation
is to be done according to the predicted levels of the measured cognitive skills.
Keywords: cognitive skills measurement; electroencephalography; short-time fourier transform;
classification

1. Introduction
Detecting cognitive states and skills is an important step towards adaptive learning, in which
the learning material and pace are adjusted to match some collected data about learners during a
learning task. In addition to the invasive approach in which learners are asked to provide some
information about their learning sessions, an automated non-invasive one is also possible. A promising
methodology for the automated collection of data during a mental task includes the use of bio-sensors
that could measure subjects’ emotions, attention, and engagement in a non-intrusive way in order to
not interfere with her/his learning process.
In this paper, an approach for measuring two basic cognitive skills that affect the outcomes of
the learning process using bio-signals is presented. The measured cognitive skills include focused
attention (FA) and working memory (WM). FA means that all subjects’ activities involve active
cognitive processes such as problem-solving and critical thinking [1]. WM, on the other hand, is a
type of short-term memory that allows subjects to store and manipulate temporary information [2].

Sensors 2018, 18, 3743; doi:10.3390/s18113743

www.mdpi.com/journal/sensors

Sensors 2018, 18, 3743

2 of 21

As presented in [3], the cognitive skills of learners in a learning task play a major role in determining
how well they perceive the knowledge being presented.
One of the bio-signals that could be measured in a convenient, non-intrusive way is the brain
activity. The electroencephalogram (EEG) is the brain electrical activity measured by mounting
electrodes on the scalp, where the activity of millions of cortical neurons produce an electrical
field that can be measured from the human scalp [4]. Analyzing EEG can be effectively used in
measuring emotions and some other psychological states of subjects, especially after advancements in
the technology and the availability of convenient wireless devices that can provide reasonable accuracy
in measuring brain activity similar to that provided by expensive medical devices.
Utilization of several biosensors to detect various cognitive skills and the application in education
is not a novel approach. Many studies have tackled the same problem with different tools and analysis
techniques. In regard to attention, the authors in [5] have built a classification model that is able to
classify the EEG signals of 24 participants to attentive and inattentive while listening to instructions.
Support vector machines are used, and the best achieved accuracy is 76.8%. In another relevant
study [6], the relation between changes in spectral analysis of EEG signals for 18 subjects and sustained
visual attention is analyzed in a real classroom setting. Changes in power values of some frequency
bands (alpha, theta, beta, and gamma [7]) are found to be features that can characterize sustained
visual attention. Also in [8], four attention states have been classified using artificial neural networks
with classification accuracies that range from 56.5–79.75%. Moving to working memory, the authors
in [9] have built a support vector machines classifier that is able to discriminate between working
memory and the recognition of cognitive states from EEG data with a classification accuracy of 79%.
In addition, the work in [10] utilizes the characteristics of power values in different frequency bands
for EEG data during working memory tasks that vary in type and complexity. Changes in theta
to beta EEG frequency bands are found to indicate the performance of WM. Currently, there are
two fundamental principles that might help in understanding brain-behavior relationships; namely,
segregation and integration [11]. The former is based on the fact that cerebral cortex can be subdivided
into distinct modules, each of them has its own structure and functionality. The latter, on the other
hand, is based on the fact that no region of the brain can perform a certain task individually, instead,
the interactions and exchange of information between different regions have to be done for any mental
or motor functionality to be performed. The analysis conducted in this paper is based on the latter
principle, where electric signals are measured from different locations on the scalp and the gathered
information is integrated and analyzed. The presented approach includes the following main steps.
First, the subjects put on a wireless headset that measures EEG signals from 14 different electrodes
distributed across the scalp. Second, they undergo a scientifically-validated cognitive assessment test.
This step helps elicit and stimulate the signals related to the cognitive skills that need to be measured.
Third, the collected EEG signals are analyzed to find out whether there are relationships between the
extracted EEG features and the measured cognitive skills.
The rest of the paper is organized as follows. In Section 2, the related work is presented.
The details of the materials and methods are presented in Section 3. In Section 4, the results of
the proposed approach are discussed. Finally, in Section 5, conclusions and directions for the future
work are outlined.
2. Related Work
The literature includes many studies that have established EEG and event-related potentials
(ERPs) as the primary tools available to scientists investigating neural indexes of cognition in different
application areas [12]. Thus, there is a large number of studies that detect different cognitive states
such as cognitive impairment, levels of alertness and drowsiness, attention, workload, and memory
from EEG signals [13–16]. Most of the available approaches employ machine learning algorithms
for building models that can predict cognitive states based on some extracted features from the
EEG signals.

Sensors 2018, 18, 3743

3 of 21

For instance, as an example of the analysis of EEG in medical applications to detect cognitive
impairment, the authors in [17] presented a methodology based on EEG to detect Alzheimer’s disease
and its prodromal form (mild cognitive impairment). 5-min of spontaneous EEG activity were
recorded using a 19-channel EEG system at a sampling frequency of 200 Hz. Subjects were asked to
stay in a relaxed state, awake, and with closed eyes during EEG acquisition. The features extracted
include spectral and non-linear features including relative power in the conventional frequency
bands, median frequency, individual alpha frequency, spectral entropy, central tendency measure,
sample entropy, fuzzy entropy, and auto-mutual information. Finally, three classifiers were trained
using a subset of the used features namely: linear discriminant analysis (LDA), quadratic discriminant
analysis (QDA), and multi-layer perceptron artificial neural network (MLP). MLP showed the highest
diagnostic performance in determining whether a subject is not healthy with sensitivity of 82.35% and
positive predictive value of 84.85%. As another example, a model for deep learning, namely deep belief
networks (DBN) was used in [18]. In this approach, DBNs were applied in a semi-supervised paradigm
to model EEG waveforms for the classification and anomaly detection of EEG signals. EEG signals were
recorded while 11 subjects received therapeutic hypothermia treatment comatose after cardiac arrest.
Then, randomly subsampled 1000 2-min segments were extracted. The authors compared the use of
raw, unprocessed data to hand-chosen features and found that DBN has comparable performance
with support vector machines (SVM) and k-nearest neighbor (KNN) classifiers and has fast test time,
even on high-dimensional raw data.
In a different application, EEG has been employed to detect neuro-activity related to encoding
different movements, viewing different images, as well as encoding drivers’ cognitive states and
alertness level. For instance, in [19], the main objective was to detect how the features extracted from
EEG vary with four tasks: slow walking, navigating and counting, communicating with radio, and
studying mission map. The experimental setting involves a user outfitted with wearable monitoring,
communication, and mobile computing equipment walking outside. The monitoring equipment is
a BioSemi Active Two EEG system with 32 electrodes. Vertical and horizontal eye movements and
blinks were recorded with electrodes below and lateral to the left eye. Using the power spectral
density of signals from seven different sites, the model classified. Classification rate was 83% using
14 features. Moreover, in [20], the authors showed how to design and train convolutional neural
networks (ConvNets) to decode movement-related information from the raw EEG without handcrafted
features and highlights the potential of deep ConvNets when combined with advanced visualization
techniques for EEG-based brain mapping. In this study, three ConvNets with different architectures,
with the number of convolutional layers ranging from 2 layers in a “shallow” ConvNet over a 5-layer
deep ConvNet up to a 31-layer residual network (ResNet), were used. The authors evaluated a
large number of ConvNet design choices and they showed that end-to-end deep ConvNets can
reach accuracies at least in the same range as other tools for decoding movement-related information
from EEG.
Also, in [21], an algorithm to decode brain activity associated with different types of images is
proposed. In this algorithm, ConvNet is modified for the extraction of features. Then, a t-test is used
for the selection of significant features and finally, likelihood ratio-based score fusion is used for the
prediction of brain activity. Multivariate pattern analysis (MVPA) is used by encoding the complete
input into ConvNet with likelihood ratio based on score fusion. Continuous EEG data were recorded
with a 128 channel with a sampling frequency of 250 Hz. The data were pre-processed where the
signals were filtered from 0.3 to 30 Hz using a bandpass filter. Eye-blink (EOG) artifacts were corrected
using an adaptive artifact correction method. The proposed method has prediction accuracy of 79.9%.
Moreover, in [22], the authors used a recurrent neural network (RNN) to decode the EEG data to help
them find out whether EEG data can be used to accurately classify whether someone is viewing a 2D
or 3D image. The collected data recordings from 12 human subjects. Each stimulus was presented for
1.65 s at a sampling rate of 256 Hz. However, the best obtained accuracy was 72% and the authors
concluded that it is not straightforward to apply RNNs to EEG data.

Sensors 2018, 18, 3743

4 of 21

As an example of how EEG was employed for the encoding of cognitive states of subjects in a
learning-related application, in [23], an intelligent tutoring system (ITS) that can dynamically tailor
its instructional strategies to impose an appropriate amount of cognitive load for each participant is
presented. The authors showed the feasibility of collecting EEG data while students learn from an
ITS using a hat-like headset. The models use EEG spectral features to differentiate between low and
high cognitive load training tasks via partial least squares regression. Linear mixed effects regressions
show that cognitive load during some of the tasks is related to performance on the questions. Also,
in [24], the authors studied whether students’ neural reliability can be quantified in a real-time manner
based on recordings of brain activity in a classroom setting using a low-cost, portable EEG system.
Video stimuli is presented while recording EEG activity as the subjects watched short video clips of
approximately 6 min duration, either individually or in a group setting. Inter-subject correlation (ISC)
is used as a marker of engagement of conscious processing. To measure the reliability of EEG responses,
correlated components analysis is used to extract maximally correlated time series with shared spatial
projection across repeated views within the same subject (inter-viewing correlation, IVC), or between
subjects (inter-subject correlation, ISC). Similarity of responses between subjects is detected which
showed the reliability of ISC as a measure for engagement. Moreover, in [25], the authors developed
a feature extraction scheme for the classification of EEG signals during a complex cognitive task,
and in rest condition with the eyes open. The discrete wavelet transform is applied on EEG signals
and the relative wavelet energy is calculated in terms of detailed coefficients (and the approximation
coefficients of the last decomposition level). The extracted relative wavelet energy features are passed
to classifiers for the classification purpose. The performances of four different classifiers (KNN, SVM,
MLP, and Naive Bayes) were evaluated. Some classifiers, including SVM and MLP, achieved over
98% accuracy in this task. However, a very small number of participants was included in this study
(only 8 participants were included). Also, in [26], EEG signals were used to assess cognitive load in
multimedia learning tasks. The EEG signals were recorded from 34 subjects using 128-channel device at
a sampling frequency of 250 Hz. In addition, some other bio-signals were recorded to help pre-process
EEG signals, including eye blinks and heart rate. The learning tasks included multimedia animations
based on plant anatomy. Then, a memory recall test was given to the subjects. Four distinct frequency
bands were used to extract the three spectral features based on entropy features. Partial directed
coherence (PDC) was also applied to determine the connectivity patterns among the selected channels
of the brain. Three types of classifiers (Naïve Bayes, SVM with linear kernel, and SVM with radial basis
function kernel) were used to classify the extracted features. The best classification accuracies were
obtained using the features of the alpha waves. In addition, in [27], an approach to measure working
memory (WM) across tasks is provided. The WM tasks used included verbal n-back, spatial n-back [28]
and the multi-attribute task battery [29]. EEG acquisition was performed in a magnetically and
electrically-shielded room using 30-channel EEG data. Then, a power spectral density (PSD) estimator
using Burg’s method was used to calculate the sums of PSD in 7 frequency bands, which resulted in
7 features of each of the 30 channels. The support vector machine regression algorithm (SVMR) with a
linear kernel function was used. Although some satisfactory results were found for some subjects in
cross-task regression, however, as reported in the study, overall results were not satisfactory.
3. Materials and Methods
3.1. Electroencephalogram (EEG) Data Acquisition
The EEG signals are measured using the wireless headset (Emotiv Epoc, San Francisco, CA, USA).
The headset has 16 channels, where 14 of them are used for collecting the EEG signals from different
parts of the scalp and two are used as reference points. The sensors measure EEG data from AF3, AF4,
F3, F4, FC5, FC6, F7, F8, T7, T8, P7, P8, O1, O2 based on the 10–20 system as shown in Figure 1. EEG
signals are sampled at 128 samples/s.

Sensors 2018, 18, 3743
Sensors 2018, 18, x FOR PEER REVIEW

5 of 21
5 of 22

Figure
the 14
14electrodes
electrodesofofthe
theused
usedEEG
EEG
headset
[30].
Figure1.1.The
Thedistribution
distribution of the
headset
[30].

3.2.
Experimental
3.2.
ExperimentalSetup
Setup
3.2.1.
Target Population
3.2.1. Target Population
The
in the
the university
universitywho
whomeet
meetthe
the
inclusion
criteria
Thepopulation
populationincludes
includes all
all the
the students
students in
inclusion
criteria
andand
were
available
for
the
actual
experiments.
Equal
opportunity
was
given
to
all
students
to
participate.
were available for the actual experiments. Equal opportunity was given to all students to participate.
The
exclusion
the following.
following.Any
Anystudent
student
with
neurological
disorder
(epilepsy,
The
exclusioncriteria
criteriawere
werebased
based on
on the
with
neurological
disorder
(epilepsy,
Alzheimer
disease
and
other
dementias,
cerebrovascular
diseases
including
stroke,
migraine
and
other
Alzheimer disease and other dementias, cerebrovascular diseases including stroke, migraine and
headache
disorders,
brain
tumors,
traumatic
disorders
of
the
nervous
system
due
to
head
trauma,
other headache disorders, brain tumors, traumatic disorders of the nervous system due to head
and
neurological
disordersdisorders
as a result
malnutrition)
would
be excluded.
Also,
thethe
students
who
trauma,
and neurological
as aofresult
of malnutrition)
would
be excluded.
Also,
students
who suffered
fromtype
any type
of skin
allergy
and
thosewho
whotook
took any
any medications
interfere
suffered
from any
of skin
allergy
and
those
medicationsthat
thatmight
might
interfere
with
the
physiologicaldata
datawere
were excluded.
excluded. To
was
sent
to all
students,
with
the
physiological
To recruit
recruitthe
thesubjects,
subjects,ananemail
email
was
sent
to all
students,
informingthem
themabout
aboutthe
theexperiment.
experiment. The
about
thethe
purpose
of the
informing
The email
emailcontained
containedinformation
information
about
purpose
of the
study,the
theexperimental
experimental procedure,
procedure, discomforts
thethe
subjects
might
suffer
fromfrom
whilewhile
putting
on
study,
discomfortsthat
that
subjects
might
suffer
putting
the
headset,
confidentiality
of
the
study,
instructions
to
be
done
on
the
day
of
the
experiment,
and
on the headset, confidentiality of the study, instructions to be done on the day of the experiment,
thethe
incentives
thatthat
they
would
receive
as aas
sign
of appreciation
for their
contribution.
In addition,
a
and
incentives
they
would
receive
a sign
of appreciation
for their
contribution.
In addition,
consent
form
was
signed
indicating
that
the
subjects
satisfy
the
inclusion
criteria
in
addition
to
the
a consent form was signed indicating that the subjects satisfy the inclusion criteria in addition to the
aforementioned information prior to the experiment. In total, the experiment included 86 subjects (72
aforementioned information prior to the experiment. In total, the experiment included 86 subjects
males, 14 females) with ages ranging from 18 to 23 years (µ = 19.76, σ = 0.951). The study was
(72 males, 14 females) with ages ranging from 18 to 23 years (µ = 19.76, σ = 0.951). The study was
approved by the Research Office in Zewail City.
approved by the Research Office in Zewail City.
3.2.2. Procedure and Task Description
3.2.2. Procedure and Task Description
In order to record the EEG signals associated with different cognitive skills, the subjects undergo
In order to record the EEG signals associated with different cognitive skills, the subjects undergo
a scientifically-validated cognitive assessment battery while putting on the EEG headset. The test is
a scientifically-validated cognitive assessment battery while putting on the EEG headset. The test
available at [31]. The test consists of a series of fixed activities that stimulate perception, memory,
is attention
availableand
at [31].
The test consists of a series of fixed activities that stimulate perception, memory,
other cognitive states. The main reason for choosing this test is that it is clinicallyattention
and
other
cognitive
states.
main reason
forpublications
choosing this
test is The
that outcome
it is clinically-proven;
proven; in addition, it has been
usedThe
in several
scientific
[32–35].
of interest
inrepresents
addition, the
it has
been
used
in
several
scientific
publications
[32–35].
The
outcome
of skills
interest
given test scores or measures of the cognitive skills for each subject. The cognitive
represents
the
given
test
scores
or
measures
of
the
cognitive
skills
for
each
subject.
The
cognitive
measured include 23 basic skills and 5 compound skills. The five compound skills include memory,
skills
measured
include perception,
23 basic skills
5 compound
skills.5 compound
The five compound
skills include
attention,
coordination,
and and
reasoning.
Each of these
skills are composed
of
memory,
attention,
perception,
and reasoning.
Each of the
these
5 compound
a set of basic
skills. coordination,
For instance, the
score of perception
is the average
scores
of six basicskills
skills;are
composed
of a set of basic
skills.
For instance,
the
score of perception
is thevisual
average
of the scores
namely recognition,
auditory
perception,
spatial
perception,
visual scanning,
perception,
and of
sixestimation.
basic skills;
recognition,
auditory
perception,
spatial perception,
visual
scanning,
visual
Onnamely
the other
hand, focused
attention
(FA) and working
memory (WM)
belong
to the basic
skills that are
measured
andother
whosehand,
scoresfocused
are directly
reflected
in the
The numeric
scores
perception,
anddirectly
estimation.
On the
attention
(FA)
andtest.
working
memory
(WM)
givento
tothe
cognitive
skills that
varyare
from
0 to 800
with a categorization
of theare
profile
to low
(𝑠𝑐𝑜𝑟𝑒 <
belong
basic skills
directly
measured
and whose scores
directly
reflected
in 200),
the test.
moderate
(200
≤ 𝑠𝑐𝑜𝑟𝑒
< to
400)
and high
(𝑠𝑐𝑜𝑟𝑒
≥ 400).
in the
conducted
experiments,
FA profile
and
The
numeric
scores
given
cognitive
skills
vary
fromThus,
0 to 800
with
a categorization
of the
scores <
were
encoded
to these
levels.
toWM,
low (score
200),
moderate
(200three
≤ score
< 400) and high (score ≥ 400). Thus, in the conducted
The
data
collection
was
extended
across
three
months,
where
three subjects, on average, were
experiments, FA and WM, scores were encoded
to these
three
levels.
included
each
day.
The
reason
for
this
is
that
we
need
to
clean
the
contacts
of the
headseton
to average,
make
The data collection was extended across three months, where
three
subjects,
sure that the signals received from them were as strong as possible. Each session lasted for an average
were included each day. The reason for this is that we need to clean the contacts of the headset

Sensors 2018, 18, x FOR PEER REVIEW

Sensors 2018, 18, 3743

6 of 22

6 of 21

of 40 min including the instructions given to the participant and the setup time. Figure 2 shows the
experimental setup.
to make sure that the signals received from them were as strong as possible. Each session lasted for
It should be mentioned that the quality of the measured signals varied from one subject to
an average of 40 min including the instructions given to the participant and the setup time. Figure 2
another due to the differences in scalp and hair thickness. More specifically, it varied from 17% to
shows
thewhere
experimental
85%,
the lowestsetup.
contact quality was obtained for female participants with thick and long hair.

Figure
2. Experimental
set-up.
subjectundergoes
undergoes the
testtest
while
wearing
the the
Figure
2. Experimental
set-up.
AAsubject
thecognitive
cognitiveassessment
assessment
while
wearing
headset.
EEG EEG
headset.

3.3.should
Artifactbe
Identification
andthat
Removal
It
mentioned
the quality of the measured signals varied from one subject to
EEGtosignals
have veryin
small
amplitudes
(approximately
40 to 100 it
µ V),
hence
they17%
easily
another due
the differences
scalp
and hair thickness.
Morefrom
specifically,
varied
from
to 85%,
get
buried
in larger
interfering
signals
that stem
the participants
electrochemical
activity
other
cells
and
where
the
lowest
contact
quality was
obtained
forfrom
female
with
thickofand
long
hair.
their ability to conduct electrical current. These signals are called artifacts and they should be
removed
from the EEG
before any analyses could be done. Artifacts can be divided into
3.3. Artifact
Identification
andsignal
Removal
external and internal artifacts [4]. External artifacts are caused by outer actions, such as interference

EEG
signals
have
small
amplitudes
(approximately
from
100 hand,
µV), hence
theytoeasily
with the
power
line very
signals
or electronic
devices.
Internal artifacts,
on 40
theto
other
are related
get buried
in
larger
interfering
signals
that
stem
from
the
electrochemical
activity
of
other
cells and
the actions made by the subjects. Those include body movements, movement of eye ball (oculogenic
their potentials),
ability to conduct
electrical
current.
These
signals
are
called
artifacts
and
they
should
be
removed
and cardiac activity.
from the There
EEG signal
beforemethods
any analyses
could beand
done.
Artifacts
can befrom
divided
external
are several
for detecting
removing
artifacts
EEG into
signals.
The and
application
filters,
statistical
autoregression
modeling,
and independent
internal
artifactsof[4].digital
External
artifacts
are analysis,
caused by
outer actions,
such as interference
with the
component
analysis
are among
the most
used methods.
the effective
these
power
line signals
or electronic
devices.
Internal
artifacts,However,
on the other
hand, areutilization
related toofthe
actions
methods
requires
prior
knowledge
about
where
the
signal
is
and
where
the
noise
comes
from.
made by the subjects. Those include body movements, movement of eye ball (oculogenic potentials),
In the conducted experiments, only the basic preprocessing is performed. The reason for that is
and cardiac activity.
twofold. First, no prior knowledge about where the signal might be is available. Second, any excessive
There are several methods for detecting and removing artifacts from EEG signals. The application
preprocessing actually distorts the signal and might mislead the analyses.
of digital The
filters,
statistical analysis,
modeling,
and independent
analysis
high-frequency
external autoregression
artifacts were originally
removed
by the 50 Hzcomponent
low pass filter
are among
theinmost
used methods.
However,
the effective
theseThus,
methods
requires prior
included
the headset.
This filter
affects frequencies
fromutilization
43 Hz andof
above.
the maximum
knowledge
about
thewas
signal
is and
where thethe
noise
comes
from.
frequency
to bewhere
analyzed
43 Hz.
Additionally,
DC offset
(mean
amplitude displacement from
In
thewhich
conducted
experiments,
only
the sampling
basic preprocessing
performed.
Thefinite
reason
for that is
zero)
was added
to the signal
during
was removedisusing
a first order
impulse
response
(FIR)
filter
with
cut-off
frequency
of
0.16
Hz,
which
resulted
in
a
preprocessed
signal
twofold. First, no prior knowledge about where the signal might be is available. Second, any with
excessive
no DC component
suitable
frequency
analysis.
Finally, some statistical analyses
preprocessing
actually (NoDC)
distortsthat
theissignal
andformight
mislead
the analyses.
were high-frequency
performed to detect
the outliers,
i.e., were
the signals
with exceptionally
large
The
external
artifacts
originally
removed by
theamplitudes
50 Hz low(greater
pass filter
than 3× mean value). Since the data collection sessions were relatively long, those samples were
included in the headset. This filter affects frequencies from 43 Hz and above. Thus, the maximum
removed. This resulted in a further preprocessed signal, denoted by NoOutlier. The percentage of
frequency to be analyzed was 43 Hz. Additionally, the DC offset (mean amplitude displacement from
removed samples as a result of the performed preprocessing varied from one subject to another with
zero)values
whichofwas
to theand
signal
during
sampling
was
removedmean,
usingand
a first
order deviation,
finite impulse
2%,added
15%, 0.3%,
0.11%,
for the
maximum,
minimum,
standard
response
(FIR)
filter
with
cut-off
frequency
of
0.16
Hz,
which
resulted
in
a
preprocessed
signal
with no
respectively.
DC component (NoDC) that is suitable for frequency analysis. Finally, some statistical analyses were
performed to detect the outliers, i.e., the signals with exceptionally large amplitudes (greater than
3× mean value). Since the data collection sessions were relatively long, those samples were removed.
This resulted in a further preprocessed signal, denoted by NoOutlier. The percentage of removed
samples as a result of the performed preprocessing varied from one subject to another with values of
2%, 15%, 0.3%, and 0.11%, for the maximum, minimum, mean, and standard deviation, respectively.

Sensors 2018, 18, 3743

7 of 21

3.4. Feature Extraction
3.4.1. Time-Domain Features
In the signal’s time domain, the extracted features are primarily statistical ones. The extracted
time-domain features include the following:
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.

Minimum
Maximum
Mean
Variance
Standard Deviation
Coefficient of Variance
Kurtosis
Skewness
25 Quartile
50 Quartile
75 Quartile
Shapiro–Wilk test (test statistic and p-value)
Hjorth mobility and complexity parameters

Each of the above features is calculated for each of the 14 channels. In addition, the Hjorth
mobility and complexity parameters are calculated as given by Equations (1) and (2) [36], respectively:
v


u
u var dx(t)
t
dt
Mobility =
var ( x (t))
Mobility
Complexity =



dx (t)
dt

(1)


Mobility( x (t))

(2)

where x (t) is the signal, var is its variance, and the forward difference formula is used for the calculation
dx (t)
of dt .
3.4.2. Frequency Domain Features
There are variations in the EEG signals that cannot be shown only in the time domain. Thus,
we transformed the collected EEG signals to the frequency (spectral) domain to be able to extract such
variations. Short-time Fourier transform (STFT) is used for transforming EEG signals to the spectral
domain. This is done by the calculation of fast Fourier transform (FFT) on short segments or windows
of fixed length on the analyzed signal. FFT is a time-efficient numerical implementation of the Fourier
transform (FT). For a given time-series signal x[n] with N sample points (n = 1, 2 . . . N), FFT is given
by Equation (3).
2πm
N
f f t(ω ) = ∑n=1 x [n]e−iωn , ω =
, 0 ≤ m ≤ N−1
(3)
N
where ω = 2πf /fs represents the angular frequency discretized in N samples and fs is the sampling
frequency [37]. STFT is an extension of the Fourier transform that addresses two main limitations of
the FT for EEG time-frequency analyses. First, FT obscures time-varying changes in the frequency
structure of the data. Second, FT assumes that the data are stationary, which is not true for EEG
signals. To address this, data in short segments have to be tapered, which attenuates the amplitude
of the data at the beginning and the end of the time segment. This is important for preventing edge
artifacts from contaminating the time-frequency results. However, tapering attenuates valid EEG
signal, thus overlapping time segments have to be used. In the conducted analysis, a Hann window

Sensors 2018, 18, 3743

8 of 21

is used with 50% overlapping. After transforming the signal to the frequency domain using STFT,
Power spectral density (PSD) is calculated. PSD is an important feature of the signal in the frequency
domain that represents the contribution of each individual frequency component to the power of the
whole signal segment. PSD can be estimated using periodogram [38], which can be calculated as given
in Equation (4).
1
per (ω ) = | f f t(ω )|2
(4)
N
Using a normalized periodogram, the relative contribution of each frequency component to the
total power can be estimated as given by Equation (5).
pernorm =

1
| f f t(ω )|2 /
N

2π ( N −1)/N

∑

per (ω )

(5)

ω =0

As suggested in cognitive physiology, the power of four main frequency bands in the EEG signals
can reflect cognitive activity in the brain. These frequency bands include theta (4–8 Hz), alpha (8–12 Hz),
beta (12–30 Hz), and gamma (30–60 Hz). Thus, the relative power of the four frequency sub-bands are
calculated and used as features for the EEG signals in the frequency domain. In addition, the total
average power in each channel is calculated.
It should be mentioned that for many signals, it is difficult to interpret raw power data at different
frequency ranges. This is mainly because the frequency spectrum tends to show decreasing power
at increasing frequencies; a phenomena that is called the 1/ f power scaling [39]. Thus, the power at
higher frequencies (e.g., gamma) has much smaller magnitude than the power at lower frequencies
(e.g., theta). In addition, it is not possible to aggregate raw power values across subjects because of
the individual differences, which affects the power values (e.g., hair and skull thickness). The 1/ f
power scaling also affects task-related power, especially in the frequencies that tend to have high
power during baseline or resting period. To address these limitations, usually baseline normalization
is performed. In this paper, decibel (dB) conversion is used, which is one of the most commonly used
approaches for baseline normalizations in cognitive electrophysiology [39]. Using decibel conversion,
the change in power relative to baseline power is calculated using Equation (6):
Normalized Power = 10· log10

activity powert f
baseline power f

!
(6)

where t, and f indicate time and frequency points, respectively.
To be able to measure the baseline power, before conducting the experiment, the EEG signals
for each subject were recorded while the participant was resting with her/his eyes open for 15 s,
followed by another 15 s with closed eyes. These 30 s, including other 6 s for getting the subject ready,
are considered as the baseline period. In addition to aforementioned benefits of baseline normalization,
normalization makes it possible to remove the personal task-unrelated factors that might affect the
results of the analysis.
3.5. Feature Selection
The total number of features included for each subject is 280. This high-dimensional feature vector
makes the analysis more susceptible to overfitting. Thus, feature selection has to be performed before
training a predictive model. There are many possible approaches for feature selection. A promising
approach for feature selection is performed using transformation of the features (predictors) to fit the
response variable (target) using regularized logistic regression [40]. In this approach, predictors are
mapped to fit the target and the coefficients of the transformed predictors are used as a method for
ranking the importance of them, where weak predictors tend to have small coefficients. In addition,
to further enhance the performance of the feature selection process, a regularization term is added
to the loss function to be minimized, which is E( X, Y ), where X is the set of predictors and Y is the

Sensors 2018, 18, 3743

9 of 21

target. Thus, the objective function to be minimized is E( X, Y ) + λkω k, where k·k denotes l1 or l2
regularizations and ω is the coefficients’ vector for the model.
3.6. Avoiding Overfitting
Another approach for speeding up learning and preventing over-fitting is to use regularization
while training a model for classification. Different regularization approaches exist and can be effectively
used with the learning algorithm to speed up learning and avoid over-fitting due to the small sample
size and/or large number of features. For instance, as generally known, a multi-layer neural network
(NN) consists of a large number of units (neurons) joined together in a pattern of connections.
Feed-forward NNs allow signals to travel one way only, from input to output, where the input
is, successively from one layer to another, multiplied by weights, summed up, then mapped to other
values to produce the output by a pre-selected transfer function. The most well-known and widely
used learning algorithm to estimate the values of the connecting weights is the back propagation
(BP) algorithm. BP is a gradient descent (GD) algorithm that is usually used to minimize an objective
function J (θ ), where θ represents the model’s parameters. There are several implementations of GD
algorithms that vary depending on the amount of data used in each step of parameters’ updates [41].
In batch GD (BGD), the complete training samples are used to calculate the updates of J (θ ), whereas
in stochastic GD (SGD), only a randomly-selected sample is used to update J (θ ). Thus, J (θ ) is updated
using Equation (7):


θ = θ − η ∇θ J θ, xi , yi

(7)

where η is the learning rate, and xi , yi are a training sample’s predictors and target, respectively for a
classification problem. BGD produces better approximations of the objective function at the expense of
time complexity. On the other hand, SGD handles the optimization problem with less time complexity
at the expense of high variances of the minimized objective function. Thus, a combination between the
two algorithms is usually used, namely mini-batch GD, where a small subset of the training samples is
used for each iteration of the parameters’ updates.
It should be mentioned that η is used to regularize the GD algorithm by giving it large values in
the beginning of the learning cycle to learn faster, then it is given small values to avoid oscillations
around the local minima of the objective function. Another method to regularize learning is to use a
momentum term γ. In this case, the rule used for minimizing J (θ ) is given by Equation (8).
θt = θt−1 − γVt−1 − η ∇θ J (θt−1 )

(8)

where Vt−1 is the update vector used in the previous time step. To further optimize the previous
equation, several optimization algorithms are introduced. For instance, the adaptive gradient algorithm
(Adagrad), adaptive moment estimation algorithm (Adam), and Adamax (a variant of Adam based on
infinity norm [41]) are among the used optimization algorithms that are based mainly on the adaptive
variation of the learning rate and momentum values, for each element of θ, to speed up convergence
and/or prevent the objective function from being trapped at local minima. Thus, the gradient of the
objective function with respect to each parameter at time t, θt,i is used to find the gradient gt,i as given
by Equation (9).
gt,i = ∇θ J (θt,i )
(9)
For instance, Adam [42] is based on storing the first and second momentums of the gradients as
given by Equations (10) and (11), respectively.
mt =

β 1 m t −1 + (1 − β 1 ) g t
1 − βt1

(10)

vt =

β 2 v t −1 + (1 − β 2 ) gt
1 − βt2

(11)

Sensors 2018, 18, 3743

10 of 21

where mt and vt are estimates of the first and second moment of the gradients, and β 1 and β 2 are
the decaying rates for these two estimates, respectively. Usually, l2 norm is used in the previous two
equations
to scale the gradients.
Sensors 2018, 18, x FOR PEER REVIEW
10 of 22
Finally, the parameters updating rule is given by Equation (12):
𝜂
= θ𝜃𝑡 −
−√ η 𝑚
𝑡+1 =
θ𝜃t+
m𝑡t
t
1
√𝑣v𝑡t +
+𝜖e

(12)(12)

where 𝜖 is a small value that prevents the division by zero.
where e is a small value that prevents the division by zero.
In AdaMax, which is a slight modification of Adam, 𝑙∞ norm is used to update the gradients
In AdaMax, which is a slight modification of Adam, l∞ norm is used to update the gradients
instead of 𝑙2 , which converges to a more stable solution with
the default values of 𝜂 = 0.002, 𝛽1 =
instead of l2 , which converges to a more stable solution with the default values of η = 0.002, β 1 = 0.9,
0.9, and 𝛽2 = 0.999.
and β 2 In
= 0.999.
multi-class classification problems, usually the objective function to be minimized represents
In
multi-class
classification
the of
objective
function between
to be minimized
represents
the cross entropy
(CE) [43], problems,
which is ausually
measure
the difference
two probability
thedistributions
cross entropy
(CE)
[43],
which
is
a
measure
of
the
difference
between
two
probability
distributions
as given by Equation (13):
as given by Equation (13):
11 𝑁N 𝐶C
logP(yi𝜖𝐶
eC)c )
CE
=
−
∑∑
∑ 𝑙𝑜𝑔𝑃(𝑦
(13)(13)
𝐶𝐸 = − N ∑
𝑖
𝑐
𝑁 i =1 c =1
𝑖=1 𝑐=1

where N is the number of observations, C is the number of classes, and yi is the label (target) of the
N is the number of observations, C is the number of classes, and 𝑦𝑖 is the label (target) of the
ithwhere
observation.
𝑖thAlthough
observation.
NNs are known to generalize well [44], they are usually susceptible to overfitting
are known
to generalize
they
are usuallyespecially
susceptible
to overfitting
due of
due to Although
the large NNs
number
of parameters
that well
need[44],
to be
optimized,
when
the number
to
the
large
number
of
parameters
that
need
to
be
optimized,
especially
when
the
number
of
observations is small and/or the features’ dimension is large. To further avoid over fitting, in addition
observations
is
small
and/or
the
features’
dimension
is
large.
To
further
avoid
over
fitting,
in
addition
to the regularized learning mentioned above, k-fold cross-validation is used. Moreover, the dropout
to the regularized
learning
k-fold cross-validation
is used.
Moreover,
the dropout
approach
[45] can be
used. mentioned
Dropout isabove,
a promising
approach that was
recently
developed
in deep
approach [45] can be used. Dropout is a promising approach that was recently developed in deep
learning to avoid deep networks from overfitting and improve their generalization ability. In this
learning to avoid deep networks from overfitting and improve their generalization ability. In this
approach, during training, only randomly chosen percentage (p) of the neurons in the fully connected
approach, during training, only randomly chosen percentage (𝑝) of the neurons in the fully connected
(dense) layers is included in the training, while the weights of the (1 − p) remaining neurons are set to
(dense) layers is included in the training, while the weights of the (1 − 𝑝) remaining neurons are set
zero. In addition, in the testing phase, the weights of the neurons are modulated by p.
to zero. In addition, in the testing phase, the weights of the neurons are modulated by 𝑝.
4. Results
4. Results
4.1. Analysis of the Cognition Scores
4.1. Analysis of the Cognition Scores
As mentioned before, the two cognitive scores that were included in the analysis as the targets
As mentioned before, the two cognitive scores that were included in the analysis as the targets
include FA and WM. Figures 3 and 4 show the distribution of the two skills in the targets. As shown
include FA and WM. Figures 3 and 4 show the distribution of the two skills in the targets. As shown
in the figures, none of the two targets follow a normal distribution. Instead, FA is positively-skewed,
in the figures, none of the two targets follow a normal distribution. Instead, FA is positively-skewed,
meanwhile,
WM
is negatively-skewed.
This
adds
additional
challenges
to to
thethe
analysis
approach,
since
meanwhile,
WM
is negatively-skewed.
This
adds
additional
challenges
analysis
approach,
it has
to
be
able
to
handle
skewness
of
the
output.
since it has to be able to handle skewness of the output.

Figure
of focused
focusedattention
attention(FA)
(FA)test
testscores.
scores.
Figure3.3. Distribution
Distribution of

Sensors 2018, 18, 3743

11 of 21

Sensors 2018, 18, x FOR PEER REVIEW

Sensors 2018, 18, x FOR PEER REVIEW

11 of 22

11 of 22

Figure
of working
workingmemory
memory(WM)
(WM)test
testscores.
scores.
Figure4.
4. Distribution
Distribution of

Furthermore,
coefficientsbetween
betweenthe
thetargets
targets
were
calculated.
However,
Furthermore,Spearman’s
Spearman’scorrelation
correlation coefficients
were
calculated.
However,
there
was
nonosignificant
between
them.
there
was
significantcorrelation
correlation
between
Figure 4. Distribution of working memory (WM) test scores.
4.2.
Feature
4.2.
FeatureExtraction
Extraction Spearman’s correlation coefficients between the targets were calculated. However,
Furthermore,
there was no significant correlation between them.

ToTobebeable
dynamics of
ofthe
thecollected
collectedEEG
EEGdata,
data,
features
were
abletotoget
getinformation
information about
about the dynamics
features
were
extracted
from
the
time
as
well
as
frequency
domains.
The
analysis
was
repeated
after
each
step
ofthe
extracted
from
the
time
as
well
as
frequency
domains.
The
analysis
was
repeated
after
each
step
of
4.2. Feature Extraction
the
implemented
preprocessing.
Figures
5–7,
respectively,
show
the
first
15
s
of
the
raw
time
series
implemented preprocessing. Figures 5–7, respectively, show the first 15 s of the raw time series EEG
To be able to get information about the dynamics of the collected EEG data, features were
one channel
itsfor
PSD
for
one
of the subjects,
followed
by those
obtained
forsame
the same
forEEG
onefor
channel
and the
itsand
PSD
oneas
of
the subjects,
followed
by those
obtained
foreach
the
extracted
from
time as
well
frequency
domains.
The analysis
was repeated
after
step of subject
subject
after
removing
the DC
offset
and the
outliers.
As shown
in the
removing
DC
has
after
removing
the DC preprocessing.
offset
and
theFigures
outliers.
As
shown
inshow
the figures,
removing
thetime
DCthe
has
resulted
the
implemented
5–7,
respectively,
the
firstfigures,
15
s of the
raw
series
resulted
in
negative
EEG
values
in
the
time
domain.
Furthermore,
removing
the
outliers
has
resulted
EEGEEG
for one
channel
andtime
its PSD
for oneFurthermore,
of the subjects,removing
followed by
obtained
for the same
in negative
values
in the
domain.
thethose
outliers
has resulted
in a signal
in asmaller
signal
number
of
spikes
in the
time
domain.
Moreover,
Figures
8 and
9baseline
show
subjectwith
aftersmaller
removing
the DC
offset
anddomain.
the
outliers.
As
shownFigures
in
the figures,
removing
the
has the
with
number
of spikes
in the
time
Moreover,
8 and
9 show
theDC
EEG
baseline
signals
forEEG
onevalues
of the
and theEEG
task-related
EEG signals,
respectively
time
resulted
negative
insubjects
the time domain.
Furthermore,
the outliers
has resulted
signals
forEEG
one in
of
the subjects
and
the
task-related
signals,removing
respectively
(in
time
and (in
frequency
in a signal with
smallerAs
number
of spikes
the time low
domain.
Moreover,have
Figures
8 and
9 show
the as
and frequency
domains).
shown
in theinfigures,
frequencies
high
power
values
domains).
As shown
in the
figures,
low
frequencies
have highEEG
power
values
as compared
to high
baseline
EEG
signals
for
one
of
the
subjects
and
the
task-related
signals,
respectively
(in
time
compared to high frequencies. In addition, when compared to the EEG signal before baseline
frequencies.
In
addition,
when
compared
to
the
EEG
signal
before
baseline
normalization,
it
can be
and frequency
shown
in thetofigures,
low frequencies
have highbased
poweronvalues
normalization,
it candomains).
be shownAsthat
is easier
differentiate
between channels
their as
power
shown that
is easier
to differentiate
channels
based on to
their
values,
especially
compared
to high
frequencies. between
In addition,
when compared
the power
EEG signal
before
baselineat small
values, especially at small frequencies. It should be mentioned that the notch around 50 Hz (~0.8 Hz
normalization,
it can
shown thatthat
is easier
differentiate
between
based
on theirofpower
frequencies.
It should
be be
mentioned
the to
notch
around
50 Hzchannels
(~0.8 Hz
in terms
normalized
in terms
of normalized
frequencies)
is mainly
due
to the low-pass
filter
with cutoff
at 50 HzHz
that is
values,isespecially
at small
frequencies.
It filter
shouldwith
be mentioned
that
the that
notchisaround
50 avoid
Hz (~0.8
frequencies)
mainly
due
to
the
low-pass
cutoff
at
50
Hz
used
to
interference
used to
avoid
interference
with
the
power
line
signals.
in terms of normalized frequencies) is mainly due to the low-pass filter with cutoff at 50 Hz that is
with the power line signals.
used to avoid interference with the power line signals.

Figure 5. Raw EEG signals for one channel (upper), with its power spectral density (PSD) (lower).

Sensors 2018,
2018, 18,
18, xx FOR
FOR PEER
PEER REVIEW
REVIEW
Sensors

Sensors 2018, 18, 3743

12 of
of 22
22
12

12 of 21

Figure 5.
5. Raw
Raw EEG
EEG signals
signals for
for one
one channel
channel (upper),
(upper), with
with its
its power
power spectral
spectral density
density (PSD)
(PSD) (lower).
(lower).
Figure

Figure
6. EEG
EEG
signals
for
onechannel
channelafter
after removing
removing the
the
DC
offset
inin
time
(upper)
andand
frequency
Figure
6. EEG
signals
forfor
one
theDC
DCoffset
offsetin
time
(upper)
frequency
Figure
6.
signals
one
channel
after
removing
time
(upper)
and
frequency
(lower)
domains.
(lower)
domains.
(lower)
domains.

Figure
7. EEG
EEG
signals
for
one
channelafter
afterremoving
removing the
the
DC
offset
and
outliers
in time
time
(upper)
andand
Figure
7.
signals
one
channel
after
removing
outliers
in
(upper)
and
Figure
7. EEG
signals
forfor
one
channel
theDC
DCoffset
offsetand
and
outliers
in
time
(upper)
frequency
(lower)
domains.
frequency
(lower)
domains.
frequency
(lower)
domains.

Sensors 2018, 18, 3743
Sensors 2018, 18, x FOR PEER REVIEW

13 of 21
13 of 22

Figure
Baselinesignal
signalfor
forthe
the14
14 channels
channels in time
domains.
Figure
8. 8.Baseline
time (upper)
(upper)and
andfrequency
frequency(lower)
(lower)
domains.

Figure
EEGsignal
signalafter
afterremoving
removing baseline
baseline period
and
frequency
Figure
9. 9.
EEG
period for
for one
onesubject
subjectinintime
time(upper)
(upper)
and
frequency
(lower)
domains.
(lower) domains.

total,
time-domainfeatures
featureswere
were calculated
calculated for
toto
the
extracted
In In
total,
1515
time-domain
for each
eachchannel,
channel,ininaddition
addition
the
extracted
frequency-domain
features,
which
include
5
power
values
for
each
of
the
14-channel
EEG
signals;
frequency-domain features, which include 5 power values for each of the 14-channel EEG signals; thus,
in total
280 features
were calculated
forsubject.
each subject.
in thus,
total 280
features
were calculated
for each

Sensors 2018, 18, 3743
Sensors
Sensors 2018,
2018, 18,
18, x
x FOR
FOR PEER
PEER REVIEW
REVIEW

14 of 21
14
14 of
of 22
22

Figures 10 and 11 show how the values of each subset of the frequency domain features vary
Figures 10
10 and
and 11
11 show
show how
how the values
values of each
each subset of
of the
the frequency
frequency domain
domain features
features vary
vary
across theFigures
different categories
of each the
target. As of
shown subset
in the figures,
there are clear variations
between
across
the
different
categories
of
each
target.
As
shown
in
the
figures,
there
are
clear
variations
across the different categories of each target. As shown in the figures, there are clear variations
the power
values for each class of the targets.
between the power values for each class of the targets.
between the power values for each class of the targets.

Figure
10.
average
power
values
of
attention.
Figure
10. Distribution
Distribution
of
averagepower
power values
values across
across
different
categories
of focused
focused
attention.
Figure
10. Distribution
of of
average
acrossdifferent
differentcategories
categories
of focused
attention.

Figure 11. Distribution of average power values across different categories of working memory.

Figure
11. Distribution
averagepower
power values
values across
categories
of working
memory.
Figure
11. Distribution
of of
average
acrossdifferent
different
categories
of working
memory.

4.3. Classification
Classification
Results
4.3.
Results
4.3. Classification
Results
To be able to correctly classify the collected EEG signals into one of the 3 classes (i.e., low,

Toable
be able
to correctly
classifythe
thecollected
collected EEG
one
of the
3 classes
(i.e., (i.e.,
low, low,
To be
to correctly
classify
EEGsignals
signalsinto
into
one
of the
3 classes
average
and
high)
of
each
target,
many
classifiers
were
implemented
using
the
python
libraries
average
and
high)
of
each
target,
many
classifiers
were
implemented
using
the
python
libraries
average and high) of each target, many classifiers were implemented using the python libraries
numpy,
numpy, Keras,
Keras, scikit,
scikit, and
and scipy.
scipy. The
The trained
trained classifiers
classifiers include
include linear
linear support
support vector
vector (LinearSVC),
(LinearSVC),
numpy,
Keras,
scikit,
and
scipy.vector
The trained
classifiers
include
linear (SVC),
support
vector (LinearSVC),
random
forest
(RF),
support
classifier
with
radial
basis
function
random forest (RF), support vector classifier with radial basis function (SVC), k-nearest
k-nearest neighbor
neighbor
random
forest (RF),networks
support vector
classifier with
radial basisprocess
function (SVC),
k-nearest
neighbor
(KNN),
(KNN), neural
neural networks (NN),
(NN), decision
decision trees
trees (DT),
(DT), Gaussian
Gaussian process (GP),
(GP), boosted
boosted and
and bagged
bagged
(KNN),
neural (tree
networks
(NN), decision
trees (DT), Gaussian
process trees.
(GP),However,
boosted with
and the
bagged
ensembles
and
and
sampling
ensembles
(tree
and KNN-based),
KNN-based),
and random-under
random-under
sampling boosted
boosted
trees.
However,
with
the
skewness
of
the
data
and
the
relatively-small
number
of
observations
in
comparison
to
the
features’
ensembles
(tree
and
KNN-based),
and
random-under
sampling
boosted
trees.
However,
with
skewness of the data and the relatively-small number of observations in comparison to the features’ the
dimension,
was
that
classifiers
would
to
yield
skewness
of theit
and the relatively-small
number
of observations
to theeven
features’
dimension,
itdata
was expected
expected
that only
only a
a few
few of
of the
the
classifiers
would be
be able
ablein
tocomparison
yield good
good results,
results,
even
after
hyper-parameter
optimization.
It
should
be
mentioned
also
that
in
all
of
the
performed
after hyper-parameter
optimization.
should
be classifiers
mentioned would
also that
all to
of yield
the performed
dimension,
it was expected
that only aItfew
of the
beinable
good results,
the
of
were
tuned
search
experiments,
the hyper-parameters
hyper-parameters
of the
the implemented
implemented
classifiers
were
tuned
using
the
grid
search
even experiments,
after hyper-parameter
optimization.
It should be classifiers
mentioned
also
thatusing
in allthe
of grid
the performed
method
in
scikit-learn
library.
The
calssifiers
aforementioned
have
been
tested
using
the
time
and
method
in
scikit-learn
library.
The
calssifiers
aforementioned
have
been
tested
using
the
time
and
experiments, the hyper-parameters of the implemented classifiers were tuned using the grid
search
frequency-domain
features
computed
from
the
(1)
raw
data,
(2)
data
with
DC
component
removed
frequency-domain features computed from the (1) raw data, (2) data with DC component removed
method in scikit-learn library. The calssifiers aforementioned have been tested using the time and
(NoDC)
(NoDC) and
and (3)
(3) data
data with
with the
the DC
DC and
and outliers
outliers removed
removed (NoOutliers).
(NoOutliers).
frequency-domain
features computed
from the (1)feature
raw data, (2) data
with DC component
removed
To
avoid
overfitting,
meta-transformation
To avoid overfitting, meta-transformation feature selection
selection was
was applied
applied using
using a
a logistic
logistic
(NoDC)
and (3) classifier
data with
the k-fold
DC and
outliers
removed
(NoOutliers).
regression
with
cross
regression classifier
with k-fold
cross validation
validation (𝑘
(𝑘 =
= 10)
10) and
and the
the mean
mean score
score is
is obtained.
obtained. Also,
Also,
To
avoid
overfitting,
meta-transformation
feature
selection
was
applied
using
a
logistic
regulization
with
different
optimizitation
techniques
were
investigated.
Moreover,
dropout
was
used
regulization with different optimizitation techniques were investigated. Moreover, dropout
wasregression
used
when
NN
is
employed
as
the
classifier.
classifier
crossasvalidation
(k = 10) and the mean score is obtained. Also, regulization
whenwith
NN isk-fold
employed
the classifier.
with different optimizitation techniques were investigated. Moreover, dropout was used when NN is
employed as the classifier.

Sensors 2018, 18, 3743
Sensors 2018, 18, x FOR PEER REVIEW
Sensors 2018, 18, x FOR PEER REVIEW

15 of 21
15 of 22
15 of 22

In the following subsections, the performances of the implemented classifiers for the three data
In the following subsections, the performances of the implemented classifiers for the three data
sets (raw
NoDC, NoOutliers)
only time-domain
features, only
frquency-domain
features,
In data,
the following
subsections,using
the performances
of the implemented
classifiers
for the three
data
sets (raw data, NoDC, NoOutliers) using only time-domain features, only frquency-domain features,
andsets
combined
setNoDC,
of features,
will be
presented.
(raw data,
NoOutliers)
using
only time-domain features, only frquency-domain features,
and combined set of features, will be presented.
and combined set of features, will be presented.
4.3.1. Classification Results Using Only Time-Domain Features
4.3.1. Classification Results Using Only Time-Domain Features
4.3.1. Classification Results Using Only Time-Domain Features
Classification of Focused Attention
Classification of Focused Attention
Classification
of Focused
Attention accuracy for the implemented classifiers on raw data. As shown
Figure 12 shows
the classification
Figure 12 shows the classification accuracy for the implemented classifiers on raw data. As
in the figure,
highestthe
obtained
classification
is about 70%.classifiers
The classifiers
yielded
Figure the
12 shows
classification
accuracyaccuracy
for the implemented
on rawthat
data.
As
shown in the figure, the highest obtained classification accuracy is about 70%. The classifiers that
theshown
best accuracy
include
KNN
and GP.
It should accuracy
be mentioned
that,
after
selection,
in the figure,
the SVC,
highest
obtained
classification
is about
70%.
Thefeature
classifiers
that
yielded the best accuracy include SVC, KNN and GP. It should be mentioned that, after feature
yielded
best182
accuracy
include features
SVC, KNN
and
GP. It should
be mentioned
that,features
after feature
only
70 outthe
of the
time domain
were
selected.
In addition,
the selected
mostly
selection, only 70 out of the 182 time domain features were selected. In addition, the selected features
selection,
only
70
out
of
the
182
time
domain
features
were
selected.
In
addition,
the
selected
features
belong
to belong
shapiro-wilk
test for normality
and Hjorth
and complexity
values.values.
mostly
to shapiro-wilk
test for normality
andmobility
Hjorth mobility
and complexity
mostly belong to shapiro-wilk test for normality and Hjorth mobility and complexity values.

Figure
Performanceofofimplemented
implemented classifiers
classifiers for
raw
data
Figure
12.12.
Performance
for the
the classification
classificationofofFA
FAscores
scoresusing
using
raw
data
Figure 12. Performance of implemented classifiers for the classification of FA scores using raw data
and
only
time-domain
features.
and only time-domain features.
and only time-domain features.

Furthermore,
Figure
shows
classification
accuracy
for implemented
the implemented
classifiers
on
Furthermore,
Figure
13 13
shows
the the
classification
accuracy
for the
classifiers
on NoDC.
Furthermore, Figure 13 shows the classification accuracy for the implemented classifiers on
NoDC.
As
shown
in
the
figure,
the
highest
obtained
classification
accuracy
is
also
about
70%.
This
is
As shown in the figure, the highest obtained classification accuracy is also about 70%. This is expected,
NoDC. As shown in the figure, the highest obtained classification accuracy is also about 70%. This is
expected,
sincethe
removing
the DC component
should
affect
time domain
characteristics
the
since
removing
DC component
should not
affectnot
the
timethe
domain
characteristics
of theofsignal.
expected, since removing the DC component should not affect the time domain characteristics of the
signal.
Moreover,
the
classifiers
that
yielded
the
best
accuracy
include
LinearSVC
and
GP.
After
Moreover,
the classifiers
that yielded
bestthe
accuracy
includeinclude
LinearSVC
and GP.
feature
signal. Moreover,
the classifiers
that the
yielded
best accuracy
LinearSVC
andAfter
GP. After
feature only
selection,
only
78
out
of 182
time features
domain features
were selected.
selection,
78
out
of
182
time
domain
were
selected.
feature selection, only 78 out of 182 time domain features were selected.

Figure 13. Performance of implemented classifiers for the classification of FA scores using no DC
Figure
Performanceofofimplemented
implemented classifiers
classifiers for
DC
Figure
13.13.
Performance
for the
the classification
classificationofofFA
FAscores
scoresusing
usingnono
DC
component
(NoDC) data and only time-domain
features.
component
(NoDC)
data
andonly
onlytime-domain
time-domainfeatures.
features.
component
(NoDC)
data
and

In addition, Figure 14 shows the classification accuracy for the implemented classifiers on
addition,Figure
Figure1414shows
showsthe
the classification
classification accuracy
accuracy for
classifiers onon
In In
addition,
for the
the implemented
implemented
NoOutliers
data. As shown
in the figure,
the highest obtained classification
accuracy isclassifiers
also about
NoOutliers
data.
As
shown
in
the
figure,
the
highest
obtained
classification
accuracy
is also
about
NoOutliers
data.
As be
shown
in the figure,
highest
classification
accuracy
also
about
70%.
70%. This
may
explained
by thethe
low
ratio obtained
of removed
outliers from
eachis signal,
hence
70%.
This
may
be
explained
by
the
low
ratio
of
removed
outliers
from
each
signal,
hence
This
may
be
explained
by
the
low
ratio
of
removed
outliers
from
each
signal,
hence
approximately
approximately the same performance measures are obtained when NoOutlier data are used in
approximately
the same
performance
measures
are
obtaineddata
when NoOutlier
data are used
thecomparison
same performance
measures
are obtained
whenimplication
NoOutlier
in comparison
withinthe
with the raw
and NoDc
data. Another
mightare
be used
that there
is no information
comparison
with
the
raw
and
NoDc
data.
Another
implication
might
be
that
there
is
no
information
raw
and NoDc
data.
implication charactersitics
might be that there
no information
related
to the
FA in
related
to the
FA Another
in the time-domain
of theisoutliers.
As shown
in the
figure,
thethe
related to the
FA in the time-domain
charactersitics
outliers.
As shownthat
in the
figure,
time-domain
charactersitics
theaccuracy
outliers.
As shown
inof
thethe
figure,
theAfter
classifiers
yielded
thethe
best
classifiers that
yielded the of
best
include
LinearSVC
and GP.
feature selection,
only
76
classifiers that yielded the best accuracy include LinearSVC and GP. After feature selection, only 76
out of 182 time-domain features were selected.
out of 182 time-domain features were selected.

Sensors 2018, 18, 3743

16 of 21

accuracy include LinearSVC and GP. After feature selection, only 76 out of 182 time-domain features
Sensors 2018, 18, x FOR PEER REVIEW
16 of 22
were Sensors
selected.
2018, 18, x FOR PEER REVIEW
16 of 22
Sensors 2018, 18, x FOR PEER REVIEW

16 of 22

Figure
14.
Performance
ofimplemented
implementedclassifiers
classifiers for
for the
ofofof
FA
scores
using
NoOutliers
Figure
Performance
classifiers
the
classification
FA
scores
using
NoOutliers
Figure
14.14.
Performance
ofofimplemented
theclassification
classification
FA
scores
using
NoOutliers
data
and
only
time-domain
features.
Figure
14.
Performance
of
implemented
classifiers
for
the
classification
of
FA
scores
using
NoOutliers
data
and
only
time-domainfeatures.
features.
data
and
only
time-domain
data and only time-domain features.

Classification
of
Working
Memory
(WM)
Classification
ofof
Working
Memory
(WM)
Classification
Working
Memory
(WM)
Classification of Working Memory (WM)

As
for
theclassification
classification
WM,much
muchworse
worse results
results
using
only
time-domain
AsAs
for
the
classification
ofofof
WM,
wereobtained
obtained
using
only
time-domain
for
the
WM,
much
worse
results were
were
obtained
using
only
time-domain
As for
the means
classification
WM, much worse
were obtained
using only
time-domain
features,
which
that theofcharacteristics
of theresults
signal related
to WM cannot
be extracted
from
features,
which
means
that
thethe
characteristics
ofof
the
signal
related
from
features,
which
means
that
characteristics
the
signal
relatedtotoWM
WMcannot
cannotbebeextracted
extracted
fromthe
features,
which means
that the15characteristics
of the signal related
tofor
WM
be extracted
from
the time-domain
only. Figure
presents the classification
accuracy
thecannot
implemented
classifiers
the time-domain
only. Figure
15 presents
the classification
accuracy
the implemented
classifiers
time-domain
only. Figure
15 presents
the classification
accuracy
for thefor
implemented
classifiers
on raw
the
time-domain
only.
presents
the classification
accuracy
for the implemented
classifiers
on raw
data using
WMFigure
as the15
target.
As shown
in the figure,
the obtained
classification accuracies
onusing
raw data
using
WM
as the
target.
As
shown
in the
figure,
the obtained
classification
accuracies
data
WM
as
the
target.
As
shown
in
the
figure,
the
obtained
classification
accuracies
are
below
on
data
using
WMthere
as the
shown
the figure, the
obtained classification accuracies
areraw
below
50%,
where
aretarget.
only 77Asout
of 182intime-domain
features.
are
below
50%,
where
there
are
only
77
out
of
182
time-domain
features.
50%, are
where
there
onlythere
77 out
18277time-domain
features. features.
below
50%,are
where
areof
only
out of 182 time-domain

Figure 15. Performance of implemented classifiers for the classification of WM scores using raw data
and
time-domain
features.
Figure
15.
Performance
ofimplemented
implementedclassifiers
classifiers for
ofofof
WM
scores
using
raw
data
Figure
15.only
Performance
ofof
implemented
for
theclassification
classification
WM
scores
using
raw
data
Figure
15.
Performance
classifiers
for the
the
classification
WM
scores
using
raw
data
and
only
time-domain
features.
and
only
time-domain
and
only
time-domainfeatures.
features.
Furthermore, Figure 16 presents the classification accuracy of WM scores for the implemented
Furthermore,
FigureAs
16 presents
classification
ofof
WM
scores
for for
the the
implemented
classifiers
using NoDC.
shown inthe
the
figure, the accuracy
highest obtained
classification
accuracy
is,
Furthermore,
accuracy
WM
scores
implemented
Furthermore,Figure
Figure1616presents
presentsthe
theclassification
classification accuracy
of WM
scores
for the
implemented
classifiers
using
NoDC.
As
shown
in
the
figure,
the
highest
obtained
classification
accuracy
is,
surprisingly,
increased
to
about
63%
using
LinearSVC.
Also,
after
feature
selection,
only
83
out
of
182
classifiers
the highest
highest obtained
obtainedclassification
classificationaccuracy
accuracy
classifiersusing
usingNoDC.
NoDC.As
Asshown
shown in
in the
the figure,
figure, the
is, is,
surprisingly,
to about
63% using LinearSVC. Also, after feature selection, only 83 out of 182
time-domain increased
features were
selected.
surprisingly,
about63%
63%using
using
LinearSVC.
Also,
feature
selection,
surprisingly,increased
increasedto
to about
LinearSVC.
Also,
afterafter
feature
selection,
only 83only
out of83
182out
time-domain features were selected.

of 182
time-domain
features
selected.
time-domain
features
were were
selected.

Figure 16. Performance of selected classifiers for the classification of WM scores using NoDC data and
only time-domain
features.
Figure
16. Performance
of selected classifiers for the classification of WM scores using NoDC data and
only
time-domain
features.
Figure 16. Performance of selected classifiers for the classification of WM scores using NoDC data and

Figure 16. Performance of selected classifiers for the classification of WM scores using NoDC data and
only
time-domain
features.
only
time-domain
features.

Sensors 2018, 18, 3743

17 of 21

Sensors 2018, 18, x FOR PEER REVIEW

17 of 22

In addition,
Figure 17 presents the classification accuracy of WM for the implemented classifiers
In addition, Figure 17 presents the classification accuracy of WM for the implemented classifiers
on NoOutliers
data.
As As
shown
ininthe
highestobtained
obtained
classification
accuracy
is below
on NoOutliers data.
shown
thefigure,
figure, the
the highest
classification
accuracy
is below
50%, 50%,
whichwhich
is obtained
using
only
9797
features.
is obtained
using
only
features.

Figure
17. Performance
selectedclassifiers
classifiers for
for the
the classification
WM
scores
using
NoOutlier
data data
Figure
17. Performance
of of
selected
classificationofof
WM
scores
using
NoOutlier
and only
time-domain
features.
and only
time-domain
features.

It should
be mentioned
in general,
classification
results
using
NoDC
data
were
It should
be mentioned
that,that,
in general,
the the
classification
results
using
thethe
NoDC
data
were
higher
than thosewhen
obtained
when
both or
theNoOutliers
raw or NoOutliers
dataused.
were used.
This implies
than higher
those obtained
both
the raw
data were
This implies
thatthat
the the
outliers
outliers
that were
detected
the EEG
signals
mightsome
contain
some information
about
the predicted
that were
detected
in the
EEG in
signals
might
contain
information
about the
predicted
cognitive
cognitive skills and hence, should not be removed. Therefore, in the following subsections, the
skills and hence, should not be removed. Therefore, in the following subsections, the obtained results
obtained results on only NoDC data will be reported.
on only NoDC data will be reported.
4.3.2. Classification Results Using Only Frequency-Domain Features

4.3.2. Classification Results Using Only Frequency-Domain Features
When only the frequency-domain features are used, approximately similar results were

When
only
frequency-domain
features
used, approximately
similar
results the
were
obtained.
obtained.
Tothe
provide
some information
about are
the hyper-parameters
of the
used models,
results
To provide
some
information
about
the
hyper-parameters
of
the
used
models,
the
results
are
listed
are listed in a table. Table 1 summarizes the classification results using only the 70 frequency-domain in a
table.features.
Table 1As
summarizes
theinclassification
using
only
the 70 frequency-domain
can be shown
the table, KNNresults
gave the
highest
classification
scores of 70% and features.
51% for As
FAshown
and WM,
respectively.
It isgave
worththe
mentioning
that the selected
features
mostly
theFA
power
can be
in the
table, KNN
highest classification
scores
of 70%
andinclude
51% for
and WM,
in
the
theta
band
for
the
different
channels.
respectively. It is worth mentioning that the selected features mostly include the power in the theta
band for the different channels.
Table 1. Summary of the average scores of stratified 10-fold cross validation for the two cognitive

using only
features.
Highest
scorescross
are shown
in bold.
Tableskills
1. Summary
offrequency-domain
the average scores
of stratified
10-fold
validation
for the two cognitive skills
using only frequency-domain features. Highest scores are shown in bold. Focused
Working
Classifier

Parameters

Attention
Memory
Focused
Working
Classifier
Parameters
Linear
Support
Attention
Memory
Uses l2-norm penalty
0.69
0.42
Vector (LinSVC)
Linear Support Vector
Uses l2-norm penalty
0.69
0.42
Random Forest
(LinSVC)
Number of trees = 200
0.66
0.48
(RF)
Random Forest (RF)
Number of trees = 200
0.66
0.48
Support Vector
0.7
Uses
a
radial
basis
function
kernel
0.47
Support
Vector
Classifier
Classifier
(SVC)
Uses a radial basis function kernel
0.7
0.47
(SVC)
K-Nearest
0.7 0.7
0.5 0.5
K = 15K = 15
K-Nearest
Neigbor
Neigbor
(KNN)(KNN)
Neural Network
Two hidden
layers
withwith
40 nodes
each,
logistic
activation
Two hidden
layers
40 nodes
each,
logistic
activation and
Neural Network (NN)
0.68 0.68
0.37 0.37
regularization
parameter
of 0.001.
(NN)
and regularization
parameter
of 0.001.
Two hidden
dense
layers
of 271
andand
180180
nodes
followed
Two hidden
dense
layers
of 271
nodes
followed by a
25%
dropoutlayer,
layer,followed
followed by
by a hidden
hidden dense
layer
NN
with
by
a
25%
dropout
dense
layerof 280
NN with Dropout layers
0.66 0.66
0.39 0.39
nodes and
AllAll
layers
have
sigmoid
Dropout layers
of 280 nodes
and aa25%
25%dropout
dropoutlayer,
layer,
layers
have
activation with Adamax optimization.
sigmoid activation
with Adamax optimization.
Nodes
are expanded
until
all leaves
pureororuntil
untilall
all leaves
Nodes
are expanded
until
all leaves
areare
pure
contain less than the minimum number of samples required to
Decision Tree (DT) leaves contain less than the minimum number of samples
0.53
0.35
Decision Tree
split an internal node, which is 2. The minimum number of
required to split an internal node, which is 2. The
0.53
0.35
samples required to be at a leaf node is 1.
(DT)
minimum number of samples required to be at a leaf node
Gaussian Process (GP)
The kernel is radial-basis function
0.65
0.35
is 1.
Gaussian Naive Bayes
(GNB)

0.51

0.48

Logistic Regression (LR)

The penalty is L1 norm. Inverse of regularization strength is 1.

0.7

0.43

Ada Boost (AB)

DT as base estimator.

0.7

0.42

Sensors 2018, 18, 3743

18 of 21

4.3.3. Classification Results Using Combined Features
The two features’ sets were combined, which resulted in a total number of statistical and frequency
domain features of 280. This number was then reduced using the same feature selection approach that
was used before. The best obtained results are summarized in Table 2, together with some information
on the important hyper-parameters that yielded best results for the used classifiers. As can be shown
in the table, combining the time and frequency-domain features yielded the best results, which indicate
that both the time and frequency domains have information about the detected cognitive skills that
need to be used in encoding them. Moreover, the best obtained accuracy for FA was 84% using a
two-layer NN, meanwhile, linear support vector classifier yielded the best accuracy (about 81%) for the
detection of WM. Other classifiers, however, were not able to provide good performance on the used
data set. It is worth mentioning that the selected features in the best models include mostly power
band and Hjorth mobility and complexity values.
Table 2. Summary of the average scores of stratified 10-fold cross validation for the two cognitive skills
using statistical and frequency domain features. Highest scores are shown in bold.
Classifier Name

Parameters

Focused
Attention

Working
Memory

Linear Support Vector
(LinSVC)

Uses l2-norm penalty

0.80

0.815

Random Forest (RF)

Number of trees = 200

0.70

0.55

Support Vector Classifier
(SVC)

Uses a radial basis function kernel

0.69

0.58

K-Nearest Neigbor (KNN)

K = 15

0.69

0.46

Neural Network (NN)

Two hidden layers with 40 nodes each, logistic activation and
regularization parameter of 0.001.

0.84

0.811

Neural Network with
Dropout layers

Two hidden dense layers of 271 and 180 nodes followed by a
25% dropout layer, followed by a hidden dense layer of 280
nodes and a 25% dropout layer, All layers have sigmoid
activation, with AdaMax optimization.

0.82

0.74

Decision Tree (DT)

Nodes are expanded until all leaves are pure or until all leaves
contain less than the minimum number of samples required to
split an internal node, which is 2. The minimum number of
samples required to be at a leaf node is 1.

0.59

0.43

Gaussian Process (GP)

The kernel is radial-basis function

0.68

0.43

0.45

0.40

Gaussian Naive Bayes
(GNB)
Logistic Regression (LR)

The penalty is L1 norm. Inverse of regularization strength is 1.

0.62

0.64

Ada Boost (AB)

DT as base estimator.

0.69

0.42

5. Conclusions and Future Work
In this paper, an approach for the prediction of focused attention and working memory using EEG
is proposed. EEG signals were recorded while the subjects undertook a cognitive test that stimulated
these cognitive skills. The collected EEG signals were analyzed in the time and frequency domains to
extract a set of 280 features, which were then used to train different classifiers. The built models were
able to detect the three levels of FA and WM with good accuracy, which indicates the suitability of the
proposed approach to predict the three levels (i.e., low, average, and high) of focused attention and
working memory of learners using a wireless 14-channel EEG headset. In comparison with similar
works, the proposed models provide generalizable and consistent results since they were obtained
using a relatively-large sample size. For example in [5,46], a sample size of 24 and 4 participants
were used, respectively, where cognitive states were classified into only two classes; attentive and
non-attentive. In addition, the best obtained classification accuracies were 77% and 83%, respectively,
using SVM binary classifiers. A comparative analysis with similar work is presented in Table 3.

Sensors 2018, 18, 3743

19 of 21

It should be mentioned that only approaches with machine learning-based models are included in the
table; studies with statistical designs are not added.
Table 3. The proposed approach in comparison with related work.
Ref.

Detected Cognitive State(s)

Sample Size

Used Model(s)

Performance

[5]

Attention

24

Support Vector
Machines (SVM)

76.8%

[8]

Four Attention states

4

NN

56.5–79.75%

[9]

Discrimination between working
memory and recognition

10

SVM

79%

[19]

Four cognitive states related to an
ambulatory subject

A total of 6000 data points
taken from 1 subject

Ensemble Classifier

80%

[21]

Cognitive states related to
viewing 4 images

26

Convolutional NN

79.9%

Proposed
Approach

Three different levels (low,
average, high) for the working
memory and focused attention

86

NN and SVC

84% for WM and
81% for FA

As a future work, to avoid feature extraction and selection, using the raw EEG for the detection
of the levels of FA and WM will be examined. For this purpose, deep-learning approaches such as
convolutional and recurrent neural networks can be used. As a limitation of the study, it should be
mentioned that a few of the subjects reported difficulty in recognizing the meanings of some English
words that were included in the cognitive test, which should be considered in the future as it might
have affected the analysis. Another limitation of the study is that the data collection was performed in
a laboratory-like setting, not in a real classroom environment. Therefore, the factors that might affect
the results when the experiment is conducted in a more ecological setting need to be considered.
Author Contributions: Conceptualization, D.S. and A.B.; Data curation, Z.M., M.E.H. and D.S.; Formal analysis,
M.E.H. and D.S.; Funding acquisition, D.S. and A.B.; Investigation, Z.M., M.E.H. and T.S.; Methodology, Z.M.,
M.E.H. and D.S.; Project administration, D.S. and A.B.; Resources, A.B.; Software, M.E.H.; Supervision, A.B.;
Validation, Z.M. and M.E.H.; Visualization, Z.M. and M.E.H.; Writing—original draft, D.S.; Writing—review and
editing, Z.M. and A.B.
Funding: This research was supported by Information Technology Industry Development Agency (ITIDA);
website www.itida.gov.eg, and Smart & Creative Solutions (SMACRS LLC); website www.smacrs.com,
grant number SM-SW01.
Acknowledgments: The authors would like to acknowledge the role of the staff of Center for Learning
Technologies and the undergraduate students who have participated in administrating the data collection sessions.
Conflicts of Interest: The authors declare that they have no conflict of interest.

References
1.
2.
3.
4.
5.
6.
7.

Kearsley, G.; Shneiderman, B. Engagement theory: A framework for technology-based teaching and learning.
Educ. Technol. 1998, 38, 20–23.
Baddeley, A. Working memory: Theories, models, and controversies. Annu. Rev. Psychol. 2012, 63, 1–29.
[CrossRef] [PubMed]
Shuell, T.J. Cognitive conceptions of learning. Rev. Educ. Res. 1986, 56, 411–436. [CrossRef]
Savelainen, A. An Introduction to EEG Artifacts. Available online: https://pdfs.semanticscholar.org/5cd2/
3372a19187f7dfb61c1443879f392473e250.pdf (accessed on 12 February 2018).
Liu, N.-H.; Chiang, C.-Y.; Chu, H.-C. Recognizing the degree of human attention using EEG signals from
mobile sensors. Sensors 2013, 13, 10273–10286. [CrossRef] [PubMed]
Ko, L.-W.; Komarov, O.; Hairston, W.D.; Jung, T.-P.; Lin, C.-T. Sustained attention in real classroom settings:
An EEG study. Front. Hum. Neurosci. 2017, 11, 388. [CrossRef] [PubMed]
Klimesch, W. Eeg alpha and theta oscillations reflect cognitive and memory performance: A review and
analysis. Brain Res. Rev. 1999, 29, 169–195. [CrossRef]

Sensors 2018, 18, 3743

8.

9.

10.
11.
12.

13.
14.
15.

16.

17.

18.

19.
20.

21.
22.
23.

24.
25.

26.
27.

28.

20 of 21

Mohammadpour, M.; Mozaffari, S. Classification of EEG-Based Attention for Brain Computer Interface.
In Proceedings of the 2017 3rd Iranian Conference on Intelligent Systems and Signal Processing, Shahrood,
Iran, 20–21 December 2017; pp. 34–37.
Ghosh, P.; Mazumder, A.; Bhattacharyya, S.; Tibarewala, D.N. An EEG Study on Working Memory and
Cognition. In Proceedings of the 2nd International Conference on Perception and Machine Intelligence,
New York, NY, USA, 26–27 February 2015; pp. 21–26.
Pavlov, Y.G.; Kotchoubey, B. EEG correlates of working memory performance in females. BMC Neurosci.
2017, 18, 26. [CrossRef] [PubMed]
Genon, S.; Reid, A.; Langner, R.; Amunts, K.; Eickhoff, S.B. How to characterize the function of a brain region.
Trends Cogn. Sci. 2018, 22, 350–364. [CrossRef] [PubMed]
Berka, C.; Levendowski, D.J.; Cvetinovic, M.M.; Petrovic, M.M.; Davis, G.; Lumicao, M.N.; Zivkovic, V.T.;
Popovic, M.V.; Olmstead, R. Real-time analysis of EEG indexes of alertness, cognition, and memory acquired
with a wireless EEG headset. Int. J. Hum. Comput. Interact. 2004, 17, 151–170. [CrossRef]
Åkerstedt, T.; Folkard, S. The three-process model of alertness and its extension to performance, sleep latency,
and sleep length. Chronobiol. Int. 1997, 14, 115–123. [CrossRef] [PubMed]
Gonsalvez, C.J.; Polich, J. P300 amplitude is determined by target-to-target interval. Psychophysiology 2002,
39, 388–396. [CrossRef] [PubMed]
Abdel-Rahman, A.S.; Seddik, A.F.; Shawky, D.M. An Affordable Approach for Detecting Drivers’ Drowsiness
Using EEG Signal Analysis. In Proceedings of the 2015 International Conference on Advances in Computing,
Communications and Informatics, Kochi, India, 10–13 August 2015; pp. 1326–1332.
Abdel-Rahman, A.S.; Seddik, A.F.; Shawky, D.M. A Low-Cost Drowsiness Detection System as a Medical
Mobile Application. Available online: https://www.researchgate.net/publication/280156360_A_low-cost_
Drowsiness_detection_system_as_a_medical_mobile_application (accessed on 10 January 2018).
Ruiz-Gómez, S.J.; Gómez, C.; Poza, J.; Gutiérrez-Tobal, G.C.; Tola-Arribas, M.A.; Cano, M.; Hornero, R.
Automated multiclass classification of spontaneous EEG activity in alzheimer’s disease and mild cognitive
impairment. Entropy 2018, 20, 35. [CrossRef]
Wulsin, D.F.; Gupta, J.R.; Mani, R.; Blanco, J.A.; Litt, B. Modeling electroencephalography waveforms with
semi-supervised deep belief nets: Fast classification and anomaly measurement. J. Neural Eng. 2011, 8,
036015. [CrossRef] [PubMed]
Lan, T.; Adami, A.; Erdogmus, D.; Pavel, M. Estimating Cognitive State Using EEG Signals. In Proceedings
of the 2005 13th European Signal Processing Conference, Antalya, Turkey, 4–8 September 2005; pp. 1–4.
Schirrmeister, R.T.; Springenberg, J.T.; Fiederer, L.D.J.; Glasstetter, M.; Eggensperger, K.; Tangermann, M.;
Hutter, F.; Burgard, W.; Ball, T. Deep learning with convolutional neural networks for brain mapping and
decoding of movement-related information from the human EEG. arXiv, 2017; arXiv:1703.05051.
Zafar, R.; Dass, S.C.; Malik, A.S. Electroencephalogram-based decoding cognitive states using convolutional
neural network and likelihood ratio based score fusion. PLoS ONE 2017, 12, e0178410. [CrossRef] [PubMed]
Greaves, A.S. Classification of EEG with Recurrent Neural Networks. Available online: http://cs224d.
stanford.edu/reports/GreavesAlex.pdf (accessed on 10 March 2018).
Mills, C.; Fridman, I.; Soussou, W.; Waghray, D.; Olney, A.M.; D’Mello, S.K. Put Your Thinking Cap on:
Detecting Cognitive Load Using EEG during Learning. In Proceedings of the Seventh International Learning
Analytics & Knowledge Conference, Vancouver, BC, Canada, 13–17 March 2017; pp. 80–89.
Poulsen, A.T.; Kamronn, S.; Dmochowski, J.; Parra, L.C.; Hansen, L.K. EEG in the classroom: Synchronised
neural recordings during video presentation. Sci. Rep. 2017, 7, 43916. [CrossRef] [PubMed]
Amin, H.U.; Malik, A.S.; Ahmad, R.F.; Badruddin, N.; Kamel, N.; Hussain, M.; Chooi, W.-T. Feature extraction
and classification for EEG signals using wavelet transform and machine learning techniques. Australas. Phys.
Eng. Sci. Med. 2015, 38, 139–149. [CrossRef] [PubMed]
Mazher, M.; Aziz, A.A.; Malik, A.S.; Amin, H.U. An EEG-based cognitive load assessment in multimedia
learning using feature extraction and partial directed coherence. IEEE Access 2017, 5, 14819–14829. [CrossRef]
Ke, Y.; Qi, H.; He, F.; Liu, S.; Zhao, X.; Zhou, P.; Zhang, L.; Ming, D. An EEG-based mental workload estimator
trained on working memory task can work well under simulated multi-attribute task. Front. Hum. Neurosci.
2014, 8, 703. [CrossRef] [PubMed]
Owen, A.M.; McMillan, K.M.; Laird, A.R.; Bullmore, E. N-back working memory paradigm: A meta-analysis
of normative functional neuroimaging studies. Hum. Brain Mapp. 2005, 25, 46–59. [CrossRef] [PubMed]

Sensors 2018, 18, 3743

29.

30.
31.
32.

33.

34.
35.

36.
37.
38.
39.
40.
41.
42.
43.
44.
45.
46.

21 of 21

Santiago-Espada, Y.; Myer, R.R.; Latorella, K.A.; Comstock, J.R., Jr. The Multi-Attribute Task Battery II
(MATB-II) Software for Human Performance and Workload Research: A User’s Guide; NASA Langley Research
Center: Hampton, VA, USA, 2011.
Emotiv Website. Available online: www.emotiv.com (accessed on 12 October 2017).
Cognifit Website. Available online: www.cognifit.com (accessed on 25 November 2017).
Siberski, J.; Shatil, E.; Siberski, C.; Eckroth-Bucher, M.; French, A.; Horton, S.; Loefflad, R.F.; Rouse, P.
Computer-based cognitive training for individuals with intellectual and developmental disabilities:
Pilot study. Am. J. Alzheimer’s Dis. Other Dementias 2015, 30, 41–48. [CrossRef] [PubMed]
Preiss, M.; Shatil, E.; Cermakova, R.; Cimermannova, D.; Flesher, I. Personalized cognitive training in
unipolar and bipolar disorder: A study of cognitive functioning. Front. Hum. Neurosci. 2013, 7, 108.
[CrossRef] [PubMed]
Shatil, E.; Mikulecka, J.; Bellotti, F.; Bureš, V. Novel television-based cognitive training improves working
memory and executive function. PLoS ONE 2014, 9, e101472. [CrossRef] [PubMed]
Shah, T.M.; Weinborn, M.; Verdile, G.; Sohrabi, H.R.; Martins, R.N. Enhancing cognitive functioning in
healthly older adults: A systematic review of the clinical significance of commercially available computerized
cognitive training in preventing cognitive decline. Neuropsychol. Rev. 2017, 27, 62–80. [CrossRef] [PubMed]
Hjorth, B. Eeg analysis based on time domain properties. Electroencephalogr. Clin. Neurophysiol. 1970, 29,
306–310. [CrossRef]
Proakis, J.G.; Manolakis, D.G. Power spectrum estimation. In Digital Signal Processing; Principles, Algorithms,
and Applications, 3rd ed.; Prentice Hall: Upper Saddle River, NJ, USA, 1996.
Welch, P. The use of fast fourier transform for the estimation of power spectra: A method based on time
averaging over short, modified periodograms. IEEE Trans. Audio Electroacoust. 1967, 15, 70–73. [CrossRef]
Cohen, M.X. Analyzing Neural Time Series Data: Theory and Practice; MIT Press: London, UK, 2014.
Ng, A.Y. Feature Selection, L 1 vs. L 2 Regularization, and Rotational Invariance. In Proceedings of the
Twenty-First International Conference on Machine Learning, Banff, AB, Canada, 4–8 July 2004; p. 78.
Ruder, S. An overview of gradient descent optimization algorithms. arXiv, 2016; arXiv:1609.04747.
Kingma, D.P.; Ba, J. Adam: A method for stochastic optimization. arXiv, 2014; arXiv:1412.6980.
Markou, M.; Singh, S. Novelty detection: A review—Part 1: Statistical approaches. Signal Process. 2003, 83,
2481–2497. [CrossRef]
Atkinson, P.M.; Tatnall, A.R.L. Introduction neural networks in remote sensing. Int. J. Remote Sens. 1997, 18,
699–709. [CrossRef]
Srivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; Salakhutdinov, R. Dropout: A simple way to prevent
neural networks from overfitting. J. Mach. Learn. Res. 2014, 15, 1929–1958.
Djamal, E.C.; Pangestu, D.P.; Dewi, D.A. EEG-Based Recognition of Attention State Using Wavelet and
Support Vector Machine. In Proceedings of the 2016 International Seminar on Intelligent Technology and Its
Applications, Lombok, Indonesia, 28–30 July 2016; pp. 139–144.
© 2018 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).

