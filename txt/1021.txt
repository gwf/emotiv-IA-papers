algorithms
Article

Development of Filtered Bispectrum for EEG Signal
Feature Extraction in Automatic Emotion Recognition
Using Artificial Neural Networks
Prima Dewi Purnamasari, Anak Agung Putri Ratna and Benyamin Kusumoputro *
Department of Electrical Engineering, Faculty of Engineering, Universitas Indonesia, 16424 Depok, Indonesia;
prima.dp@ui.ac.id (P.D.P.); ratna@eng.ui.ac.id (A.A.P.R.)
* Correspondence: kusumo@ee.ui.ac.id; Tel.: +62-21-7270078
Academic Editors: Andras Farago and Toly Chen
Received: 31 March 2017; Accepted: 25 May 2017; Published: 30 May 2017

Abstract: The development of automatic emotion detection systems has recently gained
significant attention due to the growing possibility of their implementation in several applications,
including affective computing and various fields within biomedical engineering. Use of the
electroencephalograph (EEG) signal is preferred over facial expression, as people cannot control
the EEG signal generated by their brain; the EEG ensures a stronger reliability in the psychological
signal. However, because of its uniqueness between individuals and its vulnerability to noise, use
of EEG signals can be rather complicated. In this paper, we propose a methodology to conduct
EEG-based emotion recognition by using a filtered bispectrum as the feature extraction subsystem
and an artificial neural network (ANN) as the classifier. The bispectrum is theoretically superior to the
power spectrum because it can identify phase coupling between the nonlinear process components
of the EEG signal. In the feature extraction process, to extract the information contained in the
bispectrum matrices, a 3D pyramid filter is used for sampling and quantifying the bispectrum value.
Experiment results show that the mean percentage of the bispectrum value from 5 × 5 non-overlapped
3D pyramid filters produces the highest recognition rate. We found that reducing the number of
EEG channels down to only eight in the frontal area of the brain does not significantly affect the
recognition rate, and the number of data samples used in the training process is then increased to
improve the recognition rate of the system. We have also utilized a probabilistic neural network
(PNN) as another classifier and compared its recognition rate with that of the back-propagation
neural network (BPNN), and the results show that the PNN produces a comparable recognition rate
and lower computational costs. Our research shows that the extracted bispectrum values of an EEG
signal using 3D filtering as a feature extraction method is suitable for use in an EEG-based emotion
recognition system.
Keywords: bispectrum; BPNN; EEG; emotion recognition

1. Introduction
It is widely believed that psychological factors can affect a patient’s recovery process; positive
emotions, for example, affect the progression of recovery, and a patient’s emotional response to his or
her illness could also affect the type and amount of medication prescribed by doctors [1]. Psychological
treatment is one of the most important factors in a patient’s recovery and, when administered properly,
might even accelerate the healing process. Although psychologists are still debating whether positive
emotions could cure patients suffering from life-threatening illnesses, experts believe that experiencing
positive emotions (e.g., feeling happy and stress free) would improve patients’ social life, which in
turn make patients more productive in their daily activities.
Algorithms 2017, 10, 63; doi:10.3390/a10020063

www.mdpi.com/journal/algorithms

Algorithms 2017, 10, 63

2 of 20

The difficulty in utilizing the psychological approach in the patient recovery process stems from
the patient’s ability to hide emotions or the inability to express an emotional condition despite his or
her desires, such as cases where patients experience facial nerve paralysis or adhere to certain cultural
dynamics. When these situations occur, even social approaches through various communication
techniques or observations of body language still pose a challenge, as the nurse or the patient’s family
members might not be able to accompany the patient at all times. A solution can therefore be found in
an automatic emotion recognition tool or system.
There are several ways to determine human emotions automatically. One popular way is by using
a camera and image processing technique to recognize facial expressions; however, this method is
ineffective when patients suffer from facial paralysis or when they hide their genuine emotions.
Therefore, another approach should be considered, such as automatic emotion recognition that
incorporates the use of physiological signals, such as the electrocardiograph (ECG) for monitoring
the patient’s heart rate, the multi-resonance imaging (f-MRI) for monitoring blood-oxygen-level
dependence (BOLD), the eye-gaze trackers (EGT) for monitoring the width of the pupil movements or
the EEG for monitoring brain signals. The EEG-based emotion recognition system is preferred due to
the advantage of its temporal resolution [2] and the ability of the patient to move around while the
EEG-based emotion recognition system is processing the signals.
The goal of this research is to develop an automatic system for recognizing human emotions
through EEG signals. The power spectrum analysis has previously been implemented in the developed
automatic emotion recognition system [3], and in this work, the bispectrum analysis, which theoretically
is better than the power spectrum analysis, was used as the feature extraction subsystem, while ANNs
served as the classifiers. For the purpose of benchmarking the experiment setup and comparing the
recognition capability of the developed system, we have used the EEG signals from healthy subjects as
provided by the Database for Emotion Analysis Using Physiological Signals (DEAP) [4].
The rest of the paper is organized as follows: Section 2 presents the literature review on
methodologies previously proposed by other researchers; Section 3 discusses the methodology used in
this research, the materials of the study, the bispectrum processing technique, including the proposed
3D filtering method for extracting the bispectrum value, and the classifiers used (i.e., the BPNN and
the PNN); Section 4 describes in detail the experimental method and discusses the results; finally,
Section 5 concludes the paper.
2. Literature Review
Researchers have proposed many different methods for recognizing emotions through EEG
signals [2,5]. The fundamental challenge of using EEG signals to detect human emotion lies in
understanding how a particular emotional state is represented in the brain and applying the correct
computational model to accurately identify that emotion, both automatically and in real time. Human
emotion is naturally felt after a few seconds, not in a split second; therefore, it is more appropriate if
the EEG signals used for the analysis have a set time frame to avoid analyzing the signals’ amplitude
as in the event-related potentials (ERP) technique. However, the ERP technique has been used in other
emotion recognition research [6–9] because the emotional stimuli were instantaneous pictures. On the
other hand, to analyze EEG signals within a certain time duration for successful and productive use in
an emotion recognition system, a signal processing technique is needed: one widely-adopted method
is to calculate the power spectrum of the brain signals [4,10–13], and the results were an approximately
60–70% recognition rate, showing much room for improvement.
Bispectrum analysis has been used as a signal processing technique for analyzing the EEG
signal [14] and also used in medical research, such as in cases related to anesthesia [15,16],
meditation [17], vigilance tests [18] and recently in emotion recognition [19,20], but the recognition
rate result for the latter was somewhat lower than desired. Hosseini [19] used seven features from
bispectrum analysis taken from each of five EEG channels to recognize whether the subject was
stressed; Hosseini used a support vector machine (SVM) as the classifier and a genetic algorithm

Algorithms 2017, 10, 63

3 of 20

as the feature selection algorithm. Kumar [20] used the bispectrum as a feature extraction method
to identify four types of emotional states, namely the low and high arousal and the low and high
valence from the data provided in the DEAP database [4], using only two channels (Fp1 and Fp2)
as the input signals and SVM as the classifier. There were five types of features calculated after the
bispectrum analysis: normalized bispectral entropy (NBE), normalized bispectral squared entropy
(NBSE), mean-magnitude of bispectrum (MMOB), first order spectral moment (FOSM) and second
order spectral moment (SOSM). However, the use of a higher complexity feature calculation leads
to an increased computation cost and may result in decreased accuracy. In this work, we propose
a simpler method for feature extraction after the bispectrum values are calculated: to calculate the
mean of bispectrum values after they are divided, by a filtering method, into several spatial regions,
and using the percentage of the mean as the feature.
Regarding the relationship between EEG and emotion, the prefrontal cortex area of the brain
exhibits a strong correlation with human emotion. Nevertheless, through the feature selection process,
some researchers have found that other EEG channels were better suited for emotion recognition [20,21].
In the frequency domain, particular emotions are believed to affect specific frequency bands, such as
alpha [22,23], gamma [24,25] and theta [26], and some researchers have incorporated all frequencies,
delta, alpha, beta, theta and gamma [19,27], into their EEG research. In this work, we used all
frequencies of brain signals and omitted none.
For the EEG signal classification using DEAP as the dataset, both linear and non-linear methods
have been used widely; such as support vector machine (SVM) [28–32], linear discriminant analysis
(LDA) [33], quadratic discriminant analysis (QDA) [30], k-nearest neighbor (KNN) [34] for the linear
classifier; and back-propagation neural networks (BPNN) [3], probability neural networks (PNN) [35],
Bayesian neural networks (BNN) [36], deep learning networks (DLN) [37] and Elman neural networks
(ENN) [19] for the non-linear classifier. In this work, we use the BPNN as the benchmark classifier.
Although BPNN has a major drawback in terms of time, it is guaranteed to execute the best performance
because the algorithm uses a gradient descent of error. We also compared our classification results
using BPNN to the results with that of the PNN.
DEAP has been used by many researchers because a complete description of the data acquisition
is available, the data samples are large enough, i.e., 1280 samples, and the pre-processed data in the
form of MATLAB and Python matrices are also provided; these allow researchers to be focused on the
development of feature extraction and the classification methods. However, the results achieved by
different researchers are difficult to compare because they use various cross-validation methods, and
the recognition rate result varied between 40% and 90%.
3. Bispectrum Analysis of EEG Signals for an Emotion Recognition System
There is a significant amount of research focusing on human emotion recognition. However, only
a few publicly open databases of emotion are available for download, and in this research, we chose
the DEAP database [4] for two reasons. First, the DEAP database uses carefully-chosen emotional
stimuli based on statistical methods from the respondents. To ensure objectivity, the respondents who
assessed the emotional stimuli were different from the respondents who recorded the EEG signals.
Second, this database provides numerous observations. Using a publicly open database enables us to
reproduce and compare our research results with other findings.
To recognize human emotion automatically, the proposed method employs higher order statistics
to the EEG signal to produce 3D bispectrum matrices. Then, the bispectrum matrices are filtered using
a 3D pyramid filter to reduce the size of the matrices and to form a feature vector. The resulting feature
vectors are used as the input for the BPNN classifier.
3.1. EEG Database and Its Acquisition
Currently, the existing EEG databases available for research purposes mostly contain the EEG
signals of motor imagination [38,39], sleep stages [40] and epilepsy [41], and only a few EEG databases

Algorithms 2017, 10, 63

4 of 20

related to the emotional states are available, such as Mahnob-HCI [42], the SJTU Emotion EEG Dataset
(SEED) database [43] and the DEAP database [4]. This work utilized the DEAP database [4] that
provided the most EEG data from participants; emotional stimuli came from video music clips, and
the emotional states of the participants were defined in 2D emotion models (i.e., the arousal and the
valence levels [4]), producing four quadrants of emotion: the high arousal, high valence (HAHV);
the low arousal, high valence (LAHV); the low arousal, low valence (LALV); and the high arousal,
low valence (HALV) emotion classes.
In this database, the emotional stimuli for each quadrant of the 2D emotion model were portioned
equally with 10 music videos, and the excited EEG signals from the brain activity were recorded
using 32 channels of an EEG Biosemi ActiveTwo with a sampling rate of 512 Hz. The DEAP database
also provided a list of the videos and their YouTube URLs, the basic statistics of participants’ ratings,
the physiological signals, the facial expression videos and the preprocessed data. EEG signals were
recorded from 32 people (balance between women and men) between the ages of 19 and 37 years.
During the recording process, each participant was asked to rate, on a scale of 1–9, his or her emotional
response when watching the video in terms of arousal, valence, likes/dislikes, the dominance and the
familiarity through the Self-Assessment Manikin (SAM) questionnaire [4].
As EEG signals are vulnerable to noises, several noise filtering methods, such as to remove power line
noise, eye and muscle artefact removal [44,45], are usually applied. In DEAP, several pre-processing steps
that consist of a band-pass filtering at 50 Hz and 60 Hz, a high-pass filtering at 2 Hz, a down-sampling
process to 128 Hz and the removal of artefacts corresponding to electrooculography (EOG) have been
conducted [4]. The DEAP database then provides 32 matrices of 40 × 40 × 8064, representing video/trial
× physiology channels × duration of the signals, respectively; however, for our research purposes, these
original 32 matrices were divided into a 1280 person-video data matrix (32 people × 40 videos). In this
work, we only considered the EEG signals; thus, only 32 out of 40 physiology signals are used, and we
eliminated the first three-second baseline from the 63-second duration of the signals, so the resulting size
of each matrix was 32 × 7680 (EEG channels × duration of the signals). The 1280 matrices were further
divided into 320 matrices (32 people × 10 videos) for each class of emotion.
3.2. EEG Channel Selection Related to Human Emotion
The brain is the operations center of the human nervous system and can be divided into three
major parts: the brainstem, the cerebellum (hindbrain) and the cerebrum (front brain). The largest part
of the brain is the cerebrum, and the outer part of the cerebrum is the cerebral cortex, which generates
most of the voltage measured on the surface of the head. The cerebral cortex is divided into four lobes,
which are the temporal, the frontal, the parietal and the occipital lobes. Each lobe is associated with
various functions, and the frontal lobe is where we can observe brain signals associated with emotions.
An EEG captures the electrical activity of the brain by measuring the voltage fluctuations due to
the flow of the electric current between neurons in the brain. The EEG electrodes are named according
to their corresponding area of the cerebral cortex: the frontal (F), the central (C), the parietal (P),
the occipital (O) and the temporal (T). Some electrodes are placed in pairs on the left side and right
side of the head, with odd numbers given to the left electrode positions and even numbers given to the
right electrode positions. The electrodes in the middle positions are given zero (z) subscripts.
In this study, we document the effect of channel selection on the recognition rate of the emotion
classification system; then, aside from the initial 32 channels, we also assess the effect of channel selection on
the brain emotion theory, which involves all eight channels on the frontal, frontal parietal and anterior-frontal
regions. In addition to studying the 32 channels and the reduced eight channels, we also compared our
results with the 14 channels corresponding to the channels available in the BCI Emotiv device.
3.3. The Developed Methodology for Emotion Classification System
Generally, the automatic emotion recognition process can be carried out using classification
algorithms through a mathematical-based machine learning method or through a biological-based

Algorithms 2017, 10, 63

5 of 20

Algorithms 2017, 10, 63

5 of 20

ANN method. Using both learning mechanisms of the classification algorithms, the researchers’
focus is finding the best techniques for the feature extraction subsystem in order to minimize
main focus is finding the best techniques for the feature extraction subsystem in order to minimize
classification error and improve the recognition rate.
classification error and improve the recognition rate.
The developed methodology for our automatic emotion recognition system is illustrated in
The developed methodology for our automatic emotion recognition system is illustrated in
Figure 1. At the first stage, human emotions are elicited through a set of emotion stimuli, usually by
Figure 1. At the first stage, human emotions are elicited through a set of emotion stimuli, usually by
pictures, sounds or videos. Then, the EEG input signals are captured by an EEG device with several
pictures, sounds or videos. Then, the EEG input signals are captured by an EEG device with several
electrodes using the appropriate sampling frequency. In this research, the EEG input signals were
electrodes using the appropriate sampling frequency. In this research, the EEG input signals were
provided by the DEAP database, and these signals were stimulated by a set of music videos (clips).
provided by the DEAP database, and these signals were stimulated by a set of music videos (clips).
The EEG input signals from this database were already preprocessed [3] through certain steps, such
The EEG input signals from this database were already preprocessed [3] through certain steps, such as
as reducing interference noise during the EEG recording, including power line frequency noise
reducing interference noise during the EEG recording, including power line frequency noise filtering
filtering and the blink artefact removal.
and the blink artefact removal.

Figure 1. Block diagram of the EEG-based emotion recognition system.
Figure 1. Block diagram of the EEG-based emotion recognition system.

To retrieve any meaningful information hidden
hidden in the EEG signal, the original time domain
To
signals
are
processed
using
some
signal
processing
techniques, and
and the
the features
features are then calculated
calculated
signals are processed using some signal processing techniques,
and extracted.
extracted. Because
Because the
the higher
higher spectrum
spectrum analysis
analysis is
is theoretically
theoretically better
better than
than the
the power
power spectrum
spectrum
and
model, we
we processed
processed our
our signals
signals using
using bispectrum
bispectrum analysis,
analysis, and
and to
toreduce
reducethe
thebispectrum
bispectrummatrix
matrixsize,
size,
model,
method.
TheThe
features
are extracted
by calculating
several
relevant
pieces
we also
also applied
appliedaa3D
3Dfiltering
filtering
method.
features
are extracted
by calculating
several
relevant
of information
from the
filtered
bispectrum
matrix,matrix,
such as
the as
entropy,
the moment
and the
mean.
pieces
of information
from
the filtered
bispectrum
such
the entropy,
the moment
and
the
The
best
features
are
then
investigated
further
to
determine
the
highest
recognition
rate.
mean. The best features are then investigated further to determine the highest recognition rate.
As the
thefeature
featurevectors
vectors
produced
bispectrum
analysis
are usually
thanofthat
of the
As
produced
by by
bispectrum
analysis
are usually
largerlarger
than that
the power
power spectrum
thecomputation
higher computation
cost of
using bispectrum
analysis
not be
spectrum
analysis,analysis,
the higher
cost of using
bispectrum
analysis could
notcould
be avoided.
avoided.
reduce
the feature
vectors,
in turn
reduce
computationcost,
cost,aa principal
principal
To
reduceTothe
size the
of size
the of
feature
vectors,
and and
in turn
reduce
thethe
computation
component analysis (PCA) technique
technique that
that searches
searches the
the best
best principal
principal components
components (i.e.,
(i.e., the
the coefficient
coefficient
component
matrix according
according to the number of the Eigen values
values desired)
desired) is utilized.
utilized. We
We also sought
sought to determine
determine
matrix
channels used
used could
could contribute
contribute to
to reducing
reducing the
the computation
computation cost
cost
whether reduction in the number of channels
without decreasing the recognition rate.
In the
the classification
classification stage,
stage, the
the BPNN
BPNN classifier
classifier was
was chosen
chosen as the benchmark classifier among
among
In
other ANN
ANN classifiers
classifiersdue
dueto
toits
itscapacity
capacitytotoguarantee
guaranteethe
theconvergence
convergenceofofthe
theminimum
minimum
error
using
other
error
using
a
a gradient
descent
trainingmethod.
method.We
Wethen
thencompared
comparedthe
theperformance
performanceof
ofthe
the BPNN
BPNN with
with another
another
gradient
descent
training
ANN classifier, namely the PNN, that uses
uses Bayesian
Bayesian probability
probability in
in the
the classification
classification process.
process.
3.4. Feature
Feature Extraction
Extraction Based
Based on
on Bispectrum
Bispectrum Analysis
Analysis and
and 3D
3D Pyramid
Pyramid Filter
Filter
3.4.
3.4.1. Bispectrum
3.4.1. Bispectrum
The power spectrum analysis, which has been widely used in biomedical signal processing,
The power spectrum analysis, which has been widely used in biomedical signal processing,
performs power distribution calculation using a function of frequency and pays no attention to the
performs power distribution calculation using a function of frequency and pays no attention to the
signal phase information. In the power spectral analysis, the signal is assumed to arise from a linear
signal phase information. In the power spectral analysis, the signal is assumed to arise from a linear
process, thus ignoring any possible interaction between components (phase coupling). However, the
process, thus ignoring any possible interaction between components (phase coupling). However, the
brain signal is part of the central nervous system with many nonlinear sources, so it is highly likely
brain signal is part of the central nervous system with many nonlinear sources, so it is highly likely
to have a phase coupling between signals [14]. The bispectrum analysis is superior to the power
to have a phase coupling between signals [14]. The bispectrum analysis is superior to the power
spectrum analysis because in its mathematical formula, there is a correlation calculation between the
frequency components [46]; thus, the phase coupling components of the EEG signals could be

Algorithms 2017, 10, 63

6 of 20

Algorithms 2017, 10, 63

6 of 20

spectrumSome
analysis
because in of
its bispectrum
mathematical
formula,
is a correlation
revealed.
characteristics
analysis
arethere
the ability
to extractcalculation
deviations between
due to
the
frequency
components
[46];
thus,
the
phase
coupling
components
of
the
EEG
signals could
be
Gaussianity, to suppress the additive colored Gaussian noise of an unknown power spectrum
and to
revealed.
Some characteristics
of bispectrum
analysis
are thebispectrum
ability to extract
deviations
detect
nonlinearity
properties [47].
Because of its
superiority,
analysis
is used indue
thisto
Gaussianity,
to
suppress
the
additive
colored
Gaussian
noise
of
an
unknown
power
spectrum
and
research as the signal processing technique in the feature extraction step. We expect that by usingto
detect nonlinearity
[47]. Because
its superiority,
used in this system
research
bispectrum
analysis,properties
the recognition
rate ofofthe
EEG-basedbispectrum
automatic analysis
emotionisrecognition
as the
signal processing technique in the feature extraction step. We expect that by using bispectrum
will
improve.
analysis,
the recognition
rate
of the
automatic
emotion recognition
system
will improve.
For example,
suppose
there
is aEEG-based
signal S3 that
is a combination
of two other
signals,
S1 and S2,
suppose
is af1signal
S3 that
twothe
other
signals,
S11 and
S2 ,
whereFor
theexample,
frequencies
are f1 there
= 20 Hz,
= 10 Hz
and is
f3 a= combination
f1 + f2 = 30 Hz,ofand
phases
are φ
= π/6,
where
the
frequencies
are
f
=
20
Hz,
f
=
10
Hz
and
f
=
f
+
f
=
30
Hz,
and
the
phases
are
φ
=
π/6,
1
1
3
1
2
1
φ2 = 5π/8 and φ3 = φ1 + φ2, respectively. The signals S1, S2 and S3 are then defined as follows: S1 = 3
φ2 = 5π/8
φ1 + φ2t2 ,+respectively.
signals
S3 are then
defined
2 and
cos(2πf
1t + φand
1), S2φ=3 5=cos(2πf
φ2) and S3 = 8The
cos(2πf
3t + S
φ13,).SThe
resulting
signal
X(t) = as
S1 +follows:
S2 + S3 isS1
= 3 cos(2πf
+ ϕa1 ),
S2 = 5With
cos(2πf
+ ϕ2 ) andfrequency
S3 = 8 cos(2πf
+ ϕ3signal
). The X(t)
resulting
signal X(t)
= S1 +
1 t by
3 t Hz,
then
received
sensor.
the2 tsampling
of 100
is processed
to reveal
S
+
S
is
then
received
by
a
sensor.
With
the
sampling
frequency
of
100
Hz,
signal
X(t)
is
processed
3 spectrum and its bispectrum values, and the result can be seen in Figure 2. The power
its2power
to reveal (Figure
its power
its bispectrum
and
the result
can be seen
Figure
2. The
spectrum
2a)spectrum
producedand
using
FFT showedvalues,
that the
dominant
frequencies
arein10,
20 and
30
power
spectrum
(Figure
produced
using
that
frequencies
are 10,of
20
Hz;
however,
it does
not 2a)
reveal
the fact
that FFT
the showed
frequency
30the
Hzdominant
is a resulting
combination
and
30
Hz;
however,
it
does
not
reveal
the
fact
that
the
frequency
30
Hz
is
a
resulting
combination
frequencies 10 and 20 Hz. On the other hand, in bispectrum (Figure 2b), the pair of normalized
of frequencies
10 and
On to
thethe
other
hand,frequencies
in bispectrum
(Figure
2b),on
thecoordinate
pair of normalized
frequencies
0.1 and
0.2 20
HzHz.
(equal
original
10 and
20 Hz),
(0.1,0.2)
frequencies
0.1
and
0.2
Hz
(equal
to
the
original
frequencies
10
and
20
Hz),
on
coordinate
(0.1,0.2)
and
and (0.2,0.1), show a high spectrum, which means that they are strongly correlated because they
(0.2,0.1),
show
a
high
spectrum,
which
means
that
they
are
strongly
correlated
because
they
produce
produce the 30-Hz signal. Supposing that the signal S3 is noise, by using a bispectrum analysis, the
the 30-Hz
Supposing
theconsideration.
signal S3 is noise,
bywe
using
bispectrum
analysis,
thebispectrum
noise signal
noise
signalsignal.
will not
be takenthat
into
Thus,
cana conclude
that
through
will
not
be
taken
into
consideration.
Thus,
we
can
conclude
that
through
bispectrum
analysis,
the main
analysis, the main frequency components are revealed while the other frequencies are eliminated.
frequency
components
are revealed
while
theinformation
other frequencies
are power
eliminated.
Because
bispectrum
Because
bispectrum
analysis
provides
more
than the
spectrum
analysis,
it is
analysis provides
more
the power
analysis,
it isextraction
expected that
the use
expected
that the use
ofinformation
bispectrum than
analysis
in thespectrum
EEG signals’
feature
process
willof
bispectrum
analysis
in
the
EEG
signals’
feature
extraction
process
will
increase
the
recognition
rate.
increase the recognition rate.

(a)

(b)

Figure
Figure 2.2. Comparison
Comparisonbetween
between(a)(a)power
powerspectrum
spectrumwhich
whichshows
showsdominant
dominantfrequencies
frequenciesand
and
(b)
bispectrum,
which
shows
dominant
frequencies
and
their
correlation
(if
any).
(b) bispectrum, which shows dominant frequencies and their correlation (if any).

The autocorrelation of a signal is the correlation between the signal and itself at a different time;
The autocorrelation of a signal is the correlation between the signal and itself at a different time;
for example, at time t and at time t + m. The autocorrelation function of x(n) can be expressed as the
for example, at time t and at time t + m. The autocorrelation function of x(n) can be expressed as the
expectation of stationary process, defined as:
expectation of stationary process, defined as:
( ): = { ∗ ( ) ( + )}.
(1)
R xx (m) := E{ x ∗ (n) x (n + m)}.
(1)
The higher order moments are a natural generalization of autocorrelation, and the cumulants
The higher
order moments
are a natural
generalization
of autocorrelation,
and the process
cumulants
are
are a nonlinear
combination
of moments.
The first
order cumulant
(C1x) from stationary
is the
a nonlinear
of an
moments.
Thenotation.
first order
cumulant
(C1x ) cumulants
from stationary
process
is the
mean,
C1x = E combination
{x(t)}, with E{.}
expectation
The
higher order
have the
property
mean, C1x
E {x(t)},
with
an expectation
The
order
cumulants under
have the
property
invariant
to=the
shift of
the E{.}
mean;
therefore, it notation.
is practical
to higher
describe
the cumulants
zero
mean
invariant to meaning
the shift of
theif mean;
therefore,
it is practical
to describe
under
zero
mean
assumption,
that
the mean
of a process
is not zero,
then as the
the cumulants
first step, the
mean
should
assumption,
thatvalue.
if the mean
of a process
not zero, then as
the first
step,
the mean
shouldis
be
be
subtractedmeaning
from each
The second
orderispolyspectrum,
which
is the
power
spectrum,
subtracted
from
each value.
The of
second
order polyspectrum,
which
is the
the third
powerorder
spectrum,
is defined
defined
as the
Fourier
transform
the second
order cumulant,
while
polyspectrum,
as theisFourier
transformisofdefined
the second
order
cumulant,
while
which is
which
the bispectrum,
as the
Fourier
transform
of the
the third
third order
order polyspectrum,
cumulant.
the For
bispectrum,
defined
as the Fourier
transform ofof
the
third order
cumulant.
the thirdisorder
cumulant,
the autocorrelation
a signal
will be
calculated until the distance
t + τ1 and t + τ2, where τ1 and τ2 are the lag. The third order cumulant from a zero mean stationary
process is defined as [48]:

Algorithms 2017, 10, 63

7 of 20

For the third order cumulant, the autocorrelation of a signal will be calculated until the distance
t + τ 1 and t + τ 2 , where τ 1 and τ 2 are the lag. The third order cumulant from a zero mean stationary
process is defined as [48]:
C3x (τ1 , τ2 ) = E{ x ∗ (n) x (n + τ1 ) x (n + τ2 )}.

(2)

Thus, the bispectrum, B( f 1 , f 2 ), defined as the Fourier transform of the third order
cumulant, becomes:
∞

B( f 1 , f 2 ) =

∞

∑ ∑

k =−∞ l =−∞

C2x (k)e− j2π f1 k e− j2π f2 l .

(3)

The bispectrum has a specific symmetrical property that is derived from the symmetrical property
of the third order cumulant [46], which results in similarity of the six regions of the bispectrum, shown
as follows:
C3x (τ1 , τ2 ) = C3x (τ2 , τ1 ) = C3x (−τ1 − τ2 , τ2 ) = C3x (τ1, − τ1 − τ2 )
.
(4)
= C3x (−τ1 − τ2 , τ1 ) = C3x (−τ2 , −τ1 − τ2 )
Because the bispectrum matrix has a redundant region as described in (4), it is sufficient to extract
features from only one quadrant of the bispectrum matrix, and because the FFT in the calculation
of bispectrum value may result in non-imaginary values, then the absolute value of the bispectrum
is used. The pseudocode of the bispectrum calculation in the feature extraction process for an EEG
signals, derived from (1)−(4), is summarized in Algorithm 1 as follows:
Algorithm 1: Bispectrum calculation in feature extraction algorithm.
1.
2.
3.
4.
5.
6.
7.
8.
9.

Define: lag
For channel = 1 to C
Calculate autocorrelation signal X to the defined lag as (2)
Construct symmetrical matrix C3x for the first quadrant
Construct cumulant matrix C3x at other quadrant as (4)
Calculate bispectrum as (3)
Take only 1 quadrant of bispectrum matrix
Take the absolute value of the bispectrum matrix
End For

3.4.2. The 3D Pyramid Filter
The output of the previous step is one quadrant bispectrum matrix, which is a 64 × 64 matrix for
each of the 32 EEG channels, equaling a total of 131,072 elements; therefore, the number of elements is
too large to be used for calculating the extracted features. To reduce the size of these feature vectors,
we have proposed a filtering mechanism by utilizing 3D pyramid shape filters for the bispectrum
elements value so that the bispectrum value at the center of the pyramid becomes the most significant
value. From the filtered area, one or more statistical properties are derived and calculated as the
features-extracted data, which will be described in the next subsection.
To find the best filtering mechanism, two filter models are proposed, as shown in Figure 3: the
non-overlapping filters with various sizes at the base and the overlapping filters with equal sizes at the
base. Figure 2b shows that the bispectrum usually gathers near the center; thus, in this area, the filters are
dense and the bases are small. At the higher frequencies, the bispectrum usually has a very low value,
so in this area, the filters are sparse, and the bases are large. Therefore, in the non-overlapping filters,
we use 5 × 5 filters (Figure 3a), with the size of the filter varying (32, 16, 8, 4 and 4) along the x- and y-axis.
By increasing the number of filters and overlapping the filters, the quantization process is expected
to provide a better approximation; we therefore increased the number of filters up to 7 × 7 and
constructed the filters with overlapping areas at the base (Figure 3b). The size of the bases is 16 × 16

Algorithms 2017, 10, 63

8 of 20

equal elements, and there are 50% overlapping areas with the adjacent filters. However, the complexity
and the resulting
Algorithms
2017, 10, 63feature vector’s size of the 7 × 7 filter should be considered as a trade-off.
8 of 20

(a)

(b)

Figure
types of
of 3D
3D filtering
filtering using
using pyramid
pyramid models
models with
with (a)
(a) 55 ×
× 55 non-overlap
non-overlap and
(b) 77 ×
× 77
Figure 3.
3. Two
Two types
and (b)
overlapping
filters.
overlapping filters.

The height of the overlapping and non-overlapping filters in both pyramid models is equal: in
The height of the overlapping and non-overlapping filters in both pyramid models is equal: in this
this case, it is one. To filter the bispectrum matrix, each selected area was multiplied by the
case, it is one. To filter the bispectrum matrix, each selected area was multiplied by the corresponding
corresponding filter. The filtering process results in several filtered matrices with their respective
filter. The filtering process results in several filtered matrices with their respective bispectrum values,
bispectrum values, and from these filtered bispectrum matrices, the features are calculated and
and from these filtered bispectrum matrices, the features are calculated and extracted. The pseudocode
extracted. The pseudocode of the filtering mechanism for the bispectrum matrices is summarized in
of the filtering mechanism for the bispectrum matrices is summarized in Algorithm 2 as follows:
Algorithm 2 as follows:
Algorithm
2: Bispectrum
3D
forfeature
feature
extraction
algorithm.
Algorithm
2: Bispectrum
3Dfiltering
filtering for
extraction
algorithm.
1. Define:M
= number
of column
filter
1.
Define:
M = number
of column
filter
2.
N = number
of row
filter
2.
N = number
of row
filter
3. channel
For channel
3. For
= 1 to C= 1 to C
4.
4.
for m = 1for
to m
M= 1 to M
5.
for
n
5.
for n = 1 to N= 1 to N
6.
length of rectangle base of the filter
6.
CalculateCalculate
length of
rectangle base of the filter
7.
Calculate width of rectangle base of the filter
7.
Calculate width of rectangle base of the filter
8.
Construct 2D filter for length side of rectangle with triangle type
8.
Construct
2D filter for length side of rectangle with triangle type
9.
Construct 2D filter for width side of rectangle with triangle type
9.
ConstructConstruct
2D filter
widthfilter
side
of rectangle
10.
3Dfor
pyramid
based
on 2D filter with triangle type
10.
Construct
3D pyramid
based
on
2Dwidth
filter
11.
Take bispectrum
value filter
with the
length
and
of the rectangle
11.
Take bispectrum
value
with the
length
width
12.
Multiply
bispectrum
value
with and
the 3D
filter of the rectangle
13.
Perform
calculation for
feature
12.
Multiply
bispectrum
value
with the 3D filter
14.
Take
only
half
of
the
13.
Perform calculation for featuretriangle (non-redundant) of the feature matrix
15.
feature matrix to feature vector
14.
Take onlyTransform
half of the
triangle (non-redundant) of the feature matrix
16.
end
15.
Transform feature matrix to feature vector
17.
end
16.
end
18.
end
17.
end
18. end
The implementation of the filtering process using the 3D pyramid filters is illustrated in Figure 4.
The
implementation
thepyramid
filteringfiltering
processon
using
the 3D pyramid
is EEG
illustrated
In this
example,
the effect of 3D
the bispectrum
matrix filters
from the
signal in
of
Figure
In this1-Channel
example, the
of The
3D pyramid
filtering onmatrix
the bispectrum
matrix
from
the EEG
Person 4.
1-Video
1 is effect
shown.
original bispectrum
is shown in
Figure
4a, and
then,
signal
of Person
1-Videoresulting
1-Channel
1 is64shown.
The original
matrix
is shown
in
one-quarter
is extracted,
in the
× 64 matrix
shown inbispectrum
Figure 4b. The
filtering
process
Figure
4a, multiplying
and then, one-quarter
extracted,
resulting in
64 × 64 filters,
matrix and
shown
Figure
4b. The
began by
this matrixiswith
the constructed
3Dthe
pyramid
for in
this
example,
we
filtering
began by multiplying
this matrix
with
the constructed
3D pyramid
filters, for
andeach
for
use the 5process
× 5 non-overlapping
filters (Figure
3a). The
multiplication
process
was conducted
this example, we use the 5 × 5 non-overlapping filters (Figure 3a). The multiplication process was
conducted for each area of the matrix according to the size of the pyramid base; for example, the
bispectrum matrix region (0:32,0:32) was multiplied by the filter whose base size is 32 × 32 and whose
height is one. Therefore, for 5 × 5 non-overlapping filters, there will be 25 times the filtering process
through this multiplication step, and the result is shown in Figure 4c. The calculation of features from

Algorithms 2017, 10, 63

9 of 20

area of the matrix according to the size of the pyramid base; for example, the bispectrum matrix
region (0:32,0:32) was multiplied by the filter whose base size is 32 × 32 and whose height is one.
Therefore, for 5 × 5 non-overlapping filters, there will be 25 times the filtering process through this
Algorithms 2017, 10, 63
9 of 20
multiplication step, and the result is shown in Figure 4c. The calculation of features from the filtered
bispectrum
matrix is then
conducted;
in this example,
the features
are the mean
of each
the
filtered bispectrum
matrix
is then conducted;
in this example,
the features
are the(average)
mean (average)
filtered
bispectrum
matrix,matrix,
and theand
result
shown
in Figure
of
each filtered
bispectrum
theisresult
is shown
in 4d.
Figure 4d.

(a)

(b)

(c)

(d)

(e)

(f)

Figure 4. Bispectrum filtering step: (a) the original bispectrum contour plot, (b) one-quarter of the
Figure 4. Bispectrum filtering step: (a) the original bispectrum contour plot, (b) one-quarter of the
bispectrum matrix, (c) the result of the filtering process, (d) the mean as the feature, (e) the feature
bispectrum matrix, (c) the result of the filtering process, (d) the mean as the feature, (e) the feature
vector constructed from the non-redundant region of the quantized matrix and (f) the full feature
vector constructed from the non-redundant region of the quantized matrix and (f) the full feature
vectors from the 32 channels.
vectors from the 32 channels.

As the bispectrum matrix shown in Figure 4b is asymmetrical, then the feature-extracted matrix
(such as in Figure 4d) is also asymmetrical; therefore, it is sufficient to take just half of the whole
matrix. For example, in the 5 × 5 filters, the number of non-redundant elements of the matrix equal
to the sum of arithmetic sequence ∑
= 15, resulting in 15 dimensions of feature vectors, as
shown in Figure 4e. Thus, for the full 32 EEG channels used, the dimension of feature vectors will be

Algorithms 2017, 10, 63

10 of 20

As the bispectrum matrix shown in Figure 4b is asymmetrical, then the feature-extracted matrix
(such as in Figure 4d) is also asymmetrical; therefore, it is sufficient to take just half of the whole matrix.
For example, in the 5 × 5 filters, the number of non-redundant elements of the matrix equal to the sum
of arithmetic sequence ∑5n=1 n = 15, resulting in 15 dimensions of feature vectors, as shown in Figure 4e.
Thus, for the full 32 EEG channels used, the dimension of feature vectors will be 15 × 32 = 480,
as shown in Figure 4f. Obviously, for the 7 × 7 filters, there will be 28 dimensions of feature vector per
channel, resulting in 28 × 32 = 896 dimensions of feature vectors for the whole channel.
3.4.3. Feature Types Based on the Bispectrum
Bispectrum analysis has also been used for an EEG-based emotion recognition system by
Kumar [20], with several entropies, values and moments taken as the features, which we adapted as
one of the feature extraction modes in our experiments (Mode #1 feature extraction); however, this
feature produces a high dimension of feature vectors and thus requires a high computation cost. In this
work, we propose as simpler feature, which is the mean of the bispectrum value, as Mode #3 feature
extraction, producing only one feature value for each area of the filter, thereby reducing the dimension
of the feature vectors.
In previous work, we have found that the energy percentage performed well as the feature of
an EEG signal [49]; therefore, here, we considered the percentage value as the feature. The percentage
value of entropies and moments of Mode #1 becomes the Mode #2 feature extraction, and the percentage
of the mean becomes Mode #4 feature extraction. Suppose Fm is a feature and M is the number of
features per channel; then, the percentage value of the feature FPm is defined as:
FPm =

Fm
M
∑m
=1 Fm

× 100.

(5)

3.5. Back-Propagation Neural Networks
BPNN is one of the ANN classifiers constructed by the multi-layer perceptron (MLP) architecture.
BPNN provides a mechanism to update the weights and biases of each neuron’s connection by
propagating and minimizing the error in each iteration (epoch). The BPNN used in this work has
three layers of architecture: one input layer, one hidden layer and one output layer. The number of
neurons in the input layer was equal to the dimension of the feature vector, while the number of
hidden neurons amounted to half of the input neurons. Because the classification for arousal and
valence were conducted separately, each emotion was divided into high and low class, resulting in
only one neuron in the output layer. The Nguyen–Widrow method [22] was used to initialize the
weight and bias of each neuron.
At the feed-forward part of the training phase, for each epoch, the input vectors were presented
one at a time to the input layer, and their values were passed to the hidden layer. In the hidden layer,
the value received by each neuron was multiplied by its weight and added with the bias. The activation
function used was a sigmoid function, f (x) = 1/(1–ex ). On the output layer, the value received by the
neuron was multiplied by its weight and added with the bias. The activation function at the output
layer was also a sigmoid function, resulting in the output value of the ANN.
At the back-propagation part of the algorithm, the calculated error between the values in the
output layer (the predicted emotion) and the target (the actual emotion) was propagated back to adjust
the weights and the biases for the neurons in the output and the hidden layer [23]. The learning
process was carried on until the preferred minimum error was achieved; in this case, we used root
mean sum squared error (RMSSE), or until it reached the maximum number of epochs.
At the testing stage, the testing data part was fed-forward to the input layer and sent to the
hidden layer and up to the output layer using the weights and the biases calculated and obtained
from the learning stage. The testing stage was similar to the feed-forward part of the learning stage,

Algorithms 2017, 10, 63

11 of 20

but the error between the predicted emotion and the stimulated emotion was calculated to produce
the recognition rate of the automatic emotion recognition system.
3.6. Probabilistic Neural Networks
PNN is an ANN classifier that is based on Bayes theorem. With a Bayes classifier, the datum X
belongs to the class Cj when P(Cj |X) has the biggest probability value.
P(Cj | X ) =

P( X Cj ) P(Cj )
P( X )

(6)

To calculate the probability P(Cj |X), it is necessary to estimate the conditional probability P(X|Cj )
and the a priori probability of P(Cj ) of each class Cj. The calculation of P(X) is not required because P(X)
exists in every class. The a priori probability P(Cj ) could be calculated by including the entire training
data. The conditional probability P(X|Cj ) is estimated using the Parzen window p.d.f. estimation.
By assuming Gaussian distribution, the Parzen window p.d.f. estimates pj (x) as:
p j (x) =

1 N
1
e(−
 √
d
N k∑
=1 σ 2π )

1
2(

x − x jk 2
σ ) )

(7)

Here, x is the sample data with the probability being estimated, j is the class number, N is the number
of training data in class Cj and d is the dimension of the feature vector. The value of σ (sigma) in the
Gaussian distribution is the standard deviation or the smoothing parameter of the Gaussian curve;
however, the actual standard distribution was unknown and should be determined.
The PNN consists of four layers: an input layer, a pattern layer, a summation layer and a decision
layer. In the input layer, which consists of one neuron, the input vector was received and then
forwarded to the pattern layer. Each neuron at the pattern layer represents the training data that
belong to each class. We used 50% of the data samples as the training data. Thus, the number of
neurons in the pattern layer was equal to 50% of the number of data samples. In the pattern layer,
the vector input was compared to the training data in each class. In the summation layer, the Parzen
window p.d.f. formula was used to determine the class of the data. The biggest probability value
determined where the datum should belong.
4. Experiment and Results
The development of an automatic emotion recognition system using a new methodology for 3D
filtered bispectrum feature extraction and the ANN classifier has been presented in Section 3; to verify
the performance of the proposed methodology, five experiments were carried out (Exp. #1–Exp. #5).
In Exp. #1, to find the best feature extraction type based on bispectrum analysis, four feature extraction
types were compared. In Exp. #2, two types of 3D pyramid filters were compared, namely the overlap
and the non-overlapping filters. To reduce the computation cost, in Exp. #3, the number of EEG
channels used in the system was reduced. In Exp. #4, to increase the recognition rate, the number of
samples in the training data was increased to 90%. In the final experiment, Exp. #5, two types of ANN
classifiers were compared, the BPNN and the PNN, in order to find the best classifier.
The DEAP database [4] provides EEG signals from 32 people and 40 video stimuli, forming
1280 person-video samples. Originally, the DEAP database used four emotion classes, which are the
HAHV (Class 1), the LAHV (Class 2), the LALV (Class 3) and the HALV (Class 4) emotions, by way of
H = high, L = low, A = arousal and V = valence, with a balanced sample size (320 samples) resulting
from 32 people and 10 video stimuli per class. As indicated in the data acquisition section, this research
principally takes two binary classes of emotion in 2D arousal and valence planes: high arousal (Class 1
+ Class 4)/low arousal (Class 2 + Class 3) and high valence (Class 1 + Class 2)/low valence (Class 3 +
Class 4), with 640 samples for each class, accordingly.

Algorithms 2017, 10, 63

12 of 20

In the following experiments (apart from Exp. #4), 50% of the samples were used as the training
data, while the remaining data were used as the testing data, producing 320 samples per class for each
of the training and testing parts. All thirty-two people were represented in the training data, but the
video stimuli were presented to only a half. For instance, Videos 1–5 (from Class 1) and Videos 31–35
(from Class 4) were chosen as high arousal training data, while Videos 6–10 (from Class 1) and Videos
36–40 (from Class 4) were chosen as testing data. Similarly, Videos 11–15 (from Class 2) and Videos
21–25 (from Class 3) were chosen as low arousal training data, and Videos 16–20 (from Class 2) and
Videos 26–30 (from Class 3) were chosen as testing data.
For Exp. #1–Exp. #4, BPNNs were used as the benchmark classifier, with the setting parameters
of α (learning rate) = 0.2 and β (momentum) = 0.3. The number of input neurons was equal to the
dimension of the feature vectors, the number of hidden neurons is equal to half of the input neurons,
and the output of the neurons equals one. In Exp. #5, the PNN was compared with the results of the
benchmark classifier.
4.1. Exp. #1: Comparison of Bispectrum-Based Feature Extraction Modes
The following experiments in Exp. #1 were carried out to compare and select the best feature
extraction mode from the bispectrum value. There were four types of features to consider: the entropy
and moment (NBE, NBSE, MMOB, FOSM and SOSM as in [20]) with additional standard deviation
(STD) value as Mode #1, the percentage value of Mode #1 as Mode #2, the mean as Mode #3 and
the percentage of the mean as Mode #4. A 5 × 5 3D pyramid filter was used in this experiment,
producing 480 dimensions of feature vectors for Mode #3 and Mode #4. The dimensions of feature
vectors for Mode #1 and Mode #2 are 2880 because there are six features from each area of the filters
(480 × 6 = 2880). To reduce the large dimension, each feature vector was dimensionally reduced using
PCA with 99% Eigen values. The comparison result of the feature extraction type Mode #1–Mode #4 is
presented in Table 1.
Table 1. Recognition rate comparison of various feature type. MMOB, mean-magnitude of bispectrum;
FOSM, first order spectral moment; SOSM, second order spectral moment; STD, standard deviation.
Feature Type

Mode #1
Mode #2
Mode #3
Mode #4

Features
NBE, NBSE, MMOB, FOSM,
SOSM [20], STD
NBE%, NBSE%, MMOB%,
FOSM%, SOSM%, STD%
Mean
Mean%
1

Maximum Recognition Rate 1

Mean ± SD Recognition Rate

Arousal (%)

Valence (%)

Arousal (%)

Valence (%)

73.36

74.77

64.88 ± 5.32

69.36 ± 5.98

55.24

56.72

54.70 ± 0.41

56.23 ± 0.79

67.27
74.22

71.80
77.58

56.11 ± 1.97
72.83 ± 4.04

54.91 ± 0.96
70.95 ± 9.86

Shown is the maximum recognition rate from 5 repeated experiments.

Table 1 shows that the mean percentage feature (Mode #4) provides the highest recognition rate
(74.22%) for the arousal and 77.58% for the valence. From the recognition rate in Table 1, it can be
seen that for both arousal and valence emotion, Mode #1 and Mode #4 gave the best recognition rate
compared to Mode #2 and Mode #3, with Mode #4 providing a slightly better recognition rate than
Mode #1. To investigate whether any of the feature types is significantly different, one-way ANOVA
was used to compare the recognition rate of four feature type (Mode #1–Mode #4), and the resultant
p-values of both separate and combined arousal and valence were all <10−4 . This result suggests
Tukey’s HSD test to see which of the feature types is significantly different.
Table 2 presents the Tukey HSD test result for pair-wise comparisons. For arousal emotion,
Mode #1 is statistically different from Mode #4 with the p-value = 0.0114 (p < 0.05); while for valence
emotion, Mode #1 did not have significant difference with Mode #4 with the p-value = 0.9716 (p > 0.05).
Moreover, Mode #4 also has shortcomings due to the more complicated feature calculation compared
to Mode #1. From this result, we can conclude that for the arousal emotion, the mean percentage of

Algorithms 2017, 10, 63

13 of 20

the bispectrum (Mode #4) is a better feature type than entropy and moment (Mode #4), while for the
valence emotion, both Mode #1 and Mode #4 are similar. However, it can be noted that both Mode
#1 and Mode #4 are significantly different with Mode #2 and Mode #3 (p < 0.05). This experiment
also confirmed our hypothesis that the mean percentage of the bispectrum (Mode #4) provides more
accurate information than the absolute mean value (Mode #3). This finding implies that for the same
emotion, the relative/percentage bispectrum values from the EEG signals may have similarities among
people, although different people may have different amplitude in the bispectrum values.
Table 2. Result of statistical significance of Tukey–Kramer HSD test in the arousal and valence
recognition rate.
Arousal

Pairwise
Comparison
Mode #1 vs.
Mode #1 vs.
Mode #1 vs.
Mode #2 vs.
Mode #2 vs.
Mode #3 vs.

Mode #2
Mode #3
Mode #4
Mode #3
Mode #4
Mode #4

Valence

Q-Statistic

p-Value

Inference

Q-Statistic

p-Value

Inference

4.611
3.973
3.605
0.6375
8.2161
7.579

0.0015
0.0054
0.0114
0.9183
0.0000
0.0000

p < 0.05
p < 0.05
p < 0.05
p > 0.05
p < 0.05
p < 0.05

3.578
3.940
0.434
0.3620
4.0122
4.374

0.0121
0.0058
0.9716
0.9832
0.0050
0.0024

p < 0.05
p < 0.05
p > 0.05
p > 0.05
p < 0.05
p < 0.05

4.2. Exp. #2: Comparison of Overlapped and Non-Overlapped Filters
The newly-developed methodology proposed in this paper is the use of bispectrum analysis and
a 3D pyramid filtering for the feature extraction subsystem. The idea of using a 3D pyramid filter as
the filtering method is to emphasize the values in the center of the filter and devote less attention to
the values at the edges. As Table 1 shows, the mean percentage (Mode #4) with 5 × 5 non-overlapping
filters provides a good recognition rate. To improve the recognition rate, the number of filters used
for feature extraction was increased to 7 × 7 filters with overlapping filters at the base of the pyramid
model. The hypothesis is that by using overlapping filters, the values at the edge of the filters, which
have less importance in one filter, will be given higher importance in the adjacent filters; therefore,
it is expected that every element of the bispectrum matrix has equal importance. In this experiment
we assessed whether the increasing number of the filter will increase the recognition rate. However,
we only compare the 5 × 5 filters with the 7 × 7 filters as the first step, and if the higher number of
filters results in a higher recognition rate, then the number of filters could be increased more.
In this experiment, we only used the mean percentage feature (Mode #4), following our findings
in the previous experiment. Each feature type is dimensionally reduced using PCA with 99% Eigen
values. The BPNN was used as the benchmark classifier, and the recognition rate result is presented
in Table 3.
Table 3. Recognition rate results from different filter types.
Filter Type
5 × 5 non-overlapping filters
7 × 7 overlapping filters
1

Maximum Recognition Rate 1

Mean ± SD Recognition Rate

Arousal (%)

Valence (%)

Arousal (%)

Valence (%)

74.22
64.22

77.58
73.13

72.83 ± 4.04
61.14 ± 3.18

70.95 ± 9.86
72.81 ± 0.62

Shown is the maximum recognition rate from 5 repeated experiments.

Table 3 shows that the 5 × 5 non-overlapping filters gave a higher recognition rate than that
of the 7 × 7 overlapping filters, which deviates from our hypothesis. The superiority of the 5 × 5
non-overlapping filters is obvious for the arousal emotion, with a difference up to 10%. One-way
ANOVA was used to compare the recognition rate of using the different filter types in the feature
extraction process (5 × 5 non-overlapping filters and 7 × 7 overlapping filters). The resultant p-value
of the combined arousal and valence emotion recognition rate was p = 0.126 (p > 0.05), which implies

Algorithms 2017, 10, 63

14 of 20

that neither of the filter types were significantly different. However, the p-value for arousal emotion
recognition rate was p = 0.001 (p < 0.05), and the valence emotion recognition rate was p = 0.685
(p > 0.05). The p-value implies that for the arousal emotion, the use of the 5 × 5 non-overlapping filter
was better than the 7 × 7 overlapping filters; however, for the valence emotion, these filters did not
differ significantly.
The finding of this Exp. #2 deviated from our hypothesis that increasing the number of filters and
by using overlapping strategies in the adjacent filters would increase the recognition rate. The analysis
is as follows. The more filters used in the feature extraction process, the higher the dimension of feature
vectors produced. However, a higher dimension of feature vectors produces higher cumulative errors,
resulting in lower recognition rates. Another reason might be that in the 5 × 5 non-overlapping filters,
more attention (by using more numbers and smaller filters) is given to the area where the bispectrum
matrix elements show high amplitudes, whereas in the 7 × 7 overlapping filters, all elements of the
bispectrum matrix have the same significance. This finding implies that the higher amplitude areas of
the bispectrum matrix are more important than the lower ones. Further optimization of the filter type
should be investigated as future work, such as by using more numbers of filters, but the size of the
filters should be different, which is smaller at the area where the bispectrum amplitudes are high.
4.3. Exp. #3: Channel Selection for Emotion Classification
Research has shown that emotions, and their related brain signals, are associated with the frontal
lobe of the cerebral cortex; therefore, the frontal EEG electrodes were often used in the emotion
classification research. In this experiment, the effect of channel selection on the recognition rate of the
emotion classification system is assessed. The selection of eight and 14 channels is in accordance with
our previous work [3]. The reduced eight channels were selected because they are positioned on the
frontal and the near frontal area of the cerebral cortex (frontal parietal and anterior-frontal), while the
reduced 14 channels were chosen in accordance with the channels available in the BCI Emotiv device.
The channel descriptions and the results of this experiment are depicted in Table 4. BPNN was used
for the classifier, and the experiment was repeated five times.
Table 4. Channel selection and description.
∑
Channel

Channel Name

8

F3, F4, F7, F8, Fp1, Fp2,
AF3, AF4

14
32

Description

Maximum Recognition Rate 1

Mean ± SD Recognition Rate

Arousal (%)

Valence (%)

Arousal (%)

Valence (%)

Frontal + Frontal Parietal
+ Anterior-Frontal

74.45

74.06

73.38 ± 1.93

73.52 ± 1.13

AF3, F3, F7, FC5, T7, P7, O1,
AF4, F4, F8, FC6, T8, P8, O2

BCI channel

75.94

76.02

74.81 ± 0.71

75.00 ± 0.82

all channels

DEAP EEG channel

74.22

77.58

72.83 ± 4.04

70.95 ± 9.86

1

Shown is the maximum recognition rate from 5 repeated experiments.

Table 4 shows the recognition rate of the reduced eight and 14 channels compared to the
recognition rate of the full 32 channels. One-way ANOVA was used to compare the recognition
rate of using the complete and the reduced number of EEG channels. The p-value of the combined
arousal and valence recognition rate was p = 0.3 (p > 0.05), while for the arousal emotion recognition
rate, p = 0.485 (p > 0.05), and for the valence emotion recognition rate, p = 0.548 (p > 0.05); this finding
shows that the reduced eight and 14 channels did not significantly differ from that of the complete
32 channels. This experiment showed that the 14 channels in the BCI equipment are sufficient to
conduct emotion classification, with only slight differences compared to that of the 32 EEG channels.
However, this experiment ignored the possibility that the signal quality of the BCI equipment might
be lower than that of the medical-grade EEG. This experiment also shows that when the research calls
for fewer electrodes to reduce the complexity and computation cost, the use of eight electrodes from
the frontal, frontal parietal and anterior-frontal regions would be sufficient.

Algorithms 2017, 10, 63

15 of 20

4.4. Exp. #4: Comparison of the Number of Training Samples
To increase the recognition rate, the number of samples in the training data in this experiment was
increased to 90%. In the previous experiments (Exp. #1–Exp. #4), the training data composed 50% of
the whole dataset, and all samples in the dataset were used for testing. In this experiment, the amount
of training data samples was increased to 90% of all data samples, and classification was conducted
using a 10-fold cross-validation method. There were 1152 samples used for the training data out of the
1280
samples
available
in the dataset. The division of the 10-fold cross-validation was designed
Algorithms
2017, 10,
63
15 ofso
20
that if any of the video stimuli produced a different emotion from the emotion class desired, then it
data from
the people
with
Videos 1,result.
11, 21 Therefore,
and 31 were
testing
while
restthe
was
used
would
be depicted
in the
recognition
forused
Fold as
1 (F1),
thedata,
sample
datathe
from
people
for
training.
Similarly,
for
Fold
2
(F2),
sample
data
from
the
people
with
Videos
2,
12,
22
and
32
were
with Videos 1, 11, 21 and 31 were used as testing data, while the rest was used for training. Similarly,
used
for2testing,
and the
restfrom
were
used
for training,
and 2,
so12,
on,22until
10 (F10).
experiment
for
Fold
(F2), sample
data
the
people
with Videos
andFold
32 were
used This
for testing,
and
was
conducted
using
a
BPNN
classifier,
and
the
reduced
eight
channels
were
used
as
the
input.
the rest were used for training, and so on, until Fold 10 (F10). This experiment was conducted using
From
the data
shown
in Tableeight
5 it can
be seen
thatused
the recognition
a BPNN
classifier,
and
the reduced
channels
were
as the input.rate results from the 10-fold
cross-validation
areshown
92.92%infor
arousal
andbe
93.51%
for valence.
One-way
ANOVA
resulted
the pFrom the data
Table
5 it can
seen that
the recognition
rate
results from
the in
10-fold
value
=
0.669
(p
>
0.05);
thus,
there
is
no
significant
difference
between
the
fold.
This
experiment
cross-validation are 92.92% for arousal and 93.51% for valence. One-way ANOVA resulted in the
implies =that
by (p
increasing
the training
up to 90%,difference
the recognition
rate
byexperiment
an average
p-value
0.669
> 0.05); thus,
there is data
no significant
between
theincreased
fold. This
of 18.96%
and 19.42%
for arousal
valence),
and the comparison
of recognition
rates
implies
that(18.47%
by increasing
the training
data upand
to 90%,
the recognition
rate increased
by an average
of
between
using
50%
training
data
and
90%
training
data
is
depicted
in
Figure
5.
From
this
result,
18.96% (18.47% and 19.42% for arousal and valence), and the comparison of recognition rates betweenit
can be50%
concluded
amount
of training
greatly influences
results
using
trainingthat
datathe
and
90% training
datadata
is depicted
in Figure 5.theFrom
thisobtained,
result, it and
can the
be
higher
the
ratio
of
the
training
data,
the
higher
the
recognition
rate.
concluded that the amount of training data greatly influences the results obtained, and the higher the
ratio of the training data, the higher the recognition rate.
Table 5. Result of the 10-fold cross-validation recognition rate for mean percentage feature type from
8 EEG5.channels
5 × 5 non-overlapped
filters.
Table
Result ofwith
the 10-fold
cross-validation
recognition rate for mean percentage feature type from
8 EEG channels with 5 × 5 non-overlapped filters.
Emotion Type
Emotion Type

Arousal

Arousal
Valence
Valence
Arousal
(LSO)
Arousal
(LSO)
Valence
(LSO)
Valence
(LSO)

F1 (%)
F1 (%)

92.66

92.66
93.44
93.44
54.69
54.69
55.47
55.47

F3
(%)
F2 (%)
F3 (%)
92.50
92.42
92.50
92.42
93.13
93.59
93.13
93.59
51.56 50.00
50.00
51.56
59.38
59.38 55.47
55.47
F2 (%)

F4
(%)
F4 (%)
93.13
93.13
93.52
93.52
57.03
57.03
57.03
57.03

F5
(%)
F5 (%)
93.98
93.98
93.83
93.83
57.03
57.03
51.56
51.56

F6
F7
(%)
(%)
F6 (%)
F7 (%)
93.44
92.50
93.44
92.50
93.20
93.59
93.20
93.59
53.13 55.47
55.47
53.13
60.16
60.16 60.16
60.16

F8
(%)
F8 (%)
93.44
93.44
93.98
93.98
51.56
51.56
58.59
58.59

F9
(%)
F9 (%)
92.66
92.66
93.44
93.44
54.69
54.69
55.47
55.47

F10
(%)
F10 (%)
92.11
92.11
93.28
93.28
47.66
47.66
58.59
58.59

Mean
(%)
Mean (%)
92.92
92.92
93.51
93.51
53.28
53.28
57.19
57.19

Figure5.5.Recognition
Recognitionrate
ratefor
fordifferent
differentamounts
amountsof
ofdata
datatraining
trainingsamples.
samples.
Figure

In a leave subject out (LSO) training-testing method, each fold in this experiment represented
In a leave subject out (LSO) training-testing method, each fold in this experiment represented the
the video used as the stimulus as the subject, instead of the person. The recognition rate for the
video used as the stimulus as the subject, instead of the person. The recognition rate for the arousal
arousal emotion was on average 53.28% and for the valence emotion was 57.19%. This finding was
emotion was on average 53.28% and for the valence emotion was 57.19%. This finding was comparable
comparable to recent other research results using the same database, such as the leave-subject-out
to recent other research results using the same database, such as the leave-subject-out scheme of the
scheme of the person (53.42% for arousal and 52.05% for valence) [37], using a random subset 10-fold
person (53.42% for arousal and 52.05% for valence) [37], using a random subset 10-fold leave-p-out
leave-p-out cross-validation (all between 40% and 50% accuracy) [50].
cross-validation (all between 40% and 50% accuracy) [50].
4.5.Exp.
Exp. #5:
#5: Comparison
Comparison of
of BPNN
BPNN and
and PNN
PNN Classifiers
Classifiers
4.5.
Tofind
findthe
thebest
bestclassifier
classifierfor
forthe
theautomatic
automaticemotion
emotionrecognition
recognition system,
system, two
two types
types of
of ANN
ANN
To
classifiers
were
compared,
the
BPNN
and
the
PNN.
The
drawback
of
the
BPNN
was
the
computing
classifiers were compared, the BPNN and the PNN. The drawback of the BPNN was the computing
time to conduct the training phase for the classifier; thus, the PNN, which is generally faster than the
BPNN, was used in this experiment.
The BPNN setting parameter was similar to that of the previous experiments (Exp. #1–Exp. #4),
whereas for the PNN, it is difficult to find a good choice of smoothing parameter (σ), because in the
original PNN classifier, there is no mathematical calculation to determine the best smoothing

Algorithms 2017, 10, 63

16 of 20

time to conduct the training phase for the classifier; thus, the PNN, which is generally faster than the
BPNN, was used in this experiment.
The BPNN setting parameter was similar to that of the previous experiments (Exp. #1–Exp. #4),
whereas for the PNN, it is difficult to find a good choice of smoothing parameter (σ), because in
the original PNN classifier, there is no mathematical calculation to determine the best smoothing
parameter. In this experiment, the PNN smoothing parameter (σ) was chosen randomly, and the trial
was repeated 60 times to find the highest recognition rate. In this experiment, we used only eight
channels from the EEG signals, as the previous experiment showed that using eight channels would
Algorithms 2017,
10,PNN
63
16 of 20(σ)
be sufficient.
The
recognition result for various randomly-generated smoothing parameters
can be seen in Figure 6.

(a)

(b)

Figure 6. PNN results for various smoothing parameter values for (a) arousal and (b) valence emotion.
Figure 6. PNN results for various smoothing parameter values for (a) arousal and (b) valence emotion.

From the randomly-generated smoothing parameter (σ), we could find the optimum PNN
From the rate,
randomly-generated
smoothing
could
find
optimum
PNN
recognition
which is 76.09% when
σ = 0.4505parameter
for arousal(σ),
and we
75.31%
when
σ =the
0.3517
for valence,
recognition
rate,
which6.isThe
76.09%
when of
σ the
= 0.4505
for arousal
when
= 0.3517thefor
as depicted
in Figure
comparison
recognition
rate andand
the 75.31%
computing
timeσbetween
valence,
as depicted
inisFigure
6. The
comparison
of the recognition rate and the computing time
PNN and
the BPNN
presented
in Table
6.
between the PNN and the BPNN is presented in Table 6.
Table 6. Comparison of results from BPNN and PNN.
Table 6. Comparison of results from BPNN and PNN.

ANN Type Time (s) Recognition Rate (%)
BPNN
448.35
74.45
Arousal
ANN Type
(s)
Recognition
PNN Time2.09
76.09Rate (%)
448.21
74.06
BPNNBPNN 448.35
74.45
Arousal Valence
75.31
PNN PNN
2.092.09
76.09

Valence

BPNN

448.21

74.06

Sensitivity Specificity PPV
0.74
0.75
0.75
Sensitivity
Specificity
0.89
0.63
0.71 PPV
0.75
0.74 0.75 0.74 0.75
0.74
0.93
0.58 0.63 0.69 0.71
0.89

0.75

0.74

0.74

PNN
2.09 the recognition
75.31
0.93
0.58PNN for0.69
From Table 6,
we can see that
rate from both the
BPNN and the
arousal
and valence emotion was comparable, with both values hovering around 75%, with the p-value of
0.761
(p Table
> 0.05).6,However,
thethat
specificity
of PNN is
lower
than
BPNN,
and the
sensitivity
From
we can see
the recognition
rate
from
both
the BPNN
and
the PNNwas
forhigher
arousal
BPNN,
meaning
that
PNN detected
more
positive
(the around
higher emotion
level)
andthan
valence
emotion
was
comparable,
with
both
valuesvalues
hovering
75%, with
thecompared
p-value of
to (p
BPNN,
butHowever,
PNN hasthe
a lower
ability
to detect
the negative
values
emotion
0.761
> 0.05).
specificity
of PNN
is lower
than BPNN,
and(the
the lower
sensitivity
waslevel)
higher
compared
to
BPNN.
As
for
the
positive
predictive
value
(PPV)
value,
it
can
be
seen
that bothto
than BPNN, meaning that PNN detected more positive values (the higher emotion level) compared
classifiers are comparable with a slightly higher value of PPV from BPNN. If PPV is used as the “gold
BPNN, but PNN has a lower ability to detect the negative values (the lower emotion level) compared
standard” as mentioned by Parikh [51], then BPNN is superior to PNN in classifying emotion from
to BPNN. As for the positive predictive value (PPV) value, it can be seen that both classifiers are
EEG signals.
comparable with a slightly higher value of PPV from BPNN. If PPV is used as the “gold standard” as
As depicted in the receiver operating characteristics (ROC) graph in Figure 7, the BPNN is more
mentioned by Parikh [51], then BPNN is superior to PNN in classifying emotion from EEG signals.
“conservative” than the PNN because for both emotion, i.e., the arousal and the valence, the BPNN
As depicted
in the
operating
characteristics
(ROC) graph
the BPNN
BPNNmakes
is more
position
is on the
leftreceiver
side of the
PNN. From
this ROC analysis,
it caninbeFigure
stated7,that
“conservative”
than
the
PNN
because
for
both
emotion,
i.e.,
the
arousal
and
the
valence,
the
BPNN
positive classifications with strong evidence, so it makes few false positive errors. According
to
position
is
on
the
left
side
of
the
PNN.
From
this
ROC
analysis,
it
can
be
stated
that
BPNN
makes
Fawcett [52], in a real-word classification problem, there are more negative instance than the positive
positive
classifications
strong
evidence,
it makes
fewmore
false
positive errors.
According
ones; thus,
the far-leftwith
handed
side of
the ROCso
graph
becomes
interesting;
thus, from
this pointto
Fawcett
[52],
inBPNN
a real-word
classification
problem,
there are more negative instance than the positive
of view,
the
performed
better than
the PNN.

Although the maximum value achieved was not significantly different, the computing time of
the PNN for a single run was about 200-times faster than that of the BPNN. However, PNN requires
an optimization method for finding the best sigma value, whereas the BPNN does not require prior
optimization; thus, the iterative training for BPNN can be conducted only once. Nevertheless, for this
experiment, the total time for running 60 time trials of PNN is still faster (60 × 2.09 = 125.4 s) compared

Algorithms 2017, 10, 63

17 of 20

ones; thus, the far-left handed side of the ROC graph becomes more interesting; thus, from this point
of view, the BPNN performed better than the PNN.
Although the maximum value achieved was not significantly different, the computing time of
the PNN for a single run was about 200-times faster than that of the BPNN. However, PNN requires
an optimization method for finding the best sigma value, whereas the BPNN does not require prior
optimization; thus, the iterative training for BPNN can be conducted only once. Nevertheless, for tthis
experiment, the total time for running 60 time trials of PNN is still faster (60 × 2.09 = 125.4 s) compared
to a single run of BPNN (448 s), which is about 3.5-times faster. Using one-way ANOVA, the time
Algorithms 2017, 10, 63
17 of 20
difference between BPNN and PNN shows a significant difference with p < 10−4 .

Figure 7.
asas
thethe
classifier.
Figure
7. ROC
ROCplot
plotofofBPNN
BPNNand
andPNN
PNN
classifier.

5. Discussion and Conclusions
5. Discussion and Conclusions
In our developed automatic emotion recognition system, we propose a new methodology for
In our developed automatic emotion recognition system, we propose a new methodology for EEG
EEG signal feature extraction using bispectrum analysis, a 3D pyramid filtering method and an ANNsignal feature extraction using bispectrum analysis, a 3D pyramid filtering method and an ANN-based
based classifier. The proposed method could recognize two binary types of emotion—high/low
classifier. The proposed method could recognize two binary types of emotion—high/low arousal and
arousal and high/low valence—from the EEG input signals provided by the DEAP benchmark
high/low
the EEG
input signals
provided by(Exp.
the DEAP
benchmark
database. of
From
database.valence—from
From the feature
extraction
mode experiment
#1), the
mean percentage
the the
feature
extraction
modethe
experiment
(Exp. #1),
thewith
mean
percentage
of the
bispectrum
provided
bispectrum
provided
best recognition
rate
lower
complexity
(74.22%
for the
arousal the
andbest
recognition
rate
with
lower
complexity
(74.22%
for
the
arousal
and
77.58%
for
the
valence).
To
reduce
77.58% for the valence). To reduce and extract the features from the bispectrum values, we propose
and
extract
the
features
from
the
bispectrum
values,
we
propose
a
new
method
of
filtering
by
means
a new method of filtering by means of 3D pyramid filters and by comparing the 5 × 5 non-overlapping
ofand
3D the
pyramid
filters and filters
by comparing
5 × 5 non-overlapping
and the 7 ×filters
7 overlapping
filters
7 × 7 overlapping
(Exp. #2); the
we found
that the 5 × 5 non-overlapping
with various
(Exp.
#2);
we
found
that
the
5
×
5
non-overlapping
filters
with
various
pyramid
base
sizes
provide
the
pyramid base sizes provide the highest recognition rate.
highest
recognition
The
reduction rate.
of channels (Exp. #3) did not significantly affect the recognition rate. Therefore,
we The
conclude
that the
channels (Exp.
provided
in the
equipmentaffect
are sufficient
to conduct
reduction
of channels
#3) did
notBCI
significantly
the recognition
rate.emotion
Therefore,
classification,
andthe
when
it is more
suitable
to use
electrodes
to reducetocomplexity
and
we
conclude that
channels
provided
in the
BCI fewer
equipment
are sufficient
conduct emotion
computation
cost,
the
choice
of
eight
electrodes
in
the
frontal,
frontal
parietal
and
anterior-frontal
classification, and when it is more suitable to use fewer electrodes to reduce complexity and
regions is also
sufficient.
By increasing
the number
of training
samples
to 90%
of the entire
computation
cost,
the choice
of eight electrodes
in the
frontal, data
frontal
parietal
and anterior-frontal
dataset
(Exp.
#4),
the
recognition
rate
was
increased
by
18.96%;
from
this
result,
it
can
be
regions is also sufficient. By increasing the number of training data samples to 90% of the noted
entire that
dataset
the
amount
of
training
data
used
greatly
influences
the
results
obtained.
We
also
compared
the
(Exp. #4), the recognition rate was increased by 18.96%; from this result, it can be noted that the amount
benchmark
BPNN
classifier
with
a
PNN
classifier
for
the
eight
EEG
channels’
input
(Exp.
#5),
and
we
of training data used greatly influences the results obtained. We also compared the benchmark BPNN
achieved a slightly better result: 76.09% for arousal and 75.31% for valence. Although the result is not
classifier with a PNN classifier for the eight EEG channels’ input (Exp. #5), and we achieved a slightly
significantly higher, the computing time of the PNN to achieve the maximum recognition rate result
better result: 76.09% for arousal and 75.31% for valence. Although the result is not significantly higher,
is about 3.5-times faster than the BPNN.
the computing time of the PNN to achieve the maximum recognition rate result is about 3.5-times
The proposed bispectrum-based feature extraction gave a comparable result with some recent
faster than the BPNN.
research using different feature extraction methods, such as Discrete Wavelet Transform - Relative
The proposed bispectrum-based feature extraction gave a comparable result with some recent
Wavelet Energy (DWT-RWE) [3], Short Time Fourier Transform (STFT) [50], Power Spectral Density
research
using
extraction methods,
Discrete
Wavelet
Transform
- Relative
(PSD) [37]
anddifferent
Empiricalfeature
Mode Decomposition
(EMD)such
withas
Sample
Entropy
(SampEn)
[53]. Future
studies in the EEG-based emotion recognition system should focus on improving the feature
calculation from the bispectrum values. This research utilized two non-linear classifiers, namely the
BPNN and the PNN. The result of using these classifier was also comparable to other recent research
using other linear and non-linear classifier, such as SVM [50,53] and DLN [37]. However, newer
classifiers, such as group sparse canonical correlation analysis (GCCA) [54] and sparse deep belief
networks (SDBN) [55], have never been used, giving room to further future works.

Algorithms 2017, 10, 63

18 of 20

Wavelet Energy (DWT-RWE) [3], Short Time Fourier Transform (STFT) [50], Power Spectral Density
(PSD) [37] and Empirical Mode Decomposition (EMD) with Sample Entropy (SampEn) [53]. Future
studies in the EEG-based emotion recognition system should focus on improving the feature calculation
from the bispectrum values. This research utilized two non-linear classifiers, namely the BPNN and
the PNN. The result of using these classifier was also comparable to other recent research using other
linear and non-linear classifier, such as SVM [50,53] and DLN [37]. However, newer classifiers, such as
group sparse canonical correlation analysis (GCCA) [54] and sparse deep belief networks (SDBN) [55],
have never been used, giving room to further future works.
Acknowledgments: This research was partly supported by the Universitas Indonesia under the grant PITTA UI
and through a scholarship program BPPDN from Ministry of Research and Higher Education of Indonesia.
Author Contributions: B.K. conceived and designed the experiments; P.D.P. performed the derivation of the
algorithms and its experiments; B.K., A.A.P.R. and P.D.P. analyzed the data and wrote the paper.
Conflicts of Interest: The authors declare no conflict of interest.

References
1.
2.
3.

4.
5.
6.
7.

8.
9.
10.

11.
12.
13.

14.
15.

Mumford, E.; Schlesinger, H.J.; Glass, G.V. The effects of psychological intervention on recovery from surgery
and heart attacks: An analysis of the literature. Am. J. Public Health 1982, 72, 141–151. [CrossRef] [PubMed]
Kim, M.; Kim, M.; Oh, E.; Kim, S. A Review on the Computational Methods for Emotional State Estimation
from the Human EEG. Comput. Math. Methods Med. 2013. [CrossRef] [PubMed]
Purnamasari, P.D.; Ratna, A.A.P.; Kusumoputro, B. Artificial Neural Networks Based Emotion Classification
System through Relative Wavelet Energy of EEG Signal. In Proceedings of the Fifth International
Conference on Network, Communication and Computing (ICNCC 2016), Kyoto, Japan, 17–21 December
2016; pp. 135–139.
Koelstra, S.; Lee, J.; Yazdani, A.; Ebrahimi, T.; Pun, T.; Nijholt, A.; Patras, I. DEAP: A Database for Emotion
Analysis Using Physiological Signals. IEEE Trans. Affect. Comput. 2012, 3, 1–15. [CrossRef]
Jenke, R.; Peer, A.; Buss, M. Feature Extraction and Selection for Emotion Recognition from EEG. IEEE Trans.
Affect. Comput. 2014, 5, 327–339. [CrossRef]
Solomon, B.; DeCicco, J.M.; Dennis, T.A. Emotional picture processing in children: An ERP study.
Dev. Cogn. Neurosci. 2012, 2, 110–119. [CrossRef] [PubMed]
Goyal, M.; Singh, M.; Singh, M. Classification of emotions based on ERP feature extraction. In Proceedings
of the 2015 1st International Conference on Next Generation Computing Technologies (NGCT), Dehradun,
India, 4–5 September 2015; pp. 660–662.
Hajcak, G.; Macnamara, A.; Olvet, D.M. Event-Related Potentials, Emotion, and Emotion Regulation:
An Integrative Review. Dev. Neuropsychol. 2010, 32, 129–155. [CrossRef] [PubMed]
Kaestner, E.J.; Polich, J. Affective recognition memory processing and event-related brain potentials.
Cogn. Affect. Behav. Neurosci. 2011, 11, 186–198. [CrossRef] [PubMed]
Bastos-Filho, T.F.; Ferreira, A.; Atencio, A.C.; Arjunan, S.; Kumar, D. Evaluation of feature extraction
techniques in emotional state recognition. In Proceedings of the 2012 4th International Conference on
Intelligent Human Computer Interaction: Advancing Technology for Humanity (IHCI), Kharagpur, India,
27–29 December 2012.
Jatupaiboon, N.; Pan-Ngum, S.; Israsena, P. Real-time EEG-based happiness detection system. Sci. World J.
2013, 2013. [CrossRef] [PubMed]
Daimi, S.N.; Saha, G. Classification of emotions induced by music videos and correlation with participants’
rating. Expert Syst. Appl. 2014, 41, 6057–6065. [CrossRef]
Rozgic, V.; Vitaladevuni, S.N.; Prasad, R. Robust EEG emotion classification using segment level decision
fusion. In Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), Vancouver, BC, Canada, 26–31 May 2013; pp. 1286–1290.
Sigl, J.C.; Chamoun, N.G. An introduction to bispectral analysis for the electroencephalogram. J. Clin. Monit.
1994, 10, 392–404. [CrossRef] [PubMed]
Miller, A.; Sleigh, J.W.; Barnard, J.; Steyn-Ross, D.A. Does bispectral analysis of the electroencephalogram
add anything but complexity? Br. J. Anaesth. 2004, 92, 8–13. [CrossRef] [PubMed]

Algorithms 2017, 10, 63

16.

17.
18.
19.

20.
21.

22.
23.

24.
25.
26.

27.
28.

29.
30.
31.
32.
33.
34.

35.

36.

19 of 20

Hagihira, S.; Takashina, M.; Mori, T.; Mashimo, T.; Sleigh, J.W.; Barnard, J.; Miller, A.; Steyn-Ross, D.A.
Bispectral analysis gives us more information than power spectral-based analysis. Br. J. Anaesth. 2004, 92,
772–773. [CrossRef] [PubMed]
Goshvarpour, A.; Goshvarpour, A.; Rahati, S. Bispectrum Estimation of Electroencephalogram Signals
During Meditation. Iran. J. Psychiatry Behav. Sci. 2012, 6, 48–54. [PubMed]
Ning, T.; Bronzino, J.D. Bispectral Analysis of the Rat EEG During Various Vigilance States. IEEE Trans.
Biomed. Eng. 1989, 36, 1988–1990. [CrossRef] [PubMed]
Hosseini, S.A.; Khalilzadeh, M.A.; Naghibi-sistani, M.B.; Niazmand, V. Higher Order Spectra Analysis
of EEG Signals in Emotional Stress States. In Proceedings of the 2010 2nd International Conference on
Information Technology and Computer Science, Kiev, Ukraine, 24–25 July 2010; pp. 60–63.
Kumar, N.; Khaund, K.; Hazarika, S.M. Bispectral Analysis of EEG for Emotion Recognition. Proced. Comput.
Sci. 2016, 84, 31–35. [CrossRef]
Vijayan, A.E.; Sen, D.; Sudheer, A.P. EEG-Based Emotion Recognition Using Statistical Measures and
Auto-Regressive Modeling. In Proceedings of the 2015 IEEE International Conference on Computational
Intelligence & Communication Technology (CICT), Ghaziabad, India, 13–14 February 2015; pp. 587–591.
Gotlib, I.H.H.; Ranganath, C.; Rosenfeld, J.P.P. EEG Alpha Asymmetry, Depression, and Cognitive
Functioning. Cogn. Emot. 1998, 12, 449–478. [CrossRef]
Balconi, M.; Mazza, G. Brain oscillations and BIS/BAS (behavioral inhibition/activation system) effects on
processing masked emotional cues. ERS/ERD and coherence measures of alpha band. Int. J. Psychophysiol.
2009, 74, 158–165. [CrossRef] [PubMed]
Muller, M.M.; Keil, A.; Gruber, T.; Elbert, T. Processing of affective pictures modulates right-hemisphere
gamma band activity. Clin. Neurophysiol. 1999, 110, 1913–1920. [CrossRef]
Balconi, M.; Lucchiari, C. Consciousness and arousal effects on emotional face processing as revealed by
brain oscillations. A gamma band analysis. Int. J. Psychophysiol. 2008, 67, 41–46. [CrossRef] [PubMed]
Aftanas, L.I.; Varlamov, A.A.; Pavlov, S.V.; Makhnev, V.P.; Reva, N.V. Affective picture processing:
Event-related synchronization within individually defined human theta band is modulated by valence
dimension. Neurosci. Lett. 2001, 303, 115–118. [CrossRef]
Murugappan, M. Classification of human emotion from EEG using discrete wavelet transform. J. Biomed.
Sci. Eng. 2010, 3, 390–396. [CrossRef]
Li, M.; Lu, B.L. Emotion classification based on gamma-band EEG. In Proceedings of the 2009 31st Annual
International Conference of the IEEE Engineering in Medicine and Biology Society: Engineering the Future
of Biomedicine (EMBC), Piscataway, NJ, USA, 3–6 September 2009; pp. 1323–1326.
Lin, Y.P.; Wang, C.H.; Jung, T.P.; Wu, T.L.; Jeng, S.K.; Duann, J.R.; Chen, J.H. EEG-based emotion recognition
in music listening. IEEE Trans. Biomed. Eng. 2010, 57, 1798–1806. [PubMed]
Petrantonakis, P.C.; Hadjileontiadis, L.J. Emotion recognition from EEG using higher order crossings.
IEEE Trans. Inf. Technol. Biomed. 2010, 14, 186–197. [CrossRef] [PubMed]
Wang, X.; Nie, D.; Lu, B. EEG-based emotion recognition using frequency domain features and support
vector machines. Neural Inf. Process. 2011, 734–743. [CrossRef]
Yazdani, A.; Lee, J.-S.; Vesin, J.-M.; Ebrahimi, T. Affect recognition based on physiological changes during
the watching of music videos. ACM Trans. Interact. Intell. Syst. 2012, 2, 1–26. [CrossRef]
Murugappan, M.; Nagarajan, R.; Yaacob, S. Combining spatial filtering and wavelet transform for classifying
human emotions using EEG Signals. J. Med. Biol. Eng. 2011, 31, 45–51. [CrossRef]
Brown, L.; Grundlehner, B.; Penders, J. Towards wireless emotional valence detection from EEG.
In Proceedings of the 2011 33rd Annual International Conference of the IEEE Engineering in Medicine
and Biology Society (EMBS), Piscataway, NJ, USA, 30 August–3 September 2011; pp. 2188–2191.
Zhang, J.; Chen, M.; Hu, S. PNN for EEG-based Emotion Recognition. In Proceedings of the 2016 IEEE
International Conference on Systems, Man and Cybernetics, Budapest, Hungary, 9–12 October 2016;
pp. 2319–2323.
Chai, R.; Tran, Y.; Naik, G.R.; Nguyen, T.N.; Ling, S.H.; Craig, A.; Nguyen, H.T. Classification of EEG
based-mental fatigue using principal component analysis and Bayesian neural network. In Proceedings of
the 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society
(EMBC), Orlando, FL, USA, 16–20 August 2016; pp. 4654–4657.

Algorithms 2017, 10, 63

37.
38.

39.
40.

41.

42.
43.
44.

45.

46.
47.

48.
49.

50.

51.
52.
53.
54.
55.

20 of 20

Jirayucharoensak, S.; Pan-Ngum, S.; Israsena, P. EEG-Based Emotion Recognition Using Deep Learning Network
with Principal Component Based Covariate Shift Adaptation. Sci. World J. 2014, 2014. [CrossRef] [PubMed]
Goldberger, A.L.; Amaral, L.A.N.; Glass, L.; Hausdorff, J.M.; Ivanov, P.C.; Mark, R.G.; Mietus, J.E.;
Moody, G.B.; Peng, C.-K.; Stanley, H.E. PhysioBank, PhysioToolkit, and PhysioNet. Circulation 2000, 101,
e215–e220. [CrossRef] [PubMed]
Schalk, G.; McFarland, D.J.; Hinterberger, T.; Birbaumer, N.; Wolpaw, J.R. BCI2000: A General-Purpose
Brain-Computer Interface (BCI) System. IEEE Trans. Biomed. Eng. 2004, 51, 1034–1043. [CrossRef] [PubMed]
Kemp, B.; Zwinderman, A.H.; Tuk, B.; Kamphuisen, H.A.C.; Oberyé, J.J.L. Analysis of a Sleep-Dependent
Neuronal Feedback Loop: The Slow-Wave Microcontinuity of the EEG. IEEE Trans. Biomed. Eng. 2000, 47,
1185–1194. [CrossRef] [PubMed]
Zwoliński, P.; Roszkowski, M.; Zygierewicz, J.; Haufe, S.; Nolte, G.; Durka, P.J. Open database of epileptic
EEG with MRI and postoperational assessment of foci—A real world verification for the EEG inverse
solutions. Neuroinformatics 2010, 8, 285–299. [CrossRef] [PubMed]
Soleymani, M.; Lichtenauer, J.; Pun, T.; Pantic, M. A Multimodal Database for Affect Recognition and Implicit
Tagging. IEEE Trans. Affect. Comput. 2012, 3, 42–55. [CrossRef]
Zheng, W.L.; Lu, B.L. Investigating Critical Frequency Bands and Channels for EEG-Based Emotion
Recognition with Deep Neural Networks. IEEE Trans. Auton. Ment. Dev. 2015, 7, 162–175. [CrossRef]
Jadhav, P.N.; Shanamugan, D.; Chourasia, A.; Ghole, A.R.; Acharyya, A.; Naik, G. Automated detection
and correction of eye blink and muscular artefacts in EEG signal for analysis of Autism Spectrum Disorder.
In Proceedings of the 2014 36th Annual International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC), Chicago, IL, USA, 26–30 August 2014; Volume 2014, pp. 1881–1884.
Bhardwaj, S.; Jadhav, P.; Adapa, B.; Acharyya, A.; Naik, G.R. Online and automated reliable system design to
remove blink and muscle artefact in EEG. In Proceedings of the 2015 37th Annual International Conference
of the IEEE Engineering in Medicine and Biology Society (EMBC), Piscataway, NJ, USA, 25–29 August 2015;
Volume 2015, pp. 6784–6787.
Nikias, C.L.; Mendel, J.M. Signal processing with higher-order spectra. IEEE Signal Process. Mag. 1993, 10,
10–37. [CrossRef]
Kusumoputro, B.; Triyanto, A.; Fanany, M.I.; Jatmiko, W. Speaker identification in noisy environment
using bispectrum analysis and probabilistic neural network. In Proceedings of the 4th International
Conference on Computational Intelligence and Multimedia Applications (ICCIMA), Yokosuka City, Japan,
30 October–1 November 2001; pp. 282–287.
Brillinger, D. An introduction to polyspectra. Ann. Math. Stat. 1965, 36, 1351–1374. [CrossRef]
Purnamasari, P.D.; Ratna, A.A.P.; Kusumoputro, B. EEG Based Patient Emotion Monitoring using Relative
Wavelet Energy Feature and Back Propagation Neural Network. In Proceedings of the 2015 37th Annual
International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC), Milan, Italy,
25–29 August 2015; pp. 2820–2823.
Ackermann, P.; Kohlschein, C.; Wehrle, K.; Jeschke, S. EEG-based Automatic Emotion Recognition: Feature
Extraction, Selection and Classification Methods. In Proceedings of the 2016 IEEE 18th International Conference
on e-Health Networking, Applications and Services (Healthcom), Munich, Germany, 14–16 September 2016.
Parikh, R.; Mathai, A.; Parikh, S.; Chandra Sekhar, G.; Thomas, R. Understanding and using sensitivity,
specificity and predictive values. Indian J. Ophthalmol. 2008, 56, 45–50. [CrossRef] [PubMed]
Fawcett, T. An introduction to ROC analysis. Pattern Recognit. Lett. 2006, 27, 861–874. [CrossRef]
Zhang, Y.; Ji, X.; Zhang, S. An approach to EEG-based emotion recognition using combined feature extraction
method. Neurosci. Lett. 2016, 633, 152–157. [CrossRef] [PubMed]
Zheng, W. Multichannel EEG-Based Emotion Recognition via Group Sparse Canonical Correlation Analysis.
IEEE Trans. Cogn. Dev. Syst. 2016, 8920. [CrossRef]
Chai, R.; Ling, S.H.; San, P.P.; Naik, G.R.; Nguyen, T.N.; Tran, Y.; Craig, A.; Nguyen, H.T. Improving
EEG-Based Driver Fatigue Classification Using Sparse-Deep Belief Networks. Front. Neurosci. 2017, 11, 103.
[CrossRef] [PubMed]
© 2017 by the authors. Licensee MDPI, Basel, Switzerland. This article is an open access
article distributed under the terms and conditions of the Creative Commons Attribution
(CC BY) license (http://creativecommons.org/licenses/by/4.0/).

