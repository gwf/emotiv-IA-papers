COMPUTACIÓN E INFORMÁTICA
Recibido 17/05/2019
2019
Aceptado 20/07/2019

ReCIBE, Año 8 No. 1, Octubre

PROTOTIPO DE INTERFAZ HUMANOCOMPUTADORA CONTROLADA A TRAVÉS DE
GESTOS FACIALES
HUMAN-COMPUTER INTERFACE PROTOTYPE
CONTROLLED THROUGH FACIAL GESTURES.
Fernando Octavio Reynoso Martínez1
iscferhogan@gmail.com
María Antonieta Abud Figueroa1
mabud@itodepi.edu.mx
Silvestre Gustavo Peláez Camarena1
sgpelaez@itodepi.edu.mx
Lisbeth Rodríguez Mazahua1
lrodriguezm@itodepi.edu.mx
Ulises Juárez Martínez1
ujuarez@itodepi.edu.mx

1

División de Estudios de Posgrado e Investigación
Tecnológico Nacional de México / Instituto Tecnológico de Orizaba
Orizaba, Ver., México

C4-1

RESUMEN
Las interfaces humano-computadora se basan principalmente en el modelo de
ventanas, punteros y ratones como paradigma dominante para la interacción con
las aplicaciones. El uso de gestos es una adición valiosa al repertorio de técnicas
de interacción humano-computadora, sin embargo, requieren tiempo para
desarrollarse mejor. Como toda nueva tecnología presenta nuevos desafíos,
entre los cuales se encuentra el contar con herramientas que soporten el
desarrollo de aplicaciones que incluyan este tipo de interacción. Esto conlleva a
la necesidad de desarrollar la infraestructura de soporte para el manejo de
interfaces a través de gestos. Por lo que se propone un prototipo para el manejo
de interfaces humano-computadora a través de gestos faciales, con la finalidad
de establecer una base que permita desarrollar a futuro una biblioteca que
permita a los desarrolladores desarrollar interfaces gestuales de manera fácil.
PALABRAS CLAVE
Interfaces-Humano-Computadora, Gestos faciales, Arquitectura.

ABSTRACT
Human-computer interfaces are mainly based in the window, pointer, mouse
model as a dominant paradigm for the interaction with applications. The use of
gestures is a valuable addition to the repertoire of human-computer interaction
techniques, however, they require time to develop better. Like every new
technology it presents new challenges, among the which it is to have tools that
support the development of applications that include this type of interaction. This
leads to the need of developing the support infrastructure for the managing of
interfaces through gestures. So it is proposed a prototype for the managing of
human-computer interfaces through facial gestures, with the purpose of
stablishing a base that will allow in the future the creation of a library that allows
developers to develop gestural interfaces in an easy way.

KEYWORDS
Human-Computer-Interface, Facial Gestures, Architecture.

C4-2

INTRODUCCIÓN
Las interfaces humano-computadora tiene gran repertorio de técnicas para la
interacción, una de ellas es el uso de gestos faciales, esta técnica presenta un
crecimiento lento, ya que su implementación depende de diversos factores y aún
no se tienen estándares que regulen su significado de modo que los mismos
gestos signifiquen lo mismo en diferentes sistemas.
Esto conlleva a la
necesidad de desarrollar la infraestructura de soporte para el manejo de
interfaces a través de gestos. Los sistemas gestuales son uno de los caminos
futuros importantes para una interacción más holística y humana de las personas
con la tecnología, representando un gran potencial para mejorar la interacción.
Sin embargo, como toda nueva tecnología presenta nuevos desafíos, entre los
cuales se encuentra la necesidad de herramientas que soporten el desarrollo de
aplicaciones que incluyan este tipo de interacción. Por esta razón, en este trabajo
se presenta el desarrollo de un prototipo de una interfaz humano-computadora
controlada a través de gestos faciales utilizando la diadema Emotiv Epoc+. El
artículo se organiza de la siguiente forma: en la sección II se presentan los
trabajos relacionados con la interacción humano-computadora; en la sección III
se describe la parte metodológica, la cual contiene la descripción de la
arquitectura y de los módulos que conforman el prototipo; por último, en la
sección IV se mencionan las conclusiones generadas con base en los resultados
obtenidos y el trabajo a futuro.

1.1 TRABAJOS RELACIONADOS
Varios autores han realizado trabajos en el área de las interfaces controladas por
gestos. Los principales trabajos se mencionan a continuación:
Hawkes et, al. [1] establecieron como propósito probar la confiabilidad y utilidad
de la diadema Emotiv EPOC para determinar si se debe utilizar en el campo
médico ya que se piensa que una Tecnología como ésta ayudará a aquellos que
sufren de parálisis debidas a enfermedades tales como la ALS (Esclerosis lateral
amiotrofia), ya que la enfermedad no tiene cura es importante una solución que
ayude a mejorar las necesidades médicas. Para la solución se propuso
contemplar el uso de la diadema Emotiv EPOC, para el control de robot Finch
mediante expresiones faciales, se concluyó que la diadema Emotiv Epoc tuvo
éxito al ser conectada y controlar al robot Finch, pero con algunas fallas el
dispositivo reconoció los movimientos de las personas, así como también que la
diadema no es médicamente funcional o confiable.
Hornero et, al. [2] mencionaron la existencia de soluciones previas para ayuda
de la discapacidad utilizando TIC’S, a pesar de contar con soluciones existentes
sus costos son elevados, requieren de procesos de calibración por cada usuario,
lo que limita su usabilidad, por lo que su propuesta usa un acelerómetro
ADXL345, el microntrolador ATmega328A, el driver FTDI232RL, una placa de
circuito impreso, cinta elástica, Bluetooth (módulo RN41-XV), Labview para
desarrollar un módulo de modo Ratón y modo Teclado. El modo Ratón asocia
cada movimiento del usuario a las direcciones del ratón. El modo Teclado se
presenta en dos opciones de funcionamiento.
C4-3

En la primera se asocian cuatro movimientos escogidos con anterioridad por el
usuario y en la segunda opción, si el usuario presenta una mayor habilidad
motora, se asocian los diferentes movimientos que el usuario haya escogido.
Melodie Vidal et, al. [3] resaltaron la importancia de tomar en cuenta las
características de los ojos cuando se busca detectar si se sigue un objeto en
movimiento, sobre todo el hecho de que esta característica de la orientación de
objetos en movimiento es más difícil explotar que la orientación de objetos
estáticos, por lo que se necesita un método diferente a los anteriores. Su
propuesta usó una pantalla grande (89x50 cm, 1920x1080px) y un seguidor
ocular remoto Tobii X300. Se obtuvo como resultado final el método Pursuits y
tres aplicaciones interactivas.
Liang et, al. [4] resaltaron que es importante contar con una solución de
tecnologías de seguimiento de manos y reconocimiento de gestos para
simplificar las operaciones y proporcionar una interfaz intuitiva, en la que los
niños utilicen gestos con la mano para manipular marionetas virtuales para
realizar historias. Propusieron usar el controlador Leap Motion, el Leap Motion
SDK proporcionado por Leap Motion como dispositivo sensor HCI (HumanComputer Interaction, Interaction Humano-Computadora) para rastrear gestos
con las manos, Leap Motion SDK como API (Application Programming Interface,
Interfaz de Programación de Aplicaciones) para acceder a los datos de
movimiento de las manos y los dedos del dispositivo, Maya 2014 para crear todos
los modelos y animaciones 3D y Unity3D Pro V4.2 para integrar y desarrollar
todo el sistema. La arquitectura del sistema se compone principalmente de tres
partes: entrada, control de movimiento y salida.
McAdam et, al. [5] discutieron la importancia que tiene el movimiento humano de
forma natural para resolver problemas complejos en la dinámica del movimiento;
por lo cual se indicó la importancia de representar un entorno virtual
multisensorial donde el espacio de fase 3D del problema sea asignado a un
sistema de coordenadas visuales en 3D, propusieron usar una computadora Dell
T3400 de doble núcleo a 3.00 GHz, un monitor de 120Hz de 22 pulgadas y una
pluma háptica Phantom Omni 6 grados de libertad para la entrada. La
visualización 3D se implementó utilizando Microsoft DirectX en Windows 7 con
renderización estereoscópica y efectos de sonido 3D para reforzar la posición y
el movimiento de la bola en el espacio, la bola en el espacio 3D representa el
estado actual del sistema. Como resultado final se obtiene la simulación de un
sistema de bastón de carga.
Hayashi et, al. [6] resaltaron las características de las interfaces naturales de
usuario resaltando la modalidad de gestos físicos. Los autores indicaron la
importancia de contar con un sistema que no requiera ninguna memorización, ya
que a pesar de existir trabajos anteriores la capacidad de recordar estos gestos
no está clara, por lo que su propuesta usa “Microsoft Kinect SDK 1.5”, Kinect,
Matlab, Weka. Macbook Pro con Intel Core i7 a 2,6 GHz. Se obtiene como
resultado final la identificación del usuario basada en el cuerpo: una nueva forma
de identificación del usuario basada en las longitudes del segmento del cuerpo y
un gesto de agitar la mano.

C4-4

Heijboer et, al. [7] resaltaron que el puntero de luz es un dispositivo adecuado
para estudiar la viabilidad de las interacciones periféricas con una interfaz
gestual. El objetivo de este experimento exploratorio fue rediseñar los estilos de
interacción del puntero de luz de tal manera que potencialmente podría
interactuar con la periferia de la atención. Por lo tanto, se exploró qué gestos y
qué tipo de comentarios serían los más adecuados para la interacción periférica.
El puntero de luz y una lámpara estuvieron controlados por un Phidget
Interfacekit1 que está conectado a un Apple iMac ejecutando el software Max /
MSP / Jitter.
De Carolis et al. [8] resaltaron la importancia de las características NUI (Natural
User Interface, Interfaz Natural de Usuario), por lo tanto, el objetivo fue
proporcionar a los usuarios una interfaz natural para acceder a información
adicional sobre las prendas que se muestran en los maniquíes desde el exterior,
incluso cuando la tienda está cerrada. El prototipo se desarrolló utilizando un
dispositivo comercial popular que permite la interacción sin contacto: Microsoft
Kinect 21.
Mehler et al. [9] resaltaron que cada vez más interfaces humano-computadora
(HCI) son diseñadas para incorporar medios de comunicación no verbal. La
escritura gestual se centra principalmente en el nivel perceptual de las
descripciones de las imágenes. WikiNect se propuso como un sistema que
permite autoría de hipertextos mediante escritura gestual. Para la propuesta se
usó la tecnología Kinect y como es un prototipo no se mencionó algún lenguaje
de programación, dividiendo su arquitectura en segmentación, vinculación,
descripción y calificación de imágenes. El resultado final fue el módulo de gestión
de sesión.
Barrón et al. [10] discutieron la importancia de los entornos de aprendizaje
tradicionales, los cuales no proporcionan un modelo de aprendizaje individual.
Para superar este problema se propuso un sistema que utilice expresiones
faciales y señales de EEG (Electroencephalography Signals, Señales
Electroencefálicas) para el reconocimiento de la emoción. El sistema utilizó la
diadema Emotiv Epoc para registrar las señales de EEG del estudiante y Motor
de Lógica Difusa, el cual es el responsable de evaluar emociones cognitivas y
afectivas.
Las propuestas revisadas aunque utilizan interfaces naturales, la minoría
incluyen gestos faciales para la interacción, por lo que en la presente propuesta
se busca cubrir este punto.
MARCO CONCEPTUAL
En esta sección se presenta la descripción de la arquitectura y los módulos que
conforman el prototipo para el manejo de interfaces humano-computadora a
través de gestos faciales.
El patrón arquitectonico elegido para la arquitectura del prototipo es el patrón
MVC (Model-View-Controller, Modelo-Vista-Controlador), como se muestra en la
Figura 1, contiene tres componentes:
C4-5

1. Vista: Este componente presenta la interfaz gráfica la cual en este caso
es Java Swing, interfaz que será controlada por medio de gestos faciales.
2. Controlador: Este componente contiene la lógica necesaria para la
interacción entre el modelo y la vista, así como para manejar las
interacciones del usuario.
3. Modelo: Componente que contiene la definición de las funciones que
permitirán el control del apuntador de ratón a través de gestos faciales
como son sonrisa, guiño derecho, guiño izquierdo, mirada derecha y
mirada izquierda y se integran los flujos de datos al dispositivo y
proporciona acceso a la API del mismo.

Fig. 1. Arquitectura del prototipo.

El patrón arquitectónico MVC ha tenido una gran influencia en el desarrollo de
diseño de interfaces gráficas de usuario.
Su objetivo principal es promover la independencia de desarrollo de sus
componentes, si se desea hacer un cambio en algún componente no afectará a
los demás y viceversa.
El componente modelo encapsula la información utilizada por la aplicación; el
componente vista encapsula la información elegida y necesaria para la
representación gráfica de esa información; el componente controlador encapsula
lo lógico necesario para mantener el buen funcionamiento entre el modelo y la
vista y maneja los eventos de entrada del usuario [11].

C4-6

Para el desarrollo del prototipo se eligieron las siguientes tecnologías:
Emotiv Epoc + por su característica de reconocer expresiones faciales y su SDK
(Software Development Kit, Kit de desarrollo de software) brinda las
herramientas que se necesitan para administrar y escalar aplicaciones. Java se
escogió por su buena integración con el SDK de Emotiv Epoc +, todo esto
utilizando el entorno de desarrollo NetBeans.
La decisión está determinada por la conjugación de las tecnologías de la
información descritas a continuación:
•

Lenguaje de programación: Java es un lenguaje de programación
orientado a objetos, distribuido, interpretado, robusto, portable, multihilo,
dinámico e independiente de arquitecturas [12].

•

Dispositivo: Se eligió Emotiv EPOC + dado que es un
electroencefalograma multicanal y está diseñado para la investigación del
cerebro humano escalable y contextual y para aplicaciones avanzadas de
interfaz cerebro-computadora y brinda acceso a datos cerebrales de
grado profesional, su plataforma de desarrollo es compatible con Java y
a diferencia de otras cuenta con reconocimiento de expresiones faciales
(parpadeo, guiño, sorpresa, fruncir el ceño, sonreír, apretar, reír, sonreír),
dada la naturaleza del proyecto son necesarias [13].

•

Entorno de desarrollo: Se eligió NetBeans debido a su gran interfaz de
desarrollo y amplio soporte para hacer aplicaciones gráficas [14].

METODOLOGÍA
El prototipo permite mostrar el control del ratón a través de gestos faciales.
Utilizando el SDK de Emotiv se detectan los gestos, los cuales se mapean a
comandos ejecutables en el prototipo. Los comandos utilizados son:
•
•
•
•
•

Parpadeo Izquierdo: Cuando la persona hace un parpadeo con el ojo
izquierdo, el ratón se mueve con dirección hacia arriba.
Parpadeo Derecho: Cuando la persona da un parpadeo con el ojo
derecho, el ratón se mueve con dirección hacia abajo.
Mirada a la izquierda: El ratón se mueve hacia la izquierda cuando la
persona lleva su vista hacia el lado izquierdo.
Mirada a la derecha: El ratón se dirige hacia el lado derecho cuando la
persona cambia la mirada hacia el lado derecho.
Sonrisa: Cuando la persona sonríe, la acción corresponde a dar un clic
izquierdo al ratón.

C4-7

En la Figura 2 se muestra la estructura de clases para el mapeo de los comandos
basándose en el patrón de diseño Adapter.

Fig. 2. Diagrama de clases del prototipo.

En la interfaz Mouse se definen los métodos a implementar para reemplazar el
uso directo del ratón como lo son, arriba, abajo, derecha, izquierda y clic, los
cuales son implementados en la clase Adaptador. La clase Direcciones contiene
los métodos de los gestos de la cara que serán utilizados y que se basan en el
uso de las funciones proporcionadas por el SDK de Emotiv.
El diagrama de clases del prototipo resultante se muestra en la Figura 3. El
prototipo se diseñó bajo el modelo MVC. En la Vista se tiene la clase View que
hereda de la clase Apuntador, la cual siempre debe integrarse en todos los
programas en el modelo puesto que contiene las coordenadas de donde se
quiere inicializar el apuntador del ratón al comenzar a utilizar la aplicación y el
manejo del clic izquierdo.
En la parte del modelo se encuentran las interfaces y la clase mostrada en el
diagrama de clases del prototipo presentado anteriormente. Todo esto es
implementado en la clase controlador.

C4-8

Fig.3. Diagrama de clases de la implementación del prototipo.

Para la ejecución del programa es necesario humectar con una solución salina
las almohadillas que contienen los electrodos de la diadema Emotiv Epoc +.
Después de esto se debe conectar el receptor USB a la computadora e iniciar al
programa EmotivBCI para verificar que todos los electrodos estén funcionando
correctamente y que la carga de la batería es adecuada. La Figura 4 muestra la
imagen de la diadema funcionando en forma correcta.

C4-9

Fig. 4. Imagen del programa EmotivBCI, el cual muestra el estado de los
electrodos de la diadema Emotiv Epoc + y el estado de la batería.

Una vez verificado el buen funcionamiento de la diadema Emotiv, inicia el
funcionamiento del prototipo. Como se muestra en la Figura 5, el prototipo
presenta del lado derecho, los gestos que pueden utilizarse para mover el
apuntador del ratón y en qué dirección se dirigen, de modo que al realizar el
parpadeo con el ojo izquierdo, el apuntador se moverá hacia arriba, mientras que
al parpadear con el ojo derecho se moverá hacia abajo; al dirigir la mirada hacia
la izquierda, el apuntador se moverá hacia la izquierda; dirigiendo la mirada a la
derecha, el apuntador se mueve a la derecha; la activación del clic ocurrirá
cuando se realiza una sonrisa. Al dar clic teniendo el apuntador sobre alguno de
los botones, aparecerá en la caja de texto titulada “Acciones de botones” un texto
indicando el número de botón seleccionado.
De igual manera, en la parte superior de la pantalla se encuentran cuatro menús,
los cuales contienen submenús, y presentan un funcionamiento similar,
indicando su nombre en la pantalla cuando son seleccionados.
Para iniciar el programa se debe realizar cualquier gesto, lo que producirá que el
apuntador del ratón aparezca en la pantalla, como muestra la Figura 6.

C4-10

Fig. 5. Pantalla de la implementación del programa de prueba del prototipo de la
capa de funciones gestuales.

Fig. 6. Ejemplo de inicio de aplicación con gesto.

C4-11

Como resultado del uso del prototipo se encontró que tuvieron un 90 % de
efectividad las interfaces gestuales que fueron configuradas en el prototipo ya
que en de diez veces que se probó su funcionamiento fallaron una vez al ser
solicitadas. Sin embargo, se presentan algunos problemas en el funcionamiento
ya que existe un desfase debido a los movimientos involuntarios de la cara,
porque algunas veces al realizar un gesto involuntariamente se activa otro que
no se quiere y algunas veces la reacción de la diadema se retrasa. También se
encontró que el cabello abundante dificulta el reconocimiento de los electrodos
de la diadema.
CONCLUSIONES
Las interfaces humano-computadora son muy usadas hoy en día en muchos
sectores de la sociedad, tanto en la industria como en la vida diaria de las
personas. Actualmente se observan una gran cantidad de tecnologías con las
cuales el ser humano interactúa de diferentes formas, muchas aplicaciones
sirven para mejorar la vida diaria de muchas personas o simplemente de ocio.
El desarrollo del prototipo para control del apuntador del ratón a través de gestos
faciales demostró que es factible el uso de la diadema Emotiv en una interfaz
gestual.
Se demostró que Java Swing es una tecnología la cual permite trabajar el uso
de interfaces diferentes al ratón y al teclado de la computadora. Igualmente se
demostró que el patrón de diseño adapter es útil para enlazar el api de la
diadema con las clases creadas en el prototipo; se demostró que trabajar con un
el patrón arquitectónico MVC promueve la independencia en el desarrollo de
componentes; por otra parte, se concluyó que la diadema Emotiv Epoc + es un
gran avance tecnológico para el uso de interfaces gestuales, pero aún no es del
todo precisa al obtener las señales de interfaces gestuales mediante Java.
4.1 TRABAJOS A FUTURO
Como trabajo futuro se propone la creación de una biblioteca para el manejo de
interfaces humano-computadora a través de gestos faciales, que permita la
estandarización de gestos faciales, lo cual será una herramienta importante para
trabajos futuros de programadores, ya que habrá más infraestructura, que en
estos momentos es escasa, la importancia de estas bibliotecas radica en que
pueden ser usadas tanto para el desarrollo de aplicaciones para ayudar a
comunicar personas con dificultades físicas, hacer aplicaciones domóticas,
juegos y muchas aplicaciones más.
AGRADECIMIENTOS
Este trabajo cuenta con apoyo por parte del Consejo Nacional de Ciencia y
Tecnología (CONACyT).

C4-12

REFERENCIAS
[1] A. Hawkes, A. Liu, R. Tashakkori, E. Jackson, J. Tate, S. Tashakkori, D. Kale,
M. Parry ,” Using the Emotiv EPOC Neuroheadset as an EEGControlled
BrainComputer Interface”, in Summer Ventures in Math and Science 2015, pp 1–
15, June 2015.
[2] G. Hornero, E. Font, J. Tejedo, O. Casas, “Interfaz gestual para el acceso a
plataformas digitales y control de plataformas para el ocio”, VI Congreso
Internacional de Diseño, Redes de Investigación y Tecnología para todos
(DRT4ALL), Madrid, 2015 pp. 667-692.
[3] M. Vidal, K. Pfeuffer, A. Bulling, H. Gellersen,” Pursuits: Eye-Based Interaction
with Moving Targets”, The ACM SIGCHI Conference on Human Factors in
Computing Systems, Paris, 2013, pp. 3147-3050.
[4] H. Liang, J. Chang, I. K. Kazmi, J.J. Zhang, P. Jiao,” Hand gesture-based
interactive puppetry system to assist storytelling for children”, in The Visual
Computer, vol 33, pp 517–531, Apr. 2017.
[5] R. McAdam, K. Nesbitt, “Leveraging Human Movement in the Ultimate
Display”, Proceedings of the Thirteenth Australasian User Interface Conference
(AUIC2013), Melbourme, 2013, pp. 11-20.
[6] E. Hayashi, M. Maas, Jason I. Hong,” Wave to Me: User Identification Using
Body Lengths and Natural Gestures”, CHI '14 Extended Abstracts on Human
Factors in Computing Systems (CHI EA '14), Toronto,2014, pp. 3453-3462.
[7] M. Heijboer, E. van den Hoven, B. Bongers, S. Bakker,” Facilitating peripheral
interaction: design and evaluation of peripheral interaction for a gesture-based
lighting control with multimodal feedback”, in Personal and Ubiquitous
Computing, vol. 20, no 1, pp 1–22, Dec. 2015.
[8] B. De Carolis, Giuseppe Palestra, “Gaze-based Interaction with a Shop
Window”, International Working Conference on Advanced Visual Interfaces, Bari,
2016, pp. 304-305.
[9] A. Mehler, A Lücking, G. Abrami, “WikiNect: image schemata as a basis of
gestural writing for kinetic museum wikis” in Universal Access in the Information
Society, vol. 14, no. 3, pp 333–349, Sep 2015.
[10] M. Barrón-Estrada, R. Zatarain-Cabada, C. Aispuro-Gallegos, C. de la Luz
Sosa-Ochoa, M. Lindor Valdez, “Java Tutoring System with Facial and Text
Emotion Recognition”. International Journal of Advanced Research in Computer
Science. 106. 49. 2015.
[11] R.N. Taylor, N. Mendvidovic, E. M. Dashofy, “Software Architecture”
Foundations, Theory and Practice, John Wiley & Sons, 2009, pp. 94-97.

C4-13

[12] “¿Por qué los desarrolladores de software eligen Java?” [Online]. Available:
https://www.java.com/es/about/ [Accessed: 04/Abril/2018].
[13] “EMOTIV EPOC+ 14 Channel Mobile EEG”, [online]. Available:
https://www.emotiv.com/product/emotiv-epoc-14-channel-mobile-eeg/.
[Accessed: 04/Abril/2018].
[14]
“The
NetBeans
Platform”,
[online].
Available:
https://netbeans.org/features/platform/index.html . [Accessed: 04/Abril/2018].

NOTAS BIOGRÁFICAS

Fernando Octavio Reynoso Martínez es Ingeniero en
Sistemas Computacionales del Instituto Tecnologico de
Orizaba y actualmente es estudiante de la Maestría en
Sistemas Computacionales en el Instituto Tecnologico de
Orizaba. Su área de investigación es el desarrollo de
aplicaciones de escritorio. Se ha desempeñado como
desarrollador en el sector privado de México.
María Antonieta Abud Figueroa, nació en la ciudad de
Orizaba, Ver. Es ingeniero en electrónica por la UAMIztapalapa, México DF en el año 1984, y maestra en ciencias
en sistemas de información por el ITESM-Morelos, en la ciudad
de Cuernavaca, Mor. en el año 1991
Ella fue profesora de tiempo completo en el ITESM Campus
Central de Veracruz entre los años 1985 y 1993; desde el año
1995 es profesora-investigadora en el área de posgrado del
Instituto Tecnológico de Orizaba, en la ciudad de Orizaba, Ver.
México. Tiene reconocimiento de Perfil Deseable PRODEP
desde el año 2005 y es líder del cuerpo académico en
consolidación “CADAIMIS”. Sus áreas de interés son la
Ingeniería de Software y las Interfaces Humano-Computadora.
La M.C. Abud actualmente es miembro del ACM.

C4-14

Silvestre Gustavo Peláez Camarena, Ingeniero Industrial
en producción en el Instituto Tecnológico de Orizaba, maestro
en ciencias especialidad en cómputo estadístico en el Colegio
de Postgraduados de Chapingo y especialidad en Educación a
Distancia en la Universidad Veracruzana. Actualmente es
profesor-investigador de la División de Estudios de Posgrado e
Investigación del Instituto Tecnológico de Orizaba. Cuenta con
reconocimiento de Perfil Deseable ante PRODEP.
Publicaciones en congresos nacionales e internacionales y
revistas con arbitraje estricto. Es miembro del cuerpo
académico CADAIMIS reconocido por PRODEP como un
cuerpo académico en formación. Miembro de ACM. Ha dirigido
tesis a nivel maestría y proyectos de titulación a nivel
licenciatura.

Lisbeth Rodríguez Mazahua, Doctora en Ciencias en
Computación, egresada del Centro de Investigación y de
Estudios Avanzados del Instituto Politécnico Nacional
(CINVESTAV-IPN). Es profesora de tiempo completo del
Instituto Tecnológico de Orizaba. Ha dirigido más de cinco
proyectos de investigación científica y aplicada financiados por
CONACYT, el Tecnológico Nacional de México y PRODEP. Es
autor de más de 50 publicaciones en revistas de reconocido
prestigio y congresos especializados. Es miembro del Sistema
Nacional de Investigadores nivel Candidato.

Ulises Juárez Martínez, Doctor en Ciencias en la
especialidad de Computación por parte del CINVESTAV. Sus
áreas de interés comprenden la adaptación en vivo de sistemas
de software, desarrollo de software orientado a aspectos,
programación generativa, líneas de productos de software y
lenguajes de programación. Actualmente es profesor
investigador del Grupo de Ingeniería de Software en el
TecNM/I. T. de Orizaba.

Esta obra está bajo una licencia de Creative Commons
Reconocimiento-NoComercial-CompartirIgual 2.5 México.
C4-15

