EMOTION DETECTION FROM EEG SIGNALS:
CORRELATING CEREBRAL CORTEX ACTIVITY
WITH MUSIC EVOKED EMOTION

Erim Yurci

MASTER THESIS UPF / 2014
Master in Sound and Music Computing

Master thesis supervisor:
Dr. Rafael Ramirez
Department of Information and Communication Technologies
Universitat Pompeu Fabra, Barcelona

II

Acknowledgments
I would like to thank my supervisor Dr. Rafael Ramirez for his generous help and
valuable guidance. I am grateful to Sergio Giraldo and Zacharias Vamvakousis from
MTG for sharing their valuable experience and knowledge with me. I also would like to
thank to Prof. Xavier Serra for giving me the opportunity to study in SMC master.

III

Abstract
This master project aims to study music evoked emotion using electroencephalography
(EEG) techniques. In particular, the first goal of the research is studying the correlation
between EEG signal characteristics and three emotional states that are evoked by music;
which are happy, sad and relax. A secondary goal of the research is correlating musical
features obtained by music information retrieval (MIR) techniques with the EEG
signals. An experiment was designed for proper EEG recording while subjects were
listening to emotionally relevant songs. Six songs with emotional content (happy, sad
and relax) were selected; three of them, were selected by us from a dataset of songs
previously classified according to their emotional content by MIR techniques. The other
three songs were selected by each participating subject according to their own
preferences. The obtained data, EEG signals and music audio, were analyzed in order to
investigate correlation among them. The EEG signals have been analyzed in order to
extract features, some of them emotionally related, and these features have been used to
predict the type of music to which the subject is listening. The obtained classifiers are
able to predict the music type with up to 98% accuracy (base-line accuracy is 16%). The
musical features were extracted in order to be correlated with the extracted EEG
features. More than 10 correlations found with the correlation coefficient value greater
than 0.25 and p-value smaller than 0.05.

IV

V

Table of Contents
1.	  INTRODUCTION	  ...............................................................................................................................	  1	  
1.1	  MOTIVATION	  ....................................................................................................................................................	  2	  
1.2	  OBJECTIVES	  ......................................................................................................................................................	  2	  
1.3	  STRUCTURE	  OF	  THE	  THESIS	  ..........................................................................................................................	  3	  
2.	  BACKGROUND	  ..................................................................................................................................	  4	  
2.1	  EARLY	  HISTORY	  OF	  EEG	  ................................................................................................................................	  4	  
2.1.1	  Discovery	  of	  Bioelectricity	  and	  Electricity	  ..................................................................................	  4	  
2.1.2	  Early	  Research	  on	  Bioelectrical	  Phenomena	  in	  Animals	  ......................................................	  5	  
2.1.3	  Discovery	  of	  EEG	  .....................................................................................................................................	  6	  
2.1.4	  Industrial	  Standardization	  of	  EEG	  .................................................................................................	  7	  
2.1.5	  EEG	  with	  Music	  ........................................................................................................................................	  9	  
2.2	  EEG	  SYSTEMS	  AND	  TECHNOLOGY	  .............................................................................................................	  11	  
2.2.1	  Electrodes	  and	  the	  10-­‐20	  System	  .................................................................................................	  11	  
2.2.2	  Important	  Frequency	  Bands	  ..........................................................................................................	  11	  
2.2.3	  Artifacts	  ...................................................................................................................................................	  13	  
3.	  STATE	  OF	  THE	  ART	  ......................................................................................................................	  14	  
4.	  MATERIALS	  AND	  METHODS	  .....................................................................................................	  18	  
4.1	  MATERIALS	  ....................................................................................................................................................	  18	  
4.1.1	  Musical	  Data	  .........................................................................................................................................	  18	  
4.1.2	  Emotiv	  EPOC	  ..........................................................................................................................................	  20	  
4.1.3	  OpenViBE	  ................................................................................................................................................	  20	  
4.1.4	  Weka	  .........................................................................................................................................................	  21	  
4.1.5	  MIRtoolbox	  .............................................................................................................................................	  21	  
4.1.6	  MATLAB	  ...................................................................................................................................................	  21	  
4.2	  METHODS	  .......................................................................................................................................................	  22	  
4.2.1	  Experiment	  .............................................................................................................................................	  22	  
4.2.2	  Data	  Analyses	  ........................................................................................................................................	  24	  
5.	  RESULTS	  AND	  DISCUSSION	  .......................................................................................................	  30	  
5.1	  EMOTIONAL	  STATE	  FEEDBACK	  RESULTS	  ................................................................................................	  30	  
5.2	  EEG	  CLASSIFICATION	  RESULTS	  .................................................................................................................	  31	  
5.3	  MUSICAL	  FEATURES	  AND	  EEG	  FEATURES	  CORRELATIONS	  .................................................................	  34	  
5.3.1	  Subjects’	  Selected	  Songs	  ...................................................................................................................	  35	  

VI

5.3.2	  Common	  Songs	  .....................................................................................................................................	  36	  
5.3.3	  Mutual	  EEG	  Feature	  Correlated	  With	  Subjects’	  Selected	  Songs	  and	  Common	  Songs	  
Musical	  Features	  .............................................................................................................................................	  39	  
5.4	  DISCUSSION	  ...................................................................................................................................................	  41	  
5.4.1	  EEG	  Classification	  Results	  Discussion	  ........................................................................................	  41	  
5.4.2	  Musical	  and	  EEG	  Features	  Correlation	  Discussion	  ...............................................................	  42	  
6.	  CONCLUSIONS	  ................................................................................................................................	  43	  
7.	  FUTURE	  WORKS	  ...........................................................................................................................	  44	  
REFERENCES	  .......................................................................................................................................	  45	  

VII

VIII

1. Introduction
Music’s emotion evoking properties are well known for centuries [1]. From Hippocrates
to al-Farabi many scientists from different eras used music for therapeutical purposes
taking advantage of its relationship with emotion [2,3]. The inner mechanism of the
brain how the emotions are really evoked has yet to be solved but it has been proven
that music as a consequence evokes emotions [4].
Today with the devices and techniques like electroencephalography (EEG),
magnetoencephalography (MEG), functional magnetic resonance imaging (fMRI), near
infrared spectroscopy (NIRS), and functional transcranial doppler sonography (fTCD)
are helping us to trace changes happening in the brain and giving us opportunities to use
brain as a subject for our researches [5].
In this research we use EEG technique to capture brain signals while subjects were
listening to emotional evoking music in three categories: happy, sad and relax. Due to
musical preferences is a subjective matter we asked to participants to pick three songs
(one song for each category). Moreover we chose three songs (one song for each
category) from a dataset that the songs were previously classified according to
emotional states by MIR techniques which of those listened by all of the participated
subjects [6]. The acquired EEG data and musical features are classified by machine
learning algorithms and correlations among them are investigated.

1

1.1 Motivation
Decoding emotional state of a person has a variety of applications. Using EEG
techniques to decode emotions provide easy facility to work in comparison to other
techniques like fMRI (loud scanner noise and narrow space), NIRS (expensive and long
time of implementation). The outcome of emotion decoding could be applied on braincomputer interface (BCI) applications and moreover this might be used in therapeutic
manners for instance helping patients suffering from depression. There have been
several approaches about EEG-based emotion detection, but there hasn’t been a strong
consensus about definite conclusions [7,8,9].
MIR techniques have been advanced in the past decade and pave the way for different
applications. For instance, musical features extracted by MIR techniques can define
mood characteristics of a song. But how these features really make alterations on our
state of the mind? Studying musical features with EEG techniques could reveal some
coherent attributes that music and emotion carry.
We hopefully in the end wish to contribute the previous works that had been done in
these fields and the works that will be held in future.

1.2 Objectives
The study has two main objectives. On one hand to decode emotional state of a person
produced by listening emotion evoking music. On the other hand to detect musical
features which are relevant with EEG signals. EEG data will be gathered from Emotiv
Epoc head set with its all electrodes placed on the scalp and musical features will be
extracted by MIR techniques.
The project aims to predict in which emotional category subjects are listening the music
and which musical features have relevant effects on subjects’ EEG signal changes.

2

1.3 Structure of the Thesis
The rest of the paper organized as follows: Chapter 2 presents background and history
about EEG system. Chapter 3 presents state of the art in EEG-based emotion
classification methods. Chapter 4 presents materials and methods used in the study.
Chapter 5 presents results and discussion. Chapter 6 presents conclusions. And chapter
7 presents future works.

3

2. Background
2.1 Early History of EEG
The EEG equipment and its applications as new technologies emerged have consistently
taken advantage from them. The history of the technology that allow EEG to function
could be traced back to 16th century till now but the discovery of bioelectricity,
potentials and currents produced within living organisms, grounded much earlier.
2.1.1 Discovery of Bioelectricity and Electricity
The first written bioelectric event is a description of the electric catfish as a fish that
“releases the troops” appeared in ancient Egyptian hieroglyph around 3100 B.C. [10].
The first written document on medical application by using electricity appeared in A.D.
46, when Scribonius Largus (Roman; c. 1-c. 50) recommended the use of torpedo fish
for curing headaches [11].
The term electric was invented by William Gilbert (British; 1544-1603). He constructed
a simple electroscope as seen in figure 1, a light metal needle pivoted on a pin, to
measure the attractive power of amber which in Greek named as electrics (ηλεκτρον)
[12]. The advancements continued in 1700’s. In this period, Benjamin Franklin
(American; 1706-1790), Luigi Galvani (Italian; 1737-1798), Michael Faraday (British;
1791-1867), and Jacques Arsene d’Arsonval (French; 1851-1940) were discoverers of
different kinds of electricity: static electricity, direct current, induction coil shocks, and
radio frequency current, respectively [13].

FIGURE 1. The first instrument to detect electricity, electroscope, Invented by William
Gilbert (Gilbert, 1600)
4

2.1.2 Early Research on Bioelectrical Phenomena in Animals
A significant study about bioelectrical phenomena in animals was done by Luigi
Galvani in 1791, professor of anatomy at the University of Bologna, that he caused
twitches of frog leg muscles when he simultaneously touches the muscles and exposed
nerves with a bimetallic arc of copper and zinc as seen in figure 2 [14]. This study is
cited as the first documented experiment in neuromuscular electric stimulation, and was
one of the first forays into the study of bioelectricity in animals. With the discovery of
the galvanometer, which is an analog electromechanical actuator used for detecting
electric current, by Hans Christian Oersted (Danish; 1777-1851) in 1820 led the way to
study bioelectricity deeper. Carlo Matteucci (Italian; 1811-1865) was the first to
measure the bioelectric current of muscle impulses in frog in 1838 by using the astatic
galvanometer [15]. Emil Du Bois-Reymond (German; 1818-1896) began to measure
biological currents in electric fish in 1840‘s by the galvanometer [16]. He also
developed non-polarizable electrodes made of clay, which were used in the first animal
and human EEG recordings later [17].

Figure 2. Stimulation experiment from Luigi Galvani (1791).

5

2.1.3 Discovery of EEG
In 1875, the very first demonstration on the brain’s electrical activity was done by
Richard Caton (British; 1842-1926). He recorded electrical activity from the exposed
brains of rabbits, cats and monkeys by using mirror galvanometer [17]. He managed to
observe variations associated with sleep, wakefulness, anesthesia, and death, he also
identified brain areas associated with motor activity including head movement,
mastication, and movement of the eyelids which were interpreted after him from his
work, making him the first EEG brain mapper as well [17].
In 1913 the very first photographic recordings of EEG signals were obtained by
Vladimir Pravdich-Neminsky (Russian; 1879-1952) in Kiev, he provided the first
literature to show EEG with time-series waveform representation [17].
In 1920 Alexander Forbes, professor in the department of physiology at Harvard
Medical School, replaced the string galvanometer with a vacuum tube to amplify EEG’s
electrical signal. This became the standard for EEG amplification in the following years
[18].
In 1924 Hans Berger (German; 1873-1941) recorded the very first human brainwave
and he coined the word electroencephalogram [17]. He also introduced the two major
brain wave pattern names “alpha wave” and “beta wave”, in figure 3 it can be seen.
Toennies, from Institute of Brain Research in Berlin-Buch J.F, made important
advancements in EEG instrumentation. In 1932, Toennies developed the first
multichannel ink-writing oscillograph for recording brain potentials, and developed
differential amplifier with the collaboration of B.H.C. Matthews from Physiology
Laboratory, University of Cambridge, England [17]. Berger’s outstanding scientific
contribution didn’t gain publicity worldwide until E.D. Adrian and B.H.C. Matthews
verified Berger’s results, they were the first ones to successfully conduct his experiment
in 1934. They were able to record simultaneously and independently from different
areas of the brain, revealing spatial and temporal relationships between signals. They
were also successfully audified and listened the brainwaves, which was the first
example of the sonification of human brainwaves for auditory display [19].
6

Besides the studies held in Europe in United States, Donald B. Lindsley (American;
1907-2003), co-founder of UCLA’s Brain Research institute, was one of the first
scientist to use the newly discovered technique of electroencephalography to record
electrical brain activity [20].

Figure 3. A page from Berger’s notebook illustrating early recordings of the human
EEG (1924).

2.1.4 Industrial Standardization of EEG
By the mid-1930s, commercial EEG systems were begun to appear. Many laboratories
made contributions to EEG methodology and practice by using systems that were
”clones” of previous designs. Albert Grass produced the first commercial EEG device;
Grass Model I in 1935, which had three channels of differential amplification and an
ink-writer that recorded on rolls of paper [17]. By the end of the World War II, the
Model III was introduced in 1946, and had the first 8-channel and 16-channel EEGs
ever made, about 5,000 systems were produced and were shipped world wide [17].
The American Electroencephalographic Society (AEEGS) was founded in 1946, the
first annual meeting was held in Atlantic City NJ in 1947 with the established
experimental and clinical EEGers. By the 1949 the first issue of “The EEG Journal” had
been published [21].
7

In the 1950s first transistorized EEG amplifiers were produced by Franklin Offner,
named as “Type T”, as seen in figure 4, which was a portable system that brought a new
standard for EEG instrumentation with advantages of low heat dissipation, high
efficiency, lower operating voltages, and small size [17]. By the mid-1950s EEG had
became formal laboratory equipment in institutions and hospitals. With the introduction
of digital technology EEG systems became more automated, controlled; signal storage,
retrieval and numeric processing capabilities got dramatically higher.

Figure 4. First portable EEG designed by Franklin Offner (1950’s).

8

2.1.5 EEG with Music
In 1954, Robert Frances attempted to measure musical perception through polygraph
readings, including EEG, GSR, heart rate and respiration rate while subjects listened to
selected musical examples [22]. In the late 1950’s, Joe Kamiya studied the phenomenon
of internal perception or the awareness of private internal experiencing. He
demonstrated that a subject could learn to produce alpha or beta brain states on demand
[23].
In 1965, Alvin Lucier managed to generate music by using his brainwaves. He designed
a performance named Music for Solo Performer, in figure 5 he can be seen in act. He
achieved to map performer’s alpha band to an array of percussion instruments [24,25].

Figure 5. Alvin Lucier is performing with an EEG; He controls instruments by his
thoughts (1965).

9

In 1967 Richard Teitelbaum, member of the innovative live electronic music group
called Musica Elettronica Viva (MEV), used EEG and ECG (Electrocardiography)
signals to control sources for electronic synthesizers in performances of Spacecraft. In
1968 heartbeat and breath sounds were added, which were sensed with contact
microphones, to EEG signals in the creation of an electronic music texture [24]. Over
the next few years, Teitelbaum continued to use EEG and other biological signals in his
compositions and experiments as triggers for nascent Moog electronic synthesizers [24].
In 1970 David Rosenboom composed and performed Ecology of the Skin, in which ten
live EEG performer-participants interactively generated immersive sonic/visual
environments using custom-made electronic circuits [25].
In 2003 James Fung and Steve Mann designed a brainwave music concert, where a
computer sensed audience reaction to generate and alter to music, reacting to their
responses to the music [26].

10

2.2 EEG Systems and Technology
2.2.1 Electrodes and the 10-20 System
To achieve an appropriate EEG recording electrodes are placed on scalp based on
internationally recognized 10-20 system. This method was developed to standardize
reproducibility; Subject’s studies could be compared over time and subjects could be
compared to each other. An example of electrode positioning for 10-20 system for
Emotiv EPOC neuroheadset can be seen in figure 7. In this system electrode locations
are named to identify the lobe and numbered to identify hemisphere location. The letters
F, T, C, P and O stand for frontal, temporal, central, parietal, and occipital lobes,
respectively. Even numbers (2,4,6,8) refer to electrode positions on the right
hemisphere, whereas odd numbers (1,3,5,7) refer to those on the left hemisphere. In
addition, the letter codes A, Pg and Fp identify the earlobes, nasopharyngeal and frontal
polar sites, respectively [27,28].
2.2.2 Important Frequency Bands
The frequency range of brain waves can be recognized from 0.5 to 500 Hz [29].
Important frequency ranges that are clinically relevant are divided in to four main
group; Alpha, beta, theta and delta bands [29]. The frequency of EEG measurements
ranges from 1 to 80 Hz, with amplitudes of 10 to 100 microvolts [30].
I.) Delta (∂) has a frequency of 4 Hz or below. It is normally seen in deep sleep [31].
Delta wave pattern looks like this:

11

II.) Theta (θ) has a frequency range between 4-8 Hz. It is associated with daydreams,
lucid dreaming and light sleep states [31]. Theta wave pattern looks like this:

III.) Alpha (∝) has a frequency range between 8 Hz- 12 Hz. It is associated with
relaxation without attention and concentration. [31]. Alpha wave pattern looks like this:

IV.) Beta (β) has the frequency range between 12 and 30 Hz. It is presented in the
excited sate of mind when person is awake and with full mental activity [31]. Beta wave
pattern looks like this:

12

2.2.3 Artifacts
Artifacts are undesired signals that are detected by an EEG but not belong to a cerebral
origin. They may occur at many points during the recording process. It can be in two
types; Physiologic, arising from body sites other than brain (i.e. eye blinking, heart
beating, muscles etc.) and extraphysilogic, generated outside of the body (i.e.
loose/broken electrode) [32].
I.) Power line artifact (extraphysilogic)
The most significant noise acquired is from the surrounding electromagnetic signals.
This noise is very much higher than the interested signal, the typical value in the EEG
without artifacts is from 10 to 100 microvolts where power line is from 10 millivolts to
1 volt [31]. Their signals are greater than 50 Hz [32].
II.) Muscle artifact (EMG)
Muscle artifacts are characterized by surges in high frequency activity and are readily
identified because of their outlying high values relative to the local background activity
[31]. Their frequencies are greater than 30 Hz.
III.) Eye blinks (EOG)
Blink artifacts are attributed to alterations in conductance arising from contact of the
eyelid with the cornea [33]. Its frequency is lesser than 4 Hz.
IV.) Sweat artifact
Sweat contains water, minerals, lactate and urea. It can react with the electrodes
altering their impedance and producing an unstable baseline. Over an extensive area of
the scalp may result in a saline bridge and gives rise to low amplitude tracings (short
circuiting) [34].

13

3. State of the Art
The study of emotions in human-computer interaction has been increased in recent
years. Many methods for estimating human emotion have been proposed in the past.
The conventional methods basically utilize audio and visual attributes to model human
emotional responses, such as speech, facial expressions, and body gestures. However,
emotions are not always manifested by means of facial expressions and voice
information. Facial and voice information is related only to behavioral expression which
can be consciously controlled and modified, and which interpretation is often
subjective. Thus, other approaches to detect emotion have been proposed which focus
on different physiological information such as heart rate, skin conductance, pupil
dilation [35,36]. As compared with audio and/or visual-based methods, the responses of
biosignals tend to provide more detailed and complex information as an indicator for
estimating emotional states [37]. In addition to periphery biosignals, signals captured
from the brain in central nervous system (CNS) have been proved to provide
informative characteristics in responses to the emotional states. The ongoing brain
activity recorded using EEG provides noninvasive measurement with temporal
resolution in milliseconds. There are two lines of parallel work involved on emotion
detection with EEG: One is the pure electro physiological work to acquire data; the
second is the interpretation and understanding of the records, the separation of the
important from the unimportant and apply machine learning algorithms to classify
emotions. There have been several approaches to EEG-based emotion detection, but
there is still little consensus about definite conclusions. In table 1 overall evaluation of
the reviewed papers can be seen.
In 2000 Choppin proposed to use EEG signals for classifying six emotions based on
emotional valence and arousal [7]. He characterized positive emotions by a high frontal
coherence in alpha, and high right parietal beta power. Higher arousal (excitation) is
characterized by a higher beta power and coherence in the parietal lobe, plus lower
alpha activity, while dominance (strength) of an emotion is characterized as an increase
in the beta / alpha activity ratio in the frontal lobe, plus an increase in beta activity at the
parietal lobe.
14

In 2003 Ishino and Hagiwara [38] proposed a system that estimated subjective feeling
using neural networks to categorize emotional states based on EEG features. They
reported an average accuracy range from 54.5% to 67.7% for each of four emotional
states.
In 2004 Takahashi [36] used a headband of three dry electrodes to classify five
emotions (joy, anger, sadness, fear, and relaxation) based on multiple bio-potential
signals (EEG, pulse, and skin conductance). He trained classifiers using support vector
machines (SVM) and reports the resulting classifying accuracy both using the whole set
of bio-potential signals, and solely based on EEG signals. The experimental results
showed that the recognition rate using a support vector machine (SVM) reached an
accuracy of 41.7% for five emotions.
In 2006 Oude [39] described an approach to recognize emotion from EEG signals measured with the BraInquiry EEG PET device. She uses a limited number of electrodes
and trains a linear classifier based on Fishers discriminant analysis. She considers audio,
visual and audiovisual stimuli and trained classifiers for positive/negative, aroused/calm
and audio/visual/audiovisual.
In 2007 Heraz et al. [40] established an agent to predict emotional states during
learning. The best classification in the study was an accuracy of 82.27% for
distinguishing eight emotional states, using k-nearest neighbors as a classifier and the
amplitudes of four EEG components as features.
In 2009 Chanel et al. [41] reported an average accuracy of 63% by using EEG timefrequency information as features and SVM as a classifier to characterize EEG signals
into three emotional states.
In 2009 also Zhang and Lee [42] proposed an emotion understanding system that
classified users’ status into two emotional states with the accuracy of 73.0% ± 0.33%
during image viewing. The system employed asymmetrical characteristics at the frontal
lobe as features and SVM as a classifier.
15

In 2010 Lin at al. [43] applied machine learning techniques to categorize EEG signals
according to subject self-reported emotional states during music listening. They propose
a framework for systematically seeking emotion specific EEG features and exploring
the accuracy of the classifiers. In particular, they applied support vector machines to
classify four emotional states: joy, anger, sadness, and pleasure.
In 2012 Ramirez and Zacharias [8] proposed a method for detecting emotion from EEG
signals using the Emotiv EPOC device. In their research subjects listened selected
sounds from IADS library of emotion-annotated sounds [44]. They used arousalvalance emotion plane like in figure 6, and selected 12 sound stimuli situated in the
extremes on the plane: three positive/aroused, three positive/calm, three negative/calm,
and three negative/aroused. They measured the EEG signal in four locations in the
prefrontal cortex: AF3, AF4, F3, and they used beta/alpha ratio as an indicator of the
arousal state. For determining the valance level they compared the activation level
between the two cortical hemispheres. They applied two machine learning techniques in
order to classify emotional state of mind; Linear Discriminant Analysis (LDA) and
Support Vector Machines (SVM). For the high-versus-low arousal, and the positiveversus-negative valence classifiers the average accuracies they obtained for SVM with
radial basis function kernel classifier were 77.82%, and 80.11%, respectively.

Figure 6. Emotional states and positions in the valance/arousal plane used by Ramirez
and Zacharias (2012).

16

AUTHOR /
YEAR
CHOPPIN
/ 2000

DEVICE (used
electrodes)

STIMULE

ACCURACY

Neural
networks

EEG (13
Electrodes)

Pictures /
Sound

64%

Audio /
Video

41.7%

PREDICTION CLASSIFIER
6 Emotions

Takashi /
2004

5 Emotions

SVM

EEG (3
Electrodes),
Pulse, Skin
Conductance

OUDE
/2006

Modality,
Arousal
and
Valance
Levels

FDA and
VA-ARO

EEG PET(5
electrodesfrontal
cortex)

IADS /
IAPS

~80%

Harez et
al. / 2007

8 Emotions

k-NN

Pendant
EEG (3
electrodes)

IAPS

~80%

Lin et al. /
2010

4 Emotions

SVM

Neuroscan
(32
channels)

Music
Soundtrack

~82%

Liu Y. et
al. / 2012

6 Emotions

Fractal
Dimensions
and VAARO

Emotiv
EPOC (14
electrodes)

IADS /
IAPS

-

Ramirez
R. et al. /
2012

Arousal
and
Valance
Levels

LDA and
SVM

Emotiv
EPOC(4
electrodesfrontal
cortex)

IADS

77.82%
80.11%

Table 1. Overview of the recent studies has been done in emotion prediction using
EEG.

17

4. Materials and Methods
4.1 Materials
4.1.1 Musical Data
Six songs with emotional content of happy, sad and relax were retrieved; Three of them
(one for each emotional content) were chosen among the database of songs that were
previously classified by emotional moods by signal processing, machine learning and
information retrieval techniques [6]. Table 2 shows the songs that were selected for this
dataset. This dataset is referred as common songs in this study due to they were listened
by all of the participant subjects during the experiment. The other three songs (one for
each emotional content) were selected by subjects particularly according to their
preferences while subjects were encouraged to choose songs that will evoke desired
emotions in the experiment. Table 3 shows the songs that were selected by subjects.
This musical dataset is referred as subjects’ selected songs in this study. The first two
minutes of the each song was used during the experiment.

Common Songs

HAPPY

SAD

Anathema-Are
Ace of Base-Life Is a
You There/A
Flower/Flowers
Natural Disaster
(1998)
(2003)

RELAX
Zero 7-In the
Waiting
Line/Simple
Things (2004)

Table 2. Shows the common songs that were listened by all participating subjects.
Songs names appear as: Name of the artist-Name of the song/Title of the album (year)

18

Subjects’
Selected Songs

HAPPY

SAD

RELAX

Slipknot-Vermillion Part
Subject 1

Subject 2
Subject 3
Subject 4
Subject 5

Subject 6

Subject 7

Subject 8

Subject 9

Subject 10
Subject 11

Daft Punk- One More
Time/Discovery (2002)

2/Vol. 3: (The Subliminal
Verses) (2004)

Soggy Bottom Boys-I Am a
Man of Constant Sorrow/O Trespassers William-Love
Brother, Where Art Thou? Is Blindness/B-Sides (2011)
(2000)
Andy McKee-Rylynn/Art Andy McKee-When She
of Motion (2006)
Cries/Dreamcatcher (2004)
Bob Marley & The
Moby-Whispering
Wailers-Is This Love/Kaya
Wind/Play (2000)
(1978)
Mumford & SonsNaxxos-New Orleans/
Liar/Mumford & Sons
(2013)
(2008)

Ed Sheeran-I See
Fire/Single (2013)

Edgar-Voyage/Voyage
(2005)
Bonobo-Cirrus/The
North Borders (2013)
A-ha-Summer Moved
On/Minor Earth | Minor
Sky (2000)
The National-Slow
Show/Boxer (2007)

Explosions In The SkyYour Hand In Mine/The
Earh Is Not a Cold Dead
Place (2003)
Bohren & Der Club of
Trentemoller, Marie Fisker- Dimmu Borgir-Avmakt
Gore-Constant
Candy Tongue/Lost (2013)
Slave (?)
Fear/Black Earth (2002)
Melody Gardot-If The
Micheal Buble-Haven’t
Adele-Someone Like
Stars Were Mine/My
Met You Yet/Crazy Love
You/21 (2011)
One and Only Thrill
(2009)
(2008)
Maria Anamaterou-Den
The Beatles-She Loves
Radiohead-You Never
Ximeroneis/Opou
You/The Beatles’ Second
Wash Up After
Agapas Kai Oppu Gis
Album (1964)
Yourself/The Bends (2009)
(2011)
Extrechinato y TuChicha Libre-Gnosienne
Banda Blanca-Sopa de
Abrazado a la
No. 1/Sonido
Caracol/Single (1991)
Tristeza/Poesia Basica
Amazonico (2008)
(2011)
Dr. Dog - Shadow of
Beatles-Hey jude minor
Gabriel Faurepeople/Shame,Shame(2010)
scale
Requiem(1887)
Klingande-Jubel/Jubel EP
(2013)

Gadsdens-The Sailor
Song/Autoheart (2007)

Table 3. Shows the songs that were chosen by subjects. Songs names appear as: Name
of the artist-Name of the song/Title of the album (year).

19

4.1.2 Emotiv EPOC
Electrical activity of the brain was captured by Emotiv EPOC neuroheadset [45], which
has 14 electrodes, plus 2 electrodes for reference and noise reduction. The electrodes are
located and labeled according to aforementioned 10-20 electrode placement system
(section 2.2.1) [27]. The available electrode positions can be seen in figure 7.

Figure 7. Available electrode positions on Emotiv EPOC according to 10-20 electrode
placement system.

4.1.3 OpenViBE
OpenViBE is an open-source graphical programming language that is mainly designed
for brain-computer interface (BCI) applications. It lets user to work with over 15 EEG
acquisition devices, Emotiv EPOC is one of them [46].
In this study OpenViBE was used to retrieve EEG signals from Emotiv EPOC
neuroheadset. And furthermore OpenViBE was used to process the acquired EEG
signals in order to retrieve necessary EEG features.

20

4.1.4 Weka
Weka is an open-source data mining software, which has a collection of machine
learning algorithms and tools for data pre-processing, classification, regression,
clustering, association rules, and visualization [47].
Weka was used to classify EEG data in this study.
4.1.5 MIRtoolbox
MIRtoolbox offers a set of functions written and to be used in Matlab, dedicated to
extraction of musical features from audio files [48].
In this study musical data was processed with MIRtoolbox and necessary musical
features were retrieved.
4.1.6 MATLAB
MATLAB is an interactive environment for numerical computation, visualization and
programming [49].
In this study MATLAB was used to investigate the correlations between the EEG
features that were retrieved by OpenViBE with the musical features that were retrieved
by MIRtoolbox.

21

4.2 Methods
Methods are presented in two sections. In section 4.2.1 experiment setup is presented,
this section gives information about how the EEG data is retrieved. In section 4.2.2 data
analysis is described, this section gives information about how the musical and EEG
data are analyzed.
4.2.1 Experiment
Experiment was designed in order to retrieve proper EEG data while subjects were
listening to music from aforementioned musical datasets (refer to table 2 and table 4).
Eleven subjects, aged between 24-29, participated to the experiment. Nine of the
participated subjects were males and two was female.
Before the experiment informed consent was presented to the subjects and experiment
was explained. The subjects were informed to sit in a confortable position, keep their
eyes closed and not to move during the experiment. Emotiv EPOC device was posed on
the scalp of the subjects and guaranteed that all of the electrodes were working properly.
Subjects were presented to listen six music excerpts as the first two minutes of the
selected songs were presented. Firstly, three excerpts from common songs dataset were
presented in the order of happy, sad and relax. Timeline of this presentation can be seen
in table 4. Secondly, three excerpts from the songs they had chosen were presented in
the order of happy, sad and relax. Timeline of this presentation can be seen in table 5.
While subjects were listening first two minutes of the songs in the mentioned order, 15
seconds of silence was given after each song. During these breaks subjects were asked
to rate their emotional state relevant to in which emotional content of music they were
previously listened to. They rated their emotional state between 1 and 5. A score of 5
denotes that subject intensively felt the relevant emotional state during the listening
while a score of 1 denotes he/she didn’t feel the related emotion at all. This selfassessed emotion rating was done with oral feedback.

22

Time (s) /
Category

0-15

Silence

X

Happy

16-135

136-150

151-270

X

271-285

286-405

X

X

Sad

X

Relax

X

Table 4. Timeline of the experiment for the common songs. Table shows time intervals
in seconds and when the songs and silences were appeared during the experiment.

Time (s) /
Category

406-420

Silence

X

Happy
Sad
Relax

421-540

541-565

566-685

X

686-700

701-820

X

X
X
X

Table 5. Timeline of the experiment for the songs acquired from subjects. Table shows
time intervals in seconds and when the songs and silences were appeared during the
experiment.

23

4.2.2 Data Analyses
The data analyses are presented in two sections. Section 4.2.2.1 presents how the EEG
data was analyzed in order to classify it with the relevant emotional category. Section
4.2.2.2 presents how the audio was analyzed in order to find correlations between EEG
data.
4.2.2.1 EEG Analysis
The raw EEG data, acquired from OpenViBE software during the experiment, was
analyzed in order to provide necessary EEG features for classification and correlation
purposes. EEG signals were acquired from all available 14 electrodes of Emotive EPOC
neuroheadset; AF3, AF4, F3, F4, F7, F8, FC5, FC6, T7, T8, P7, P8, O1, O2.
Two different time epochs were applied to the retrieved raw EEG data. Firstly 1 second
of window length and 0.1 second of hop-size were applied in order to be used for EEG
classification. Secondly 1 second of window length and 1 second of hop-size were
applied in order to be used for musical features and EEG features correlations. This
second time epoch was determined due to high computational power needs for musical
feature extraction.
For each electrode their θ (theta), ∝   (alpha) and β (beta) bands were calculated by
applying band pass filter as following parameters, respectively: 4-8 kHz, 8-12 kHz, 1230 kHz.
High-level EEG features (arousal and valence variations) were calculated. Extracted
high-level features and how they were calculated can be seen it the table 6 and table 7.    
  

Totally 83 EEG features were extracted; θ, ∝,   β bands for each electrode, high-level
EEG features and the raw EEG signals as their selves.

24

EEG Features

Description

Formula

VALENCE AF

Valence value between AF4 and
AF3 electrodes

(βAF4 / ∝AF4) – (βAF3 / ∝AF3)

VALENCE
FRONTAL

Valence value between AF4, F4
and AF3, F3 electrodes

[(βAF4+βF4) / (∝AF4+∝AF4)] –
[(βAF3+βF3) / (∝AF3+∝F3)]

VALENCE ALL

Valence value between all right
hemisphere (AF4, F4, F8…) and
left hemisphere (AF3, F3, F7…)
electrodes

[(βAF4+βF4…+βO2) /
(∝AF4+∝AF4…+βO2)] –
[(βAF3+βF3…+∝O1) /
(∝AF3+∝F3…+∝O1)]

VALENCE RAW
AF4

Valence value between raw AF4
and AF3 signal

AF4 – AF3

VALENCE RAW
F4

Valence value between raw F4 and
F3 signal

F4 – F3

VALENCE RAW
F8

Valence value between raw F8 and
F7 signal

F8 – F7

VALENCE RAW
FC6

Valence value between raw FC6
and FC5 signal

FC6 – FC5

VALENCE RAW
T8

Valence value between raw T8 and
T7 signal

T8 – T7

VALENCE RAW
P8

Valence value between raw P8 and
P7 signal

P8 – P7

VALENCE RAW
O2

Valence value between raw O2 and
O1 signal

O2 – O1

AROUSAL AF

Arousal value for AF electrodes

(βAF4 + βAF3) / (∝AF4 + ∝AF3)

AROUSAL
FRONTAL

Arousal value for AF and F
electrodes

(βAF4 + βAF3 + βF4 + βF3) /
(∝AF4 + ∝AF3 + ∝F4 + ∝F3)

AROUSAL ALL

Arousal value for all available 14
electrodes

(βAF4 + βAF3 …+ βO2 + βO1) /
(∝AF4 + ∝AF3 …+ ∝O2 + ∝O1)

Table 6. High-level EEG features, their descriptions and the formulas that were used to
calculate them. Features continues in table 7

25

EEG Features

Description

Formula

AROUSAL AF3

Arousal value for AF3
electrode

βAF3 / ∝AF3

AROUSAL AF4

Arousal value for AF4
electrode

βAF4 / ∝AF4

AROUSAL F3

Arousal value for F3 electrode

βF3 / ∝F3

AROUSAL F4

Arousal value for F4 electrode

βF4 / ∝F4

AROUSAL F7

Arousal value for F7 electrode

βF7 / ∝F7

AROUSAL F8

Arousal value for F8 electrode

βF8 / ∝F8

AROUSAL FC5

Arousal value for FC5
electrode

βFC5 / ∝FC5

AROUSAL FC6

Arousal value for FC6
electrode

βFC6 / ∝FC6

AROUSAL T7

Arousal value for T7 electrode

βT7 / ∝T7

AROUSAL T8

Arousal value for T8 electrode

βT8 / ∝T8

AROUSAL P7

Arousal value for P7 electrode

βP7 / ∝P7

AROUSAL P8

Arousal value for P8 electrode

βP8 / ∝P8

AROUSAL O1

Arousal value for O1 electrode

βO1 / ∝O1

AROUSAL O2

Arousal value for O2 electrode

βO2 / ∝O2

Table 7. High-level EEG features, their descriptions and the formulas that were used to
calculate them. Continuation of table 6

26

In order to predict music category the subjects were listening to, data classification
methods were applied using the Weka program. The extracted high-level EEG features
were input to Weka and four different classification methods were applied using 10-fold
cross validation; Support Vector Machines (SVM) [50], Decision Tree, Multilayer
Perceptron (MLP) [51] and kth Nearest Neighbor (k-NN) [52].
In order to detect emotional state of the subjects the mean values of high-level EEG
features were calculated. Subjects that were satisfying accepted arousal and valence
constraints were plotted. For a subject, valence constraint is fulfilled if the happy and
relax valence averages are greater than the average for sad. The arousal constraint is
fulfilled if the happy arousal average is greater than the arousal average for both sad and
relax.
4.2.2.2 Audio Analysis
Audio analysis covers the process of musical feature extraction. Musical features were
extracted using MIRtoolbox in MATLAB environment. 43 musical features were
retrieved; 20 of them were belonging to timbre, 3 of them to dynamics, 4 of them to
pitch, 8 of them to rhythm and 8 of them to tonality properties of the music. Frame
based extraction were applied using 1 second of window length and 1 second of hopsize and also standard deviations for the necessary musical features that had array of
results were calculated. Extracted musical features, their description and their category
is given in the table 8 and table 9. The extracted musical audio datasets are mentioned in
table 2 and table 3.

27

Musical Features

Description

Musical
Category

Attack Time

Time between the start of the signal and when it reaches
its peak

Timbre

Attack Slope

Average slope of the attack time

Timbre

STD (Attack Slope)

Standard deviation of the attack slope

Timbre

Roughness Mean

Mean of the estimation of the sensory dissonance,
frequency ratio of each pair of sinusoids [53]

Timbre

Brightness Mean

Mean of the amount of energy above cut-off frequency

Timbre

STD (Brightness)

Standard deviation of the brightness

Timbre

Spectral Centroid Mean

Mean of the barycenter of the spectrum

Timbre

STD (Spectral Centroid)

Standard deviation of the spectral centroid

Timbre

Zero-Crossing Rate Mean

Mean of the number of time the signal value cross zero
axe.

Timbre

STD (Zero-crossing Rate)

Standard deviation of the zero-crossing rate

Timbre

Spectral Spread

The variance of the spectral centroid

Timbre

Spectral Skewness

Asymmetry of the spectrum around its mean value.

Timbre

Entropy Mean

Mean of the Shannon entropy of the signal

Timbre

STD (Entropy)

Standard deviation of the entropy

Timbre

Spectral Flux Mean

Mean of the distance between the spectrum of each
successive frames

Timbre

Flatness

Value of the noisiness / sinusoidality of a spectrum

Timbre

Regularity Mean

Mean of the irregularity of a spectrum

Timbre

STD (Regularity)

Standard deviation of the regularity

Timbre

MFCC Mean

Mel Frequency Cepstral Coefficients is shape of the
spectrum with coefficients. (The coefficients of the
MFCC appear in parenthesis, i.e. MFCC(1),…)

Timbre

MFCC Delta Mean

Mean of the first order if the MFCC along time

Timbre

Rms

Root-Mean Square Energy. It is the root average of the
square of the amplitude [48]

Dynamics

STD (Rms)

Standard deviation of the calculated rms.

Dynamics

Low Energy

The percentage of frames showing less-than-average
energy [53]

Dynamics

Table 8. Musical features that were extracted by MIRtoolbox, their descriptions and
their categories. Continues at table 9
28

Musical Features

Description

Musical
Category

Pitch Mean

Mean of the pitch value

Pitch

STD (Pitch)

Standard deviation of the pitch

Pitch

Inharmonicity Mean

Mean of the amount of partials that are
not multiples of the fundamental
frequency. Returns values between 0 and
1

Pitch

STD (inharmonicity)

Standard deviation of the inharmonicity

Pitch

Fluctuation

Rhythmic periodicity along auditory
channels [48]

Rhythm

Fluctuation Centroid

Centroid of the fluctuation

Rhythm

Fluctuation Peak Position Mean

Mean of the positions of the fluctuation
peaks appeared

Rhythm

Fluctuation Peak Value Mean

Mean of the fluctuation peak values

Rhythm

Tempo Mean

Mean of the Periodicities detected from
the onset detection curve [48]

Rhythm

STD (Tempo)

Standard deviation of the tempo

Rhythm

Pulse Clarity Mean

Mean of the Rhythmic clarity [54]

Rhythm

STD (Pulse Clarity)

Standard deviation of the pulse clarity

Rhythm

Chromagram Mean

Mean of the energy distribution along
pitches

Tonality

STD (Chromagram)

Standard deviation of the chromagram

Tonality

Chromagram Entropy

Entropy of the chromagram

Tonality

Key Strength Mean

Mean of the key strength along each
possible key candidate. Returns values
between -1 and +1

Tonality

STD (Key Strength)

Standard deviation of the key strength

Tonality

Mode Mean

Mean of the modality. Returns values
between -1 and +1 as close to the minor
or major modes, respectively

Tonality

STD (Mode)

Standard deviation of the mode

Tonality

HCDF

Harmonic Change Detection Function is
the flux of the tonal centroid [55]

Tonality

Table 9. Musical features that were calculated by MIRtoolbox, their descriptions and
their categories. Continuation of table 8

29

5. Results And Discussion
The results are presented in three sections. In section 5.1, subjects’ self-assessment of
their emotional response to listening to different music pieces is presented. In section
5.2 the results of EEG data classification are presented, and finally, in section 5.3 the
correlations between musical features and EEG features are reported. These results are
discussed at the end of the chapter.

5.1 Emotional State Feedback Results
The average of the subjects’ self-assessed emotional state scores is shown in table 10.
The scores range between 1 and 5. A score of 5 denotes that a subject intensively felt
the relevant emotional state during music listening while a score of 1 denotes he/she
didn’t feel the related emotion at all. The common happy song was proved to be the
least emotion-evoking song as the average score for it is 2.6 out of 5. The common sad
song has an average score of 3.3 while the common relax song has an average score of
4.1 out of 5. At the same time, subject chosen songs produced more intense emotions
(subject-happy, subject-sad and subject-relax songs averages score of 4.6, 4,4 and 4.6,
respectively).

Common Common Common
Happy
Sad
Relax
2.6

3.3

4.1

Subject
Happy

Subject
Sad

Subject
Relax

4.6

4.3

4.6

Table 10. The average rates of feedbacked emotional state that music’s evoked while
subjects were listening to relevant songs.

30

5.2 EEG Classification Results

EEG data obtained from 6 different music categories (happy, sad, relax categories for
common and subject selected songs) was used to train classifiers with Support Vector
Machines (SVM), decision tree, Multilayer Perceptron (MLP) and k Nearest Neighbor
(k-NN) classification algorithms to predict the music category. Only high-level EEG
features (table 6 and table 7) were used to train the classifiers. The reported results were
obtained using 10-fold cross validation.
The obtained accuracy results, for each subject and for each classification method, are
shown in table 11. K-NN and decision tree algorithms obtained the best accuracies with
averages of 98.2% and 88.9%, respectively. Average accuracies of Multilayer
Perceptron (MLP) and SVM are 75.8% and 53.5%, respectively while the base-line
accuracy is 16.6.

Subject #

SVM %

Decision Tree %

MLP %

k-NN %

Subject 1

78.9

93.7

84

99.3

Subject 2

50.5

90.3

71

98.7

Subject 3

45.9

89.7

71.4

98.7

Subject 4

34.2

79.3

81.9

99.1

Subject 5

60.6

90.6

80.7

99.1

Subject 6

75

92.7

85.3

99.5

Subject 7

45.21

81

62.7

97.2

Subject 8

38.7

85.4

68.9

94.6

Subject 9

40.1

92.7

72.5

96.7

Subject 10

55.4

91.7

75.5

98

Subject 11

64

90.4

80.1

99.3

Average

53.5

88.9

75.8

98.2

Table 11. Classification results of 6 musical category with high-level EEG attributes
using classifiers: SVM, Decision Tree, MLP, and k-NN with 10 folds cross-validation
while the base-line classification accuracy is 16.6
31

For illustration purposes, the confusion matrix showing the classification accuracies
results of the multilayer perceptron classifier for subject 10 is shown in table 12. Due to
overall tendency of confusion matrixes stayed similar for each subject and for each
classification method the rest of the matrixes were not included here. For each category
there were 1270 instances. As the confusion matrix shows the correctly predicted
instances were above 907 for each category for subject 10 with MLP classifier. For
instance, 1207 instances of happy common EEG features were predicted correctly and
the rest 63 instances were mispredicted as sad common.

Predicted
Category Happy
Sad
Relax
/ Real
Common Common Common
Category

Happy
Subject

Sad
Subject

Relax
Subject

Happy
Common

1207

63

0

0

0

0

Sad
Common

54

1108

53

11

42

2

Relax
Common

32

36

1011

74

69

48

Happy
Subject

0

5

45

945

111

164

Sad
Subject

0

18

95

155

907

95

Relax
Subject

0

7

26

244

68

925

Table 12. Confusion matrix of the subject 10. Multilayer perceptron used as a classifier
with 10-fold cross validation

32

The mean value of each high-level EEG feature for each category was calculated in
order to plot how many subject satisfied the valence and arousal constraints as appears
in figure 6. For a subject, the valence constraint is fulfilled if the happy and relax
valence averages are greater than the average of the valence for sad. The arousal
constraint is fulfilled if the happy arousal average is greater than the arousal average for
both sad and relax. These constraints represent the expected patterns among arousal and
valence values for the three categories. In table 13, the high-level EEG features that
satisfied these constraints for at least 5 subjects in one category are shown. In addition,
the important arousal and valence features are shown in the table, as they were found
relevant for mode estimation in previous studies [26,8]. The important arousal and
valence features included in the table even they weren’t satisfied by 5 subjects are
Arousal AF, Arousal Frontal, Arousal All, Valence AF and Valence All.

Category

AROUSAL AROUSAL AROUSAL AROUSAL AROUSAL AROUSAL AROUSAL AROUSAL AROUSAL
AF
FRONTAL
ALL
AF3
FC5
FC6
O1
T7
F7

Common
Songs

4

4

4

5

7

6

3

5

5

Subject
Selected
Songs

2

3

4

1

3

5

6

5

0

Category

VALENCE VALENCE VALENCE VALENCE VALENCE VALENCE AROUSAL AROUSAL
AF
FRONTAL
ALL
RAW F4
RAW T8
Raw P8
P7
F8

Common
Songs

3

6

4

5

5

5

5

5

Subject
Selected
Songs

3

4

4

2

3

2

4

4

Table 13. Number of subjects that satisfies arousal and valence constrains. High-level
EEG feature mean values that satisfied the arousal and valence constraints. The highlevel EEG features were chosen among EEG features that were found relevant in
previous studies [26,8] and from the features that had at least 5 subjects in one of the
category

33

5.3 Musical Features and EEG Features Correlations
Musical features and EEG features correlations results are shown in three sections.
Firstly, correlations for the subjects’ selected songs will be shown, secondly
correlations of the common songs will be shown, and finally a common EEG feature
found coherent for common and subjects’ selected songs will be shown. The musical
features used to obtain these results were mentioned in the previous chapter. All of the
calculated EEG features are included in these results; used EEG features were θ, ∝  and  
β bands of each of the 14 electrodes, raw signals of the electrodes and high-level EEG
features as mentioned in table 6 and table 7. The EEG data used in these results was
obtained by using 1 seconds of window size and 1 seconds of hop-size, as it was the
case for calculating the features of the musical data.
In order to obtain the most correlated features some thresholds were chosen. The
correlation dataset that is used in the following sections was chosen among the
correlations that had correlation coefficient values greater than 0.1 and p-values lower
than 0.05.

34

5.3.1 Subjects’ Selected Songs
Correlations that have more than 0.25 correlation coefficient, with p-values of less than
0.05 and which were present in at least 7 subjects are shown in figure 8. In total, there
were 10 different music-EEG feature pairs, which were satisfied these constraints.
Among all 10 pairs, β-T7 EEG feature was found correlated with different musical
features. Rms slope and STD (Rms) musical features provided the best correlation
coefficient value as 0.325 and 0.317, and with a p-value of 0.006 and 0.0001,
respectively (7 subjects fulfilled the required constraints). Entropy and β-T7 music-EEG
feature pair was found correlated in 9 subjects with a correlation coefficient value of
0.28 and with a p-value of 0.001. MFCC (1), Rms, Spectral Centroid, MFCC (2) and
zero-crossing rate were found correlated in 8 subject with correlation coefficient values
of 0.292, 0.287, 0.266, 0.263, 0.252 and with p-values of 0.002, 0.008, 0.0009, 0.003,
0.001, respectively. Spectral Flux and MFFC(3) features were founded correlated in 7
subjects with correlation coefficient values of 0.273 and 0.253, and with p-values of
0.001, 0.003, respectively.

Figure 8. Correlation pairs between subject’s selected song’s musical features and their
related EEG features. The plotted features were selected among correlations that had
more then 0.25 correlation coefficient value, had less then 0.05 P value and that were
satisfied by at least 7 subject

35

5.3.2 Common Songs
The correlations between the musical features of the songs that were presented to all of
the subjects and their EEG data is shown in figure 9 and figure 10. The same constraints
were applied to plot correlations with the subject’s selected songs; The plotted features
were selected among correlations that had more then 0.25 correlation coefficient value,
had less then 0.05 p-value and that were satisfied by at least 7 subjects.
There were a total of 26 correlations that satisfied the constraints. The highest
correlations were seen between Flatness & Valence Raw F8, Entropy & Valence Raw
FC6, MFCC(1) & β-T7, Spectral Centroid & Valence Raw FC6 with the correlation
coefficient values of 0.323, 0.321, 0.315, 0.314 in 7,8,7,8 subjects, respectively and
with P-values below 0.01.

36

Figure 9. Correlation pairs for common songs’ musical features and the EEG features.
The plotted features were selected among correlations that had more then 0.25
correlation coefficient value and that had more than 6 subjects. The figure continues at
figure 10.

Figure 10. Continuation of figure 9. Shows correlations that had more than 0.25
correlation coefficient and had more than 6 subjects.

37

Even though there were 26 correlations the obtained p-values were not as low as the
ones obtained in subjects’ selected songs. In order to provide better p-values another
constraints were determined; by lowering the subject limit to 6 and raising the
correlation coefficient limit to 0.3. New pairs that were satisfying this constraint is
shown in figure 11. In this case, Entropy & β-T7, Spectral Centroid & β-T7, Flatness &
Valence Raw F4, Flatness & ∝ FC6, MFCC (3) & Valence Raw FC6 showed higher
correlation coefficient values and lower p-values (<0.005). The correlation coefficient
values were 0.372, 0.369, 0.351, 0.336, and 0.324, respectively while each correlation
satisfied by 6 subjects.

Figure 11. As an addition to figure 9 and figure 10 correlation pairs that had more than
0.3 correlation coefficient value and that had at least 6 subjects.

38

5.3.3 Mutual EEG Feature Correlated With Subjects’ Selected Songs and
Common Songs Musical Features

The β T7 EEG feature appeared in all of the correlations that satisfied the determined
constraints for the subjects’ selected songs musical features and EEG features
correlation. And it appeared 5 times in the common songs musical features and EEG
features correlation for the first constraint among 26 correlations and for the second
constraint determined it appeared 4 times among 8 correlations. Table 14 shows the
mutual β T7 EEG features and musical features that had more than 0.25 correlation
coefficient, had less than 0.05 p-value and at least satisfied by 6 subject or if the
correlation was satisfied more than 6 subjects. Entropy, MFCC (1), MFCC (3), RMS
Slope, RMS, Spectral Centroid, Spectral Flux and Zero-crossing Rate were the musical
features that satisfied those constrains with the p-values below 0.008.

39

Correlation

#Subjects Out of 11

Correlation
Coefficient Mean

P Value Mean

Entropy&β_T7

9

0.278161

0.003102

Entropy&β_T7

6

0.371804

0.000071

MFCC(1)&β_T7

8

0.291541

0.002659

MFCC(1)&β_T7

7

0.315388

0.009845

MFCC(3)&β_T7

7

0.253211

0.003368

MFCC(3)&β_T7

8

0.249528

0.004211

Rms_Slope&β_T7

7

0.324841

0.006283

Rms_Slope&β_T7

6

0.310533

0.003302

Rms&β_T7

8

0.286898

0.008007

Rms&β_T7

6

0.261657

0.006183

Spc_Centroid&β_T7

8

0.265806

0.000909

Spc_Centroid&β_T7

6

0.369254

0.003205

Spc_Flux&β_T7

7

0.273208

0.001889

Spc_Flux&β_T7

7

0.297838

0.007446

Zero_Cross&β_T7

8

0.252043

0.001439

Zero_Cross&β_T7

7

0.224081

0.008915

Table 14. Shows the mutual β_T7 EEG feature correlation with subject chosen song
features and common song features. The black written correlations represent subjects’
chosen songs and blue written correlation represents common songs. Correlations were
chosen if their correlation coefficient value was above 0.25, P value was below 0.05 and
at least satisfied by 6 subjects or if the correlation was satisfied at least by 7 subjects.

40

5.4 Discussion
Discussion is divided in to two parts. In the first part EEG classification results will be
discussed and in the second part musical and EEG features correlations results will be
discussed.
5.4.1 EEG Classification Results Discussion
Obtained classification accuracies for the 6 categories were high; using k-NN
classification method a 98.2% of mean accuracy was obtained. Although each category
was predicted well with using different classifier methods, categories that share same
emotional music type did not show any relation. The expected result was common
songs and subjects’ selected songs would evoke similar emotional states (for instance
happy common song and happy subject selected song will evoke same emotional states)
and classification results between them would show some commonality. In fact time
might had been played a role in the classification. In the confusion matrix presented in
table 12, correctly predicted and mispredicted classes could be seen. There was a time
dependent tendency on mispredicted instances. As time interval gets wider between
classes the mispredicted instances get lower. The expected results were as mentioned
before to see more mispredicted instances along the classes that share same emotional
content of music.
Also subjects’ arousal and valence values could not presented a proof that listened
music was evoking a relevant emotion. The number of subjects that satisfied the
expected arousal and valence properties is given in table 13. Less then half of the
subjects provide the expected arousal and valence properties.

41

5.4.2 Musical and EEG Features Correlation Discussion
The subjects’ selected songs’ musical features showed the highest correlations only with
β T7 EEG feature along 10 distinct correlation with an average of p-value smaller than
0.004 and with an average correlation coefficient greater than 0.28. Musical features
that were correlated with β T7 EEG feature were MFCC (1), Entropy, Spectral Flux,
Spectral Centroid, MFCC (2), MFCC (3) and Zero-crossing Rate which were belonging
to timbral properties of the music while Rms Slope, STD (Rms), Rms were related with
dynamic properties of the music.
Common songs, with the same constraints applied to the subjects’ selected songs,
showed more correlations. There were 26 correlations that satisfied the constraints, 22
of them belonging the timbral, 3 of them belonging to pitch, and 1 of them belonging to
dynamics properties of music. Unlike the subjects’ selected songs, 5 of the correlations
were belonging to β T7 EEG feature. And with the 8 correlations Valence Raw FC6 was
the most appeared EEG feature among correlations. The reason why common songs
showed more correlation than subjects’ selected songs could be because of subjects
weren’t expect to listen these songs, songs were surprise for them and/or even they
might not heard the songs before. On the other hand subjects were ready to hear their
choice of songs and they might not be evoked much as common songs. So the common
music might have a better effect on state of the mind due to there were 2.6 times more
correlations than the subjects’ selected songs correlations.
The musical features of common and subjects’ selected songs showed mutual
correlations between β T7 EEG feature. From the musical features that were proved to
be mutual, Entropy, MFCC (1), MFCC (3), Spectral Centroid, Spectral Flux, Zero
Crossing rate belong to timbral characteristic of the music, and Rms and Rms Slope
belong to dynamics characteristic of the music.

42

6. Conclusions
In this study, music and emotion was investigated using electroencephalography (EEG)
and music information retrieval (MIR) techniques. An experiment was designed in
order to obtain EEG data while subjects were listening to music in six categories with
three emotional content (happy, sad and relax); Three of the songs were preselected by
us, while the other three was selected by subjects according their preference. 11 subjects
participated to the experiment. Firstly EEG data was analyzed in order to classify these
6 categories. Machine learning techniques were applied to train classifiers to predict the
category based on the EEG data of the subjects. The used algorithms for this purpose
were SVM, Decision Tree, MLP and k-NN. The EEG data was also analyzed, with
various arousal and valence features, in order to show if the EEG features present the
emotional states of the subjects. Secondly, music audio was analyzed in order to show if
the EEG features and musical features were correlated along time.
We manage to classify our 6 different categories with high accuracy. The achieved
average accuracies were 55% for SVM, 88% for Decision Tree, 75% for MLP and 98%
for k-NN. But we couldn’t find relations between the categories with the same
emotional content. Arousal and valence values alone were not able to satisfactorily
describe categories with same emotional content whereas subjects’ self reported
emotional states indicate the contrary.
Some of the musical features were found to be correlated with EEG features. The most
important EEG feature that was found to be correlated with musical features was β T7.
The most correlated musical features with EEG features were timbral features while
pitch, rhythm and tonality hadn’t shown that much correlation with EEG features. β T7
showed correlation with 8 different musical features among both subjects’ selected
songs and common songs, with an average 0.29 correlation coefficient and 0.004 pvalue.

43

7. Future Works

The role of window and hop-size wasn’t investigated in this research. Musical and EEG
features might be calculated with different window and hop-size and their effects on
results might be investigated.
In the experiment the duration of one song excerpt was 120 seconds and maybe for the
subject it was very hard to evoke the expected emotion during the entire song. The most
emotionally relevant time interval of the EEG data could be marked and it could be used
for EEG classification and even in musical and EEG features correlation calculations.
This should be investigated further.
Feature selection methods could be applied to EEG data for classifying the type of the
songs subjects were listening to. This could provide useful information for detecting
which EEG features are the most relevant for discriminating the different categories.

44

References
[1] Misic, P., D. Arandjelovic, S. Stanojkovic, S. Vladejic, and J. Mladenovic. "Music
Therapy." European Psychiatry 1.25 (2010): 839. Academic Search Premier. Web.
(2011)
[2] Antrim, Doron K. "Music Therapy." The Musical Quarterly 30.4 (2006): 409.
JSTOR. Web. 9 (2011)
[3] Amber Haque, "Psychology from Islamic Perspective: Contributions of Early
Muslim Scholars and Challenges to Contemporary Muslim Psychologists", Journal of
Religion and Health 43 (4): 357-377 [363] (2004).
[4] Johnson-Laird, P.N. & Oatley, K., Emotions, music, and literature. In L. Feldman
Barrett, M. Lewis & J. Haviland-Jones (Eds.). Handbook of Emotions, Third Edition
(2008)
[5] B.-K. Min, M. J. Marzelli, and S.-S. Yoo, “Neuroimaging-based approaches in the
brain-computer interface.,” Trends in biotechnology, vol. 28, no. 11, pp. 552– 560,
Nov. 2010.
[6] Serra, X., Larier C., Automatic Classification of Musical Mood by Content-Based
Analysis. PhD thesis, Dept. of Information and Communication Technologies,
Universitat Pompeu Fabra, Barcelona, Spain [](2011)
[7] Choppin, A.: Eeg-based human interface for disabled individuals: Emotion
expression with neural networks. Masters thesis, Tokyo Institute of Technology,
Yokohama, Japan (2000)
[8] Rafael Ramirez, Zacharias Vamvakousis: Detecting Emotion from EEG Signals
Using the Emotive Epoc Device. F.M. Zanzotto et al. (Eds.): BI 2012, LNCS 7670, pp.
175–184 (2012)
45

[9] Chanel, G., Kronegg, J., Grandjean, D., Pun, T.: Emotion Assessment: Arousal
Evaluation Using EEG’s and Peripheral Physiological Signals. In: Gunsel, B., Jain,
A.K., Tekalp, A.M., Sankur, B. (eds.) MRCS 2006. LNCS, vol. 4105, pp. 530–537.
Springer, Heidelberg (2006)
[10] Howes, George J.: "The phylogenetic relationships of the electric catfish family
Malapteruridae (Teleostei: Siluroidei)". Journal of Natural History 19: 37–67.
doi:10.1080/00222938500770031.(1985)
[11] Kellaway P.: Bull. Hist. Med. 20: 112-37. (1946)
[12] Gilbert W (1600): De Magnete, Magneticisque Corporibus, et de Magno Magnete
Tellure; Physiologica Nova Plumiris et Argumentis et Experimentis Demonstrata, Peter
Short, London. (Transl. SP Thompson, London: The Gilbert Club, 1900: facsimile ed.
New York: Basic Books, 1958: transl. PF Mottelay, 1893, facsimile ed.: Dover, New
York, 1958.)
[13] Geddes LA: Kouwenhoven WB. Med. Instrum. 10:(2) 141-3.(1976)
[14] Galvani L.: De viribus electricitatis in motu musculari. Commentarius. De
Bononiesi Scientarium et Ertium Instituto atque Academia Commentarii 7: 363-418.
(Commentary on the effects of electricity on muscular motion. Burndy Library edition,
1953, Norwalk, Conn.).(1791-1953)
[15] Matteucci C: Sur le courant électrique où propre de la grenouille. Second memoire
sur l'électricité animale, faisant suite à celui sur la torpille. Ann. Chim. Phys. (2ème
serie), 67: 93-106. (1838)
[16] Andrew Brouse: A young person’s guide to brainwave music: forty years of audio
from the human EEG. (2004)

46

[17] Thomas F. Collura: History and Evolution of Electroencephalographic Instruments
and Techniques, Journal of Clinical Neurophysiology 10(4):476-504, Raven Press, Ltd.,
New York.(1993)
[18] Larry R. Squire: The history of neuroscience in autobiography, Volume 1. Society
for Neuroscience, 1996.
[19] Adrian E, Matthews B.:The Berger rhythms: potential changes from the occipital
lobes in man (1934)
[20] Chalupa, L.M.: Obituaries: Donald B. Lindsley. American Psychologist, (60)2,
193-194 (2005)
[21] http://www.acns.org/about-acns/history
[22] R. Francès, The Perception of Music, trans. from the 1958 edition by W.J. Dowling
(Hillsdale, NJ: Lawrence Erlbaum Associates) (1988).
[23] Kamiya, 1969, 1994; Gaarder & Montgomery, p. 4 (1977)
[24] Andrew Brouse: A young person’s guide to brainwave music: forty years of audio
from the human EEG. (2004)
[25] Lucier, "Statement On: Music for Solo Performer" (1971), in D. Rosenboom, ed.,
Biofeedback and the Arts, Results of Early Experiments, 2nd Ed. (Vancouver: Aesthetic
Research Centre of Canada Publications) pp. 60-61. (1976)
[24] R. Teitelbaum, "In Tune: Some Early Experiments in Biofeedback Music (196674)" (1974)
[25] http://davidrosenboom.com/media/ecology-skin
[26] http://wearcam.org/deconcert/
47

[27]Trans Cranial Technologies Ltd., “10 / 20 System Positioning Manual.”
Trans Cranial Technologies ldt., Wanchai, Hong Kong, p. 20, (2012)
[28] Teplan M.. Fundamentals of EEG Masurement. Measurement Science Review, vol.
2, pp. 1–11, 2002
[29] Roy Sucholeiki: Normal EEG Waveforms. (2008, November)
[30] Kandel, E.R., Schwartz, J.H., Jessell, T.M.: Principles of Neural Science. Mc Graw
Hill (2000)
[31] Manuel, I., & Núñez, B.: Artifact Detection. Brain (2010, June)
[32] Fatourechi, M., Bashashati, A., Ward, R.K., Birch, G.E.: EMG and EOG artifacts
in brain computer interface systems: A survey. Clininical Neurophysiology (118), 480–
494 (2007)
[33] D.A. Overton and C. Shagass, "Distribution of eye movement and eye blink
potentials over the scalp," Clinical Neurophysiology, 1969.
[34] Neuro Care Launches - Common Artifacts in electroencephalography. [Online].
http://www.neurocarelaunches.com/learningex/neurology/ICU/clinical/artifact.htm
[35] Partala, T., Jokiniemi, M., Surakka, V.: Pupillary responses to emotionally
provocative stimuli. In: ETRA 2000: Proceedings of the 2000 Symposium on Eye
Tracking Research & Applications, pp. 123–129. ACM Press, New York (2000)
[36] Takahashi, K.: Remarks on emotion recognition from bio-potential signals. In:2nd
International Conference on Autonomous Robots and Agents, pp. 186–191 (2004)

48

[37] J. Kim and E. Andre, “Emotion recognition based on physiological changes in
music listening,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 30, no. 12, pp. 2067–
2083, Dec. 2008.
[38] K. Ishinoand M. Hagiwara,“A feeling estimation system using a simple
electroencephalograph,” in Proc. IEEE Int. Conf. Syst., Man Cybern., vol. 5, pp. 4204–
4209. (2003)
[39] Bos, D.O.: EEG-based Emotion Recognition: The Influence of Visual and
Auditory Stimuli (2006)
[40] A. Heraz, R. Razaki, and C. Frasson, “Using machine learning to predict learner
emotional state from brainwaves,” in Proc. 7th IEEE Int. Conf. Adv. Learning Technol.,
pp. 853–857.(2007)
[41] G. Chanel, J. J. M. Kierkels, M. Soleymani, and T. Pun, “Short-term emotion
assessment in a recall paradigm,” Int. J. Human-Comput. Stud., vol. 67, no. 8, pp. 607–
627, Aug. 2009.
[42] Q. Zhang and M. H. Lee,“Analysis of positive and negative emotions in natural
scene using brain activity and GIST,” Neurocomputing, vol. 72, no. 4–6, pp. 1302–
1306, Jan. 2009.
[43] Lin, Y.-P., Wang, C.-H., Jung, T.-P., Wu, T.-L., Jeng, S.-K., Duann, J.-R., Chen,
J.-H.: EEG-Based Emotion Recognition in Music Listening. IEEE Transactions on
Biomedical Engineering 57(7) (2010)
[44] Bradley, M.M., Lang, P.J.: International Affective Digitized Sounds (IADS):
Stimuli, Instruction Manual and Affective Ratings. The Center for Research in
Psychophysiology, University of Florida, Gainesville, FL, USA (1999)
[45] Emotiv Systems Inc. Researchers, http://www.emotiv.com/researchers/

49

[46] Y. Renard, F. Lotte, G. Gibert, M. Congedo, E. Maby, V. Delannoy, O. Bertrand,
and A. Lécuyer, “OpenViBE: An Open-Source Software Platform to Design, Test,
and Use Brain–Computer Interfaces in Real and Virtual Environments,” Presence
Teleoperators and Virtual Environments, vol. 19, no. 1, pp. 35–53, 2010.
[47] E. Frank, M. Hall, and G. Holmes, “Weka-a machine learning workbench for
data mining,” Data Mining and ..., pp. 1–11, 2010.
[48] Lartillot, O. & Toiviainen, P.. Mir in matlab (ii): A toolbox for musical feature
extraction from audio. In Proceedings of ISMIR 2007. Vienna, Austria.
[49] http://www.mathworks.com/products/matlab/
[50] Cristianini, N., Shawe-Taylor, J.: An Introduction to Support Vector Machines.
Cambridge University Press (2000)
[51] Rosenblatt, Frank. x. Principles of Neurodynamics: Perceptrons and the Theory of
Brain Mechanisms. Spartan Books, Washington DC, 1961
[52] Altman, N. S: An introduction to kernel and nearest-neighbor nonparametric
regression. The American Statistician 46 (3): 175–185. (1992)
[53] Tzanetakis, Cook. Musical genre classification of audio signals. IEEE T r. S peech
and Audio Processing, 10(5),293-302, 2002.
[54] Olivier Lartillot, Tuomas Eerola, Petri Toiviainen, Jose Fornari, "Multi-feature
modeling of pulse clarity: Design, validation, and optimization", International
Conference on Music Information R etrieval, Philadelphia, 2008.
[55] Plomp & Levelt "Tonal Consonance and Critical Bandwidth" Journal of the
Acoustical S ociety of America, 1965.

50

