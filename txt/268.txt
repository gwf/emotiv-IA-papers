arXiv:2006.06961v2 [cs.CV] 18 Jun 2020

The eyes know it: FakeET- An Eye-tracking Database to
Understand Deepfake Perception
Parul Gupta

Komal Chugh

Indian Institute of Technology Ropar
2016csb1048@iitrpr.ac.in

Indian Institute of Technology Ropar
2016csb1124@iitrpr.ac.in

Abhinav Dhall

Ramanathan Subramanian

Monash University/Indian Institute of Technology
Ropar
ahinav.dhall@monash.edu

Indian Institute of Technology Ropar
s.ramanathan@iitrpr.ac.in

ABSTRACT

1

We present FakeETâ€“ an eye-tracking database to understand
human visual perception of deepfake videos. Given that the
principal purpose of deepfakes is to deceive human observers,
FakeET is designed to understand and evaluate the ease with
which viewers can detect synthetic video artifacts. FakeET
contains viewing patterns compiled from 40 users via the
Tobii desktop eye-tracker for 811 videos from the Google
Deepfake dataset, with a minimum of two viewings per video.
Additionally, EEG responses acquired via the Emotiv sensor
are also available. The compiled data confirms (a) distinct eye
movement characteristics for real vs fake videos; (b) utility of
the eye-track saliency maps for spatial forgery localization and
detection, and (c) Error Related Negativity (ERN) triggers
in the EEG responses, and the ability of the raw EEG signal
to distinguish between real and fake videos.

The commonplace availability of image and video forgery
software has led to the widespread creation of image and
video-based deepfakes for purposes such as trolls, disinformation campaigns and political propaganda. Even if deepfakes
may not completely mislead people, they nevertheless contribute to uncertainty and distrust of media content, posing
a significant challenge to democratic societies [22]. Therefore,
artificial intelligence (AI)-based fake detection (FD) techniques are critical for governments to inform citizens and
shape public policy.
While human performance has for long been the â€˜gold
standardâ€™ for AI methods to match and surpass, deepfakes
are expressly designed with the objective of spoofing human
detection. Indeed, computers can examine visual information
that are inconspicuous to humans [16]. Therefore, it would
be reasonable to envisage optimized FD via human-machine
cooperation; e.g., information forensic experts could hypothesize probable forgeries in an image/video with relative ease,
while state-of-the-art AI methods could follow-up to validate these hypotheses. While user impressions have been
utilized to evaluate the efficacy of forgery techniques [8, 17],
user behavioral cues have never been utilized for FD to our
knowledge. Specifically, we argue that implicit user cues such
as gaze patterns and neural responses can be very useful in
this regard, as they can be acquired with minimal effort via
wearable sensors. As evidence of this argument, we present
FakeET1 , a dataset of gaze and EEG recordings acquired
from 40 users exposed to Google Deepfake [4] videos.
Figure 1 presents two exemplar frames from a real and
fake video, and corresponding gaze heatmaps. The deepfake
video includes forgeries localized to the mouth region, and
the original and forged image regions are denoted via green
and red rectangles. When users are posed with the fake video
detection task, they explore the scene for visual irregularities
while viewing a real video; conversely, an explanatory viewing
behavior is observed when users infer a forgery candidate.
Their subsequent visual processing is focused on verifying
if the probable forged region is indeed different from its
neighborhood. This results in diffused gaze patterns for real
videos, and more focused patterns for fake videos.

CCS CONCEPTS
â€¢Human-centered computing â†’ HCI theory, concepts and
models;

KEYWORDS
deepfake, visual perception, eye-tracking, EEG
ACM Reference format:
Parul Gupta, Komal Chugh, Abhinav Dhall, and Ramanathan
Subramanian. 2016. The eyes know it: FakeET- An Eye-tracking
Database to Understand Deepfake Perception. In Proceedings of
ACM Conference, Washington, DC, USA, July 2017 (Conferenceâ€™17), 9 pages.
DOI: 10.1145/nnnnnnn.nnnnnnn

Permission to make digital or hard copies of all or part of this work
for personal or classroom use is granted without fee provided that
copies are not made or distributed for profit or commercial advantage
and that copies bear this notice and the full citation on the first page.
Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy
otherwise, or republish, to post on servers or to redistribute to lists,
requires prior specific permission and/or a fee. Request permissions
from permissions@acm.org.
Conferenceâ€™17, Washington, DC, USA
Â© 2016 ACM. 978-x-xxxx-xxxx-x/YY/MM. . . $15.00
DOI: 10.1145/nnnnnnn.nnnnnnn

1

INTRODUCTION

which would be made publicly available on paper acceptance

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.
Overall, this work makes the following research contributions: (1) To our knowledge, FakeET represents the first
compilation of user behavioral responses for deepfake detection; (2) Distinctive patterns characteristic of information
irregularities can be observed for both the eye-gaze and EEG
modalities, and (3) Through extensive experimental analysis,
we show the utility of both the eye-gaze and EEG modalities
in enabling automated deepfake detection.

2

RELATED WORK

We review works that have (a) attempted FD by mining
multimedia content, and (b) examined user impressions of
synthesized deepfakes in this section.

2.1

Figure 1: Real vs fake video viewing patterns: Two frames
and eye-track heatmaps are shown for a real (left column)
and a manipulated video (right column). Original and forged
mouth regions are shown via green and red rectangles. We
observed an exploratory viewing behavior for real videos, and
an explanatory viewing pattern for fake videos. This results
in diffused gaze for real vs compact gaze for fake videos; e.g.,
visual attention to the onlooker and other scene characters can
be noted for the real frames, but is limited to the manipulated
face in the fake video frames. View in color and under zoom.

The exploratory vs explanatory viewing patterns can be
quantified via a number of eye track-based measures such as
number of fixations (more fixations on real videos), scan-path
length (longer scan-paths on real videos), repeated fixations
conveying propensity for scene exploring (fewer repeated
fixations on fakes) and fixation entropy indicating consistency/stochasticity in visual processing (lesser entropy for
fakes). Evidently, these are useful user-centric features for the
purpose of detecting deepfakes. Furthermore, neural triggers
such as Error-Related Negativity (ERN), which occur when
users become aware of erroneous information, are observable
from EEG patterns acquired for fake videos. Via experiments,
we demonstrate that (a) gaze heatmaps can act as a reliable
prior by deep neural networks for examining forgeries, and
(b) the EEG data acquired during real/fake video viewings
can achieve better-than-chance FD performance.

Content-based FD

Since deepfakes have gained popularity in recent years, there
has been increasing number of efforts in designing robust
forgery detectors. Some of the initial works focused on inconsistencies exhibited in the physical or physiological aspects
in the deepfake content. For example, [12] used relatively
simple visual aspects such as eye colour, missing reflections,
and missing details in the eye and teeth areas. Similarly, [10]
exploited the observation that many deepfake videos lack
reasonable eye blinking due to the use of online portraits as
training data. Detection systems based on head movements
have also been proposed in the literature. Yang et al. in
[26] proposed that 3D head poses estimated from a face image can be used to detect errors introduced due to splicing
synthesised face regions into the original image in deepfake
creation.
Some of the recent works have employed Deep Neural
Networks (DNNs) for distinguishing between real and fake.
In [13], Nguyen et al. proposed a CNN system that uses
multi-task learning to simultaneously detect fake videos and
locate the manipulated regions. The network was based on
an auto-encoder. Another interesting research by Li and Lyu
[11] hypothesised that manipulation techniques can create
images of limited resolution, which need to be further warped
to match the original faces in the source video which leaves
artifacts. Hence, they use CNNs to detect artifacts from
the detected face regions and the surrounding areas. Afchar
et al. [1] proposed MesoInc-4, an inception inspired [21]
convolutional neural network with a small number of layers.
A capsule network architecture is proposed in [14] which
requires fewer parameters to train than very deep networks.

2.2

User-centered Evaluation

The user studies conducted for the field of Deepfakes have
aimed to evaluate human performance in the task of forgery
detection. For instance, FaceForensics++ [17] conducted the
study as per-frame binary classification problem where each
user was instructed to classify an image as real or fake in a
limited time period. Different types of image qualities were
used and the average human performance was observed to
decrease with deteriorating quality. DeeperForensics-1.0 [8],
another large-scale dataset asked the users about the realness

The eyes know it: FakeET- An Eye-tracking Database to Understand Deepfake Perception
Conferenceâ€™17, July 2017, Washington, DC, USA
3.1.1 Videos. We used the Google/Jigsaw DeepFake Detection dataset [4] for our user study. This dataset comprises
3068 fake and 363 real videos, out of which 811 videos (331
real, 480 fake) were used in our study. Furthermore, the
fake videos were categorized into easy and difficult-to-detect
forgeries based on visual inspection; this taxonomy resulted
in 244 easy and 236 difficult-to detect fake videos.
3.1.2 Participants. : 40 subjects (28 male, age 24.5 Ã‚Å›
4.3 and 12 female, age 22.8 Ã‚Å› 6.8) took part in our study.
Most subjects were university graduate and undergraduate
students, who were naive to the purpose of our study. All
subjects had normal or corrected vision, and provided informed consent for participation as per the ethics approval
guidelines. Participants were paid a nominal fee as a token
of appreciation.

Figure 2: Overview of our user-study protocol.

of a video clip and provide rating on a scale of five. Video
clips from various other datasets were included as well and the
main purpose of this study was to examine the quality of their
dataset as compared to others. Another interesting study
which explains the contribution of deepfakes in spreading
misinformation is that of [23]. The authors conclude that
deepfakes sow uncertainty in the viewersâ€™ mind which in
turn can lead to reduction of trust in news on social media
platforms.

2.3

Analysis of Related Work

Based on the prior work, we note that hardly any user-centric
FD methods exist; users have only been utilized to evaluate
the efficacy of deepfakes. Most FD methods are purely AI
driven, and focus on FD exclusively as a machine learning
task. On the contrary, in this work, we explore the utilizing
user-centric information for FD. Also, we demonstrate FD
using eye gaze and EEG information. To this end, FakeET
is the first database containing implicit human annotations
as additional information (or additional user labels, similar
to [15, 25]) for empowering machine-based FD methods.

3

USER-CENTERED ANALYTICS

This section describes (a) our user study, (b) differences
in gaze patterns for real vs fake videos, (c) utility of the
compiled eye-track saliency maps for FD, and (d) analysis of
the EEG recordings.

3.1

Materials & Methods

3.1.3 Stimuli. : We created a total of 10 video sets, with
each set containing an equal number of real and fake videos,
i.e., each set comprised 50 real, 25 easy and 25 hard-to-detect
fake videos. Given the fewer number of real videos in the
Google deepfake dataset, some real videos were used across
multiple sets. All videos had a resolution of 1920 Ã— 1080
pixels, and a frame rate of 24 fps. All videos were clipped to
20 seconds duration for experimental purposes.
3.1.4 Protocol. : Our experimental protocol was developed using the Matlab Psychtoolbox [3]. Each user viewed a
set of 100 videos, and each set of videos was presented to 4
users in random order (10 video sets Ã— 4 users/set = 40 users).
Our experimental protocol is depicted in Fig. 2. During each
video presentation, termed as a trial, a fixation cross was presented for 500ms, followed by the video playing for 20s. Upon
video viewing, users were presented with a two-alternative
forced choice (2AFC) task for recording their first impressions.
Users had to label the video as real or fake via a radio button. As users viewed the videos, their eye-movements were
captured via the Tobii TX300 eye-tracker with a sampling
rate of 300 Hz. Also, their EEG responses were recorded
with the 14-channel Emotiv Epoc+ device, which captures
EEG samples at 128 Hz frequency. The video presentation
was synced with the eye-tracker and EEG recordings through
the use of event markers. To minimize calibration errors and
user fatigue, the experiment was split into 4 parts comprising
25 trials each, interspersed with relaxation breaks. All users
took around 60 minutes to complete the experiment.

3.2

Gaze Pattern Analysis

Table 1 summarizes statistics of the FakeET dataset, in terms
of the eye-gaze and EEG recordings available for research.
While a substantial amount of EEG data was visually found
to be noisy, and ignored for our analyses, we nevertheless
note that sophisticated machine learning algorithms designed
for noisy data (e.g., multiple instance learning, Siamese neural networks) can be explored to improve EEG-based FD as
in [18]. In terms of eye-gaze recordings, each video corresponds to a minimum of 2, and a maximum of 16 eye-track
files (some data was lost due to subjects closing their eyes,

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.
Table 1: FakeET in a nutshell.

# Videos
811 (331 real, 480 fake)

# Users- ET
40

# Viewings/video
min: 2, max: 16
mean: 5 Â± 2.2

# Users- EEG
30

EEG description
Corrupt EEG epochs: 1236 real, 1220 fake
Clean EEG epochs: 636 real, 793 fake

Figure 3: Comparing viewing patterns differences among real, easy and difficult-to-detect fake videos via eye-track measures.
tracker errors, etc.). The following sections demonstrate the
validity of the compiled eye-gaze and EEG data.
We first examined if the compiled gaze recordings (a) were
distinctive for real and fake videos, so that they could be
utilized as features for FD, and (b) if employing the eye-track
heatmap as a prior for deep neural networks optimized FD
performance.
3.2.1 Statistical Analyses. As seen from Figure 1, the explorative vs explanative user viewing patterns for real vs fake
videos can be captured via a number of low-level eye-gaze
measures. To this end, upon processing the raw gaze data,
and discovering fixations and saccades via the EyeMMV
toolbox [9], we computed the following measures:
(1) Mean number of fixations: falling on 20s of video.
More fixations should indicate greater scene exploration.
(2) Mean fixation duration: over each video. Longer
fixation durations indicate visual processing by the
users to assimilate and understand the scene content.
(3) Fixation entropy: conveying the randomness with
which viewers fixate on scene regions. Entropy values can theoretically vary from [0, âˆž], with 0 denoting that every viewer fixates on the same exact
scene location, while âˆž denotes absolute viewing
stochasticity. Intuitively, presence of visual attractors in the scene, such as a manipulated scene region,
should result in lower viewing entropy, while scene
explorations are correlated with high entropy.
(4) Number of re-fixations: Computing the number of
recurring fixations within a scene region, and
(5) Scan-path length: Length of user scan paths over the
video scene. Longer scan paths indicate explorative
viewing.
We then compared these measures for the real, easy and
difficult-to-detect fake videos. As noted in Sec. 3.1.1, fake
videos were further categorized into easy and difficult-to
detect fakes by two of the paper authors. This was owing
to the inconspicuous manipulations existing in some videos,
and since our user-study was performed with naive viewers,

we hypothesized different user responses for the easy and
difficult fakes. We posited that if users had greater difficulty
in detecting the difficult manipulations, there should be
significant differences in viewing patterns for the easy and
difficult fakes.
Fig. 3 compares viewing patterns for the real vs easy and
difficult fakes. A one-way Analysis of Variance (ANOVA) test
was performed to identify significant differences in eye-track
measures. We noted the main effect of (real/easy/difficult)
viewing condition (with p < 0.05) for the mean number of
fixations (more fixations on real videos), entropy (greater
viewing entropy for real videos), number of re-fixations (more
re-fixations on real videos) and the scan-path length (longer
scan-paths on real videos) measures, confirming that viewing
differences existed for real vs fake videos. We then examined differences for easy vs difficult-to-detect fakes via the
two-sample t-test. This analysis revealed no differences, except for the number of re-fixations measure, implying that
detecting difficult fakes entailed a greater amount of visual
processing. There were however, no major differences in the
mean fixation durations over the three conditions. Overall,
extracting eye-gaze statistics revealed (a) the existence of differences in eye-gaze metrics for real vs fake videos, implying
that eye-gaze features are discriminative for FD, and (b) not
much differences existed for easy vs difficult fakes, except
for a greater degree of visual processing on difficult fakes for
inference.
3.2.2 FD via eye-track heat maps. As observed in our
study, the user gaze pattern is more spatially spread across
the scene in case of a real (un-manipulated) video. On
contrary in the case of a fake (manipulated) video, the userâ€™s
gaze pattern is spatially focused around the face area only.
Grimes et al. [5] conducted an interesting study to investigate,
how the subjectsâ€™ attention is divided between the audio
and visual channels, while watching television news. In
one of their experiments, they introduced manipulations in
the visual channel, and found that the subjectâ€™s attention
increased. This could be attributed to manipulated region
being more salient to the human visual system. In other

The eyes know it: FakeET- An Eye-tracking Database to Understand Deepfake Perception
Conferenceâ€™17, July 2017, Washington, DC, USA

Figure 4: Different CNN inputs (Section 3.2.2): Top and bottom rows show frames from fake and real video, respectively.
Left column shows input to the CNN, when full-frame is used.
Right column shows input to CNN, when the gaze map is overlaid on the original frame (left column). Note that these regions, where the image content is visible, represent the parts
of image, where the subjects in the study fixated.
words, the subject paid more attention when the videos
had an unusual visual channel, as compared to the real
un-manipulated videos. This is similar to the observation
regarding Fig. 1 in our study. The fake videos have some
perturbations, which can be mostly observed in the visual
channel, thus diverting the usersâ€™ gaze (or attention) towards
such regions. To empirically validate these observations, we
trained a CNN to predict if a video is real or fake.
The base of the CNN in our study is a standard 3D ResNet
[7]. Table 2 summarises the network structure. We trained
the network in three different settings based on the following
network inputs: (a) full-(video) frame ; (b) full-frame overlaid
by user gaze maps, and (c) the cropped face region only. In
the first setting, network is input with the full frame. In
the second case, we overlay the full frame with the binarized
gaze pattern map. The rationale behind adding the gaze
pattern map is to augment information pertaining to important (or equivalently, possible forged) scene regions from a
user perspective. In the third scenario, we crop the facial
region using S3 FD face detector [27] to pass the detected
face region as the input to the network. The third scenario
is designed on the observation that all available deepfake
datasets contain video manipulations limited to the facial
region, and therefore, the machine can assume a-priori that
the face needs to be mined for visual/statistical irregularities.
Method:
The dataset D = {v 1 , y 1 , v 2 , y 2 , ..., v N , y N }
consists of N = 805 videos (331 real and 474 fake), and we
split it into 60 : 20 : 20 train-validation-test sets. Here, v i
denotes the input video and the label y i  {0, 1} indicates
whether the video is real (y i = 1) or fake (y i = 0). To encode
the temporal changes at a finer level, we divide each video into
D-second long segments - {si1 , si2 , ..., sin }, where n denotes

segment count for an input video v i . Each segment either
contains the chunk of full frames, or cropped faces only, or the
full frames masked with the binarised gazemaps, depending
upon the setting under which the network has to be trained.
We use cross-entropy loss to train our model, whose input is
sit , a video sequence of size (C Ã—hÃ—w Ã—D Ã—f ), where C (= 3)
refers to the RGB color channels of each frame, h, w are the
frame height and width and f is the video frame rate and D
is the duration of each chunk (D=1 in our implementation).
Fig. 4 shows sample frames from real and fake videos along
with user gaze maps overlaid on them. The first column is
an illustration of the input when the network is trained with
full-frame information (which includes a great deal of background noise), while the second column is for the scenario
when the network takes in frame information augmented with
eye gaze/attention masks. As it can be observed from the
figure, the gaze map for a fake sample is centered around the
manipulated face only whereas for a real sample, the userâ€™s
gaze also explores the objects around a person to verify the
naturalness of a scene.
Results: The video-wise area under the curve (AUC) metric was used for evaluation. Table 3 shows the comparison
of the three settings. It is observed that full-frame input
achieved 49.03% Ã‚Å› 0.83. When the frame information is
overlaid with gaze-based masks derived from the user study,
the AUC increases to 54.39% Ã‚Å› 0.01. This validates the
observation that with the gaze maps, more focused information is input to the network. This can also be related
to recent concept of attention layers in CNN. Instead of
explicitly adding attention layer, we are adding gaze maps
generated through user study. This sets the scene for a collaborative effort for FD between human and machines by
bringing humans-in-the-loop.
The cropped faces gives a higher video-level AUC score
= 61.75% Ã‚Å› 1.25. The is better than both the full-frame
version of cropped faces as well as the gazemaps used as
masks. This can be attributed to the fact that while the

CNN Structure
conv1
conv2_x
conv3_x
conv4_x
conv5_x
average pool
fc7, 256Ã—7Ã—7, 4096
batch_norm_7, 4096
fc8, 4096, 1024
batch_norm_8, 1024
dropout, p = 0.5
fc10, 1024, 2
Table 2: Structure of the CNN architecture (initial layers are
the same as in the 3D ResNet architecture [6]).

Conferenceâ€™17, July 2017, Washington, DC, USA
Full-frame

Full-frame overlaid
with gaze map

Face-only

49.03 Ã‚Å› 0.83

54.39 Ã‚Å› 0.01

61.75 Ã‚Å› 1.25

Table 3: Comparison of performance of CNN (Section 3.2.2)
based on different inputs. Note the increase in the performance, when the gaze maps are added to the full-frame input.
gaze is likely to fall on the manipulated facial region, it may
also fall on some background pixels, inherently due to eye
movements or tracker errors. Further, it is important to note
that in the current datasets, manipulations are localized to
the face region.
With evolutions in deepfake synthesis, manipulations need
not necessarily be performed on faces, and may involve other
scene regions as well (e.g., personâ€™s hands, clothing, scene
background, etc.). In such situations, the face prior assumption would fail, but gaze information could still convey such
irregularities, as human eyes are sensitive to â€˜center-surroundâ€™
differences in the video content. Therefore, the gaze cue
influences the CNN to focus on certain scene regions and
thereby improving FD performance; even if the focus may
not be achieved as perfectly as with face crops, the eye-gaze
prior is more generalizable than the face prior as deepfakes
evolve.

3.3

EEG Data Analysis

As viewers attempted to detect fake videos, their cognitive
responses were recorded via an Emotiv EEG headset. Emotiv
is a commercial and portable EEG device which comprises
14 electrodes, has a 128 Hz sampling rate, and is typically
used for gaming applications; however, it has recently been
used to successfully analyze user cognitive responses with a
high degree of accuracy [2, 18, 24]. Due to erroneous and
corrupted recordings, we could only use EEG data for 30 of
the 40 users who participated in our study.
Corresponding to each trial, i.e., video presentation timeline, we extracted epochs of 10.5 seconds (0.5s pre-stimulus
for baseline power subtraction plus 10s of stimulus viewing).
This epoch time-length was deemed sufficient based on eyetrack findings; each epoch was therefore represented by a 14
(EEG channels) Ã— 1344 (10.5s Ã— 128 Hz) matrix. Baseline
subtraction, EEG band-limiting to [0.1, 45] Hz, visual epoch
removal and Independent Component Analysis (ICA) were
performed to eliminate corrupted epochs along with head,
eye and muscle-movement artifacts. The cleaning resulted
in 1429 clean epochs of which 636 arose from correctly userlabeled fake video trials, and 739 accurately labeled real video
trials. Experiments were performed on this clean EEG data.
3.3.1 ERP analysis: Event-related potentials (ERPs) denote the average time and phase-locked neural response following an event of interest; e.g., P300 denotes an ERP typically
elicited 300ms following presentation of faces. Error related
Negativity (ERN) is an ERP triggered when the user becomes aware of obvious error(s) committed by self, peers or

Trovato and Tobin, et al.
with the presented information. Authors of [24] successfully
captured the ERN pattern, characterized by a negative peak
followed by a positive peak, when the user becomes aware of
an error committed by a collaborator during an interactive
task.
Figure 5 presents ERPs associated with the f7, af3 and
fc5 Emotiv electrodes, and indicate neural activity in the
frontal-central part of the brain. ERPs (y-values) are plotted
upside down as per convention, with the blue and red curves
denoting ERPs for real and fake videos respectively. One
can clearly observe an ERN-like pattern in the fake ERPs
around 1.3s (dashed vertical line) post video onset. Note
that this pattern is inconspicuous for the real ERPs. We
attribute this â‰ˆ 1.3s latency to the video content; most videos
in the Google Deepfake [4] involve a â‰ˆ 1s establishing shot,
typically comprising a camera pan or zoom or actor motion,
to introduce the character(s) of interest . Once the scene is
understood by the viewer, cognitive inference of anomalous
scene regions appears fairly instantaneous. ERP findings (a)
confirm distinctive patterns in user cognitive responses upon
visual processing of fake videos, and (b) validate the utility
of the compiled EEG data.
3.3.2 EEG-based FD. We determined the efficacy of the
EEG modality for FD via classification experiments on the
clean EEG data via following baselines.
Majority label assignment (Maj-Class): Where every test
sample is assigned the label of the majority class in the
training set.
Random label assignment (Random): A random prediction
score s âˆˆ [0,1] is assigned to the test sample, and the test
label l is assigned as l = s > 0.5.
Naive Bayes (NB): Known as the minimum risk classifier,
NB assigns the test label based on the maximum a-posteriori
(MAP) criterion. Despite its simplicity, NB is known to
achieve competitive performance for many real world problems.
1
Logistic: Assigns a score s âˆˆ [0,1] as s = 1+eâˆ’w
T x , where
x, w denote the test feature and a vector of learned regression
weights respectively.
k-Nearest neighbors (k-NN): Where a test sample is assigned the class label representing the membership of the
majority of its k nearest neighbors. We found k = 3 to achieve
optimal classification performance.
Decision tree (Dec-Tree): Which represents a rule-based
classifier to predict outcomes; class labels are determined at
the terminal/leaf nodes of the tree.
Linear SVM (Lin SVM): which represents a very powerful classifier for small-to-medium scale datasets. SVM performance was fine-tuned by varying the mis-classification
tolerance parameter C within [10âˆ’6 , 106 ].
Time-series CNN (T-CNN): In addition to the above shallow learners, we also applied a 3-layer deep convolutional
neural network (CNN), comprsing two 1-D convolutional
layers followed by a fully-connected layer, employed for EEG
data analysis in [18].

The eyes know it: FakeET- An Eye-tracking Database to Understand Deepfake Perception
Conferenceâ€™17, July 2017, Washington, DC, USA

Figure 5: ERPs for the real (blue) vs fake (red) EEG response at the f7 (left), af3 (middle) and fc5 (right) electrode channels.
An ERN pattern (-ve peak followed by +ve peak) can be observed around 1.3s post video onset. x denotes time (s), while y
denotes potential (ÂµV ). ERPs are plotted upside down as per convention.
Results and Discussion: As over-fitting is a common problem where few high-dimensional training examples are available (we had 1429 epochs, each of which was of 14 Ã— 10344
size), we first applied Principal Component Analysis to retain 95% data variance upon vectorizing the epoch data; this
reduced epoch dimensionality to 679. PCA-ed data was input
to each of the classification algorithms, and the EEG-based
FD results are presented in Table 4. Along with the AUC
measure which is employed for evaluation in Section 3.2.2, we
also computed the F1-score (harmonic mean of precision and
recall), and the accuracy measure for benchmarking. We also
performed 20 repetitions of five-fold cross-validation (total
of 100 runs), to examine generalizability of the results.
Since AUC computation requires classifier prediction scores
(probabilities), AUC scores are not available for the Maj-Class,
k-NN and Dec-Tree methods. Due to the imbalanced nature
of our EEG data where the fake epochs are in minority, majority class assignment results in an accuracy of 0.56, equal
to the real class proportion. However, Maj-Class achieves
zero recall for the fake class, resulting in an F1-score of 0.
The Random classifier expectedly achieves an accuracy of
0.5, AUC of 0.5, and a slightly lower F1-score of 0.47. Cumulatively, the simplistic Maj-Class and Random classifiers
convey that (a) the F1-score is more sensitive to class imbalance than AUC, and (b) classifiers that work better on
the minority fake class are more likely to achieve a higher
AUC/F1-score.
To benchmark the performance of other classifiers, we
compared their AUC and F1-score distributions against the
Random outputs via the two-sample t-test. Starred values
in Table 4 denote distributions significantly higher than the
Random classifier. From the observed results one can conclude that (a) Logistic regression and linear SVM perform
similar to a random classifier, (b) the Dec-Tree and T-CNN
algorithms achieve a significantly higher F1-score than Random, (c) the k-NN classifier achieves the second-highest F1
metric of 0.54, and (d) the Naive Bayes classifier achieves
optimal EEG-based FD with AUC and F1 scores of 0.55.
Cumulatively, EEG-based classification results reveal that
(1) Better-than-chance FD is achievable only using PCA-ed

EEG features and off-the-shelf classification algorithms, and
(2) While a modest FD performance is achieved in this work,
better EEG descriptors (Power Spectral Density features,
EEG spectrograms, etc) and superior classification strategies
may be explored for possible improvements.

4

DISCUSSION

FD techniques have been primarily focusing on face manipulation in videos. As manipulation techniques progress in the
future, it is going to be non-trivial for users to identify fakes.
In our experiments, we observe that the EEG and gaze maps
show that the explanatory behavior is observed, when a user
becomes aware of any irregularity in the video (similar to
prior studies [19, 20], where emotional content was found to
be a strong attractor of visual attention). This opens up a
new direction for the effort of detecting manipulated video
using human-machine cooperation. The research question
would then be: Based on implicit user cues acquired while
watching a video, can an AI-based FD technique be used to
validate (with explanations) if the video is fake?
The AI based tool can further make a more informed decision using the video content and the implicit user feedback, as
compared to only mining the video signal. This is important
as in the future, manipulations will be added to not just the
face but to the scene background, audio information and other
attributes such as clothes of video subject(s), scene lighting,
etc. These complex manipulations would be a direct consequence of evolving video manipulation techniques. Therefore,
AI methods alone may be inadequate due to training data
bias, and the difficulty in creating varied and representative
training sets, which can simulate the whole gamut of video
manipulations. In this regard, a human-in-the-loop appears
a promising direction for FD, and the exploitation of contextual information in the form of human eye-gaze and EEG
annotations, which can be acquired in a facile manner, would
enable the AI system to optimize decision-making.

Conferenceâ€™17, July 2017, Washington, DC, USA

Trovato and Tobin, et al.

Table 4: Classification performance with different algorithms on EEG epochs (+ve class proportion = 636/1429 = 0.445).All
scores are computed over 20 repetitions of 5-fold cross-validation (total 100 runs). AUC/F1 distributions significantly higher
than the random output (with p < 0.05), as determined via a 2-sample t-test, are denoted by *.

5

Classifier

Maj-Class

Random

NB

Logistic

k-NN

Dec-Tree

Lin SVM

T-CNN

Acc

0.56Â±0.00

0.50Â±0.03

0.52Â±0.02

0.50Â±0.02

0.52Â±0.03

0.52Â±0.03

0.51Â±0.03

0.51Â±0.03

AUC

-

0.50Â±0.04

0.55Â±0.03*

0.50Â±0.02

-

-

0.51Â±0.03

0.51Â±0.03

F1

0.00Â±0.00

0.47Â±0.03

0.55Â±0.02*

0.45Â±0.03

0.54Â±0.04*

0.51Â±0.07*

0.46Â±0.04

0.50Â±0.03*

CONCLUSION & FUTURE WORK

In this work, we present the first study that explores the use
of user behavioral cues for fake detection. A novel datasetâ€“
FakeEt containing 811 samples from 40 subjects is curated
via a user study, and contains two modalities: eye gaze and
EEG signals. The gaze maps show an interesting spread of
fixations outside of the face area(s) for real videos. This
exploratory gaze behavior is opposite to the explanatory one,
in the case of fake videos. Subjects explored the scene less
in fake videos and fixated more around the face, which is
the typica; region of manipulation in the video. Augmenting input frames with gaze maps, while training a CNN for
fake detection, shows that the network implicitly learns from
the gaze patterns and the performance improves by approximately 5%. Similar observation is also made in the case
of EEG signal analysis that a subjectâ€™s EEG responses are
discriminative for predicting if the viewed video is fake.
As future work, we will explore: (a) strategies for humanmachine collaboration for deepfake detection; (b) learning
a joint embedding to capture similarities between EEG and
eye gaze patterns; (c) transfer learning from saliency prediction networks to gaze map generation, for increasing the
number of data samples and better scene understanding; (d)
explore fusion techniques for adding eye gaze maps through
embeddings into the CNN input, and (e) analyze the effect
of audio manipulation on EEG signals.

REFERENCES
[1] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao
Echizen. 2018. Mesonet: a compact facial video forgery detection
network. In 2018 IEEE International Workshop on Information
Forensics and Security (WIFS). IEEE, 1â€“7.
[2] Maneesh Bilalpur, Seyed Mostafa Kia, Manisha Chawla, TatSeng Chua, and Ramanathan Subramanian. 2017. Gender and
Emotion Recognition with Implicit User Signals. In International
Conference on Multimodal Interaction (ICMI Ã¢Ä‚Å¹17). Association for Computing Machinery, New York, NY, USA, 379Ã¢Ä‚Åž387.
https://doi.org/10.1145/3136755.3136790
[3] D. H. Brainard. 1997. The Psychophysics Toolbox. Spatial Vision
10 (1997), 433â€“436.
[4] Nicholas Dufour, Andrew Gully, Per Karlsson, Alexey Victor
Vorbyov, Thomas Leung, Jeremiah Childs, and Christoph Bregler.
[n.d.]. DeepFakes Detection Dataset by Google & JigSaw.
[5] Tom Grimes. 1991. Mild AuditoryÃ¢Ä‚Å˜Visual Dissonance in Television News May Exceed Viewer Attentional Capacity.
[6] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2017. Can
Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and
ImageNet? CoRR abs/1711.09577 (2017). arXiv:1711.09577
http://arxiv.org/abs/1711.09577

[7] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. 2017. Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition. CoRR abs/1708.07632 (2017). arXiv:1708.07632
http://arxiv.org/abs/1708.07632
[8] Liming Jiang, Wayne Wu, Ren Li, Chen Qian, and Chen Change
Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for RealWorld Face Forgery Detection. arXiv:cs.CV/2001.03024
[9] Vassilios Krassanakis, Vassiliki Filippakopoulou, and Byron
Nakos. 2014. EyeMMV toolbox: An eye movement post-analysis
tool based on a two-step spatial dispersion threshold for fixation
identification. Journal of Eye Movement Research 7, 1 (Feb.
2014). https://doi.org/10.16910/jemr.7.1.1
[10] Y. Li, M. Chang, and S. Lyu. 2018. In Ictu Oculi: Exposing AI
Created Fake Videos by Detecting Eye Blinking. In 2018 IEEE
International Workshop on Information Forensics and Security
(WIFS). 1â€“7.
[11] Yuezun Li and Siwei Lyu. 2018. Exposing DeepFake Videos By
Detecting Face Warping Artifacts. In CVPR Workshops.
[12] F. Matern, C. Riess, and M. Stamminger. 2019. Exploiting
Visual Artifacts to Expose Deepfakes and Face Manipulations. In
2019 IEEE Winter Applications of Computer Vision Workshops
(WACVW). 83â€“92.
[13] Huy H. Nguyen, Fuming Fang, Junichi Yamagishi, and Isao
Echizen. 2019. Multi-task Learning For Detecting and Segmenting
Manipulated Facial Images and Videos. CoRR abs/1906.06876
(2019). arXiv:1906.06876
[14] H. H. Nguyen, J. Yamagishi, and I. Echizen. 2019. Capsuleforensics: Using Capsule Networks to Detect Forged Images and
Videos. In ICASSP 2019 - 2019 IEEE International Conference
on Acoustics, Speech and Signal Processing (ICASSP). 2307â€“
2311.
[15] Syed Omer Gilani, Ramanathan Subramanian, Yan Yan, David
Melcher, Nicu Sebe, and Stefan Winkler. 2016. PET: An Eyetracking Dataset for Animal-centric PASCAL Object Classes. In
IEEE Conference on Multimedia & Expo.
[16] Cecilia Pasquini, Giulia Boato, and Fernando PÃƒÄ¾rez-GonzÃƒÄ…lez.
2017. Statistical Detection of JPEG Traces in Digital Images in
Uncompressed Formats. IEEE Trans. Information Forensics
and Security (2017), 2890â€“2905.
[17] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian
Riess, Justus Thies, and Matthias Niessner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In The
IEEE International Conference on Computer Vision (ICCV).
[18] A. Shukla, S. S. Gullapuram, H. Katti, M. Kankanhalli, S. Winkler, and R. Subramanian. 2020. Recognition of Advertisement
Emotions with Application to Computational Advertising. IEEE
Transactions on Affective Computing.
[19] Abhinav Shukla, Harish Katti, Mohan Kankanhalli, and Ramanathan Subramanian. 2018. Looking Beyond a Clever Narrative: Visual Context and Attention Are Primary Drivers of Affect
in Video Advertisements. In ACM International Conference on
Multimodal Interaction. 210Ã¢Ä‚Åž219.
[20] Ramanathan Subramanian, Divya Shankar, Nicu Sebe, and David
Melcher. 2014. Emotion modulates eye movement patterns and
subsequent memory for the gist and details of movie scenes. Journal of Vision 14, 3 (03 2014), 31â€“31.
[21] Christian Szegedy, Sergey Ioffe, and Vincent Vanhoucke. 2016.
Inception-v4, Inception-ResNet and the Impact of Residual
Connections on Learning.
CoRR abs/1602.07261 (2016).
arXiv:1602.07261
[22] Cristian Vaccari and Andrew Chadwick. 2020. Deepfakes and
Disinformation: Exploring the Impact of Synthetic Political Video

The eyes know it: FakeET- An Eye-tracking Database to Understand Deepfake Perception
Conferenceâ€™17, July 2017, Washington, DC, USA

[23]

[24]

[25]

[26]

[27]

on Deception, Uncertainty, and Trust in News. Social Media +
Society 6, 1 (2020). https://doi.org/10.1177/2056305120903408
Cristian Vaccari and Andrew Chadwick. 2020.
Deepfakes and Disinformation: Exploring the Impact of Synthetic Political Video on Deception, Uncertainty, and
Trust in News.
Social Media + Society 6, 1 (2020),
2056305120903408. https://doi.org/10.1177/2056305120903408
arXiv:https://doi.org/10.1177/2056305120903408
Chi Thanh Vi, Izdihar Jamil, David Coyle, and Sriram Subramanian. 2014. Error Related Negativity in Observing Interactive Tasks. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems (CHI Ã¢Ä‚Å¹14). Association
for Computing Machinery, New York, NY, USA, 3787Ã¢Ä‚Åž3796.
https://doi.org/10.1145/2556288.2557015
Bihan Wen, Ye Zhu, Ramanathan Subramanian, Tian-Tsong
Ng, Xuanjing Shen, and Stefan Winkler. 2016. COVERAGE
Ã¢Ä‚Åž A NOVEL DATABASE FOR COPY-MOVE FORGERY
DETECTION. In IEEE International Conference on Image
processing (ICIP). 161â€“165.
X. Yang, Y. Li, and S. Lyu. 2019. Exposing Deep Fakes Using
Inconsistent Head Poses. In ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP). 8261â€“8265.
Shifeng Zhang, Xiangyu Zhu, Zhen Lei, Hailin Shi, Xiaobo Wang,
and Stan Z Li. 2017. S3fd: Single shot scale-invariant face detector. In Proceedings of the IEEE International Conference on
Computer Vision. 192â€“201.

