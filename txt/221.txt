JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

1

Low-Cost Brain Sensing for Emotion Recognition

arXiv:1810.04582v3 [eess.SP] 30 Apr 2019

Payongkit Lakhan, Nannapas Banluesombatkul, Vongsagon Changniam, Ratwade Dhithijaiyratn,
Irawadee Thawornbut, Ekkarat Boonchieng and Theerawit Wilaiprasitporn, Member, IEEE

Abstract—This paper presents the first concrete investigation of
emotion recognition capability using a low-cost, open-source EEG
amplifier entitled OpenBCI. The most important aspect of this
type of study is effective emotion elicitation. In existing related
state-of-the-art works, movie clips are widely used for audiovisual emotion elicitation. In this study, two-hundred healthy
people of various ages participated in an experiment to select
effective clips. The methods for selecting the top 60 most
effective clips from a total of 120 candidates consist of regular
self-assessment, effective tags, and unsupervised learning. An
additional 43 participants gathered to view the selected clips to
enable the collection of both emotional EEG data and peripheral
physiological signals. The data on emotion recognition tasks were
analyzed to predict whether the elicited EEG data had a high
or low level of valence/arousal, to evaluate the performance of
OpenBCI toward emotion-related applications. The experimental
results were found to be comparable with those of existing studies
from expensive EEG amplifiers. This is because we used a similar
emotion elicitation method with the same algorithm for emotion
recognition.
Index Terms—Low-cost EEG, OpenBCI, Emotion recognition,
Affective computing, EEG datasets.

I. I NTRODUCTION

T

HE twenty-first century is experiencing a dramatic increase in wearable devices for physiological data acquisition with inexpensive sensors, storage, and computational resources offering acceptable accuracy. Such devices are rapidly
becoming game changers in human behavioral studies. The
achievement of an affective state or emotion recognition from
brain sensing in the prediction of human mental responses
from external stimuli is one of the main challenges in human
behavioral research. Electroencephalography (EEG) is a noninvasive technique for sensing brain activity via variations in
biopotential on the scalp. According to our literature survey
on recently published articles using “Low-Cost EEG Emotion”
as keywords, the majority of studies used EEG headsets from
EMOTIV EPOC+ [1], while others used EMOTIV INSIGHT,
MYNDPLAY [2], NeuroSky [3] and MUSE [4]. Only a few
studies on emotion recognition use OpenBCI (low cost with an
This work was supported by Robotics AI and Intelligent Solution Project,
PTT Public Company Limited, The Thailand Research Fund and Office of the
Higher Education Commission under Grant MRG6180028 and Junior Science
Talent Project, NSTDA, Thailand.
P. Lakhan, N. Banluesombatkul and T. Wilaiprasitporn are with Bioinspired Robotics and Neural Engineering Lab, School of Information Science
and Technology, Vidyasirimedhi Institute of Science & Technology, Rayong,
Thailand theerawit.w at vistec.ac.th
V. Changniam is with Department of Tool and Materials Engineering, King
Mongkut’s University of Technology Thonburi, Bangkok, Thailand
R. Dhithijaiyratn is with Department of Electrical Engineering, Chulalongkorn University, Bangkok, Thailand
I. Thawornbut is with British International School, Phuket, Thailand
E. Boonchieng is with Center of Excellence in Community Health Informatics, Chiang Mai University, Chiang Mai, Thailand

open-source EEG amplifier) [5]. Therefore, this study focuses
on the feasibility of using OpenBCI for emotion recognition.
Section II presents a comparison between all of the abovementioned devices.
Previous research studies on emotion-related studies use
low-cost EEG, which can be mainly categorized into two
domains, based on emotion-eliciting stimuli. The first domain
consists of audio-visual stimulation (video clips). Two studies
[6], [7], aimed to construct public datasets for further research
and develop the recognition algorithm and other applications.
One research team integrated EMOTIV EPOC+ with a bracelet
sensor [8] and eye tracker [9] to produce a low-cost emotion
recognition system [10]. An evolutionary computation was
then proposed as a competitive algorithm in emotion recognition and EMOTIV INSIGHT then used for performance
evaluation [11]. Furthermore, MUSE and certain other physiological datasets were used in another work involving boredom
detection while viewing video clips [12]. As an example
of its practical application, a real-time emotion recognition
framework was recently proposed using EMOTIV EPOC+
while participants were viewing Chinese movies [13]. The
second domain uses auditory stimulation, especially music,
as emotion-eliciting EEG. The use of EMOTIV EPOC+ was
demonstrated for the automatic detection of brain responses
from three types of music (neutral, joyful, melancholic), and
EEG connectivity features played an essential role in the
recognition tasks [14]. Another study, using the same device,
reported the correlation of EEG with classical music rhythm
[15], [16]. Music therapy is one of the applications emanating
from music-elicited emotion [17]. Moreover, in the study
of auditory-related emotion, one research group reported the
development of envisioned speech recognition using EEG [18].
Motivated by the findings of the aforementioned research
domains on emotion-related works, many other researchers
have also used low-cost EEG devices. Examples of such
studies are provided in this paragraph to enable the reader
to get a sense of the impact of low-cost devices. Two papers
have reviewed past literature on the study of human behavior
using brain signals for consumer neuroscience and neuromarketing applications [19], [20]. In addition, a consumer-related
research study showcased the effect of color priming on dress
shopping, measured by EEG [21]. Another group reported the
feasibility of using asymmetry in frontal EEG band power
(especially the Alpha band) as a metric for user engagement
when reading the news [22]. Band power asymmetry was also
introduced as a study feature on the meditation effect toward
emotional responses [23]. Some groups explored simultaneous
EEG recordings from multiple subjects such as the research
on synchronized-EEGs while students were studying in the
same classroom [24], or similar research on the emotional

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

engagement between a performer and multiple audiences [25].
Other research groups have studied longitudinal EEG data
from individual subjects such as an investigation involving
103-day single channel EEG data on emotion recognition in
daily life [26]. Nowadays, fundamental knowledge on emotion
recognition or affective computing using brain wave activity
have been applied in broad areas such as a study on people
with depression [27], stress on construction sites [28] and
varying the ambient temperature in buildings [29].
As previously mentioned, this paper focuses on a feasibility study of emotion recognition using OpenBCI. In recent
years, there have been a few related works, one of which
demonstrated the usefulness of OpenBCI as a biofeedback
instrument (EEG, EMG) for an application entitled IBPoet
[30]. Three parties participated in the IBPoet demonstration: a
reader, readee, and the audience. Once the reader had relayed
the selected poem, the readee sent biofeedback (emotional
responses) to the reader via a vibration band and heat glove.
The reader and the system adapted according to the emotional
responses to give the audience a greater feeling of pleasure.
Similarly, another research group used the same device to
evaluate and practice oral presentation [31]. The other findings
related to typical research on emotion recognition, but the
experimental subjects were in very small numbers (some
have several subjects) [32]–[35]. Moreover, EEG software for
automated emotion recognition to support low-cost devices,
including OpenBCI, was proposed in late 2018 [36]. Thus, it
could be inferred that the demand for OpenBCI is increasing.
The major aim of this paper is to provide a concrete investigation of emotion recognition capability using OpenBCI. The
most important aspect of this type of study is effective emotion
elicitation. In existing related state-of-the-art works, movie
clips are widely used for audio-visual emotion elicitation
[6], [37]–[40]. Here, two-hundred healthy people of various
ages participated in an experiment to select effective clips. In
addition, 43 more participants gathered to view the selected
clips for the collection of both emotional EEG data and
peripheral physiological signals. The capability of OpenBCI
was then evaluated using similar tasks with the same algorithm
as the existing works on emotional EEG to create datasets
using high-end EEG [6], [37], [38]. Thus, the results were
comparable to the existing works. The task is designed to
predict whether the elicited EEG data has a high or low
level of valence/arousal. The algorithm involves simple feature
extraction, such as power spectral density (PSD). Furthermore,
a support vector machine (SVM) constructed the classifier.
Finally, the experimental results were comparable with the
state-of-the-art datasets from high-end EEG.
The remainder of this paper provides the methodology
in Section II, with the results and discussion reported and
explained in sections III and IV, respectively. Finally, section
V summarizes the paper.
II. M ETHODOLOGY
This section begins with an experiment to select effective
film trailers for emotional elicitation in Experiment I. Low-cost
EEG and peripheral physiological sensors are then introduced

2

in Experiment II. The selected trailers from Experiment I are
used to elicit human moods while EEG and the peripheral
physiological signals are recorded for further analysis. The
experiments follow the Helsinki Declaration of 1975 (as
revised in 2000), approved by the internal review board of
the Chiang Mai University, Thailand.
A. Experiment I: Elicited Video Selection
Film trailers are among the affective tags labeled by the
Internet Movie Database (IMDb), in which drama (happiness),
comedy, romance, action, sci-fi, horror, and mystery are the
main categories. These are simply defined in our study as three
genres: Happiness, Excitement, and Fear. To obtain effective
trailers containing emotional stimuli, we followed a selection
process as illustrated in Figure 1. Five participants randomly
helped each other to pick 40 famous film trailers per genre,
resulting in 120 trailer clips in total. The trailer clips contained
English soundtracks with native subtitles (Thai). Afterward,
the 120 clips were randomly split into four groups of 30 each.
The four groups of 50 participants (n = 200) with almost equal
numbers of males and females, ranging from 15–22 years old,
were then assigned to view one trailer group. To deal with the
different trailer durations, the final minute of each clip was
selected for the experiment. Finally, each participant assessed
his/her emotions through the qualitative measure of scoring
for Valence (V), Arousal (A), Happiness (H), Fear (F), and
Excitement (E) (qualitative measures on a continuous scale of
1–9) after each clip had finished playing.
To analyze the qualitative data assessed by a group of
sample participants, the k-means clustering method was used
[46], with H, F and E scores as the features for finding three
extremely different emotional elicitations (number of clusters
or k = 3). As a consequence of the clustering, 20 clips were
selected from each cluster closest to the centroids for further
use in Experiment II. Thus, 60 emotion-eliciting clips were
selected in total. Due to the same clips being scored by
different participants, the k-mean outputs from the same clips
may not be classified into the same clusters. This presented
a problem in the calculation of average Euclidean distances
from the outputs to the centroids of the clusters. To overcome
this issue, the majority cluster (mode value) of each clip was
obtained and the k-mean outputs were then filtered out from
the same clips which did not fall into the majority cluster.
Finally, the average Euclidean distances were calculated from
the remaining outputs of each clip to the centroid of its cluster.
In this way, the first 20 clips closest to the centroids of each
cluster could be ranked for use in the following experiment.
B. Experiment II: Emotion Recognition Using OpenBCI
1) Sensors: According to the literature survey, Table I
presents a comparison of a brand-new low-cost EEG entitled
OpenBCI and existing low-cost EEGs. Each product was scientifically validated against high-end EEGs by measuring the
recognized brain responses. Three studies validated EPOC+
against EEG from ANT Neuro [53], Neuroscan [54] and gtec
[55]. The studies contained the measurement of P300, EventRelated Potential (ERP), and Emotion, respectively. Frontal

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

3

1

2

3

4

5

Select film trailers
categorized by IMDB

Five participants
select 120 famous film trailers

Randomly split into four groups
(30 trailers per group)

200 participants view trailers
from one out of the four groups

Each participant assesses his/her emotions
for each trailer (H, F, E — 1-9)

A

B

C

D

clip 01 / 30
H
F
E
NEXT

= 40 trailers for Happiness

= 50 participants

= 40 trailers for Excitement
= 40 trailers for Fear

1

2

Gather all H, F, E scores
from all trailers and participants

Perform k-mean clustering (k=3)
among those points

3

4

Remove data points which are not in
their majority cluster for every trailer

Select the first 20 closest points
To the centroids of each cluster

9

9

8

8

8

8

7
6
5
4
3
2
1
9

8

7 6

7
6
5
4
3
2
1
9

5

Fea
r

4 3

(F)

2

1

1

2

3

4

5

6

Excitement

7

8

9

(E)

8

7 6

7
6
5
4
3
2
1
9

5

Fea
r

4 3

(F)

2

1

1

2

3

4

5

6

Excitement

7

8

9

Happiness (H)

9

Happiness (H)

9

Happiness (H)

Happiness (H)

(a)

8

7 6

= H, F, E score of one trailer
from one participant

7
6

5

4 3

(F)

2

1

1

2

3

4

5

6

Excitement

7

8

9

20 trailers from green

5
4
3
2
1
9

Fea
r

(E)

output
20 trailers from blue

20 trailers from red
8

7 6

5

Fea
r

(E)

= emotion scores of trailer A

4 3

(F)

2

1 1

2

3

4

5

6

Excitement

7

8

9

(E)

= centroids of each group

Remove all points of A
in red & green clusters

Number of = 600 data points
(200 participants x 30 clips per each)

(b)

Fig. 1: The protocol of experiment I began with the random selection of 120 film trailers from IMDB by 5 participants. The
200 participants then view 30 trailers each and assess their emotions for each trailer (a). To obtain effective film trailers for
emotional elicitation, k-mean clustering is then performed (b).
TABLE I: A comparison of existing Low-Cost EEG devices
Device Name
OpenBCI
EMOTIV EPOC+
EMOTIV INSIGHT
Myndplay Myndband
NeuroSky MindWave Mobile
Interxon MUSE

Price [USD]

Sampling Rate [Hz]

No. of Channels

Open Source

Raw Data

Scientific Validation

750/1800
799
299
200
99
199

250
256
128
512
512
220

8/16
14
5
3
1
4

Yes
SDK
SDK
SDK*
Yes
SDK

Yes
Yes*
Yes*
Yes
Yes
Yes

MRCPs [41]
P300 [42], ERP [43], emotion [6]
frontal EEG [44]
frontal EEG [44], ERP [45]

*additional cost, SDK stands for Software Development Kits

Fig. 2: Experimental setup.

EEGs recorded from NeuroSky and MUSE were validated by
two baseline EEGs entitled B-Alert (from [56]) and Enobio
(from [57]). MUSE was further validated with the EEG
from Brain Products GmbH [58] for ERP research. Recently,
one research group reported the performance of OpenBCI
based on Texas Instrument ADS1299 (biopotential amplifier)
using the movement-related cortical potential measurement. In
comparison to the product from Neuroscan [54], there were no
statistically significant differences between the two devices.

Here, we use OpenBCI to record emotional EEG. OpenBCI
is the only low-cost EEG with both open-source code (software) and hardware (schematic). The advantage of an opensource device is that it allows the development of many realtime applications directly via typical programming language
for research without additional charges. Furthermore, electrode
placement is also flexible compared to the other devices.
To cover brain wave information across the scalp, an eightchannel EEG electrode montage was selected (F p1 , F p2 ,
Fz , Cz , T3 , T4 , Pz , and Oz ) with reference and ground on
both mastoids. This montage is a subset of the international
standard 10–20 system for EEG electrode placement. As well
as EEG, we also recorded peripheral physiological signals
from wearable devices entitled Empatica4 or E4 [59]. Multiple
sensors were equipped in E4, including electrodermal activity
(EDA), skin temperature (Temp), and blood volume pulse
(BVP) with their derivatives (heart rate, inter-beat interval,
etc.). Recently, one research team used E4 to record electrodermal activity (EDA) for arousal and valence recognition [60].
Their findings from 25 healthy participants were promising.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

4

Fig. 3: The experimental program starts with the acquisition of each participants demographics, hours of sleep, level of tiredness,
and favorite movie genre. Clips are subsequently played in full-screen mode and a self-emotional assessment form provided
after each clip. (Screenshot from film trailers [47]–[52])

2) Participants: The participants in this study consisted
of 43 healthy people aged between 16 and 34 (21 male
and 22 female). They were asked to wear only two devices:
OpenBCI (EEG) and Empatica4 (E4). The participants wore
E4 like a watch, although there were extension cables with gel
electrodes attached (Kendall Foam Electrodes) to measure the
EDA at the middle knuckle of the index and middle fingers.
Both devices recorded simultaneously. During the experiment,
participants were asked to place their chin on the designated
chin rest, and move as little as possible in order to prevent
artifact interference. The experimental setup is presented in
Figure 2.
3) Experimental Protocol: We developed software to
present the emotional stimulus clips and recorded EEG with
peripheral physiological signals from the wearable devices as
shown in Figure 3. The first page of the program consists of
a survey to collect personal information from the participants
(age, gender, hours of sleep, level of tiredness, and favorite
movie genre). As in Experiment I, the video clips for the
participants were randomly selected into 43 groups with 15
clips each. Each group consisted of nine identical clips including three per cluster (the top three clips from each cluster
as in Experiment I). The remaining six clips were selected
randomly: two per cluster. Following each clip, participants
completed a survey (scroll bars) to evaluate their emotions,
including V, A, H, F, and E (on a continuous scale of 1–9).
4) Feature Extraction: In order to obtain stability and
specific emotional responses from the EEG and peripheral
physiological signals, the program started recording signals
after the stimulus clip had been played for two seconds. In
order to avoid filtering artifacts, the first and the last two
seconds of recorded signals of each clip were trimmed during
viewing. In total, there were 56 seconds of signals for each
elicited clip. Typical EEG-preprocessing was incorporated, including common average reference (CAR) to find the reference
signal for all electrode channels, with independent component
analysis (ICA) applied to remove artifact components. Even
if there was no electrooculography (EOG) in the experiments,

TABLE II: Features extracted from EEG and empatica signals
in each clip
Signal

Extracted features

EEG(32)

θ (3–7 [Hz]), α (8–13 [Hz]), β (14–29 [Hz]) and
γ (30–47 [Hz]) power spectral density for each
channel.

EDA(21)

average skin resistance, average of derivative, average of derivative for negative values only, proportion
of negative samples in the derivative vs all samples,
number of local minima in the EDA signal, average
rising time of the EDA signal, 14 spectral power
in the 0–2.4 [Hz] bands, zero crossing rate of Skin
conductance slow response 0–0.2 [Hz], zero crossing
rate of Skin conductance very slow response 0–0.08
[Hz].

BVP(13)

average and standard deviation of HR, HRV, and inter beat intervals, energy ratio between the frequency
bands 0.04–0.15 [Hz] and 0.15–0.5 [Hz], spectral
power in the bands 0.1–0.2 [Hz], 0.2–0.3 [Hz], 0.3–
0.4 [Hz], low frequency 0.01–0.08 [Hz], medium
frequency 0.08–0.15 [Hz] and high frequency 0.15–
0.5 [Hz] components of HRV power spectrum.

Temp.(4)

average, average of its derivative, spectral power in
the band 0–0.1 [Hz], 0.1–0.2 [Hz].

ICA components were removed upon inspection by the expert.
Both CAR and ICA were implemented using the MNE-python
package [61]. Conventional feature extraction, as shown in
Table II, was incorporated into the recorded data, including
pre-processed EEG, EDA, BVP, and Temp. These features
were based on previous works [6], [37], [38], as the baseline
for comparison with the results of this study.
5) Emotion Recognition: The studies on emotion recognition were analyzed in four directions, the first of which
was based on a simple threshold for labeling two classes of
emotional states: low-high. Whereas, the second, third, and
fourth were based on k-mean clustering using the same two
low-high classes.
•

Low-High: Valence, Arousal, Happiness, Fear, Excite-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

•

•

•

ment
The recorded signals were taken from each participant
using one clip as a single sample. Hence, there were 645
(43 participants × 15 clips) samples in total. We labeled
each sample using self-emotional assessment scores (V,
A, H, F, and E). For binary classification purposes, we
manually set the threshold of each score at 5 (from scores
range 1–9). For example, samples with H scores lower
than 5 were labeled as low H, and high H in vice versa.
Here, we had six binary classification tasks with a lowhigh of V, A, H, F, and E. Table II shows the set of
input features for the SVM using a linear kernel classifier
for all tasks. We incorporated gridsearchcv to achieve
an optimal set of SVM parameters. Leave-one-clip-out
cross-validation was performed on these experiments.
Since only nine out of 15 clips were viewed by all
participants, nine-fold cross-validation was conducted. In
each fold, one common clip was selected as a test sample
and the others were used as training samples.
Low-High: Valence, Arousal
Using the same population as in the previous subsection,
k-mean clustering was conducted using the participants’
V and A scores. According to four combinations of V
and A models in a conventional emotion-related study
(low (L) or high (H) V and A scores), the number of
clusters or k-mean was set to four (LVLA, LVHA, HVLA,
LVHA). The samples in each cluster were then labeled
following their own clusters, either low or high V and
A. The classification tasks were conducted as in the
previous subsection except on this occasion we were only
interested in V and A classification tasks (not on H, F,
and E). Here, the same clip may not have an identical
label, so nine-fold cross-validation was performed based
on samples rather than leaving-one-clip out.
EEG electrode channels
In this study, the electrode setups were explored for the
future hardware design of a user-friendly EEG device
capable of emotion recognition. Here, three or four out
of eight electrode channels were strategically selected
for a hardware design perspective. The process began
by studying the middle line of the human scalp (Fz ,
Cz , Pz Oz ), and another eight sets of channels were
then created (F P1 , F P2 , Fz ), (F P1 , F P2 , Cz ), (F P1 ,
F P2 , Cz ), (F P1 , F P2 , Pz ), (F P1 , F P2 , Oz ), (T3 , T4 ,
Fz ), (T3 , T4 , Cz ), (T3 , T4 , Pz ) and (T3 , T4 , Oz ). These
were taken from a combination of either the temporal or
frontal lobe and one channel from the middle line. Binary
classification tasks (low-high V and A) were subsequently
performed as for the previous task (Low-High: Valence,
Arousal) and the results compared among the sets of
channels.
EEG frequency bands
The aim of this study is to find important frequency
bands for EEG signals. From four basic frequency band
signals, θ (3–7 [Hz]), α (8–13 [Hz]), β (14–29 [Hz])
and γ (30–47 [Hz]), the following combinations of the
relevant frequencies were created: (θ, α), (θ, β), (θ, γ),
(α, β), (α, γ), (β, γ), (θ, α, β), (θ, α, γ), (θ, β, γ), (α,

5

(a)

(b)

(c)

(d)

Fig. 4: (a) was the scatter plot of all qualitative samples from
Experiment I. k-mean clustering on (a) provided the output
cluster as in (b). After removing the qualitative samples not
belonging to the same majority cluster of the samples from
each clip, the remaining candidates were performed in (c).
Eventually, in (d), the 20 points per cluster were retained
to give the nearest distances to the centroids for emotional
stimulation in Experiment II.

β, γ). Finally, binary task classification (low-high V and
A) was performed as in the previous task (Low-High:
Valence, Arousal) and the results subsequently compared
among the sets of frequency bands.
III. R ESULTS
The results of this study are sequentially organized according to the experiments. The k-mean clustering results
for the stimulus clip selection were used to gather the most
effective clips for EEG and peripheral physiological signals in
Experiment I. Finally, in Experiment II, the gathered datasets
were analyzed using a simple machine learning algorithm and
the results compared to the previously related datasets for
application of affective computing. The results were comparable to the existing works because we used a similar
elicited emotion method with the same algorithm for emotion
recognition.
A. Experiment I: Elicited Video Selection
The qualitative results, Happiness (H), Fear (F), and Excitement (E) scores from all participants on each candidate
clip were scattered on three-dimensional spaces (H, F, E),
as shown in Figure 4 (a). In Figure 4 (b), all scattered
points (samples) were labeled (colorized) according to three
output clusters using the k-mean method. As explained in
Section II: Experiment I, the qualitative results or scattered
points were removed from a clip not labeled the same as the
majority of labels in that clip. Thus, the scatter points of each
cluster were subsequently obtained as shown in Figure 4 (c).
Finally, only 20 points in each cluster (60 clips in total) were
retained to provide the nearest distances to the centroids for
emotional stimulation in Experiment II, Figure 4 (d). Here,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

6

TABLE III: Selected clips used to elicit emotion during the recording of EEG and peripheral physiological signals in Experiment
II. The mean and standard deviations of qualitative results from each clip were also calculated and inserted into this table. The
samples (Samp.) denote the number of participants who performed emotional self-assessment using qualitative measures after
watching the clip.
ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
53
54
55
56
57
58
59
60

Movie clip trailers
The Help
Star Wars: The Last Jedi
Suicide Squad
Pacific Rim
War for the Planet of the Apes
God Help the Girl
Rogue One: A Star Wars Story
Blade Runner 2049
Hope Springs
Ghost in the Shell
Point Break
The Hunger Games
Crazy, Stupid, Love.
Arrival
Mr. Hurt
American Assassin
G.I. Joe: Retaliation
Beginners
Open Grave
Flipped
The Choice
Danny Collins
The Big Sick
Monsters University
Kung Fu Panda 3
Baby Driver
The Good Dinosaur
About Time
Ordinary World
Lion
Shrek Forever After
Chappie
Guardians of the Galaxy Vol. 2
The Intern
La La Land
Ice Age: Collision Course
Frozen
Transformers: The Last Knight
Divergent
Why Him?
The Boy
Jigsaw
Shutter
Ladda Land
No One Lives
Tales from the Crypt
Orphan
Unfriended
Poltergeist
Jeruzalem
Leatherface
The Babadook
Oculus
The Witch
Trick ’r Treat
The Woman in Black
The Possession
Crimson Peak
Program na winyan akat
The Pact

Affective Tags (IMDb)
Drama
Action, Adventure, Fantasy, Sci-Fi
Action, Adventure, Fantasy, Sci-Fi
Action, Adventure, Sci-Fi
Action, Adventure, Drama, Sci-Fi, Thriller
Drama, Musical, Romance
Action, Adventure, Sci-Fi
Drama, Mystery, Sci-Fi
Comedy, Drama, Romance
Action, Drama, Sci-Fi
Action, Crime, Sport
Adventure, Sci-Fi, Thriller
Comedy, Drama, Romance
Drama, Mystery, Sci-F
Comedy, Romance
Action, Thriller
Action, Adventure, Sci-Fi
Comedy, Drama, Romance
Horror, Mystery, Thriller
Comedy, Drama, Romance
Drama, Romance
Biography, Comedy, Drama
Comedy, Drama, Romance
Animation, Adventure, Comedy
Animation, Action, Adventure
Action, Crime, Drama
Animation, Adventure, Comedy
Comedy, Drama, Fantasy
Comedy, Drama, Music
Biography, Drama
Animation, Adventure, Comedy
Action, Crime, Drama
Action, Adventure, Sci-Fi
Comedy, Drama
Comedy, Drama, Music
Animation, Adventure, Comedy
Animation, Adventure, Comedy
Action, Adventure, Sci-Fi
Adventure, Mystery, Sci-Fi
Comedy
Horror, Mystery, Thriller
Crime, Horror, Mystery
Horror, Mystery, Thriller
Horror
Horror, Thriller
Horror
Horror, Mystery, Thriller
Drama, Horror, Mystery
Horror, Thriller
Horror
Crime, Horror, Thriller
Drama, Horror
Horror, Mystery
Horror, Mystery
Comedy, Horror, Thriller
Drama, Fantasy, Horror
Horror, Thriller
Drama, Fantasy, Horror
Horror, Thriller
Horror, Mystery, Thriller

we hypothesized that the selected clips (Table III) using the
proposed unsupervised learning were the most effective for
emotional elicitation.
B. Experiment II: Emotion Recognition Using OpenBCI
As explained in Experiment II, studies on emotion recognition using OpenBCI were analyzed in four directions. Here,
the experimental results are reported for each direction.
• Low-High: Valence, Arousal, Happiness, Fear, Excitement

Valence
4.28 ± 2.08
4.98 ± 2.15
4.73 ± 2.02
4.10 ± 2.03
4.73 ± 2.06
4.38 ± 2.45
5.09 ± 1.78
4.44 ± 2.47
4.78 ± 2.46
4.77 ± 2.10
4.65 ± 2.33
5.42 ± 2.26
4.82 ± 2.54
4.84 ± 2.22
4.50 ± 2.21
4.19 ± 2.18
4.69 ± 2.39
4.42 ± 2.45
3.70 ± 1.90
5.05 ± 2.69
4.58 ± 2.11
4.54 ± 2.20
4.44 ± 2.10
5.55 ± 2.07
6.11 ± 2.17
5.29 ± 2.10
6.43 ± 2.15
5.25 ± 2.60
4.88 ± 1.72
5.36 ± 2.30
5.87 ± 2.12
5.47 ± 2.26
6.15 ± 2.40
6.34 ± 2.02
5.44 ± 2.24
6.38 ± 2.36
5.88 ± 2.40
4.57 ± 2.06
5.87 ± 1.81
5.85 ± 2.24
3.85 ± 2.09
4.04 ± 2.14
3.53 ± 2.20
4.61 ± 2.20
4.20 ± 2.04
3.83 ± 2.22
4.07 ± 2.35
4.34 ± 2.57
4.13 ± 2.44
4.20 ± 2.12
3.92 ± 2.11
3.62 ± 2.06
4.12 ± 2.11
3.69 ± 2.17
4.50 ± 2.27
4.23 ± 2.29
4.83 ± 2.47
3.93 ± 2.15
3.95 ± 2.11
4.37 ± 2.00

Arousal
3.34 ± 1.96
4.36 ± 2.08
4.12 ±1.90
3.81 ± 2.16
4.04 ± 2.20
3.09 ± 2.02
4.65 ± 2.15
4.34 ± 2.38
3.56 ± 2.12
4.28 ± 2.27
4.49 ± 2.40
4.76 ± 2.18
4.01 ± 2.52
4.82 ± 2.28
3.59 ± 2.20
4.80 ± 2.33
4.11 ± 2.31
3.08 ± 2.18
4.70 ± 2.30
3.75 ± 2.40
4.12 ± 1.98
3.55 ± 2.07
3.21 ± 1.80
4.01 ± 2.12
4.46 ± 2.31
4.65 ± 2.27
4.63 ± 2.33
4.29 ± 2.68
3.93 ± 1.72
4.62 ± 2.65
3.85 ± 2.37
4.43 ± 2.20
4.61 ± 2.34
5.06 ± 2.13
4.09 ± 2.37
4.96 ± 2.40
4.17 ± 2.38
4.18 ± 1.96
4.87 ± 2.00
4.60 ± 2.40
4.92 ± 1.97
4.68 ± 2.15
4.71 ± 2.25
4.81 ± 2.23
4.84 ± 2.28
4.41 ± 2.21
5.18 ± 2.11
5.40 ± 2.57
5.28 ± 2.55
4.82 ± 2.14
4.77 ± 2.39
4.84 ± 2.19
5.81 ± 1.90
4.69 ± 2.44
5.59 ± 2.30
4.67 ± 2.27
5.32 ± 2.29
4.79 ± 2.33
4.98 ± 2.48
5.56 ± 2.40

Happiness
4.41 ± 2.12
4.06 ± 2.16
4.36 ± 2.18
3.93 ± 2.04
2.57 ± 1.90
5.00 ± 2.45
3.34 ± 2.17
3.15 ± 2.02
5.09 ± 2.34
3.41 ± 2.13
3.31 ± 2.33
3.66 ± 2.09
5.12 ± 2.64
3.28 ± 2.03
4.66 ± 2.19
2.90 ± 1.88
3.46 ± 2.09
4.97 ± 2.20
1.90 ± 1.14
5.43 ± 2.48
4.63 ± 2.16
5.01 ± 2.09
4.56 ± 2.15
6.15 ± 2.12
6.44 ± 2.37
5.19 ± 2.32
6.47 ± 2.18
5.97 ± 2.32
5.47 ± 1.68
5.01 ± 2.49
6.29 ± 2.29
4.54 ± 2.45
5.85 ± 2.40
6.31 ± 1.98
5.55 ± 2.29
6.92 ± 1.94
6.31 ± 2.41
4.10 ± 2.33
4.75 ± 2.27
6.03 ± 2.30
1.78 ± 1.06
2.07 ± 1.37
1.68 ± 0.97
1.95 ± 1.47
1.94 ± 1.35
2.21 ± 1.88
1.85 ± 1.40
1.98 ± 1.93
1.91 ± 1.70
2.02 ± 1.42
1.89 ± 1.27
1.67 ± 1.05
1.76 ± 1.29
1.89 ± 1.38
1.93 ± 1.42
1.94 ± 1.54
1.82 ± 1.44
2.33 ± 1.71
1.72 ± 1.20
1.86 ± 1.42

Fear
1.26 ±
1.79 ±
1.73 ±
1.31 ±
2.67 ±
1.27 ±
2.04 ±
2.39 ±
1.19 ±
2.07 ±
2.08 ±
2.06 ±
1.21 ±
2.64 ±
1.23 ±
2.48 ±
1.55 ±
1.23 ±
4.03 ±
1.16 ±
1.43 ±
1.17 ±
1.28 ±
1.24 ±
1.25 ±
1.53 ±
1.19 ±
1.22 ±
1.22 ±
1.79 ±
1.26 ±
1.69 ±
1.44 ±
1.23 ±
1.49 ±
1.21 ±
1.24 ±
1.91 ±
1.95 ±
1.25 ±
5.21 ±
4.65 ±
5.23 ±
5.62 ±
4.97 ±
4.67 ±
5.11 ±
5.34 ±
5.85 ±
5.00 ±
4.93 ±
5.34 ±
6.07 ±
5.12 ±
5.68 ±
5.21 ±
5.90 ±
4.81 ±
5.52 ±
6.23 ±

0.37
1.00
1.41
0.52
1.76
0.47
1.23
1.73
0.17
1.57
1.41
1.34
0.25
1.90
0.21
1.92
0.93
0.27
2.40
0.08
0.68
0.09
0.35
0.23
0.24
0.88
0.41
0.34
0.20
1.25
0.29
0.73
1.01
0.39
1.28
0.26
0.40
1.22
1.34
0.39
2.23
2.18
2.34
2.13
2.56
2.36
2.15
2.53
2.36
2.23
2.53
2.11
1.81
2.40
2.26
2.33
2.42
2.43
2.45
2.05

Excitement
2.56 ± 1.66
5.09 ± 2.12
4.37 ± 2.14
4.25 ± 2.12
4.83 ± 2.08
3.01 ± 1.85
5.28 ± 2.27
4.81 ± 2.29
2.84 ± 1.89
5.01 ± 2.27
5.12 ± 2.56
5.38 ± 2.05
3.06 ± 2.35
5.66 ± 2.40
2.41 ± 1.43
5.03 ± 2.36
5.02 ± 2.54
2.42 ± 1.48
4.55 ± 2.46
2.88 ± 2.12
3.24. ± 1.71
2.76 ± 1.68
2.93 ± 1.90
4.44 ± 2.14
4.14 ± 2.11
5.30 ± 2.28
4.22 ± 2.07
4.25 ± 2.24
3.45 ± 1.98
3.93 ± 2.35
4.35 ± 2.49
5.31 ± 2.31
5.56 ± 2.50
3.74 ± 2.34
3.15 ± 1.99
4.87 ± 2.36
4.35 ± 2.36
4.87 ± 1.97
5.84 ± 2.05
4.06 ± 2.44
5.01 ± 2.16
4.91 ± 2.18
4.63 ± 2.29
4.88 ± 2.07
5.07 ± 2.33
4.68 ± 2.41
4.68 ± 2.27
5.37 ± 2.55
5.20 ± 2.60
4.90 ± 2.27
4.90 ± 2.50
4.74 ± 2.17
5.36 ± 2.28
4.54 ± 2.37
5.55 ± 2.34
4.79 ± 2.16
5.66 ± 2.38
5.10 ± 2.16
5.00 ± 2.26
5.62 ± 2.50

Samp.
82
81
81
44
41
43
42
44
45
47
40
42
43
44
42
43
48
44
43
44
82
82
86
47
47
44
43
43
49
48
44
47
48
42
47
43
47
42
49
43
82
86
81
42
47
44
40
43
43
49
47
43
42
42
42
42
42
43
43
44

In these tasks, ground truth labels were created using
self-emotional assessment scores (V, A, H, F, and E)
with the threshold empirically set at 5. Scores higher
than the threshold were assigned to the high-level class
and vice versa for scores lower than the threshold. The
input features extracted from EEG, E4, and Fusion (EEG
and E4) were used to train and test the model. Table IV
presents the mean accuracy and mean nine-fold F1 scores.
The condition with Fusion of EEG and E4 as input
features reached 68.21 and 74.41% of mean accuracy for

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

7

TABLE IV: Accuracy and F1 score for low-high recognition

OpenBCI (EEG)
E4
Fusion

Valence
[1.79 : 1]

Arousal
[2.03 : 1]

0.6718
0.6563
0.6821

0.7286
0.6950
0.7441

Accuracy
Happiness
Fear
[2.41 : 1]
[5.20 : 1]
0.7158
0.7158
0.7054

Excitement
[0.66 : 1]

Reward
[0.60 : 1]

Valence

Arousal

0.7054
0.6848
0.6873

0.6615
0.6253
0.6512

0.5900
0.3922
0.5990

0.6523
0.4765
0.6723

0.8450
0.8450
0.8424

F1 Score
Happiness
Fear
0.5760
0.5760
0.5359

0.6172
0.6172
0.5610

Excitement

Reward

0.5848
0.3974
0.5488

0.5904
0.3808
0.5847

Numbers inside brackets are class ratios [low : high].
Bold indicates the best results for the proposed datasets.

TABLE VII: Accuracy and F1 score from varying
EEG frequency bands

Fig. 5: k-mean clustering using valence and arousal scores
labeled
TABLE V: Comparison accuracy and F1 scores for low-high
classification with the other datasets using exactly the same
feature extraction and recognition methods.
Modality

Accuracy
Valence
Arousal

F1 Score
Valence
Arousal

OpenBCI (EEG)
E4
OpenBCI & E4 (Fusion)
class ratio

0.6718
0.6563
0.6821
1.79:1

0.7286
0.6950
0.7441
2.03:1

0.5900
0.3922
0.5990
1.79:1

0.6523
0.4765
0.6723
2.03:1

OpenBCI (EEG) :k-mean
E4 :k-mean
OpenBCI & E4 (Fusion) :k-mean
class ratio

0.6460
0.5943
0.6718
0.68:1

0.6977
0.6693
0.7054
1.87:1

0.6107
0.3770
0.6369
0.68:1

0.6193
0.4563
0.6403
1.87:1

DREAMER (EEG) [6]
DREAMER (ECG) [6]
DREAMER (Fusion) [6]
DEAP (EEG) [37]
DEAP (Peripheral) [37]
MAHNOB-HCI (EEG) [38]
MAHNOB-HCI (Peripheral) [38]
DECAF (MEG) [40]
DECAF (Peripheral) [40]

0.6249
0.6237
0.6184
0.5760
0.6270
0.5700
0.4550
0.5900
0.6000

0.6217
0.6237
0.6232
0.6200
0.5700
0.5240
0.4620
0.6200
0.5500

0.5184
0.5305
0.5213
0.5630
0.6080
0.5600
0.3900
0.5500
0.5900

0.5767
0.5798
0.5750
0.5830
0.5330
0.4200
0.3800
0.5800
0.5400

Numbers at class ratio row are class ratios [low : high].
Bold indicates the best results for all datasets.

TABLE VI: Accuracy and F1 score form channel
selection
Channel

Accuracy
Valence
Arousal

F1 Score
Valence
Arousal

F P1 , F P2 , Fz
F P1 , F P2 , Cz
F P1 , F P2 , Pz
F P1 , F P2 , Oz
T3 , T4 , Fz
T3 , T4 , Cz
T3 , T4 , Pz
T3 , T4 , Oz
Fz , Cz , Pz , Oz

0.6162
0.6162
0.6046
0.6162
0.6813
0.6511
0.6209
0.6325
0.6139

0.3782
0.3782
0.5148
0.3782
0.6421
0.6067
0.5687
0.5869
0.5394

0.6767
0.6558
0.6674
0.6767
0.6767
0.6767
0.6767
0.6767
0.6883

0.3989
0.4681
0.5087
0.3989
0.3989
0.3989
0.3989
0.3989
0.5524

Bold indicates the best results for our proposed datasets.

Frequency band

Accuracy
Valence
Arousal

F1 Score
Valence
Arousal

θ
α
β
γ
θ, α
θ, β
θ, γ
α, β
α, γ
β, γ
θ, α, β
θ, α, γ
θ, β, γ
α, β, γ

0.5943
0.5943
0.5943
0.5659
0.5943
0.5943
0.5659
0.5943
0.6305
0.6227
0.5943
0.6253
0.6098
0.6202

0.3704
0.3704
0.3704
0.4510
0.3704
0.3704
0.4537
0.3704
0.5895
0.5757
0.3704
0.5823
0.5636
0.5875

0.6563
0.6563
0.6563
0.6563
0.6563
0.6563
0.6563
0.6486
0.6486
0.6408
0.6486
0.6460
0.6408
0.6718

0.3919
0.3919
0.3919
0.3919
0.3919
0.3919
0.3919
0.4156
0.4962
0.4208
0.4156
0.4943
0.4208
0.5693

Bold indicates the best results for our proposed datasets.

•

V and A, respectively. However, input features consisting
of either EEG or E4 provided better results than Fusion
for H, F, and E. In terms of F1 score, the results were
consistent with the mean accuracy. Furthermore, EEG,
E4, and Fusion as input features were compared in binary
classification tasks. One-way repeated measures ANOVA
revealed significant differences in the classification of
V (F(2, 16) = 48.726, p < 0.05) and A (F(1.196,
9.570) = 61.931, p < 0.05) (low-high) levels. Pairwise
comparison of E4 (A: 0.477 ± 0.032, V: 0.392± 0.018)
was significantly worse than both EEG (A: 0.652 ±
0.016, V: 0.590 ± 0.024) and Fusion (A: 0.672± 0.013,
V: 0.559 ± 0.025) in terms of F1 Score, p < 0.05.
Low-High: Valence, Arousal As shown in Table III, V
and A scores were widely distributed according to their
large standard deviation values. A larger population might
have a higher standard deviation value. Hence, the fixed
threshold is not suitable. To solve this problem, k-mean
clustering was proposed for labeling low-high levels of V
and A, and binary classification tasks subsequently performed. Figure 5 shows the results following completion
of k-mean clustering. We empirically labeled according
to the VA model [62], with the Blue and Red group for
low-A (LA), and the Green and Yellow group for high-A
(HA). In terms of valence, the Red and Green group was
selected for low-V (LV), and the Blue and Yellow group
for high H (HV). Table V presents the mean accuracy
and mean nine-fold F1 scores. The condition with the
Fusion of EEG and E4 as the input features reached mean

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

•

accuracy at 67.18 and 70.54% for V and A, respectively.
In terms of F1 score, Fusion features provided the best
results for V and A (0.6369 and 0.6403, respectively).
One-way repeated measures ANOVA revealed significant
differences in the classification of V (F(2, 16) = 26.871,
p < 0.05) and A (F(2, 16) = 104.430, p < 0.05)
(low-high) levels. Pairwise comparison was performed,
indicating that E4 (A: 0.377 ± 0.017, V: 0.456± 0.026)
was significantly worse than both EEG (A: 0.611 ±
0.029, V: 0.619 ± 0.024) and Fusion (A: 0.637± 0.026,
V: 0.640 ± 0.015) in terms of F1 Score, p < 0.05.
Table V also reports the V and A classification results
from DREAMER [6], DEAP [37], MAHNOB-HCI [38]
and DECAF [40] datasets using exactly the same feature
extraction and classification methods.
EEG Electrode Channels and Frequency Bands The
results from channel selection are presented in Table VI
for the mean accuracy and mean F1 scores from nine
folds. In the low-high classification task for V, the condition with EEG channels from T3 , T4 , Fz as the input
features reached mean accuracy at 68.13% and mean F1score at 0.6421. However, for A, mean accuracy reached
68.83% and the mean F1-score reached 0.5524 using Fz ,
Cz , Pz , Oz . Finally, the results obtained from varying
EEG frequency bands as shown in Table VII, indicate
the mean accuracy and mean F1 scores from nine folds.
Classification accuracy for V and A reached 62.02 and
67.18%, respectively, using EEG features from α, β and
γ bands. In terms of F1 score, the accuracy results were
similar. The features from α, β and γ bands provided the
best results in V and A classification tasks (0.5875 and
0.5693, respectively).
IV. D ISCUSSION

This paper aims to discuss the academic merits of this work
for scientific purposes. One of the most important aspects
of gathering an affective EEG with peripheral physiological
signals is the use of an elicited emotion method. State-of-theart works and datasets in this study are based on audio-visual
stimulation or movie clips. The strong point of this paper in
comparison to previous works is that it involves a conservative
clip selection from 200 participants of various ages (15–22year-olds), with k-mean clustering proposed as the method
for ranking candidate clips. Finally, a list of clips for affective
computing studies is shown in Table III. These are all open to
the public via YouTube, for ease of use in further study.
Another strong point is the evaluation of the emotion
recognition task for low-high levels of valence and arousal. In
previous research, the threshold is empirically set according to
the self-emotional assessment scores for ground truth labeling
[6], [37], [38], [40]. However, the list of selected clips in
Table III shows high standard deviations in the self-assessment
scores. In a large population, simple thresholding might not
work well since different people are likely to experience
varying levels of low or high valence and arousal. Thus, a
simple mathematical based algorithm; k-mean clustering, is
proposed for labeling the ground truth. As reported in Table V,

8

no statistical difference was found between the two methods
of ground truth labeling. Hence, labeling by k-mean clustering
might work well in future studies with a larger population or
number of participants. Moreover, emotion recognition using
OpenBCI was comparable to state-of-the-art works from highend or expensive EEGs (both in accuracy and F1 Score)
as shown in Table V, including one involving magnetoencephalography (MEG) which is a functional brain mapping
method (considerably more expensive than EEG).
Table VI and Table VII show the study results of EEG
factors, involving electrode channel selection for the future
development of user-friendly devices in emotion recognition
and frequency selection to obtain more effective EEG frequency bands as classification features. According to the
aforementioned tables, T3 , T4 , Fz achieved the best results for
valence classification both in terms of accuracy and F1 score,
while the middle line (Fz , Cz , Pz , Oz ) was promising for the
further improvement of an emotion recognition algorithm in
both valence and arousal for low-high classification. Taking
both results together, T3 , T4 , Fz and Fz , Cz , Pz , Oz with all
EEG frequency bands as input features offer the best path for
developing practical, usable low-cost devices for mental state
recognition, especially concerning valence and arousal.
Finally, this study is the first concrete performance evaluation using a low-cost, open-source device for emotion
recognition entitled OpenBCI. In comparison to the highend EEG amplifier with a greater number of electrodes and
sampling frequencies, OpenBCI demonstrated competitive capability. A low-cost, open-source device is a game changer
for encouraging developers or researchers to improve emotion
recognition algorithms. It can facilitate further progress toward
online applications since the device is inexpensive and within
the reach of the majority of people. For example, one team
recently proposed the algorithm k-nearest neighbors, based
on dynamic time warping, to deal with physiological signals
and the affective computing using datasets of E4 [63]. We
might be able to modify this work for OpenBCI. There are
two related works on deep learning with EEG in our group
which can also be adapted and incorporated into emotion
recognition tasks [64], [65]. Moreover, emotion recognition
using peripheral physiological data from a real-time bracelet
sensor or E4 remains a challenge. The E4 is a simple wearable
device and resembles a watch so is very useful as a long-term
affective state or mental health monitoring system.
V. C ONCLUSION
In summary, we introduced the low-cost and open-source
EEG entitled OpenBCI and its proposed capabilities for
emotion recognition. In the experiments, the dataset includes
EEG and peripheral physiological signals from 43 participants
while viewing the elicited trailer clips. Subsequently, the
self-assessment emotional scores from the participants were
measured according to the scales of valence and arousal.
Furthermore, for both valence and arousal, the k-means clustering algorithm was applied to create ground truth labels from
low and high clusters. In contrast, previous works performed
labeling by empirical thresholding. Finally, the experimental

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

results were found to be competitive with state-of-the-art
datasets from high-end EEG. The ultimate goal of this study is
for the experimental data obtained to provide inspiration and
benefit for researchers and engineers with the utilization of
OpenBCI as a recording device in the development of online,
emotional EEG-related applications. The data will be available
(upon request) to other researchers for further development and
study.
R EFERENCES
[1]
[2]
[3]
[4]
[5]
[6]

[7]
[8]
[9]
[10]

[11]
[12]
[13]

[14]
[15]

[16]

[17]
[18]
[19]
[20]
[21]

[22]

[23]

https://www.emotiv.com/.
https://store.myndplay.com/.
http://neurosky.com/.
https://choosemuse.com/.
http://openbci.com/.
S. Katsigiannis and N. Ramzan, “Dreamer: a database for emotion
recognition through eeg and ecg signals from wireless low-cost offthe-shelf devices,” IEEE journal of biomedical and health informatics,
vol. 22, no. 1, pp. 98–107, 2018.
J. A. M. Correa, M. K. Abadi, N. Sebe, and I. Patras, “Amigos: A
dataset for affect, personality and mood research on individuals and
groups,” IEEE Transactions on Affective Computing, 2018.
https://www.empatica.com/research/e4/.
https://www.tobiipro.com/.
J.-M. Lpez-Gil, J. Virgili-Gom, R. Gil, T. Guilera, I. Batalla, J. SolerGonzlez, and R. Garca, “Method for improving eeg based emotion
recognition by combining it with synchronized biometric and eye
tracking technologies in a non-invasive and low cost way,” Frontiers
in Computational Neuroscience, vol. 10, p. 85, 2016.
“Evolutionary computation algorithms for feature selection of eegbased emotion recognition using mobile sensors,” Expert Systems with
Applications, vol. 93, pp. 143 – 155, 2018.
“Detecting boredom from eye gaze and eeg,” Biomedical Signal Processing and Control, vol. 46, pp. 302 – 313, 2018.
Y. Liu, M. Yu, G. Zhao, J. Song, Y. Ge, and Y. Shi, “Real-time
movie-induced discrete emotion recognition from eeg signals,” IEEE
Transactions on Affective Computing, vol. 9, no. 4, pp. 550–562, Oct
2018.
“Toward automatic detection of brain responses to emotional music
through analysis of eeg effective connectivity,” Computers in Human
Behavior, vol. 58, pp. 231 – 239, 2016.
A. Martnez-Rodrigo, A. Fernndez-Sotos, J. M. Latorre, J. MonchoBogani, and A. Fernndez-Caballero, “Neural correlates of phrase
rhythm: An eeg study of bipartite vs. rondo sonata form,” Frontiers
in Neuroinformatics, vol. 11, p. 29, 2017.
A. Fernndez-Soto, A. Martnez-Rodrigo, J. Moncho-Bogani, J. M.
Latorre, and A. Fernndez-Caballero, “Neural correlates of phrase
quadrature perception in harmonic rhythm: An eeg study using a
brain?computer interface,” International Journal of Neural Systems,
vol. 28, no. 05, p. 1750054, 2018, pMID: 29298521.
R. Ramirez, J. Planas, N. Escude, J. Mercade, and C. Farriols, “Eegbased analysis of the emotional effect of music therapy on palliative
care cancer patients,” Frontiers in Psychology, vol. 9, p. 254, 2018.
P. Kumar, R. Saini, P. P. Roy, P. K. Sahu, and D. P. Dogra, “Envisioned speech recognition using eeg sensors,” Personal and Ubiquitous
Computing, vol. 22, no. 1, pp. 185–199, 2018.
M.-H. Lin, S. N. Cross, W. J. Jones, and T. L. Childers, “Applying eeg
in consumer neuroscience,” European Journal of Marketing, vol. 52, no.
1/2, pp. 66–91, 2018.
M. Yadava, P. Kumar, R. Saini, P. P. Roy, and D. P. Dogra, “Analysis
of eeg signals and its application to neuromarketing,” Multimedia Tools
and Applications, vol. 76, no. 18, pp. 19 087–19 111, 2017.
A. J. Casson and E. V. Trimble, “Enabling free movement eeg tasks
by eye fixation and gyroscope motion correction: Eeg effects of color
priming in dress shopping,” IEEE Access, vol. 6, pp. 62 975–62 987,
2018.
I. Arapakis, M. Barreda-Angeles, and A. Pereda-Banos, “Interest as a
proxy of engagement in news reading: Spectral and entropy analyses of
eeg activity patterns,” IEEE Transactions on Affective Computing, pp.
1–1, 2018.
N. Jadhav, R. Manthalkar, and Y. Joshi, “Effect of meditation on
emotional response: An eeg-based study,” Biomedical Signal Processing
and Control, vol. 34, pp. 101–113, 2017.

9

[24] A. T. Poulsen, S. Kamronn, J. Dmochowski, L. C. Parra, and L. K.
Hansen, “Eeg in the classroom: Synchronised neural recordings during
video presentation,” Scientific Reports, vol. 7, p. 43916, 2017.
[25] P. Zioga, F. Pollick, M. Ma, P. Chapman, and K. Stefanov, “enheduannaa
manifesto of falling live brain-computer cinema performance: Performer
and audience participation, cognition and emotional engagement using
multi-brain bci interaction,” Frontiers in neuroscience, vol. 12, p. 191,
2018.
[26] Y. Dai, X. Wang, P. Zhang, and W. Zhang, “Wearable biosensor
network enabled multimodal daily-life emotion recognition employing
reputation-driven imbalanced fuzzy classification,” Measurement, vol.
109, pp. 408–424, 2017.
[27] H. Cai, X. Zhang, Y. Zhang, Z. Wang, and B. Hu, “A case-based
reasoning model for depression based on three-electrode eeg data,” IEEE
Transactions on Affective Computing, 2018.
[28] H. Jebelli, S. Hwang, and S. Lee, “Eeg-based workers’ stress recognition
at construction sites,” Automation in Construction, vol. 93, pp. 315–324,
2018.
[29] X. Shan, E.-H. Yang, J. Zhou, and V. W.-C. Chang, “Human-building
interaction under various indoor temperatures through neural-signal
electroencephalogram (eeg) methods,” Building and Environment, vol.
129, pp. 46–53, 2018.
[30] J. Rosenthal and G. Benabdallah, “Ibpoet: an interactive & biosensitive
poetry composition device,” in Proceedings of the 2017 ACM International Joint Conference on Pervasive and Ubiquitous Computing and
Proceedings of the 2017 ACM International Symposium on Wearable
Computers. ACM, 2017, pp. 281–284.
[31] R. Munoz, R. Villarroel, T. S. Barcelos, A. Souza, E. Merino, R. Guiñez,
and L. A. Silva, “Development of a software that supports multimodal
learning analytics: A case study on oral presentations,” Journal of
Universal Computer Science, vol. 24, no. 2, pp. 149–170, 2018.
[32] M. Mohammadpour, S. M. R. Hashemi, and N. Houshmand, “Classification of eeg-based emotion for bci applications,” in 2017 Artificial
Intelligence and Robotics (IRANOPEN), April 2017, pp. 127–131.
[33] M. Mohammadpour, M. M. AlyanNezhadi, S. M. R. Hashemi, and
Z. Amiri, “Music emotion recognition based on wigner-ville distribution
feature extraction,” in 2017 IEEE 4th International Conference on
Knowledge-Based Engineering and Innovation (KBEI), Dec 2017, pp.
0012–0016.
[34] M. A. Hafeez, S. Shakil, and S. Jangsher, “Stress effects on exam
performance using eeg,” in 2018 14th International Conference on
Emerging Technologies (ICET), Nov 2018, pp. 1–4.
[35] L. Rahman and K. Oyama, “Long-term monitoring of nirs and eeg
signals for assessment of daily changes in emotional valence,” in 2018
IEEE International Conference on Cognitive Computing (ICCC), July
2018, pp. 118–121.
[36] R. Munoz, R. Olivares, C. Taramasco, R. Villarroel, R. Soto, M. F.
Alonso-Sánchez, E. Merino, and V. H. C. de Albuquerque, “A new
eeg software that supports emotion recognition by using an autonomous
approach,” Neural Computing and Applications, pp. 1–17.
[37] S. Koelstra, C. Muhl, M. Soleymani, J.-S. Lee, A. Yazdani, T. Ebrahimi,
T. Pun, A. Nijholt, and I. Patras, “Deap: A database for emotion analysis;
using physiological signals,” IEEE Transactions on Affective Computing,
vol. 3, no. 1, pp. 18–31, 2012.
[38] M. Soleymani, J. Lichtenauer, T. Pun, and M. Pantic, “A multimodal
database for affect recognition and implicit tagging,” IEEE Transactions
on Affective Computing, vol. 3, no. 1, pp. 42–55, Jan 2012.
[39] W. Zheng, W. Liu, Y. Lu, B. Lu, and A. Cichocki, “Emotionmeter:
A multimodal framework for recognizing human emotions,” IEEE
Transactions on Cybernetics, pp. 1–13, 2018.
[40] M. K. Abadi, R. Subramanian, S. M. Kia, P. Avesani, I. Patras, and
N. Sebe, “Decaf: Meg-based multimodal database for decoding affective
physiological responses,” IEEE Transactions on Affective Computing,
vol. 6, no. 3, pp. 209–222, 2015.
[41] U. Rashid, I. Niazi, N. Signal, and D. Taylor, “An eeg experimental study
evaluating the performance of texas instruments ads1299,” Sensors,
vol. 18, no. 11, p. 3721, 2018.
[42] M. Duvinage, T. Castermans, M. Petieau, T. Hoellinger, G. Cheron,
and T. Dutoit, “Performance of the emotiv epoc headset for p300-based
applications,” Biomedical engineering online, vol. 12, no. 1, p. 56, 2013.
[43] N. A. Badcock, P. Mousikou, Y. Mahajan, P. De Lissa, J. Thie, and
G. McArthur, “Validation of the emotiv epoc® eeg gaming system for
measuring research quality auditory erps,” PeerJ, vol. 1, p. e38, 2013.
[44] E. Ratti, S. Waninger, C. Berka, G. Ruffini, and A. Verma, “Comparison
of medical and consumer wireless eeg systems for use in clinical trials,”
Frontiers in human neuroscience, vol. 11, p. 398, 2017.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015

[45] O. E. Krigolson, C. C. Williams, A. Norton, C. D. Hassall, and F. L.
Colino, “Choosing muse: Validation of a low-cost, portable eeg system
for erp research,” Frontiers in neuroscience, vol. 11, p. 109, 2017.
[46] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, “Scikit-learn: Machine learning in Python,” Journal of Machine
Learning Research, vol. 12, pp. 2825–2830, 2011.
[47] B. Chris and L. Jennifer, “Frozen,” USA: Walt Disney Pictures, 2013.
[48] M. Bay, “Transformers: The last knight,” USA: Paramount Pictures,
2017.
[49] J. Collet-Serra, “Orphan,” USA: Warner Bros. Pictures, 2009.
[50] D. Chazelle, “La la land,” USA: Lionsgate, 2016.
[51] N. Meyers, “The intern,” USA: Warner Bros. Pictures, 2015.
[52] D. Villeneuve, “Arrival,” USA: Paramount Pictures, 2016.
[53] https://www.ant-neuro.com/.
[54] https://compumedicsneuroscan.com/.
[55] http://www.gtec.at/.
[56] https://www.advancedbrainmonitoring.com/.
[57] https://www.neuroelectrics.com/.
[58] https://www.brainproducts.com/.
[59] “The e4 wristband is a wearable research device that offers real-time
physiological data acquisition and software for in-depth analysis and
visualization.” https://www.empatica.com/en-eu/research/e4/.
[60] A. Greco, G. Valenza, L. Citi, and E. P. Scilingo, “Arousal and valence
recognition of affective sounds based on electrodermal activity,” IEEE
Sensors Journal, vol. 17, no. 3, pp. 716–725, 2017.
[61] A. Gramfort, M. Luessi, E. Larson, D. Engemann, D. Strohmeier,
C. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, and M. Hmlinen,
“Meg and eeg data analysis with mne-python,” Frontiers in Neuroscience, vol. 7, p. 267, 2013.
[62] J. A. Russell, “A circumplex model of affect.” Journal of personality
and social psychology, vol. 39, no. 6, p. 1161, 1980.
[63] A. Albraikan, D. P. Tobón, and A. El Saddik, “Toward user-independent
emotion recognition using physiological signals,” IEEE Sensors Journal,
2018.
[64] T. Wilaiprasitporn, A. Ditthapron, K. Matchaparn, T. Tongbuasirilai,
N. Banluesombatkul, and E. Chuangsuwanich, “Affective eeg-based
person identification using the deep learning approach,” arXiv preprint
arXiv:1807.03147, 2018.
[65] A. Ditthapron, N. Banluesombatkul, S. Ketrat, E. Chuangsuwanich,
and T. Wilaiprasitporn, “Universal joint feature extraction for p300
eeg classification using semi-supervised autoencoder,” arXiv preprint
arXiv:1808.06541, 2018.

10

