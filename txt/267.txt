PhyAAt: Physiology of Auditory Attention to Speech Dataset

arXiv:2005.11577v1 [cs.HC] 23 May 2020

a

Nikesh Bajaja,b , Jesús Requena Carrióna , Francesco Bellottib
Centre for Intelligent Sensing, School of Electronics Engineering and Computer Science (EECS),
Queen Mary University of London, London, UK
b
Elios Lab, Dipartimento di Ingegneria Navale, Elettrica, Elettronica e delle
Telecomunicazioni (DITEN), University of Genoa, Italy
{n.bajaj, j.requena}@qmul.ac.uk, franz@elios.unige.it

A BSTRACT
Auditory attention to natural speech is a complex brain process. Its quantification from physiological
signals can be valuable to improving and widening the range of applications of current braincomputer-interface systems, however it remains a challenging task. In this article, we present a
dataset of physiological signals collected from an experiment on auditory attention to natural speech.
In this experiment, auditory stimuli consisting of reproductions of English sentences in different
auditory conditions were presented to 25 non-native participants, who were asked to transcribe the
sentences. During the experiment, 14 channel electroencephalogram, galvanic skin response, and
photoplethysmogram signals were collected from each participant. Based on the number of correctly
transcribed words, an attention score was obtained for each auditory stimulus presented to subjects.
A strong correlation (p << 0.0001) between the attention score and the auditory conditions was
found. We also formulate four different predictive tasks involving the collected dataset and develop a
feature extraction framework. The results for each predictive task are obtained using a Support Vector
Machine with spectral features, and are better than chance level. The dataset has been made publicly
available for further research, along with the python library phyaat to facilitate the preprocessing,
modeling, and reproduction of the results presented in this paper. The dataset and other resources are
shared on webpage - https://phyaat.github.io.
Keywords Auditory attention · Physiological signals · EEG · GSR · PPG · Attention of speech · Predictive modeling ·
PhyAAt

1

Introduction

Auditory attention is a cognitive process of great importance that still remains poorly understood [1]. It plays a vital role
in many human activities, from aircraft flying to learning in a classroom environment. Thus, understanding attention
can be important to improve the effectiveness, experience, and performance of many tasks [2]. A better understanding
of auditory attention could also lead to improved brain-computer-interface (BCI) systems that use emotional and mental
states to execute actions.
Auditory attention and auditory perception are closely related processes; while perception is the process of interpreting
information, attention is the process by which discrete pieces of information are selected or discarded [3]. Auditory
perception is a complex brain process that involves encoding sounds into patterns of neural activity [4]. This process
can also modulate other physiological responses, such as the heart rate and the sympathetic tone. The brain activity can
be investigated in the Electroencephalogram (EEG), and the heart rate and sympathetic response can be extracted from
Photoplethysmogram (PPG) and Galvanic Skin Response (GSR) signals, respectively. Other physiological modalities
such as the Electromyogram, the Electrooculogram, and the Electrocardiogram are also available.
Physiological signals are used in many applications from healthcare to BCI system. Datasets of physiological signals
are available for a wide variety of applications, from paralysis [5] and epilepsy [6] studies, to applications for patients in
a complete locked-in state [7, 8, 9] to emotion recognition [10, 11, 12, 13, 14] and serious games [15, 16, 17]. Widely
used EEG datasets for BCI applications include motor imagery datasets [18, 19, 20]. There are several attention related
datasets, including datasets for covert and overt visual attention [21, 22, 23] and for auditory-visual attention shift [24].
Specifically for auditory attention based on EEG signals, an auditory oddball paradigm has been presented, where the
response of participants to oddball sounds inserted in streams of sinusoidal tones was analysed [25, 26].

https://phyaat.github.io

Figure 1: Experiment Design

Among the wide variety of physiological datasets available, to the best of our knowledge there is a lack of datasets
designed to study auditory attention to natural speech. With the goal of supporting future research in the field of
auditory attention, we have designed an experiment based on the dichotic listening task [27], and collected a dataset of
physiological signals that include 14 channel EEG, PPG and GSR. The dataset is available to the wider community for
it to be used freely1 . Based on this dataset, a model could be trained to predict from physiological signals the attention
level of individuals in different scenarios, for instance during the delivery of a lecture. This dataset does not restrict
itself to auditory attention studies and can also be used to investigate other mechanisms involved in brain auditory
processing of natural speech. One example of such applications is the design of the difficulty levels of a game based on
the level of auditory attention [28].
The rest of the article is organised as follows. Section 2 explains the experimental design, materials and procedures.
Section 3 describes the collected dataset, labels and file structure. In Section 4, we analyse the correlation between
attention score and auditory conditions. In Section 5, we formulate the four predictive tasks and Section 6 describes the
framework to extract the features from physiological responses. Section 7 demonstrates the results of the predictive
tasks using the data from one subject. Section 8 concludes the presented work and discusses several directions for
future research. The details of resources made available are explained in the Resources section at the end of the paper.

2

Experiment

2.1

Overview of the experiment design

An experiment based on diochotic listening task was designed to create the dataset for auditory attention analysis. The
main components of this experiment are shown in Figure 1. Unlike the diochotic listening task, only one audio stimuli
per trial was presented to a participant. Audio stimuli were reproduced under three different auditory conditions (N ,S,
and L) and following previous studies, the attention score A for each trial was computed from a transcribed audio
message, by counting the number of correctly identified words [29, 30, 31, 32]. While the establishing attention score,
minor spelling errors and typos were ignored such as did/dids, beautiful/beutiful, dog/dogs. For the entire duration of
the experiment, three different modalities of physiological responses (R) were recorded at the sampling rate of 128Hz.
The details of the methods and materials are explained in subsequent subsections.
1

https://phyaat.github.io

2

https://phyaat.github.io

2.2

Stimuli

A total collection of 5000 audio files, each containing semantically correct English language sentence was obtained
from the Tatoeba Project [33]. The length of stimuli ranges from 3 to 13 words per sentence. All the stimuli were
reproduced by the same speaker to avoid variations in physical properties such as pitch, rate of speech, etc. From the
collection of semantic stimuli, a total 1700 non-semantic stimuli were created by suitably inserting random isolated
words in between. The following are examples of semantic and non-semantic stimuli:
Tx1: I am going to study.
Tx2: I would like to read some books about the Beatles.
Tx3: Let’s touch enjoyable go.
Tx4: I have a hey big are we dog.
In the above examples, stimuli Tx1 and Tx2 are semantic and stimuli Tx3 and Tx4 are non-semantic stimuli generated
by inserting the words highlighted in bold font. From each group of semantic and non-semantic stimuli, six additional
groups of stimuli were created by adding different levels of background noise. The signal to noise ratios (SNR) of
each background noise level were -6 dB, -3 dB, 0 dB, 3 dB, 6 dB and noise-free (∞ dB). In summary, we created two
groups of audio stimuli, semantic and non-semantic, of lengths ranging from 3 to 13 words and six different levels of
background noise was added to each audio stimuli during reproduction.
2.3

Physiological signals

We recorded three types of physiological responses from each participant, namely EEG, GSR, and PPG. A 14 channel
Emotiv Epoc wireless device was used for recording EEG signals [34]. The EEG signals were collected using the
API provided by the device manufacturer with the highest available sampling rate of 128Hz. The DC component was
removed from EEG signals while recording. Two small copper plates placed on the index finger and middle finger were
used to measure the GSR. The resulting GSR signal was low-pass filtered and recorded at a rate of 128 Hz. Both the
raw and low-pass filtered signals were recorded.
A pulse sensor [35] positioned on the ring finger was used to record the PPG response, which measures microvascular
blood volume changes. Pulse rate and the inter-beat interval (IBI) were obtained from the PPG response using software
code provided by the sensor manufacturer [36].
In summary, 19 signals streams were recorded from each participant, i.e. 14 EEG channels, 2 GSR streams (raw and
low-pass filtered) and 3 PPG streams (raw signal, pulse rate, and IBI). All the physiological responses were labeled
accordingly.
2.4

Participants

A total of 25 university students, 21 male, 4 female, 1 left handed, 24 right handed, participated in our study. All the
participants were non-native English speakers (i.e. their first language was other than English) and had no known
auditory processing disorder. The age of the participants ranged from 16 to 34 years. The majority of nationalities among
the participants were Indian and Italian and the majority of the first languages were Arabic, Italian and Malayalam.
2.5

Experiment procedure

After mounting all the sensors e.g. EEG headset, GSR plates, and PPG pulse sensor, participant were provided with
a passive ear-phone and presented with a computer interface. Participants were then asked to enter demographic
information that included nationality, gender, age-group, and first language. Participants were also asked to rate their
English language skills in terms of listening, writing, reading, and speaking. Once all the information was entered,
participants could initiate the experiment.
A total of 144 stimuli were randomly selected without replacement, for each participant. Out of 144, 72 were selected
from the semantic group and 72 from the non-semantic group. In each group, the 72 stimuli were equally split into six
background noise levels, i.e. 12 stimuli were assigned to each noise level. Furthermore, the 12 stimuli within each
noise level had different lengths. Specifically, 5 stimuli were short with an average of 4 words (L1), 4 stimuli had an
average of 8 words (L2), and 3 stimuli were long, with an average of 12 words (L3). The chosen lengths followed
closely previous studies [29]. The variation in length within each category was ±1 word and we assumed that this
variation did not have a significant effect on perception, as suggested by previous studies [37, 38].
The experiment thus consisted of 144 trials per participant, during which they had to transcribe one auditory stimulus.
Each trial was divided into three tasks, namely Listening, Writing, and Resting. The timeline of a trial is shown in
3

https://phyaat.github.io

(a) A participant during the experiment

(b) Timeline of a trial

Figure 2: Experiment procedure

Figure 2b. A participant could actively start a trial by pressing the play button on the computer interface, after which
a listening task would start by reproducing one of the 144 audio stimuli. Once the audio was finished, participants
were allowed to write the response in a text-box. Participants were not allowed to replay any stimuli and the interface
remained disabled while stimuli were being played. After the transcription was written, participants could press the
submit button to save the response and end the writing task. To start new trial, participants had to press the play button
again. The time period between writing and the next listening task was labeled as resting.
Participant were explained the entire procedure beforehand. The average time taken for a participant to complete the
experiment was 40 ± 10 minutes. Figure 2a shows one of the participants conducting the experiment.

3

Collected dataset

The collected dataset includes the physiological responses of the 25 participants. Signals and time segments are
conveniently labeled so as to identify auditory conditions and tasks, and the computed attention scores for each stimuli
is also included. A segment of physiological responses and corresponding labels is shown in Figure 3 and includes the
14 EEG channels, the low-pass GSR signal and the raw PPG response. Listening, writing, and resting segments are
identified by different background colors (green, white, and blue, respectively) and the attention score for listening
segments is also shown. A summary of collected dataset is described in Table 1.
As shown in Figure 4, the collected dataset is structured in a directory containing 25 sub-directories, one for each participant, named as S1, S2, ... S25. Each sub-directory contains two files namely S[x]_Signals.csv and S[x]_Textscores.csv,
where [x] is the participant ID, e.g. S1 corresponds to the participant 1. The signal file includes 19 streams of signals,
time-stamped, and three labels, namely Label_N, Label_S, and Label_T, corresponding to noise level, semanticity, and
task respectively. For the noise level ∞ dB, the numerical value 1000 is used. Semanticity is encoded as 0 and 1, for
semantic and non-semantic respectively. Tasks are encoded as 0, 1, and 2 for listening, writing, and resting respectively.
The labels before first trial are set to -1. The text score file includes the attention score (Label_A) for each trial, along
with noise level, semanticity, length of stimulus, and timestamp. The dataset has been anonymised and the time-stamp
in each files are set to the same time, i.e. 00 hours. The dataset also contains the demographic information and English
level self-rating score of each participant. Finally, a ReadMe file is included.
A python library phyaat has been created to allow users to download the dataset and extract the segments with the
corresponding labels to use for predictive modeling. User guide and instructions are also provided 2 .

4

Analysis of Attention Score

The Spearman rank correlation between the attention score and the auditory condition (i.e. noise level, semanticity, and
length of stimulus) was carried out and its results are shown in Figure 5. It can be observed that attention score has a
strong positive correlation (r = 0.7) with the background noise level and a negative correlation with the semancity
(r = −0.24) and length of the stimulus (r = −0.23), all with a p-value p < 0.0001. Furthermore, a repeated-measure
ANOVA test was performed on the attention score considering each auditory conditions individually and jointly. The
2

https://phyaat.github.io/introduction

4

https://phyaat.github.io

Figure 3: A segment of collected physiological responses with corresponding labels. Green color background is
corresponds to listening task, white for writing, and blue for resting.
p-values were found to be low (p << 0.001), suggesting a significant impact of the auditory conditions on the attention
scores.

5

Formulation of predictive tasks

Based on the analysis of the attention score and the experiment design (Figure 1), four different predictive tasks have
been formulated.
5.1

T1: Attention score prediction

In this task, the objective is to estimate the auditory attention level A of a participant based on the physiological signals
recorded during the listening segment. Given a set of features Fr extracted from the listening segment this objective can
be formulated as finding a model fA (·) such that the attention score can be approximated by the quantity
A0 = fA (Fr )

(1)

The model fA (·) is defined as the one minimising the expected risk
E(fA ) = E[L(A, fA (Fr ))],
5

(2)

https://phyaat.github.io

Table 1: Dataset summary
Types of signal
EEG channels
Total signals streams

Sampling rate
Participants
Number of stimuli
per participants
Independent
variables
Noise levels
Semanticity levels
Length of stimulus

Average duration of a
stimuli
Average duration of
entire recording of a
participant
Self-rating

EEG, GSR, PPG
14 channels; AF3, AF4, F3, F4, FC5, FC6,
F7, F8, T7, T8, P7, P8, O1, O2
19 streams; 14 from EEG, 2 from GSR- instant sample and moving averaged, 3 from
PPG - raw signal, beats per minutes (BPM),
interval between beats (IBI)
128Hz
25 participants; 21 male, 4 female
144 randomly selected stimuli, 72 semantic
and 72 non-semantic
Noise level, Semanticity, Length of stimulus
6 levels; -6, -3, 0, 3, 6 and ∞ dB SNR
2 levels; 0-semantic, 1-non-semantic
3 to 13 words per stimulus, grouped into
three categories; L1 (small), L2 (medium)
and L3 (long)
3(±1.2) sec
40(±10) mins

Writing, listening, reading and speaking
skill of English language, rating scale 1-5

(a) Noise level

(b) Semanticity

Figure 4: File structure of dataset directory

(c) Length of stimulus

Figure 5: Correlation of attention score and auditory conditions, p < 0.0001

where L(·, ·) is the chosen loss function and E[·] is the expectation operator. The choice of the loss function can be
defined as the mean square error, the mean absolute error or a combination of the two, such as the Huber loss.
The analysis of the attention score (c.f. Section 4) shows that there is a strong correlation between attention score and
auditory conditions, such as noise level and semanticity. Therefore our knowledge about the auditory conditions can
be informative for predicting the auditory attention. Although in general auditory conditions are not known, in our
controlled experiment we do have access to their exact values. Consequently, we can formulate a predictive task for the
auditory attention, that use the value of the auditory conditions as input.
5.2

T2: Noise level prediction

Assuming that the background noise level influences the physiological responses of a person, a model fN (·) can be
obtained to provide an estimation N 0 of the noise level experienced by a subject, based on the features extracted from
6

https://phyaat.github.io

the physiological responses during a listening segment,
N 0 = fN (Fr )

(3)

Since the defined noise level is an ordinal quantity, this predictive task can be formulated as a regression problem
with a loss functions similar to the ones suggested for the predictive task T1. Alternatively, this predictive task can be
formulated as a classification problems, where each of the six levels of noise constitute different classes.
5.3

T3: Semanticity of stimulus prediction

Assuming that the semanticity of a stimulus S modulates the physiological responses of a subject, a model fS (·) can be
obtained to estimate the semanticity experienced by subject from the set of features FR :
S 0 = fS (Fr )

(4)

Since semanticity is a binary valued auditory condition, this task can be formulated as a classification problem. Choices
of loss functions include the cross-entropy or Hinge loss.
5.4

T4: LWR classification

Tasks T1, T2 and T3 are based on listening segments only and therefore assume that the subject not engaged in any
activities other than listening. However, there are scenarios where we might be interested in evaluating the activity
that a subject might be engaged in. The objective of the LWR-classification task is to predict which activity T is being
performed by the subject, considering listening, writing or resting as the candidate activities. Similar to other predictive
tasks, this can be modeled as to predict T 0 for unseen data, as follows
T 0 = fT (Fr )

(5)

where fT (Fr ) is a model that uses a set of features Fr extracted from a segment of physiological responses.

6

Feature extraction framework

In the context of the experiment described in Section 2.5, a segment corresponds to a continuous time interval during
which a complete task (listening, reading or resting) is executed. By processing the physiological signals recorded
during an entire segment, a set of features Fr can be extracted and used as an input in any of the predictive tasks
T1 through T4. Other time intervals that do not map to complete tasks could also be used to extract features. We
will call these time intervals windows, to distinguish them from our definition of segment. A graphical illustration of
segment-wise and window-wise feature extraction is shown in Figures 6a and 6b, respectively. Different models could
be built to estimate the target predictions based on features extracted from short windows or from an entire segment. An
interesting question is to determine the minimum window length that will produce a set of features Fr that contain
enough information to produce an accurate prediction.
By allowing moving windows, window-wise feature extraction could be used to train temporal models, such as Dynamic
Bayesian Network or RNN, for predictive tasks T1 and T3. Features extracted from each window could be modelled as
non-Independent and Identically Distributed (IID) samples. In the case of predictive tasks T2 and T4, window-wise
feature extraction could be used to train temporal or non-temporal models and the set of features extracted from each
window could be treated as IID samples instead.

7

Results of predictive tasks

In this section a basic SVM model is created for each of the predictive tasks defined in Section 5, by using data from a
single subject, namely S1. First the EEG signals from subject S1 are pre-processed by using a high-pass filter with cutoff
frequency of > 0.5Hz and artifact are removed by using an automatic and tunable wavelet approach [39] with default
parameter setting, namely db3, β = 0.1, N = 128 and λs (·). After the pre-processing stage, six spectral features are
extracted from each segment from the 14 EEG channels. The spectral features correspond to the total power in the
commonly used EEG frequency bands, namely delta (0.5–4 Hz), theta (4–8 Hz), alpha (8–14 Hz), beta (14–30 Hz), low
gamma (30–47 Hz) and high gamma (47–64 Hz).The spectral power in each frequency band is computed based on
Welch’s method. In summary, a total of 84 spectral features (6 × 14 = 84) are extracted from each EEG segment.
During the modeling stage, SVM with RBF kernel are used. Task T1 and T2 are modeled with a Support Vector
Regressor. In task T2, noise level ∞ dB is mapped to 10 dB. Task T3 and T4 are modeled with Support Vector
7

https://phyaat.github.io

(a) Segment-wise feature extraction

(b) Window-wise feature extraction

Figure 6: Feature extraction framework

Classifier. Mean absolute error (MEA) and accuracy are used as performance metrics for regression (T1 & T2) and
classification (T3 & T4), respectively.
Since the sample size for each task (i.e. 143 for T1, T2, T3 and 143 × 3 = 429 for T4) is small, K-Fold cross validation
with K = 10 folds is used and to avoid overfitting a regularization parameter C is included. The average training and
testing performance for each task together with the final value of the parameter C are reported in Table 2 and shown
in Figure 7. It can be observed from Figure 7 that performance for each predictive task is better than chance level
measure. Since the class ratio is balanced for classification tasks, the chance level is 1/number of classes (e.g. 1/2 for
T3 and 1/3 for T4). For regression, the standard deviation σ is shown in Figure 7. These results are reproducible and
the corresponding python scripts are provided as part of the phyaat library.
Table 2: Results of basic modeling using SVM for each predictive task with subject S1
Predictive Task
SVM
Training Testing Metric
T1: Attention score C = 10
23.80
29.65
MAE
T2: Noise level
C=1
4.07
4.75

8

T3: Semanticity

C=2

0.95

0.56

T4: LWR

C=2

0.97

0.81

Accuracy

Conclusions and Future work

In this paper, we have presented a dataset of physiological signals for predictive analysis of auditory attention to natural
speech. Our dataset is novel in its kind and contains three different physiological signals, namely EEG, GSR, and
PPG. Signals were recorded during an experiment conducted with 25 subjects, which involved three tasks (listening,
writing and resting) executed in 144 trials . In the listening stage, subjects were presented with an audio stimulus,
which is to be transcribed in following writing stage, followed by a resting before the subsequent trial. In each trial,
audio stimuli were presented in different auditory conditions, which included background noise level, semanticity,
and length of the stimulus. The attention score from transcription of each trial was computed. Based on the collected
dataset and experiment design, four predictive tasks have been formulated and two approaches of feature extraction
have been proposed. For demonstration purposes, SVM models using EEG spectral features have been created for each
predictive tasks using EEG data from a single subject. The correlation analysis of the attention score and the results of
the predictive tasks demonstrate the potential of the collected dataset.
8

https://phyaat.github.io

Figure 7: Results of basic modeling using SVM for each predictive task with subject S1

In order to advance the research in the field of auditory attention, the collected dataset has been made openly available
to the wider research community, together with supporting tools (See Resources section below) To improve the
performance of the predictive tasks, more sophisticated models such as DBN, RNN and LSTM can be used with
spectral, wavelet-based and other features. Our dataset provides an excellent opportunity to investigate the auditory
attention mechanisms of the brain.

Resources
The dataset and supporting source code is distributed as a python library phyaat (https://pypi.org/project/phyaat/) on
the Python Package Index (PyPI) platform. A user guide and scripts to reproduce the results presented in this paper,
can be downloaded from the project homepage (https://phyaat.github.io/). Supporting functions in the library include
pre-processing, feature extraction and modeling functions.

References
[1] Hilary Gomes, Sophie Molholm, Christopher Christodoulou, Walter Ritter, and Nelson Cowan. The development
of auditory attention in children. Frontiers in Bioscience, 5(1):d108–120, 2000.
[2] Jon Driver. A selective review of selective attention research from the past century. British Journal of Psychology,
92(1):53–78, 2001.
[3] Elizabeth Styles. Attention, perception and memory: an integrated introduction. Psychology Press, 2004.
[4] Frédéric E Theunissen, Sarah MN Woolley, Anne HSU, and Thane Fremouw. Methods for the analysis of auditory
processing in the brain. Annals of the New York Academy of Sciences, 1016(1):187–207, 2004.
[5] Andrea Kübler and Niels Birbaumer. Brain–computer interfaces and communication in paralysis: extinction of
goal directed thinking in completely paralysed patients? Clinical neurophysiology, 119(11):2658–2666, 2008.
[6] Ralph G Andrzejak, Kaspar Schindler, and Christian Rummel. Nonrandomness, nonlinear dependence, and
nonstationarity of electroencephalographic recordings from epilepsy patients. Physical Review E, 86(4):046206,
2012.
[7] Ivan Volosyak. SSVEP-based Bremen–BCI interface—boosting information transfer rates. Journal of neural
engineering, 8(3):036020, 2011.
[8] MA Lopez-Gordo, E Fernandez, S Romero, F Pelayo, and Alberto Prieto. An auditory brain–computer interface
evoked by natural speech. Journal of neural engineering, 9(3):036013, 2012.
[9] MA Lopez-Gordo, Ricardo Ron-Angevin, and Francisco Pelayo Valle. Auditory brain-computer interfaces for
complete locked-in patients. In International Work-Conference on Artificial Neural Networks, pages 378–385.
Springer, 2011.
[10] Kyung Hwan Kim, Seok Won Bang, and Sang Ryong Kim. Emotion recognition system using short-term
monitoring of physiological signals. Medical and biological engineering and computing, 42(3):419–427, 2004.
9

https://phyaat.github.io

[11] Johannes Wagner, Jonghwa Kim, and Elisabeth André. From physiological signals to emotions: Implementing
and comparing selected methods for feature extraction and classification. In Multimedia and Expo, 2005. ICME
2005. IEEE International Conference on, pages 940–943. IEEE, 2005.
[12] Guillaume Chanel, Julien Kronegg, Didier Grandjean, and Thierry Pun. Emotion assessment: Arousal evaluation
using EEG’s and peripheral physiological signals. In International workshop on multimedia content representation,
classification and security, pages 530–537. Springer, 2006.
[13] Sander Koelstra, Christian Muhl, Mohammad Soleymani, Jong-Seok Lee, Ashkan Yazdani, Touradj Ebrahimi,
Thierry Pun, Anton Nijholt, and Ioannis Patras. Deap: A database for emotion analysis; using physiological
signals. IEEE Transactions on Affective Computing, 3(1):18–31, 2012.
[14] Yuan-Pin Lin, Chi-Hong Wang, Tzyy-Ping Jung, Tien-Lin Wu, Shyh-Kang Jeng, Jeng-Ren Duann, and Jyh-Horng
Chen. EEG-based emotion recognition in music listening. IEEE Transactions on Biomedical Engineering,
57(7):1798–1806, 2010.
[15] Guillaume Chanel, Cyril Rebetez, Mireille Bétrancourt, and Thierry Pun. Emotion assessment from physiological
signals for adaptation of game difficulty. IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems
and Humans, 41(6):1052–1063, 2011.
[16] Qiang Wang, Olga Sourina, and Minh Khoa Nguyen. EEG-based "serious" games design for medical applications.
In Cyberworlds (cw), 2010 international conference on, pages 270–276. IEEE, 2010.
[17] J Matias Kivikangas, Guillaume Chanel, Ben Cowley, Inger Ekman, Mikko Salminen, Simo Järvelä, and Niklas
Ravaja. A review of the use of psychophysiological methods in game research. journal of gaming & virtual
worlds, 3(3):181–199, 2011.
[18] C Brunner, R Leeb, G Müller-Putz, A Schlögl, and G Pfurtscheller. BCI Competition 2008–Graz data set A.
Institute for Knowledge Discovery (Laboratory of Brain-Computer Interfaces), Graz University of Technology, 16,
2008.
[19] Robert Leeb, Felix Lee, Claudia Keinrath, Reinhold Scherer, Horst Bischof, and Gert Pfurtscheller. Brain–
computer communication: motivation, aim, and impact of exploring a virtual apartment. IEEE Transactions on
Neural Systems and Rehabilitation Engineering, 15(4):473–482, 2007.
[20] Jaeyoung Shin, Alexander von Lühmann, Benjamin Blankertz, Do-Won Kim, Jichai Jeong, Han-Jeong Hwang,
and Klaus-Robert Müller. Open access dataset for eeg+ nirs single-trial classification. IEEE Transactions on
Neural Systems and Rehabilitation Engineering, 25(10):1735–1745, 2017.
[21] Fabio Aloise, Pietro Aricò, Francesca Schettini, Angela Riccio, Serenella Salinari, Donatella Mattia, Fabio
Babiloni, and Febo Cincotti. A covert attention P300-based brain–computer interface: Geospell. Ergonomics,
55(5):538–551, 2012.
[22] P Aricò, F Aloise, F Schettini, S Salinari, D Mattia, and F Cincotti. Influence of P300 latency jitter on event related
potential-based brain–computer interface performance. Journal of neural engineering, 11(3):035008, 2014.
[23] Matthias S Treder, Ali Bahramisharif, Nico M Schmidt, Marcel AJ Van Gerven, and Benjamin Blankertz.
Brain-computer interfacing using modulations of alpha activity induced by covert shifts of attention. Journal of
neuroengineering and rehabilitation, 8(1):24, 2011.
[24] R Čeponienė, M Westerfield, M Torki, and J Townsend. Modality-specificity of sensory aging in vision and
audition: evidence from event-related potentials. Brain research, 1215:53–68, 2008.
[25] S Halder, M Rea, R Andreoni, F Nijboer, EM Hammer, SC Kleih, N Birbaumer, and A Kübler. An auditory
oddball brain–computer interface for binary choices. Clinical Neurophysiology, 121(4):516–523, 2010.
[26] Martijn Schreuder, Benjamin Blankertz, and Michael Tangermann. A new auditory multi-class brain-computer
interface paradigm: spatial hearing as an informative cue. PloS one, 5(4):e9813, 2010.
[27] René Westerhausen and Kenneth Hugdahl. The corpus callosum in dichotic listening studies of hemispheric
asymmetry: a review of clinical and experimental evidence. Neuroscience & Biobehavioral Reviews, 32(5):1044–
1054, 2008.
[28] Nikesh Bajaj, Francesco Bellotti, Riccardo Berta, Jesùs Requena Carriòn, and Alessandro De Gloria. Auditory
attention, implications for serious game design. In International Conference on Games and Learning Alliance,
pages 201–209. Springer, 2018.
[29] Arthur J Compton. Aural perception of different syntactic structures and lengths. Language and speech, 10(2):81–
87, 1967.
[30] Esther Strauss, William H Gaddes, and Juhn Wada. Performance on a free-recall verbal dichotic listening task and
cerebral dominance determined by the carotid amytal test. Neuropsychologia, 25(5):747–753, 1987.
10

https://phyaat.github.io

[31] Gregory JH Colflesh and Andrew RA Conway. Individual differences in working memory capacity and divided
attention in dichotic listening. Psychonomic Bulletin & Review, 14(4):699–703, 2007.
[32] Kenneth Hugdahl, Thomas Bodner, Elisabeth Weiss, and Thomas Benke. Dichotic listening performance and
frontal lobe function. Cognitive Brain Research, 16(1):58–65, 2003.
[33] Allan Simon Trang Ho. Tatoeba : open collaborative multilingual sentence dictionary. https://tatoeba.org.
last accessed: 2017/01/19.
[34] Emotiv Epoc EEG headset, 14 channel. https://www.emotiv.com/epoc/. last accessed: 2017/01/19.
[35] Pulse sensor. https://pulsesensor.com/. last accessed: 2017/01/19.
[36] Source code for pulse sensor, github repository. https://github.com/WorldFamousElectronics/
PulseSensor_Amped_Arduino. last accessed: 2017/01/19.
[37] Michael Page and Dennis Norris. The primacy model: a new model of immediate serial recall. Psychological
review, 105(4):761, 1998.
[38] Ian Neath and James S Nairne. Word-length effects in immediate memory: Overwriting trace decay theory.
Psychonomic Bulletin & Review, 2(4):429–441, 1995.
[39] Nikesh Bajaj, Jesús Requena Carrión, Francesco Bellotti, Riccardo Berta, and Alesandro De Gloria. Automatic and
tunable algorithm for eeg artifact removal using wavelet decomposition with applications in predictive modeling
during auditory tasks. Biomedical Signal Processing and Control, 55:101624, 2020.

11

