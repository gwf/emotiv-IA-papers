Assessing emotional responses induced in virtual
reality using a consumer EEG headset: A
preliminary report
Marko Horvat*, Marko DobriniÄ‡*, Matej Novosel* and Petar JerÄiÄ‡**
Zagreb University of Applied Sciences, Department of Computer Science and Information Technology,
Zagreb, Croatia
**
Blekinge Institute of Technology, Karlskrona, Sweden
marko.horvat@tvz.hr

*

Abstract â€“ We report on a pilot study involving emotion
elicitation in virtual reality (VR) and assessment of
emotional responses with a consumer-grade EEG device.
The stimulation used HTC Vive VR system showing
pictures from NAPS database within a specifically designed
virtual environment. The stimulation consisted of two
distinct sequences with 10 pictures of happiness and 10
pictures of fear. Each picture was contained in a separate
virtual room that the participants traveled through along a
preset path. The estimation employed EMOTIV EPOC+ 14channel EEG headset and a custom-developed application.
The software wirelessly received EEG signals from alpha,
beta low, beta high, gamma and theta bands, time-stamped
them and dynamically stored in a relational database for
subsequent analysis. Our preliminary results show that
statistically significant correlations between valence and
arousal ratings of pictures and EEG bands are present but
highly personalized. Simultaneous correct placement of VR
and EEG headsets is demanding and precise localization of
electrodes is difficult. In fact, if emotion estimation is not
strictly necessary we recommend using devices with fewer
electrodes. Nevertheless, we found the EEG to be effective.
By acknowledging its limitations, and using the headset in
the correct context, experiments involving emotions may be
significantly amended.

I.

INTRODUCTION

The aim of this paper is to evaluate the feasibility of
detecting emotional reactions induced in virtual reality
(VR) using a commercial EEG system. Previous studies
have clearly demonstrated that elicitation of specific
emotions with films, videos and pictures is challenging
but possible [1][2][3]. High stimuli realism, sufficient
personal relevance, and strong engagement have been
suggested as the most significant factors contributing to
immersion and presence leading to a successful elicitation
[1][4][5][6]. Key advantages of exposure in VR
overexposure in vivo are 1) greater flexibility, 2) safety, 3)
cost-effectiveness [7].
VR and multimedia systems have been previously
employed for detection, prevention, and treatment of
phobias, anxiety and other stress-related mental disorders
[5][7][8]. They have proven themselves very useful in a
whole spectrum of therapies such as exposure therapy [9],
stress inoculation therapy [10], stress resistance training
[11], relaxation [12] and biofeedback [13]. Additionally,

MIPRO 2018/CIS

multimodal systems that combine VR and different
multimedia formats have been successful as assistive tools
in psychotherapy of PTSD patients [14][15]. However,
only recent advancements in computer hardware have
made it possible to render photorealistic graphics in realtime and with sufficient detail to provoke emotional
responses on the same level as real-world photographs
[16].
II.

RELATED WORK

Recently, a number of related studies concerning
detection of emotion-related phenomena using customergrade EEG equipment have been published. One research
proved initial usability of low-cost EEG equipment paired
with VR in the detection of low-intensity and long
duration emotion states such as moods [17][18]. This
research obtained results that are in accordance with
scientific literature regarding frontal EEG asymmetry,
which supports the possibility of using the portable EEG
as a reliable instrument to measure emotions in VR. In
another research, the same type of low-cost EEG was
demonstrated to be useful in providing emotional
awareness information for synchronous collaborative
editing systems [19]. These previous investigations
motivated the exploration of emotional responses on EEG
bands using low-cost commercial devices, as employed in
our study. Also, scholars have been successful in
combining commercial facial analysis software and EEG
in emotion detection [20]. They have achieved 53%
accuracy in emotion classification by fusing both
information sources, which is higher than 19% accuracy
when using only facial expression analysis tool.
III.

COMMERCIAL-GRADE VR AND EEG SYSTEMS IN
THE ASSESSMENT OF EMOTIONAL RESPONSES

HTC Vive is a high-end VR display device made by
HTC and Valve Corporation (Figure 1) [21]. It is
commercially available since April 2016. The device was
designed to take advantage of available physical space
and to use it as a free 3D environment within VR. This is
accomplished by means of special sensors that allow a
person to be tracked while moving through a

1175

predetermined space in the room. Tracking sensors are
mounted on pedestals.
Movement involves moving users around the virtual
world as they move in the room, but it also tracks
movements and position of the headset so it is possible to
move the camera inside the game and, for example, to
descend into the VR environment or to approach and
explore the objects in the simulation.

data, the user can define the rate of data collection per
second, as well as minimums and maximums of recorded
input data. This process is enabled due to the high
possibility of interference with diode signals. Some of
these issues can be reproduced by exerting a force on the
diodes or facially expressing emotion.

Fig. 1. HTC Vive headset (center) and controllers (left and
right) [21].

In addition to the head mounted device (HMD) there
are also two motion controllers that allow users to interact
with objects within the VR environment. In VR, they are
most often presented in the form of a hand or some
facility that the user uses in the environment.
Recording of EEG signals was accomplished with
EMOTIV EPOC+ 14-channel mobile system (Figure 2)
[22]. This device is one of the few commercially
available solutions that meet most of the required clinical
electroencephalograph standards [23]. It was primarily
developed for use in advanced Brain-Computer Interface
(BCI) and for neurological research, but it can be applied
in other areas such as research of emotion and attention.
The system is highly mobile, easily operated and provides
a powerful Software Development Kit (SDK) to
programmers. Raw signals can also be recorded if
needed.

Fig. 2. EMOTIV EPOC+ 14-channel EEG wireless headset
[22].

Using the EMOTIV SDK we developed a Java desktop
application (Figure 3) that can wirelessly record EEG
signals from alpha, beta low, beta high, gamma and theta
bands, time-stamped them and dynamically stored in a
SQL database. The recording application was
synchronized with the VR stimulation. Before measuring

1176

Fig. 3. Application interface for baseline acquisition and
calibration of EEG measurements.

Before the recording starts, the user first performs a
calibration or baseline measurement based on which data
is measured together with stimulus presentation. The
recording of baseline data lasts for 2 minutes, during
which the user is asked to relax and have a neutral facial
expression. After this process, it is possible to access the
analysis window, where the evaluation of baseline
measurement and test measurements is presented. The
application also supports measuring participantâ€™s EEG
signals while reproducing auditory stimuli in the form of
music or various white noise signals.
IV.

EXPERIMENTAL PROTOCOL

A homogeneous group of N=6 participants (5 men, 1
woman) with an average age 26.67 years (std = 1.11)
participated in the experiment. Participants were visually
stimulated with images from the Nencki Affective Picture
System (NAPS) [24] embedded in a 3D VR environment.
The NAPS affective multimedia database has 1356
realistic pictures and, together with its extensions NAPS
BE [25] and NAPS ERO [26], is currently the largest
such database with general semantic content. Picturesâ€™
context contains semantic categories, multi-word stimuli
annotations and normative ratings in discrete and
dimensional models of affect [27].
Each participant was exposed to two emotion
elicitation sequences. A single sequence consisted of 10
VR rooms with 10 NAPS pictures, one picture in each
room. The first sequence displayed pictures with
dominating happiness and the second with dominating
fear. The pictures were selected based on discrete
emotion values from NAPS BE: 10 pictures had the
highest level of happiness and the other 10 the highest
values of fear. The length of each stimulus was exactly 15
seconds after which the participant enters an empty room

MIPRO 2018/CIS

without any emotion provoking picture to rest for 10
seconds. Therefore, in total each participant was exposed
to 20 pictures in 20 VR rooms. Spatial and visual
characteristics of all rooms, including those in rest
phases, were identical (Figure 4). The rooms were
initially designed to be emotionally neutral. Only emotion
provoking content were the NAPS pictures (Figure 5).
Participants followed a preset path through the VR
stimulation. They could freely change their direction of
view but they could not change the path or speed of the
movement in the 3D environment.

Fig. 6. On the left, a participant is wearing VR and EEG
headsets during the experiment. One of the HTC Vive VR
sensors is visible in the background. The supervisor station is on
the right.
Fig. 4. A VR room during the development of stimulation
sequences with Unreal Engine tools.

Fig. 5. A VR room with a picture from the NAPS database
inserted on one of the walls.

Participants wore HTC Vive VR and EMOTIV
EPOC+ headsets at the same time (Figure 6). This proved
to be technically challenging because these systems were
not designed for simultaneous use. Some HTC Vive
elastic straps had to be loosened to provide space for
EEG sensors. Also, before the experiment participants
were advised to support by hand the front section of the
VR headset because of its significant weight.
During the entire experiment, the illumination in the
laboratory was dimmed as required by HTC Vive manual
and psychophysiological laboratory guidelines [28].
Participants removed the headgear and rested between the
two sequences. The exposure resumed only after the
participants were fully relaxed.

MIPRO 2018/CIS

V.

RESULTS

In the experiment, we monitored alpha (ğ›¼), beta low
(ğ›½ğ¿ ), beta high (ğ›½ğ» ), gamma (ğ›¿) and theta (ğœƒ) bands. The
signals were recorded at 128 Hz sampling rate. EMOTIV
EPOC+ acquisition software automatically filtered out
the noise and removed artifacts if electrodes were
appropriately positioned on a participantâ€™s scalp. Features
were derived off-line from the signalsâ€™ values stored in
the SQL database. In this preliminary analysis we
cal,culated one feature: absolute difference between
signal average ( ğ´ğ‘‰ğº ) and baseline for each stimulus
(ğµğ´ğ‘†).
ğ´ğ‘‰ğºğµğ´ğ‘†ğ›¼ = |ğ›¼ğ´ğ‘‰ğº âˆ’ ğ›¼ğµğ´ğ‘† |
ğ´ğ‘‰ğºğµğ´ğ‘†ğ›½ğ¿ = |ğ›½ğ¿ ğ´ğ‘‰ğº âˆ’ ğ›½ğ¿ ğµğ´ğ‘† |
ğ´ğ‘‰ğºğµğ´ğ‘†ğ›½ğ» = |ğ›½ğ» ğ´ğ‘‰ğº âˆ’ ğ›½ğ» ğµğ´ğ‘† |

(1)

ğ´ğ‘‰ğºğµğ´ğ‘†ğ›¿ = |ğ›¿ğ´ğ‘‰ğº âˆ’ ğ›¿ğµğ´ğ‘† |
ğ´ğ‘‰ğºğµğ´ğ‘†ğœƒ = |ğœƒğ´ğ‘‰ğº âˆ’ ğœƒğµğ´ğ‘† |
The feature ğ´ğ‘‰ğºğµğ´ğ‘† was obtained in 10 sec fixed
windows for each stimulus in both sequences. The feature
was then correlated (Pearson correlation coefficient r)
with valence (ğ‘‰ğ‘ğ‘™) and arousal (ğ´ğ‘Ÿ) emotion values from
the NAPS database for the displayed picture. Correlation
measures between aggregated average ğ´ğ‘‰ğºğµğ´ğ‘† for all
participants and picture emotion values are shown in
Table 1. Valence and arousal of pictures in the happiness
sequence are denoted ğ‘‰ğ‘ğ‘™ğ» and ğ´ğ‘Ÿğ» , while valence and
arousal in the fear sequence are ğ‘‰ğ‘ğ‘™ğ¹ and ğ´ğ‘Ÿğ¹ ,
respectively.

1177

TABLE I
Correlation measures (r) among aggregated average ğ´ğ‘‰ğºğµğ´ğ‘†
feature for all participants and picture emotion values per each
EEG band.

ğ›¼

ğ›½ğ¿

ğ›½ğ»

ğ›¿

ğœƒ

ğ‘‰ğ‘ğ‘™ğ»

-0,112

0,009

0,113

0,054

-0,063

ğ´ğ‘Ÿğ»

0,019

0,003

0,182

-0,017

0,081

ğ‘‰ğ‘ğ‘™ğ¹

0,168

0,071

0,123

0,063

-0,259

ğ´ğ‘Ÿğ¹

0,019

-0,054

-0,012

0,051

0,047

As can be seen in the table the largest correlation
pairs in the happiness sequence are (ğ›½ğ» , ğ´ğ‘Ÿğ» ) and
(ğ›½ğ» , ğ‘‰ğ‘ğ‘™ğ» ). The fear sequence stimulated statistically
more significant correlations (ğœƒ, ğ‘‰ğ‘ğ‘™ğ¹ ) and (ğ›¼, ğ‘‰ğ‘ğ‘™ğ¹ ) .
This is in agreement with findings from previous studies
where stimulation of negative polarity emotions is more
powerful than with positive emotions. Alpha and theta
bands indicate that participants were simultaneously less
relaxed and experienced stronger cognitive processing
than with stimuli in the positive sequence, as should be
expected. Although this study is only preliminary, it
acquired quality data for further analyses.
VI.

DISCUSSION AND CONCLUSION

The experiment demonstrated that VR is a great asset
for emotion elicitation techniques surpassing the potential
of pictures, films, and video clips. Contemporary VR
systems enable development of provoking storylines with
personally meaningful narratives. This is of the utmost
importance for a successful emotion stimulation. The
other vital factors are realistic and engaging visuals.
Continuous progress in computer graphics has made it
possible to eliminate differences between photographs
and computergenerated graphics. We found that using
modern development tools creation of VR environments
is swift and intuitive. Subsequent customizations are also
greatly simplified. During the design, it is very important
that a VR room is emotionally unprovocative and that
participants are actually looking at a stimulus in the
room. This can be accomplished by forcefully directing
their line of sight when entering a room. Constant
monitoring of participantsâ€™ point of view from the
supervisorâ€™s station should be obligatory.
Unfortunately, consumer-grade EEGs have not yet
reached capabilities of professional laboratory devices.
Nevertheless, we found EMOTIV EPOC+ EEG ideally
suited for student experiments and quite adequate in the
detection of emotion-related phenomena where exact
placement of electrodes is not crucial. The system is very
easy to work with, develop new code and record signals.
However, the headset is not ideally suited for usage in
tandem with a VR device. At the current level of
development, these two technologies (EEG and VR) have
conflicting requirements in experimental protocols. Better
immersion requires obscuring upper face region and
forehead with a large display, and elastic straps needed

1178

for immobilizing the heavy screen cover wide scalp
regions. Calibration of EEGâ€™s electrodesâ€™ location is
particularly demanding. Electrodes must be placed within
c. 5âˆ’10 mm of the ideal spot to record signals with an
acceptable artifact level (i.e. noise). This is especially
noticeable for prefrontal (dorsolateral and ventrolateral)
electrodes because the VR display almost completely
obscures these brain regions and cannot be moved.
However, VR headset straps can be loosened and even
partially removed to allow access to parietal and occipital
brain lobes. However, this may limit participantsâ€™ head
movements or even cause some physical discomfort and
must be taken into account during the planning of an
experiment.
EEG spectral bands are a good measure of attention
and alertness. To obtain power spectral analysis of EEG
activity, a simpler single-electrode headset can be used.
Such device is more easily manageable and could be
compatible with a VR headset simplifying the whole
problem with the simultaneous use of EEG and VR
devices.
Our experience in this study shows that a number of
questions must be considered to accomplish an emotion
experiment with consumer-grade equipment. But if the
protocol is carefully designed all issues can be
successfully solved. Accurate electrode localization
remains an open challenge, however.
VII. ACKNOWLEDGEMENT
We would like to kindly thank the company â€œLittle
Green Men Gamesâ€ (Intercorona d.o.o.) and their
employees for the contribution of the HTC Vive
equipment and participation in the experiment.
REFERENCES
[1]
[2]
[3]

[4]

[5]

[6]

[7]
[8]

J. Rottenberg, R. D. Ray, and J. J. Gross, â€œEmotion elicitation
using films,â€ Handbook of emotion elicitation and assessment,
Oxford University Press, New York, pp. 9âˆ’28, 2007.
M. Soleymani, M. Pantic, and T. Pun, â€œMultimodal emotion
recognition in response to videos,â€ IEEE transactions on affective
computing, vol. 3(2), pp. 211â€“223, 2012.
M. Horvat, D. Kukolja, and D. Ivanec, â€œComparing affective
responses to standardized pictures and videos: A study report,â€ In
MIPRO, 2015 Proceedings of the 38th International Convention,
IEEE, pp. 1394âˆ’1398, May 2015.
D. Kukolja, S. PopoviÄ‡, M. Horvat, B. KovaÄ, and K. Ä†osiÄ‡,
â€œComparative analysis of emotion estimation methods based on
physiological measurements for real-time applications,â€
International journal of human-computer studies, vol. 72(10), pp.
717âˆ’727, 2014.
K. Ä†osiÄ‡, S. PopoviÄ‡, M. Horvat, D. Kukolja, B. DropuljiÄ‡, B.
KovaÄ, and M. JakovljeviÄ‡, â€œComputer-aided psychotherapy based
on multimodal elicitation, estimation and regulation of emotion, â€
Psychiatr. Danub., vol. 25(3), 340â€“346.
M. Krijn, P. M. Emmelkamp, R. Biemond, C. D. W. de Ligny, M.
J. Schuemie, and C. A. van der Mast, â€œTreatment of acrophobia in
virtual reality: The role of immersion and presence,â€ Behaviour
research and therapy, vol. 42(2), pp. 229â€“239, 2004.
G. Riva, â€œVirtual reality in psychotherapy,â€ Cyberpsychology &
Behavior, vol. 8(3), pp. 220â€“230, 2005.
J. A. Spurgeon and J. H. Wright, â€œComputer-assisted cognitivebehavioral therapy,â€ Current Psychiatry Reports, vol. 12(6), pp.
547âˆ’552, 2010.

MIPRO 2018/CIS

[9]

[10]

[11]

[12]
[13]

[14]

[15]

[16]

[17]

[18]

T. D. Parsons and A. A. Rizzo, â€œAffective outcomes of virtual
reality exposure therapy for anxiety and specific phobias: A metaanalysis,â€ Journal of behavior therapy and experimental
psychiatry, vol. 39(3), pp. 250âˆ’261, 2008.
S. PopoviÄ‡, M. Horvat, D. Kukolja, B. DropuljiÄ‡, and K. Ä†osiÄ‡,
â€œStress inoculation training supported by physiology-driven
adaptive virtual reality stimulation,â€ Studies in Health Technology
and Informatics, vol 14, pp. 50âˆ’54, 2009.
K. Ä†osiÄ‡, S. PopoviÄ‡, M. Horvat, D. Kukolja, B. DropuljiÄ‡, I.
KostoviÄ‡, M. JudaÅ¡, M. RadoÅ¡, M. RadoÅ¡, L. Vasung, B. B. SpajiÄ‡,
S. DoriÄiÄ‡, and D. MesiÄ‡, â€œVirtual reality adaptive stimulation in
stress resistance training,â€ In HFM-205 Symposium on "Mental
Health and Well-Being across the Military Spectrum", January
2011.
M. Krijn, P. M. Emmelkamp, R. P. Olafsson, and R. Biemond,
â€œVirtual reality exposure therapy of anxiety disorders: A review,â€
Clinical psychology review, vol. 24(3), pp. 259â€“281, 2004.
C. Repetto, A. Gorini, C. Vigna, D. Algeri, F. Pallavicini, and G.
Riva, â€œThe use of biofeedback in clinical virtual reality: the
INTREPID project,â€ Journal of visualized experiments: JoVE, vol.
33, pp. 128â€“132, 2009.
B. K. Wiederhold and M. D. Wiederhold, â€œVirtual reality for
posttraumatic stress disorder and stress inoculation training,â€
Journal of Cybertherapy & Rehabilitation, vol. 1(1), pp. 23â€“35,
2008.
K. Ä†osiÄ‡, S. PopoviÄ‡, M. Horvat, D. Kukolja, B. DropuljiÄ‡, B.
KovaÄ, and I. Fabek, â€œMultimodal paradigm for mental readiness
training and PTSD prevention,â€ In NATO Advanced Study
Institute on Invisible Wounds: New Tools to Enhance PTSD
Diagnosis and Treatment, 2012.
C. G. Courtney, M. E. Dawson, A. M. Schell, A. Iyer, and T. D.
Parsons, â€œBetter than the real thing: Eliciting fear with moving and
static computer-generated stimuli,â€ International Journal of
Psychophysiology, vol. 78(2), pp. 107â€“114, 2010.
A. Rodriguez Ortega, B. Rey Solaz, A. Raya, and M. Luis,
â€œValidation of a low-cost EEG device for mood induction
studies,â€ In Annual review of cybertherapy and telemedicine,
Interactive Media Institute (IMI), vol. 11, pp. 43â€“47, 2013.
A. Rodriguez Ortega, B. Rey Solaz, A. Raya, and M. Luis,
â€œEvaluating virtual reality mood induction procedures with

MIPRO 2018/CIS

[19]
[20]

[21]
[22]
[23]
[24]

[25]

[26]

[27]
[28]

portable EEG devices,â€ In Annual Review of Cybertherapy and
Telemedicine, Interactive Media Institute (IMI), vol. 11, pp. 131â€“
135, 2013.
M. C. Pichiliani, C. M. Hirata, and T. Fraga, â€œExploring a brain
controlled interface for emotional awareness,â€ In Collaborative
Systems (SBSC), 2012 Brazilian Symposium on, pp. 49â€“52, 2012.
T. Matlovic, P. Gaspar, R. Moro, J. Simko, and M. Bielikova,
Emotions detection using facial expressions recognition and EEG.
In Semantic and Social Media Adaptation and Personalization
(SMAP), 2016 11th International Workshop on, pp. 18â€“23, 2016.
HTC Vive, https://www.vive.com/eu/, retrieved on 16 February
2018.
EMOTIV EPOC+, https://www.emotiv.com/epoc/, retrieved on 16
February 2018.
R. Maskeliunas et al., â€œConsumer-Grade EEG Devices: Are They
Usable for Control Tasks?,â€ Ed. Jafri Abdullah, PeerJ, vol. 4,
2016.
A. Marchewka, Å. Å»urawski, K. JednorÃ³g, and A. Grabowska,
â€œThe Nencki Affective Picture System (NAPS): Introduction to a
novel, standardized, wide-range, high-quality, realistic picture
database,â€ Behavior research methods, Springer, vol. 46(2), pp.
596â€“610, 2014.
M. Riegel, Å. Å»urawski, M. Wierzba, A. Moslehi, Å. Klocek, M.
Horvat, A. Grabowska, J. Michalowski, K. JednorÃ³g, and A.
Marchewka, â€œCharacterization of the Nencki Affective Picture
System by discrete emotional categories (NAPS BE),â€ Behavior
research methods, Springer, in press.
M. Wierzba, M. Riegel, A. Pucz, Z. LeÅ›niewska, W. Å. Dragan,
M. Gola, K. JednorÃ³g, and A. Marchewka, â€œErotic subset for the
Nencki Affective Picture System (NAPS ERO): cross-sexual
comparison study,â€ Frontiers in Psychology, vol. 6(1336), 2015.
C. Peter and A. Herbon, â€œEmotion representation and physiology
assignments in digital systems,â€ Interacting with Computers,
Oxford Journals, vol. 18(2), pp. 139â€“170, 2006.
J. J. Curtin, D. L. Lozano, and J. J. Allen, â€œThe
psychophysiological laboratory,â€ Handbook of emotion elicitation
and assessment, Oxford University Press, New York, pp.
398âˆ’425, 2007.

1179

