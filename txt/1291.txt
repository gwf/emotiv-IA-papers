Downloaded from orbit.dtu.dk on: Jun 14, 2019

Design of Cognitive Interfaces for Personal Informatics Feedback

Jensen, Camilla Birgitte Falk

Publication date:
2016
Document Version
Publisher's PDF, also known as Version of record
Link back to DTU Orbit

Citation (APA):
Jensen, C. B. F. (2016). Design of Cognitive Interfaces for Personal Informatics Feedback. Kgs. Lyngby:
Technical University of Denmark (DTU). DTU Compute PHD-2015, No. 383

General rights
Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright
owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.
 Users may download and print one copy of any publication from the public portal for the purpose of private study or research.
 You may not further distribute the material or use it for any profit-making activity or commercial gain
 You may freely distribute the URL identifying the publication in the public portal
If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately
and investigate your claim.

Design of Cognitive Interfaces for
Personal Informatics Feedback

Camilla Birgitte Falk Jensen

Kongens Lyngby 2015
PhD-2015-383

Technical University of Denmark
Department of Applied Mathematics and Computer Science
Matematiktorvet, building 303B, 2800 Kongens Lyngby, Denmark
Phone +45 4525 3351, Fax +45 45882673
compute@compute.dtu.dk
www.compute.dtu.dk IMM-PhD-2015-383

Summary

The emergence of embedded low-cost sensors in mobile devices allows us to capture unprecedented data about human behavior. Hence personal informatics
systems are becoming an integrated part of our everyday life: Capturing various
aspects from our health, work-life, to economic balance, and utility consumption. All of which are aimed to provide knowledge of oneself, on which we can
reflect. Many personal informatics systems are characterized by mainly focusing
on collecting and analyzing data, rather than translating the data into meaningful feedback. This dissertation presents challenges related to personal informatics
systems, and propose an approach to design cognitive interfaces, which considers
both users’ motivations, needs, and goals.
In this thesis I propose a new personal informatics framework, the feedback loop,
which incorporates lean agile design principles. Including hierarchical modeling
of goals, activities, and tasks to create minimal viable products. While considering how micro-interactions based on an understanding of data, couples with
user needs and the context they appear in, can contribute to creating cognitive
interfaces. Designing cognitive interfaces requires a focus on translating data into
meaningful feedback, which the users can reflect on in order to gain insights. Thus
I present tools such as personalized baselines and thresholds to enable reflection,
while creating personalized goals, scenarios, trade-offs in order to provide actionable feedback, which can help users to adjust their behavior. Although feedback
can be provided in many different ways, it basically consists of audio, visual,
and haptic components, which combined may reinforce each other to support the
underlying interaction.

ii
The papers included in this thesis cover selected parts of the feedback loop. For
instance, examining emotional responses to pleasant and unpleasant media content from brain activity, reveals the large amount of data and extensive analysis
required to apply this to future personal informatics systems. In addition we analyse challenges related to temporal aspects of the feedback loop, when users attempt
to self-regulate their brain activity based on a real-time feedback. This leads to
identification of underlying audio, visual and haptic feedback components, which
combined may support the underlying interaction within personal informatics.
And with the emerging availability of sensor packed wearable devices, haptic
feedback may become an inherent part of personal informatics systems, which
could enhance the interaction based visual feedback.

Resume

Udviklingen af billige mobile sensor teknologier giver os mulighed for at indsamle
hidtil usete mængder data om menneskelig adfærd. Derved er personal informatics systemer ved at blive en integreret del af vores hverdag, med opsamling af
data om forskellige aspekter fra sundhed, arbejdsliv til økonomisk balance, og ressource forbrug. Dette har til formål at opbygge viden om en selv, hvorpå vi kan
reflektere. Mange personal informatics systemer er karakteriseret ved primært
at fokusere på at indsamle og analysere data, snarere end at oversætte data til
meningsfuld feedback. Denne afhandling præsenterer udfordringer i forbindelse
med personal informatics systemer, og foreslår en tilgang til design af kognitive
grænseflader, som omfatter brugernes motivationer, behov og mål.
I denne afhandling opstiller jeg en ny ramme for personal informatics systemer,
the feedback loop, som inkorporerer lean agile design principper. Heriblandt hierarkisk modellering af bruger behov relateret til mål, aktiviteter og opgaver som
basis for design af minimal viable products. Hvis mikro-interaktioner baseres på
en forståelse af data, brugernes behov og den sammenhæng de optræder i, kan
de bidrage til at skabe kognitive grænseflader. Design af kognitive grænseflader
kræver fokus på at oversætte data til meningsfyldt feedback, som brugerne kan
reflektere over, og få indsigt i. Således præsenterer jeg værktøjer såsom personlige referencepunkter og grænseværdier, der muliggøre refleksion og opfyldelse af
personlige mål. Scenarier, afvejninger, og hensyn til konteksten medfører at feedbacken bliver handlingsorienteret og dermed kan hjælpe brugerne til at justere
deres adfærd. På trods af at feedback kan blive præsenteret på mange forskellige
måder, består de grundlæggende af auditive-, visuelle- og haptiske komponenter,
der kan kombineres og derved forstærke hinanden og understøtte den underlig-

iv
gende interaktion.
Artiklerne der er inkluderet i denne afhandling illustrerer aspekter af the feedback loop. Heriblandt undersøges følelsesmæssige reaktioner på behagelige og ubehagelige billeder ud fra hjerneaktivitet, hvilket illustrerer den omfattende data
indsamling og analyse som kræves for at dette kan anvendes i fremtidige personal informatics systemer. Derudover analysere vi udfordringerne forbundet med
tidsmæssige aspekter af the feedback loop, når brugere forsøger at selvregulere
deres hjerneaktivitet baseret på et real-time feedback. Dette fører til identifikation af underlæggende auditive, visuelle og haptiske feedback-komponenter, der
på tværs af modaliteter kan understøtte den underliggende samspil med personal
informatics systemer. Med udviklingen af mobile sensor teknologier, wearables, vil
haptisk feedback kunne blive en integreret del af personal informatics systemer,
hvilket kan supplere interaktion med den visuelle feedback.

Preface

This thesis was prepared at the Department of Applied Mathematics and Computer Science in the Cognitive Systems Section at the Technical University of
Denmark in fulfillment of the requirements for acquiring a Ph.D. degree in engineering.
The thesis consists of an extensive summary report and a collection of five research papers written during the period from August 2012 to August 2015.

Lyngby, 15-August-2015

Camilla Birgitte Falk Jensen

List of Publications

Papers included in this thesis
A Arkadiusz Stopczynski, Carsten Stahlhut, Michael Kai Petersen, Jakob Eg
Larsen, Camilla Birgitte Falk Jensen, Marieta Georgieva Ivanova, Tobias S Andersen, and Lars Kai Hansen. Smartphones as pocketable labs:
Visions for mobile brain imaging and neurofeedback. International journal
of psychophysiology, 91(1):54–66, 2014.
B Camilla
Larsen.
Cognitive
on, pages

Birgitte Falk Jensen, Michael Kai Petersen, and Jakob Eg
Emotional responses as independent components in EEG. In
Information Processing (CIP), 2014 4th International Workshop
1–6. IEEE, 2014.change

C Camilla Birgitte Falk Jensen, Michael Kai Petersen, Jakob Eg Larsen,
Arkadiusz Stopczynski, Carsten Stahlhut, Marieta Georgieva Ivanova, Tobias Andersen, and Lars Kai Hansen. Spatio temporal media components
for neurofeedback. In Multimedia and Expo Workshops (ICMEW), 2013
IEEE International Conference on, pages 1–6. IEEE, 2013.
D Mohamad Eid, Georgios Korres, and Camilla Birgitte Falk Jensen. Soa
thresholds for the perception of discrete/continuous tactile stimulation. In
Quality of Multimedia Experience (QoMEX), 2015 Seventh International
Workshop on. IEEE, 2015.
E Camilla Birgitte Falk Jensen, Georgios Korres, Carsten Bartsch, and
Mohamad Eid. Vibrotactile alarm system for reducing sleep inertia. Affective Computing, IEEE Transactions on, 2015. Submitted for publication.

viii

Additional publications not included in this thesis
F Camilla Birgitte Falk Jensen, Marieta G Ivanova, Arkadiusz Stopczynski, Michael Kai Petersen, Tobias Andersen, and Carsten Stahlhut. Training your brain on a tablet. In 34th Annual International Conference of the
IEEE Engineering in Medicine and Biology Society (EMBC). IEEE, 2012.
G Camilla Birgitte Falk Jensen, Michael Kai Petersen, and Jakob Eg
Larsen. Differentiating emotional responses to images and words. In 34th
Annual International Conference of the IEEE Engineering in Medicine and
Biology Society (EMBC). IEEE, 2014
H David Kristian Laundav, Camilla Birgitte Falk Jensen, Per Bækgaard,
Michael Kai Petersen, and Jakob Eg Larsen. Your heart might give away
your emotions. In Multimedia and Expo Workshops (ICMEW), 2014 IEEE
International Conference on, pages 1–6. IEEE, 2014.

Acknowledgements

First of all I wish to express my deepest gratitude to my two supervisors Associate
Professor Jakob Eg Larsen and Associate Professor Michael Kai Petersen for
providing continuous support and guidance. I am truly thankful that you both
opened my eyes to this exciting world of personal informatics and neuroscience,
and maybe even more importantly I am grateful for you always being filled with
enthusiasm even if you had to explain your ideas several times, and for your
willingness to discuss numerous practical and abstract topics.
A special thanks to Post Doc Arkadiusz Stopczynski for our collaboration and
all your excellent programming skills. Also a special thanks to Ph.D. student Per
Bækgaard for long discussions on exciting ideas and providing valuable feedback.
I also wish to thank Post Doc Ivana Konvalinka for your expertise on social
and emotional neuroscience and the hands-on experience I gained through our
collaboration. I also thank Associate Professor Tobias Andersens for sharing your
insights on psychological areas and your knowledge about experimental design.
Similarly, I want to thank the rest of my colleagues in Cognitive Systems for the
social and professional interactions that we have had.
In addition I would like to thank Assistant Professor Mohamad Eid for inviting
me to join his team at New York University, Abu Dhabi, and for the great
experience.
Finally, I would like to thank my wonderful family and friends for supporting me
throughout my Ph.D. project.

x

Contents

Summary

i

Resume

iii

Preface

v

List of Publications

vii

Acknowledgements

ix

1 Introduction
1.1 Cognitive Interfaces for Personal Informatics . . . . . . . . . . . .
1.2 Outline & Contributions . . . . . . . . . . . . . . . . . . . . . . . .
2 Personal Informatics
2.1 Quantified Self . . . . . . . . . . .
2.2 Personal Informatics Frameworks .
2.2.1 Hierarchical Goals . . . . .
2.2.2 Properties of the Stages . .
2.2.3 Barriers . . . . . . . . . . .
2.3 Limitations of Current Frameworks
2.4 Summary . . . . . . . . . . . . . .

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

.
.
.
.
.
.
.

1
2
3
7
9
10
12
13
14
15
18

3 The Feedback Loop
19
3.1 Lean Agile Design Process . . . . . . . . . . . . . . . . . . . . . . . 21
3.1.1 User Story Mapping . . . . . . . . . . . . . . . . . . . . . . 22
3.1.2 Micro-Interactions . . . . . . . . . . . . . . . . . . . . . . . 23

xii
3.2

CONTENTS
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27

4 Personalization
4.1 Designing for Personalities . . . . . . . . . . . . . . . . . . . . . . .
4.2 Emotional Responses in EEG . . . . . . . . . . . . . . . . . . . . .
4.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

29
31
35
41

5 Cognitive Interfaces
5.1 Visualization . . . .
5.2 Infer Meaning . . . .
5.3 Actionable Feedback
5.4 Summary . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

43
44
48
49
52

6 Neurofeedback Training
6.1 Neurofeedback . . . . . . . . . . . . . . . . .
6.1.1 Previous Neurofeedback Interfaces . .
6.1.2 Feedback on Multiple Temporal Levels
6.1.3 Neurofeedback Experiment . . . . . .
6.2 Interface Components . . . . . . . . . . . . .
6.2.1 Visual Feedback Components . . . . .
6.2.2 Audio Feedback Components . . . . .
6.2.3 Haptic Feedback Components . . . . .
6.2.4 Multi-Modal Compositions . . . . . .
6.3 Summary . . . . . . . . . . . . . . . . . . . .

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

.
.
.
.
.
.
.
.
.
.

53
54
55
56
57
61
61
62
62
63
65

7 Haptic Interfaces
7.1 Direct Haptic Feedback . . . . . . . .
7.1.1 Perceiving Continuous Motion
7.1.2 Designing Haptic Patterns . . .
7.2 Indirect Haptic Feedback . . . . . . .
7.3 Summary . . . . . . . . . . . . . . . .

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

.
.
.
.
.

67
70
72
74
77
79

8 Perspectives
8.1 Integration of Design Principles . . . . . . . . . . . .
8.2 Creating Supportive Interfaces . . . . . . . . . . . .
8.2.1 Towards Multi-Modal Interface Components .
8.3 Moving Science into the Wild . . . . . . . . . . . . .

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

81
82
83
84
84

9 Conclusion

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.

.
.
.
.
.

.
.
.
.
.

87

A Smartphones as pocketable labs: Visions for mobile brain imaging and neurofeedback
89

CONTENTS

xiii

B Emotional responses as independent components in EEG stimulation
103
C Spatio temporal media components for neurofeedback

111

D SOA thresholds for the perception of discrete/continuous tactile
stimulation
119
E Vibrotactile alarm system for reducing sleep inertia

127

Bibliography

139

Chapter

1

Introduction

Today, there is a personal informatics system for almost any aspect of a person’s life, such as moods felt, health symptoms experienced, exercises performed,
computer applications used, steps taken, electricity consumed, and hours slept.
- Human behavior and our cognitive and emotional states have been studied
in many domains, including psychology, sociology and neuroscience. With the
technological development of sensors, mathematical models and increasing computational power, many aspects of our lives have become digital, making it easy
to monitor everything from global systems such as transportation, utility usage,
transactions to individual monitoring of everything from steps, heart rate, coffee
consumption etc. Thus, data on human behavior, cognition and emotions are
no longer reserved for the scientific community, but are steadily being applied in
governmental planning and legislation; in business strategies for consumer products; and in the everyday life of individuals, who seek knowledge and insight into
their behavior, physical and psychological being. This is a new era of personal
informatics and cognitive systems, which poses challenges related to the design
of interfaces enabling people to interact with their own data and gain insights.

2

Introduction

1.1

Cognitive Interfaces for Personal Informatics

Personal informatics is a quantitative approach to obtaining knowledge of oneself on which we can reflect [Li et al., 2010]. The field of personal informatics
stretches from information on physical and psychological conditions (heart rate,
insulin levels, mood etc.) to behavioral information (gps location, shopping expenses, shopping preferences, music taste) or social connections (Facebook, conversation patterns and dominance, physical closeness to bluetooth visibility). In
this thesis I will map out challenges related to personal informatics applications,
and propose an approach to design interfaces and systems that not only fulfill
users needs, but can be thought of as cognitive interfaces.
Cognition may be defined as the ability to infer meaning from perceptual sensory data by applying aspects of attention, memory, knowledge, and reasoning
[Reisberg, 1997]. Thus we consider cognitive interfaces as systems that can translate data into meaningful feedback, systems that can potentially learn from user
behavior and interaction, thereby becoming adaptable.
As in programming, where ’Hello World’ is the simplest version of an interface,
one of the simplest cognitive interfaces of personal informatics is FitBit1 . The
step counter, FitBit, translates accelerometer data into steps, calories, stairs
climbed and relates them to personal goals. Thereby the otherwise meaningless
data becomes accessible and meaningful to the user. More advanced systems such
as GN ReSound’s hearing aids2 use data streams from phones to adapt to the
surrounding environment. These kinds of context-aware systems are sprouting up
everywhere, as well as systems such as Google Now3 , which based on sensor data
learns from our past patterns, habits, routines, and interactions. Google Now
provides contextual updates, which it believes the user will be interested in, such
as sports scores from his favorite team, and traffic information about his usual
commute at the appropriate time. Thereby Google Now tries to accommodate
user needs and desires.
Some of the most complex cognitive systems we have to date are able to comprehend semantic relations, such as IBM with Watson’s4 natural language processing. Watson is able to translate speech to text, interpret information (or
questions), connect it to underlying concepts, make decisions (or provide answers) based on trade-off analysis or confidence levels. Although Watson is not
1 fitbit.com
2 gnresound.dk
3 google.com/landing/now/
4 ibm.com/smarterplanet/us/en/ibmwatson/

1.2 Outline & Contributions

3

a dedicated personal informatics system, it can quickly become one, by feeding
it personal data, like health data.
I will focus on how we can incorporate ways of inferring meaning from data to
make cognitive interfaces that can help people gain insights and make behavioral
changes in their lives. In this process I will also touch upon emotional aspects,
such as motivation, preferences and emotional responses. Emotions have traditionally been presented as the ’black sheep’ of cognitive science, due to their
ability to distort memories and because they can be the reason for irrational
or illogical decisions [Reisberg, 1997]. However, recently, emotions have started
to be viewed as inherently integrated with cognition, in the sense that emotions
help adjust cognitive processes such as perception, attention, memory, experience,
self-belief [Damasio, 2008]. Thus I will consider emotional aspects in relation to
personal informatics and creating personalized interfaces.

1.2

Outline & Contributions

In addition to the current chapter, the thesis consists of six chapters, four published papers, and one submitted paper. The chapters ties together the contributions from the papers and relate them to the context of cognitive interfaces for
personal informatics. In summary, the remainder of the thesis is structured as
follows:
Chapter 2 provides a foundation for the rest of the thesis, by defining personal
informatics and describing the motivation of Quantified Self’ers, and the
challenge between early adopters and users expecting a functional product.
In addition the widespread personal informatics framework, Stage-based
Model [Li et al., 2010] is described and its limitations are discussed. This
provides motivation to establish a new personal informatics framework that
accommodates lean agile design principles.
Chapter 3 presents the framework, the feedback loop, build on an iterative loop,
consisting of personal informatics stages including scope, data collection,
analysis & visualization, infer meaning, adjust behavior and outcome. The
framework incorporates lean agile design principles, such as a hierarchical modeling of high-level outcomes scoped in relation to activities and
tasks, resulting in a minimal viable product. In addition it describes microinteractions which contributes to the users experience: With triggers accommodating context and the users routines; rules consisting of algorithms

4

Introduction
analyzing the data; and feedback, translating the data into meaningful information. If done right, these micro-interactions can make systems seem
intelligent, and can be referred to as cognitive systems.

Chapter 4 illustrate how personality traits can provide insights to users’ motivations and preferences, which can help to scope a personal informatics
system to a specific type of users. Personality traits reveal only overall
attributes of large groups of people, while more individual preferences are
needed for a truly personalized system. Additionally methods for extracting individual emotional responses to pleasant and unpleasant pictures from
brain activity are examined. This stresses the amount of data analysis and
extensive machine learning techniques required.
Chapter 5 presents examples of visualizations of time-series data and how patterns, trends, and outliers of time-series data can be enhanced through
visualizations. Furthermore we focus on how to infer meaning from data by
use of baselines and thresholds, which can enable reflection. While making
the feedback more actionable from personalized goals, presenting different
scenarios and trade-offs and providing feedback at the appropriate time and
in the right context in order to facilitate behavior change.
Chapter 6 describes previous neurofeedback interfaces and demonstrates how
the design of these interfaces can have an effect on the training outcome.
This also illustrates the importance of considering the temporal aspects of
feedback, especially when the feedback is provided in real-time and and
changes are measured on a temporal level of milliseconds. The feedback interfaces are examined as a combination visual, audio or haptic components,
which combined may reinforce each other can support the training activity.
Chapter 7 centres on haptic feedback, since the development of sensor packed
wearable devices, makes haptic feedback an obvious candidate for personal
informatics systems. It examines how a few haptic components can be combined to create a perceived continuous motion. While a second experiment
reveals how fast different haptic patterns are perceived and whether they
are considered to be more pleasant or unpleasant than others. However if
these haptic patterns should not only serve as a means for providing information, but also enhance the user experience and create associations of real
touch interaction, they can benefit from other interface components, like
the combination of visual and haptic feedback in the beating heart from
Apple Watch.
Chapter 8 discusses aspects touched upon in the thesis, their implications for
further research. This includes considerations of temporal aspects of feedback, and the tight coupling between stages in the feedback loop. Also con-

1.2 Outline & Contributions

5

siderations on how to access users high-level motivations, needs and goals,
which are essential for scoping the personal informatics systems. Last a
discussion on moving from classical scientific experimental setup into the
everyday context of personal informatics systems.
Chapter 9 summarizes the main contributions presented in this dissertation.
Paper A Smartphones as pocketable labs: Visions for mobile brain
imaging and neurofeedback, presents the Smartphone Brain Scanner
which is build from a consumer EEG headset connected to a mobile device.
In addition the technical limitation of noise in a mobile environment are
discussed, while prospects of using 3D source reconstruction techniques can
aid these limitations. The paper presents a platform for developing EEG
applications with real-time 3D source reconstruction. Examples of experiments carried out with the Smartphone Brain Scanner, including imagined
finger-tapping, emotional responses from written words, and neurofeedback
training.
Paper B Emotional responses as independent components in EEG, discusses the possibilities for discriminating emotional responses, with the
perspective of applying these to create more personalized interfaces that
adapt to our preferences in real-time. While hypothesizing that retrieval of
emotional response in mobile usage scenarios could be enhanced through
spatial filtering, we compared a standard electrode-based analysis against
an approach based on independent component analysis.
Paper C Spatio temporal media components for neurofeedback, outline
previous neurofeedback interfaces used to train the ability to self-regulate
brain activity, which can be viewed as an example of a personal informatics system. The paper presents an experiment involving two different designs of neurofeedback training and demonstrates how these interfaces are
constructed from audio, visual components and temporal settings, which
appear to have a strong influence on the ability to control brain activity.
Paper D SOA thresholds for the perception of discrete/continuous
tactile stimulation, sets out to identify the lower and upper threshold of
the Stimulus Onset Asynchrony (SOA) for perceived continuous motions.
The range between lower and upper thresholds can be utilized to create
continuous stimulation of the skin, which can be perceived at moving at
various speeds.
Paper E Vibrotactile alarm system for reducing sleep inertia, utilizes
the haptic feedback in an alarm clock, eliminating the noise disturbance,
when sharing the same sleeping space with roomates, spouses or family

6

Introduction
members. The paper investigates the emotional ratings and attention level
of attention towards different haptic patterns, in order to choose a haptic
pattern that complements a pleasant awakening.

Chapter

2
Personal Informatics

Personal informatics is a quantitative approach to obtaining knowledge of oneself - behavior, habits, physical state, thoughts, and mental state - and goes by
many names self-tracking, quantified self, life-logging, and living by numbers. The
famous statement "know thyself" from ancient Greek culture is often mentioned
as a basic human need. Whether it is a basic need or not, we experience an increased interest and a growing market for applications measuring various aspects
of our life - from one’s diet, exercise or sleep to more specific measures such as
amounts of coffee, email activity, utility consumption.
However, tracking behavior is far from a modern concept. People have been
tracking their behavior for centuries through diaries, bookkeeping etc. Benjamin
Franklin, who was a very self-aware man, tracked aspects of his life and behavior.
He understood the difference between good intentions and turning them into
action. From his autobiography, "The Private Life of the late Benjamin Franklin"
we know that he measured his life on 13 virtues in order to live an ’ideal’ life,
see Figure 2.1. These virtues consisted of tangible factors which were easy to
evaluate such as temperance (eat not to dullness; drink not to elevation). The
virtues also included factors that applied to both physical and abstract terms such
as order (let all your things have their places; let each part of your business have
its time), and some more philosophical factors such as Sincerity (Use no hurtful

SELF-TRACKING

8

Personal Informatics

Figure 2.1: Benjamin Franklin’s chart, showing his 13 virtues (Temperance,
Silence, Order, Resolution, Frugality, Industry, Sincerity, Justice,
Moderation, Cleanliness, Tranquility, Chastity, Humility) and the
weekdays

deceit; think innocently and justly, and, if you speak, speak accordingly). While
some of these would be easy to map down in a binary system, others would require
evaluation of what is ’just’ or how others would perceive a certain behavior, is it
’hurtful’? Even by evaluating his life in this orderly fashion, Benjamin admitted
that he was never able to live the virtues perfectly, but felt he had become a
better and happier man for having made the attempt1 . Today we would refer to
Benjamin Franklin as a first mover or early adopter within personal informatics.
EARLY
ADOPTERS

Terms like early adopters, first movers or lead users describe a group of people,
who are the first to develop and use new technologies and applications. These
innovators and visionaries are of special interest when developing new products,
because they can help articulate behaviors, desires, and needs that might be
dwelling in the rest of the population, but which are less visible and distinguishable. This user-driven development is commonly found in extreme sports, but
within personal informatics, the early adopters are known as Quantified Self’ers.
Thus we will take a closer look at this group of people - their motivation and
needs, and the barriers they experience. We will examine how to create personal
informatics systems that not only motivate but may adapt and become action1 artofmanliness.com/2008/06/01/the-virtuous-life-wrap-up/

2.1 Quantified Self

9

Figure 2.2: Moore argues that a chasm exists between the engaged and forgiving early adopters and the majority expecting a fully functional
product [Moore, 2002].

able. This must be done while keeping in mind that a product will only become
a success if it bridges the chasm between early adopters of a technology and early
majority expecting a fully functional consumer product[Moore, 2002], see Figure
2.2.

2.1

Quantified Self

The Quantified Self-movement became a reality when Gary Wolf and Kevin Kelly
introduced the concept in the tech-magazine Wired, and created the forum, quantifiedself.com. Today Quantified Self’ers is a diverse group of life hackers, data
analysts, computer scientists, health enthusiasts, productivity gurus and patients
[Choe et al., 2014]. While some use existing personal informatics tools, others access api’s, create plug-ins, or build their own. The needs and motivations of these
users are likewise diverse: Some suffer from chronic or life-threatening diseases
that require immediate attention. Others strive to live healthier lives through exercise, diets or life-work balance. And for some it is curiosity about their habits
and behavior that motivate them, such as how their music consumption patterns

QUANTIFIED
SELF

10

Personal Informatics

evolve 2 . From a study in 2014 examining Quantified Self’ers’ tools, motivations
and insights, Choe et al. stated the most popular tracking categories are: Physical activity (40%), food (31%), weight (29%), sleep (25%), and mood (13%).
However many also report tracking other things including cognitive performance,
blood glucose, location, heart rate, symptoms, knowledge, stress, body fat, productivity, snoring, movies, posture, medicine, skin condition, home energy usage,
clothes, and public transit usage[Choe et al., 2014].
Although this group of people is still relatively small, we see examples of how
their needs are affecting broad populations through federal regulations, such as
in the case of the Nightscouts. The Nightscouts is a group of parents with children suffering from diabetes who have a strong need for accessible applications
monitoring their children’s glucose levels in real-time. However, they grew tired
of waiting for the slow development of applications due to legislation. So under
the slogan #WeAreNotWaiting3 they created an open source solution by hacking
an existing product from DexCom. They wanted to get access to their own data
accepting the responsibility that follows. Thereby they gained access to glucose
data from a DexCom monitor strapped around the abdomen. The hardware sensor in itself needed a high level of regulatory approval (class III product) from the
FDA (Food and Drug Administration). However the application, which only displays data, is now being classified as Class II, which means that it does not need
an approval but only to be registered by the agency and follow certain controls.
This can be seen as an example of how the government has been forced to allow
easy development of personal informatics apps that integrate with sensors. This
is evidence of how much power a small group of people have, and how personal
informatics systems are crossing the chasm.
These are examples of highly motivated people, creating systems that accommodate their own personal needs. However if we want to create successful personal
informatics systems that can cross the chasm, we need a framework enabling us
to navigate through the design process of personal informatics systems.

2.2
STAGE-BASED
MODEL

Personal Informatics Frameworks

An attempt has been made by Li et al., to provide a theoretical foundation for
personal informatics [Li et al., 2010]. By doing so they have created a framework,
the Stage-Based Model, dividing the process into sequential stages, see Figure 2.3:
2 quantifiedself.com/2014/10/tim-ngwena-music-listening-habits/
3 twitter.com/hashtag/wearenotwaiting

2.2 Personal Informatics Frameworks

11

Figure 2.3: Illustration of the Stage-Based Model inspired from Li et al.’s
illustration[Li et al., 2011].

Preparation is where the user decides what to track and which tools to use.
Collection during which data is collected.
Integration is where the data is prepared, combine and transformed.
Reflection here the user can reflect upon the data.
Action where users can take action on their newly gained knowledge, change
behavior or set new goals.

They suggest that design of personal informatics systems should be holistic: Considering all stages and the barriers that could occur and even cascade through the
stages. Li et al. also describe these stages as being iterative. However this is primarily in respect to users’ changing information needs, focus or tools. Hence after
tracking the amount of calories consumed, the user might find it more interesting
to track nutritional value of the food, and therefore changes tool.
In the final stage, the action stage, Li et al. describe how users can choose
to change behavior based on their new-found knowledge, and how they might
tailor their behavior to match their goals, indicating that many users of personal
informatics systems are goal oriented.

12

Personal Informatics

Figure 2.4: Goals can be viewed as a hierarchy from high-level, system goals,
to sequences of actions or tasks which constitute the higher-level
goals.

2.2.1
TYPES OF
GOALS

Hierarchical Goals

In addition to the Stage-Based Model, Li et al. examine the goals of different
personal informatics users[Li et al., 2011]. Relying on Powers’ theory of perceptual control that describes goals as a hierarchy, from idealistic system goals, to
very specific sequence goals, see Figure 2.4[Powers, 1973]:

System Goals is the thought of an ideal self: an ideal relationship, society.
Principle Goals are the goals required to achieve the ideal self. This could be
becoming physically fit, looking attractive, or becoming more effective.
Program Goals are more specific and often measurable. This could be exercising 3 times a week, loosing 10 pounds, or only checking emails once a day
to avoid distractions while working.
Sequence Goals are the sequence of actions or activities, what makes program
goals possible. This could be putting on running shoes, going outside,
avoiding sugar, or turning off email notification etc.

According to Li et al. the program goals are essential to achieve behavioral
changes. However they do not describe how the program goals should be defined

2.2 Personal Informatics Frameworks

13

on the basis of the principle goals. Should a person, whose principle goal is
to become physically fit, run three times a week, once every day, or instead
swim twice a week? This might depend on the user’s personal preferences or
past experiences. In addition Li et al. ignore the effect of the sequence goals.
Consequently, they ignore the fact that these sequence goals can easily become
barriers that hinder the user in achieving his goals, for example, sitting down on
the couch when getting home from work instead of putting on running shoes. The
importance of these sequences of actions are supported by the work of Kim et al.
[Kim and Paulos, 2010]. Kim et al. claim that users who succeed in sustaining
the use of personal informatics applications are those who adopts the tool into
their daily routines. In order to adopt the usage into the user’s routines it is the
sequence of actions leading to a specific behavior which is important.

2.2.2

Properties of the Stages

The stages in the Stage-Based Model are described as being either user-driven
or system-driven, where either the user or system is in charge of the tasks within
that stage. In a user-driven system, the user might be collecting data manually
and/or afterwards analyzing it himself. This can become very time and attention
consuming, which can easily lead to fatigue and lack of motivation. In contrast,
a system-driven setup typically has automatic data collection, driven by the advances in sensor technology. This makes it possible to collect data continuously,
such as heart rate or respiration, thereby providing a more detailed picture of how
they fluctuate. Another advantage is that sensors can collect data on behavior
that we are unconscious about, for example by tracking sleep phases. The downsides of a system-driven approach can be less accurate data, e.g. by mistaking
lying passively on the couch with sleeping. A system-driven approach can also
be limited in the visualization and analysis it provides, thereby not illustrating
the data of interest to the user.
Li et al. recommend an "appropriate balance between user- and system-driven"
systems [Li et al., 2010]. However if we want the personal informatics systems
to cross the chasm, the tedious and difficult tasks such as collecting data and
analyzing data need to be minimized, e.g. by automation. Instead the focus
should be on creating better rules, for translating data and inferring meaning
enabling the user to reflect on it, and gain insights. Thus the systems should
be designed to accommodate potential incorrect data, by allowing users to edit
and correct data, and learn from these mistakes. Analysis should be based on
personal baselines, thresholds, and goals.

USER- OR
SYSTEMDRIVEN

14

MULTI- OR
UNI-FACETED

Personal Informatics

Li et al. also describe systems as either uni-faceted or multifaceted. Uni-faceted
means that they are only concerned with one aspect about the behavior. e.g.
physical activity (including heart rate, calorie spend, distance covered, pace etc.).
Whereas multi-faceted considers multiple aspects in combination, such as physical
activity and sleep patterns. Recently more systems have started monitoring multiple aspects, but only few are combining the data streams, looking at how these
aspects affect each other. An example of a multi-faceted system is Spire4 , which
combines the physical activities (sitting, standing, and moving) with breathing
patterns (like deep breaths or quick gasps). By analyzing these, Spire claims to
"infer state of mind (tense, calm, focus)" 4 . However it does not provide tools
for analyzing how physical activity and mental states affect each other on a longterm scale. Will regular exercise stabilize mood swings? Thus many of these
multi-faceted applications lack the ability to infer meaning from the data. This
results in applications that mainly display huge amounts of data with no clear
user need or goal in mind.
Li et al. advocate for multi-faceted systems, however these will only work if
they have a clear understanding of which behavioral patterns and physiological
measures effect each other, and how to infer meaning from them. Instead systems
should rather be excellent at solving one specific need rather than trying to grasp
everything - they should start of as a minimal viable product.

2.2.3
CHANGING
TRACKING
TOOLS

Barriers

Li et al. describe several barriers within each stage, which might affect later
stages. Among the barriers are, exporting and importing data: If the user decides
to change between tools that do not support importing and exporting data, old
data might be lost. This will lead to fragmented or scattered data in the reflection
stage.
Another example of barriers is during the collection stage, where the data might
depend on subjective measures, such as an estimate of calories in a meal, or if
there is no standard for entering data such as judgment of relationship or mood.
This will affect the quality of the measures and the ability to compare it and
reflect upon it.

WHICH DATA
TO COLLECT

In the recent study from Choe et al., the motivation, tools and insights from enthusiastic Quantified Self’ers have been examined [Choe et al., 2014]. The study
4 spire.io

2.3 Limitations of Current Frameworks

15

revealed more barriers, including tracking the right things. E.g. instead of tracking when and where a panic attack happens, it would be interesting to track the
triggers and context. As Choe et al. point out, a common mistake in personal
informatics is failing to capture the triggers and context, instead only focusing on
tracking e.g. symptoms or outcome measures [Choe et al., 2014]. This problem
of not tracking the triggers, is actually a result of not understanding the users’
needs or the relations between behavioral aspects.
Another problem is tracking too many thing, which occurs in the collection
stage and is often associated with the multi-faceted systems. Tracking too many
things can quickly become overwhelming and exhausting, which might lead to
inconsistent tracking or even to lack of motivation and tracking fatigue: "I can
honestly say that I’ve made the classic newbie self-tracking mistake which is that
I track everything" [Choe et al., 2014]. The problem is also relevant later in
the integration stage, where the data format or simply the extensive amount
of data makes it difficult to analyze, to find correlations between data streams
and therefor to interpret the data: "It’s not that we lack the information, we’re
virtually drowning in it. The obstacle is that we don’t have the proper tools to
interpret the significance of our data" [Choe et al., 2014].
Both of these problems are actually related to a lack of scoping in the design
process: What are the questions you seek answers to? What are the needs and
motivation? It also shows a lack of understanding of the behavioral measures.
The fact is that we are now able to monitor and cross examine a lot more biometrics and behavioral measures without being certain of which are affecting each
other. Thus there is a risk of mistaking correlations with causality; what is the
relationship between behavioral aspects? E.g. are we drinking coffee because
we are tired, or are we lacking sleep because of the caffeine in the coffee we are
drinking (Figure 2.5)?

2.3

Limitations of Current Frameworks

In summary Li et al.’s stage-based personal informatics framework can be understood as a socio-technical model in the sense that it tries to merge engineering
aspects such as data collection and integration with user-centered aspects such
as reflection and behavioral aspects of the action stage. Although there are a lot
of good considerations, it is not clear how to apply the Stage-Based Model in an
iterative design process. The Stage-Based Model lacks detailed considerations of
needs, motivations and goals: Li et al. focus on action as an outcome and limit

TRACKING TOO
MUCH

16

Personal Informatics

Figure 2.5: Causality of data: Does your coffee consumption reflect your lack
of sleep? Or do you lack sleep due to your coffee consumption?

reflection to a single stage. They only consider iterations in terms of changing
needs, rather than it being an integrated part of the design process.
SCOPE &
OUTCOME

The framework does not address the links between needs, motivations or how
these are connected to high level values and goals. Likewise there is no consideration of how to transform these high level goals to more tangible goals.
Furthermore the sequences of actions required to reach the desired goals are ignored. Instead the main focus is limited to the barriers and how these might
cause the user to fail in reaching the final stage, action, which seems to be the ultimate goal. Thus the goal seems to be changing behavior and improving oneself,
rather than gaining insight, which is only briefly mentioned as a side product
of Reflection. This is not an uncommon approach in personal informatics, as
has also been pointed out by Baumer, who criticizes personal informatics studies
for focusing more on outcomes related to Reflection rather than reflection itself
[Baumer et al., 2014].

INFER MEANING

Perhaps this is due to the misleading definition of reflection as a stage between
integration and action. Some would argue that reflection can happen at different
times and on different cognitive levels. Already when the user decides to start
tracking, reflection is happening to some degree. The user might want to investigate how he is using his time, thereby indirectly he has reflected on his current
usage or at least on his assumed usage. Reflection could also happen during the
collection or action stage: When the user finds himself checking facebook updates while being aware that his computer usage is now being monitored with
RescueTime5 . Thus the mere awareness of tracking behavior can lead to reflection. Others like Baumer have also noted that many personal informatics studies
5 rescuetime.com

2.3 Limitations of Current Frameworks

17

use reflection as a keyword without defining it[Baumer et al., 2014]. And many
of those who do, refer to Schön’s concept of reflection-in-action [Schön, 1983].
Reflection-in-action describe the ability to improvise or make decisions during
an activity. This would be described as learning by doing or by experience. However Schön also describes a concept of reflection-on-action, which is more similar
to Li et al.’s reflection on data, where the process of reasoning on past behavior
leads to insights. Instead of reflection we will refer to this as inferring meaning
from data, which might lead to reflection or even insights.
Finally, Li et al. suggest that systems should be iterative between stages. However the model only describes iterations from action to preparation, mainly in
respect to prospects of incorporating new data, tool and processes. Thereby the
framework becomes somewhat flexible to the users’ changing information need to a redirection of the user’s goal or questions. However, the stage-based model
does not consider iterations in a shorter loop, through only some of the stages.
By doing this, we can consider the user’s transitions between stages, from knowledge, reflecting on it leading to action: How the data should be visualized to
infer meaning from data, upon which the user can reflect and hopefully gaining
insights. Or in the case where behavior change is the goal, considerations on how
systems can accommodate action leading to successful behavior change.
Thus we propose an alternative model with needs and motivations as the starting
point, while incorporating both high-level and low-level goals. The model will
include iterations between stages, a feedback loop. We will suggest elements
within the stages that can help infer meaning from data, aiming at providing
insights.

ITERATIONS &
LOOPS

18

2.4

Personal Informatics

Summary

Personal informatics systems are becoming more common in our everyday life
and span from systems monitoring our health, work-life, to economic balance,
and utility consumption. So far these systems have been highly influenced by the
Quantified Self’ers, who have tested, evaluated, discussed, and modified systems
to accommodate their own needs. This group of people are highly motivated
and more tolerant towards bulky, unstable or demanding systems. In contrast,
the majority of the market would not be able to modify the system nor be as
forgiving. For personal informatics systems to cross the chasm, we need a design
framework relying on lean agile development processes.
In contrast the established personal informatics frameworks are limited by:
• Not focusing on the scope & outcome - including understanding users needs,
motivation and goals which might be changing behavior, or exploring patterns and gaining new insights.
• Not focusing on how to infer meaning from data, enabling reflection and
insights - including an understanding of relations between behavioral patterns and physiological measures; and personalizing the data analysis and
visualization.
• Not considering the iterative feedback loop taking place when using a personal informatics system - including when and how feedback is provided to
the user and how feedback is provided to the system through interaction
and behavioral patterns.

Chapter

3
The Feedback Loop

We will start by establishing a model that is centered around an iterative feedback
loop. Focusing on the scope and its relation to high level outcome, which remains
essential in lean design process, will help to define the minimal viable product.
Scope is where the user decides what to track and how. This is based on the
underlying user needs and motivation, which is connected to the user’s
values and goals.
I will examine the connectivity between motivations, needs, and goals, while
linking them to an iterative design process. The coupling between personality traits and motivations will be described and emotional responses to
pleasant and unpleasant stimuli will be investigated.
Data Collection is where data related to physical sensors or behavioral patterns are gathered.
The difficulties and complexity of data collection and analysis are illustrated
in relation to capturing individual emotional responses.
Analysis & Visualization is where the data is integrated, transformed, analyzed and presented.

20

The Feedback Loop

Figure 3.1: The feedback loop, consists of the following stages: scope, data collection, analysis & visualization, inferring meaning, adjust behavior
and outcome, while focusing on the underlying motivation, needs
and goals.
The tight coupling between analysis and visualization will be illustrated
with neurofeedback interfaces.
Infer Meaning is where the user can infer meaning from data by means of
visualization, and by interacting with it - zooming, selecting, and filtering.
Furthermore, this might lead to reflection.
I explore how visual elements can help inferring meaning and enable reflection and association. While applying some of these in a neurofeedback
inferface to study the effect it has on training.
Adjust Behavior is where the user can adjust behavior, based on the inferred
meaning.
Neurofeedback training demonstrates users ability to adjust behavior based
on the visualization. In addition I illustrate how haptic feedback can initiate
behavioral change.
Outcome is where the user sees the effect of tracking, either by having achieved
or failed his high level goal, or by simply gaining insight, which can lead to
a changing focus, goal or question, and perhaps setting a new scope.
The effect of the outcome and the temporal aspects of the feedback loop is
discussed when designing neurofeedback interfaces.

3.1 Lean Agile Design Process

21

Figure 3.2: Being able to scope the system based on users’ motivation, needs,
and goals; the desired outcome, is essential for creating a successful
personal informatics system.

The loop through data collection, analysis & visualization, infer meaning and/or
adjust behavior is continuous, as long as the user has the same need and goal
in mind. The loop can be applied on many levels to describe small microinteractions or at a larger scale, to achiever goals on different levels. By looping
through these stages several times, the user can gain insight on long-term trends
and patterns, or might see the effect of behavior changes. Moreover, the system
might also learn from the user’s interaction and behavioral patterns, and become
adaptable, and can be described as a cognitive system.
We will examine how we can incorporate aspects of lean agile design processes
into the feedback loop, as a guideline for developing personal informatics systems.

3.1

Lean Agile Design Process

In any design process it is important to set the right scope for the system. This
is easier said than done as it requires understanding the users, their needs and
what motivates them, and narrowing these down to create a minimal viable
product. The minimal viable product is the minimal version of product, which
solves a problem for the user, while allowing developers to gain insights through

22

The Feedback Loop

testing. It is a strategy for fast development with minimum effort and resources
[Ries, 2011].Thus we will start by zooming in on the scope and outcome of the
feedback loop, see Figure 3.2.
MOTIVATIONS
& NEEDS

Motivation represents both desires and needs and is one of the reasons for selfimposed, planned behavior and reflects personal preferences and values. Needs
are basic elements that can be objective such as food and shelter, or subjective
such as self-esteem or feeling secure. However in this thesis we will view needs a
little more broadly. They are not just what provides a foundation to live ’healthy’
lives, they are rather what is necessary to live ’our’ lives, both in a practical and
ideal sense. Thus, needs explain what and motivation can explain why. In this
sense, motivation and needs can be seen as the starting point to achieving goals,
which is therefor tightly connected to the desired outcome.

GOALS &
SEQUENCES

In the previous chapter we saw how motivation, needs, and goals, are often taken
for granted when describing personal informatics systems. They assume that
motivation naturally leads to behavior change: If you want to become physically
fit, you will naturally succeed at this, if you have a program goal, like running
three times a week. There is no consideration of how these goals are defined, or
which fits the individual user. They also neglect the sequences of actions required
to achieve the desired goal.
Thus, we will investigate how to incorporate both high-level goals and sequences
of actions in the design process. Going from abstract measures of motivations,
needs and goals to tangible tasks which can be transformed into programmable
flow-charts.

3.1.1
USER STORY
MAPPING

User Story Mapping

One way of connecting high level goals with activities and sequences is by the
use of User Story Mapping [Patton and Economy, 2014]. User Story Mapping
is a tool for agile software development and is used for scoping user needs into
activities and a hierarchy of prioritized tasks: A story line is created from the
different users and their needs or goals. To achieve these goals one requires a
set of activities, which in turn translate to a sequence of actions. Each of these
activities consists of a set of tasks. These are the detailed steps supporting
the activity. By defining these tasks to avoid dependencies between them, and
by prioritizing them in a hierarchy, the developer can scope the system into a
minimal viable product, see Figure 3.3. From here the developer can improve
the system with more nice-to-have features in the later iterations. Through this

3.1 Lean Agile Design Process

23

Figure 3.3: By decoupling tasks and prioritizing them, the first iteration of
product development can be usable as a minimal viable product.

process it becomes clear which tasks are necessary, and which do not contribute
to the activities and goals.
By focusing on one user, and his main goal or need, the activities and tasks
required to solve this, we have started defining a minimal viable product. Based
on this, a prototype is created and tested, enabling the developers to get insight
and evaluate the goals, activities and tasks, which can lead to altering the original
scope, see Figure 3.4.
By applying the user story mapping to our feedback loop we can both transform
the high level goals into sequences of actions, and describe these actions or activities as a set of tasks, which can easily turn into programming flow-charts, see
Figure 3.5.

3.1.2

Micro-Interactions

These tasks can be described in details as micro-interactions. Micro-interactions
provide a detailed flow-chart view of the underlying rules and triggers that define
a system and can enhance the user experience [Saffer, 2013]. Micro-interactions

MICROINTERACTIONS

24

The Feedback Loop

Figure 3.4: User Story Mapping as part of an iterative process, helping to scope
the product by testing prototypes, thereby evaluating and redefining the scope of the product.

Figure 3.5: By considering the activities of each stage in the feedback loop,
we can transform this into the lean design framework, User Story
Mapping.

3.1 Lean Agile Design Process

25

Figure 3.6: Spire consists of micro-interactions - interpreting whether the user
is tense or relaxed based on breathing patterns and activity level.
consist of a chain of events which can be described as five different types of
actions:
Triggers initiate a change of state or mode
Rules determine what change entails
Feedback signals that a rule has been activated and preferably what it entails
Modes are meta rules, that overwrite the current rule
Loop determines when to exit a mode, based on time past, new triggers or
previous history
The triggers can be either user or system initiated. A user initiated trigger could
be when the user switches his phone to silent mode, or when he taps a button to
set an alarm, whereas a system initiated trigger could be when a system switches
to a silent night-mode after 10pm. The rule is then to replace previous audio
notifications or incoming calls with a vibration or simply a visual display. The
phone would stay in this mode until the user switches it back to audio. The
feedback is the small vibration indicating that the silence button is switched on
and an audio icon indicating audio on and off. The modes are meta rules, thus
when the user has set an alarm, this will go off even though the phone is on silent
mode. The alarm mode will then overrule the previous silence rule. The loops
determines when to return to the previous mode. Thus when the phone is on
night-mode, this will continue looping over this mode until morning.
These micro-interactions are often small fundamental elements that contribute to
the user experience, and the rules are what makes these systems seem intelligent.

PERSONAL
INFORMATICS
SYSTEM

26

The Feedback Loop

In personal informatics we see them in systems like Spire1 . For Spire the scope
is to "reduce tension and guide the user towards a calm and focused mind" 1 .
Spire relies on the natural human feedback mechanism breathing rate, which is
one of the few ways of consciously influencing the nervous system. Changing
the state of mind simply by changing your breathing pattern. Taking a deep
breath when angry or tense might result in becoming more relaxed. This is done
by transforming accelerometer data into breathing rate as well as activity modes
(sitting, standing, and moving). If the user’s breathing rate has increased without
the user moving much, the system will interpret this as the user being tense. The
system notifies the user while suggesting to take a deep breath, see Figure 3.6.
Here the trigger is a combination of data thresholds - a fast breathing pattern
and low physical activity. While the rule is the interpretation; being tense, calm
or focused. The feedback is the notification. However if the user does not agree
with the systems interpretation, and does not feel tense - the system can easily
seem unintelligent. Thus getting these micro-interactions right is important in
striving to create cognitive interfaces.

1 spire.io

3.2 Summary

3.2

27

Summary

By establishing a model around an iterative feedback loop, we have created a
starting point for a personal informatics system around the basic component of
providing feedback to the user and receiving feedback on which the system can
learn from the user’s interaction and behavioral patterns, and thereby become
cognitive. The feedback loop consists of the following stages:
• scope
• data collection
• analysis & visualization
• infer meaning
• adjust behavior (optional)
• outcome

The feedback loop focuses on the scope & outcome of the system, which are essential for creating a minimal viable product. We incorporate User Story Mapping in
order to get from high-level needs and goals to sequential activities which can be
fitted into the user’s routines, and tasks on programming level. With User Story
Mapping the goals, activities and tasks can be decoupled and prioritized helping
to scope a minimal viable product, which can be prototyped, tested, evaluated
and modified until ready to be programmed into a commercial product.
The detailed interaction with the systems are referred to as micro-interactions.
These small fundamental elements are what contributes to the user experience
and makes the system seem intelligent, but only if done right. The microinteractions consist of triggers, rules, feedback, modes, and loops. In personal
informatics systems the triggers, rules, and feedback are what makes the system
cognitive. This includes understanding the users and their routines, understanding the data in relation to correlation and causality, and how to translate and
infer meaning from data.

Chapter

4
Personalization

As we saw in the previous chapter, understanding users, their motivations, and
preferences is essential for scoping the product. This chapter focuses on how
we can gain insights to users’ motivations and preferences based on personality
traits, in order to personalize the content, and provide feedback in an appealing
way. In addition, I examine the prospects of accessing users’ emotional responses
from cortical brain activity. This will illustrate the importance of the scope and
how extensive data collection and analysis can be, see Figure 4.1.
Since personality and personality traits often reflect behavior, preferences, motivations and values in life, there has been extensive work on understanding
and analyzing these in a systematic way. One of the most well-known and
thoroughly researched frameworks is the big five developed by McCrae et al.
[McCrae and Costa, 2003]. The big five is a categorization of five primary personality traits:
Agreeableness a person scoring high in agreeableness, will often show compassion towards others, be helpful, cooperative while being modest and have
a high morale. A person with a low score would be self-centered, proud,
have a hard time trusting others, and think that people should rely on
themselves instead of others.

PERSONALITY
TRAITS

30

Personalization

Figure 4.1: We explore how we can get insights into users’ motivations, preferences and emotional responses, while illustrating the complexity
of collecting and analyzing brain data of individual emotional responses.
Conscientiousness a person scoring high in conscientiousness will be described
as driven, organized, responsible, and persistent. While a person scoring
low will be carefree, unstructured, self-doubting, and feel content with no
need for ambitious goals.
Extraversion a person scoring high in extraversion would be very energetic,
optimistic, seeking the company of others, and is comfortable taking charge
and leading groups. A person scoring low would be calm, laid back, private,
serious and need time for himself.
Neuroticism a person scoring high will be anxious, insecure, fiery and sensitive
to others’ opinions, while a person with low scores would be confident,
self-controlled, content, and calm under pressure.
Openness a person scoring high in openness is appreciative of art, imaginative,
has a philosophical approach, challenges authorities and is eager towards
new experiences and various activities. A person scoring low will rely on
familiarity and traditions, is pragmatic, prefers facts and is respectful of
authorities.

Each of the five primary traits has six facets, which further characterize the

4.1 Designing for Personalities

31

individual. A person might have a combination of these traits, however some are
more strongly represented than others.
By understanding what motivates the users with different personalities, we can
get an indication of their preferences and values. This is why IBM’s Watson1 has
been trained to infer personality types from social media profiles such as Facebook
and Twitter [Golbeck et al., 2011]. The outcome is measures of which characteristics are most prominent in a user, compared to the average of the population
of Facebook or Twitter users. In this way, Watson offers companies insights on
their customers’ personalities through social media logins, thereby helping companies target products and marketing more effectively. We can therefore easily
imagine that information about users’ personality traits will become accessible
to developers of personal informatics systems. Thus we can start to include aspects such as motivation when designing systems, which can effect the way of
presenting data, content, purpose, and whether the system should be actionable
or explorative, allowing for detailed interaction with the data.

4.1

IBM WATSON

Designing for Personalities

Even though users with different personalities have similar needs, different ways
of solving these needs might have an appeal to different personality types. This
has been demonstrated in people’s preferences towards book-reviews
[Nass and Lee, 2000], where people preferred reviews presented by a reviewer with
a personality trait similar to their own (introvert vs. extrovert), and were even
more likely to buy the book. Thus applications can be adjusted to personality
traits, by customizing the content, how it is presented and what needs it should
fulfill. To demonstrate this, we look at three applications which help the user
keep track of consumed calories.
Carrot Hunger2 , has a very simple interface. The main screen provides feedback
on the amount of remaining calories, in three ways - from the size of the avatar
(skeleton, normal and overweight), the number on the pedestal, and the level of
green slug. The simple, yet artistic interface could easily appeal to people with
high Openness. The application has a sarcastic commentator, that will ruthlessly
shame the user whenever he consumes more calories than the daily threshold, and
will electrocute his avatar, unless he pays a small fee or he will be publicly shamed
on his twitter account. Thus people who choose to use Carrot Hunger will likely
1 watson-pi-demo.mybluemix.net
2 www.meetcarrot.com/hunger

PERSONAL
INFORMATICS
SYSTEMS

32

Personalization

Figure 4.2: Illustration of how Watson would present personality traits of a
person with high Openness and Conscientiousness.

4.1 Designing for Personalities

(a) Carrot Hunger

(b) MyFitnessPal

33

(c) MealLogger

Figure 4.3: Three different approaches to personal informatics systems for
tracking calories. (a) Carrot Hunger display a clear warning, when
the consumed calories have exceeded the daily threshold, with the
red colors and electrocution of the avatar. (b) MyFitnessPal shows
a classic bar plot with three colors indicating the amounts of carbs,
fat, and protein consumed during the past week. (c) MealLogger
displays a score board in the top of the screen and depicts meals
and activities as items in a grid.

be cheerful and perhaps competitive, both of which are facets of Extraversion.
The application is also very action oriented, with one big button in the center
of the screen for recording calories. And it only provides a limited history of
behavior - the number of days the user has been able to stay within the calorie
threshold. Thus the user does not need to spend much time engaged in the app,
which is also perfect for people who score high onExtraversion - they tend to live
busy, fast-paced lives. Carrot Hunger excels by having a clear-cut scope and users
of Carrot Hunger are likely to be motivated by its simple, yet candid interface,
which is only meant for a bare minimum of interaction.
In contrast MyFitnessPal3 provides a more extensive interface with a detailed
history. The history includes a diary of all consumed food, which the user can
investigate for nutritional information, and which allows the user to correct inaccurate food entries. The average nutritional information of fat, protein, carbs,
3 myfitnesspal.com

34

Personalization

sugar, fiber, cholesterol etc. from past days, weeks, and months are displayed in
traditional bar and pie charts. This interface appeals to users who are interested
in the details of their behavior. Thus people who choose MyFitnessPal would be
likely to score high in Conscientiousness and low in Openness, since these will
prefer traditional approaches. They have a need for structure and are motivated
by feeling in control.
Another diet application is MealLogger4 , where people share pictures of their
meals in communities. People in the communities can comment on each other’s
pictures, get recipes and motivate each other. This concept is appealing to people
with high Agreeablesness, who care care for the opinions of others. The history
is presented as a grid of photos and icons, which leads to more information when
tapped. The interface is intended for the user who spends time on exploring
nice photos, recipes and engaging in the community. Thus this application might
be interesting for people with Extraversion, who find the company of others
rewarding and motivating.
These three examples illustrate how interfaces can appeal to people with different
personality traits. This applies both in terms of how actionable or exploratory
an interface should be, the content, the level of detail, and how it is delivered
(e.g. sarcastic as in Carrot Hunger).
QUANTIFIED
SELF’ERS

If we look at the personality types of early adopters including many of the Quantified Self’ers5 , these would be likely to score high in Openness, being open towards
new technologies, driven by a curiosity that questions the common ways of doing
things. They could also score high in Conscientiousness, being determined to
get the answers they are looking for and dedicated to use the technologies in
order to achieve their goals. This is how Watson would portray these personality
characteristics, their facets, see Figure 4.2. These personality characteristics can
be used to explain what needs and values a person has, which can explain how a
person would behave and what motivates him.
Though there has been a lot of research focusing of the effect of personality
traits and consumer behavior, there are only a few examples within personal
informatics. Most of these focus on implications on scenarios with gamification
or educational purposes. One study examines correlations between personality traits, preference towards different game affordances (challenges, rewards,
progress, etc.), and what behaviors they tracked [Karanam et al., 2014]. They
found that people with high conscientiousness and openness preferred rewards,
4 meallogger.com
5 quantifiedself.com/personality/

4.2 Emotional Responses in EEG

35

Figure 4.4: The averaged global field power plotted for the three conditions,
neutral (green), pleasant (red), unpleasant (blue), used to define
relevant windows for further analysis.

whereas people with high extraversion and openness were motivated by challenges. This illustrates some broad tendencies coupling motivational factors with
personality traits, which are relevant in the context of personal informatics and
the design of feedback interfaces. However if we can access emotional responses
to content in real-time, we will be able to create more effective systems that can
adapt to users’ preferences, without extensive training or prior usage.
Yet this standard way of categorizing users by personality traits will only reveal
overall attributes. To make a system truly personal, it requires access to individual differences, which amongst other things are affected by past experiences,
memories and emotions.

4.2

Emotional Responses in EEG

Being able to tap into users’ personal experience and emotional responses would
be of great value and enable the possibility of personalized content. With the
development of new types of mobile neuro-imaging headsets, this can become
a reality in the future. In an outline we demonstrate the tentative beginning
with a series of activities and experiments, Appendix A[Stopczynski et al., 2014].
However as we will see in the following, working with brain data is not trivial.

36

Personalization

In prospect of extracting emotional responses from brain activity, we conducted
an experiment displaying affective pictures, Appendix B [Jensen et al., 2014].
The pictures were a subset of pictures from the International Affective Picture
System (IAPS) [Lang et al., 1997] which consist of almost 1000 photos ranging
from everyday objects to mutilated bodies and erotic nude scenes. These photos
were originally rated by 100 college students on a valence and an arousal scale
and have been scientifically accepted as a normative rating.
EVENT
RELATED
POTENTIALS

By recording electroencephalography (EEG) while participants view the photos,
we capture the electric potential from the scalp representing the neural activity in
the cortex. Traditionally, emotional responses have been analyzed by averaging
the waveform appearing immediately after the stimulus is presented, known as
the event related potential (ERP). The emotional responses appear as slight µV
changes, which is highly susceptible, if captured in a noisy mobile environment.
Thus we compare a classic electrode-based analysis, the Global Field Power, to
an independent component analysis (ICA), in hope of getting a more robust
neural signature which could potentially be applied in a mobile context recording
emotional responses.
We presented a series of affective pictures to four male participants while recording their neural activity on a standard BioSemi ActiveTwo system using 64 electrodes. The pictures were a subset of the IAPS from two studies on emotional
responses, one with 96 pictures [Lang and Bradley, 2007], the other with 66 pictures [Larsen et al., 2003]. The total of 162 pictures were divided into three
conditions (pleasant, unpleasant and neutral), based on their valence and arousal
ratings.

ELECTRODEBASED
ANALYSIS

In the classic electrode based analysis we used the Global Field Power to identify
relevant time windows (P1 (75-125ms), EPN (130-225ms), P3 (250-315ms), and
LPP(325-525), see Figure 4.4), and a scalp map of those time windows to identify
relevant electrodes, see Figure 4.5. Based on these we plotted the average ERPs
across all participants, and found only a significant effect in the early posterior
negativity (EPN) window 4.6. This early negativity corresponds well to earlier
findings [De Cesarei and Codispoti, 2006] and has been related to an increased
attention demand, and primarily captures difference in valence, whether a picture is pleasant or unpleasant. These early responses might even be visible before
the stimulus is fully processed, suggesting that perhaps the dominating color of
skin in nude scenes and colors of skin and blood in mutilated bodies are decisive. These early responses to pleasant and unpleasant images can relate to the
basic approach or avoidance instinct [Lang and Bradley, 2010]. Whereas early
ERP components are believed to relate to bottom-up processing of stimuli, the
later components have been suggested to indicate higher-order processes such as

4.2 Emotional Responses in EEG

37

Figure 4.5: Scalp maps for the time periods P1, EPN, P3 and LPP for each of
the conditions (unpleasant, neutral and pleasant).

38

Personalization

Figure 4.6: An average ERP of channels covering the temporal occipital lobe
(P7, P8, POz, PO7, and PO8) shows the three conditions pleasant
(red), unpleasant (blue) and neutral (green). A statistical one-way
anova on each channel marks the significant areas (α=0.01) of the
ERPs in grey.

memory retrieval, and semantic evaluations [Foti et al., 2009]. Thus the lack of
later ERP components could indicate that these are more sensitive to individual
processing.
COMPONENTBASED
ANALYSIS

By using ICA we decomposed the EEG data into scalp maps which where then
grouped and reduced by PCA. After this, we clusted similar ICA components by
a K-means algorithm, thereby following the standard EEGlab procedure
[Delorme et al., 2011]. From this, three out of 20 clusters revealed several significant time windows showing difference between one or more conditions. All three
clusters were shared among all participants and consisted of a large amount of independent components. The most profound cluster, cluster 6, showed both early
and late emotional responses to pleasant pictures. These results are somewhat
similar to those found by applying principal component analysis (PCA) on emotional responses [Foti et al., 2009], with early responses distinguishing pleasant
from unpleasant images, whereas later responses were affected by high arousal
scores. This indicates that these later responses are characterized by the intensity
of emotional involvement [Gianotti et al., 2008], which could be associated with
memory coding and semantic processing.

4.2 Emotional Responses in EEG

39

Figure 4.7: Cluster 6, based on PCA dimensionality reduction and K-means
clustering (K=10,σ=3) of 97 ICA scalp maps found within all conditions, with corresponding ERPs for pleasant (red), unpleasant
(blue) and neutral (green) images. Significant intervals for differentiating between the emotions are indicated in grey.

Where many personal informatics systems suffer from poor scoping, we need an
extremely well defined scope when dealing with brain data. As illustrated with
the electrode-based analysis, we need a well defined scope, in terms of clear definitions of relevant time windows, and of relevant electrode and a clear distinction
between emotions as illustrated in the strong contrast of IAPS, from mutilated
bodies to erotic nude scenes. And even with all this, it still requires many samples
across multiple participants to distinguish between brain responses to pleasant
and unpleasant stimuli.
The prospects of not only identifying basic emotional responses of attraction
or avoidance, but also accessing more individual responses, such as the later
responses related to associations, memories and past behavior, requires more advance machine learning methods. As we saw with the component-based analysis,
transforming raw EEG data into usable components required both ICA, PCA
and K-means clustering, which are far beyond the classic excel analysis used in
many existing personal informatics systems.
For these new types of mobile brainscanners to become usable in personal informatics, making them adaptable by generating personalized content, they not
only have to do these types of extensive analysis, they will also have to do them
on a single trial level, preferably without any extensive training period.

40

Personalization

As just illustrated, gaining insights to users’ individual emotional responses is not
trivial. However, gaining insight into the users’ general personality traits, and
thereby what motivates the users, can be used to scope the systems. Personality
traits also reveal preferences on how feedback is presented, both in terms of how
actionable or exploratory it should be, and in terms of the content - the level of
detail, and how it is delivered. Thus we will take a close look at feedback, in
terms of visualization of data, and how we can infer meaning from it.

4.3 Summary

4.3

41

Summary

Systems like IBM’s Watson has demonstrated how it is possible to extract personality traits from people’s Facebook and Twitter accounts. Personality traits
can provide insight into users’ motivations and preferences, which can help in
scoping a personal informatics systems to a specific type of people, both in terms
of how actionable or exploratory it should be, the content, the level of detail, and
how it is delivered.
These personality traits reveal only overall attributes of large groups of people.
However, if we want to access more personal differences, we need to gain knowledge of past experiences, memories and emotions. With the development of new
types of mobile brainscanners, it could become possible to create individual personalized context and make systems that could adapt to the individual emotional
responses.
As we illustrate in an experiment on emotional responses from pleasant and
unpleasant pictures, it requires a large amount of data and extensive machine
learning to extract meaningful results. At the moment, this is far beyond the
classical data collection and analysis of any existing personal informatics system.

Chapter

5

Cognitive Interfaces

As touched upon in the introduction, cognitive systems include systems that are
able to learn from the user’s past behavior, or are based on contextual information. However, interfaces can also be cognitive in their ability to translate data
into meaningful feedback. And by making the feedback not only meaningful but
also actionable, the user is more likely to adjust behavior if this adjustment is
necessary to achieve his goals. This corresponds to the analysis & visualization,
infer meaning, and adjust behavior stage of the feedback loop, see Figure 5.1.

In order to benefit from the collected data, they need to be accessible and translated into something understandable in order to get insights. To do this, the data
are often visualized. This thesis will not cover all types of visualization, but will
focus on visualizations of time-series data and how we can apply analytic tools,
such as filtering, baselines, thresholds, etc. to infer meaning from the data.

44

Cognitive Interfaces

Figure 5.1: By translating data with the use of analytic elements, such as baseline and thresholds, we can infer meaning enabling reflection and
insight, thereby creating cognitive interfaces.

5.1

Visualization

How to visualize the data depends on the insights we want to provide. If the
user wants to change behavior, the visualization should be actionable. However,
if the goal is to explore unknown territory or is to answer a specific question, the
visualization should incorporate interaction, allowing the user to dig down in the
details. Thus before visualizing any data, decisions have to be made on what
message the data should convey and therefor also decisions need to be made on
the underlying analysis of the data, and the level of interaction. Thus a personal
informatics system assumes to know what the user wants and which questions
they want answers for.
The visualization also depends on the type and amount of data we have - are they
discrete, continuous or in a network? Are they events, pictures, or annotations,
etc.? When visualizing data, we try to exploit the abilities of the human eye
to detect structures, patterns, correlations, trends or outliers. Thus we should
present the data in a way that makes these patterns, correlations, trends or
outliers clear.
TIME-SERIE
DATA

Common for a lot of personal informatics data is that they are a time-series, since

5.1 Visualization

45

Figure 5.2: Moves app, shows a summary of physical activities using circle sizes
and stacked bars as a detailed timeline with colors characterizing
types of physical activities and icons illustrating events in-between
the physical activities.

we often compare present and past behavior. Thus we might want to examine
the temporal location of the data; when does the data element exist in time? Is
there a temporal texture of the data; how often does the data occur? What is the
changing rate of the data? Are they repeating in a pattern? Do data elements
occur in a specific order, a sequence? Do data elements co-occur (synchronization
or correlation between data)?
Most personal informatics applications providing temporal data choose to display
them as a linear time axis, with either line graphs, like classical financial trading
developments, or bar plots for comparing entities. However most of our patterns
are typically circular, such as seasonal changes or daily routines, which tend to
repeat on a temporal level. Other data can be structured as a sequence where
one data element follows another in order, or data can be a hierarchical structure
where one data element can be followed by a number of possible data elements.
E.g. you might not wake up at the same time every day, however even if you
wake up later one day, the next thing you do is take a shower and get dressed.
And then you might eat breakfast followed by brushing your teeth, or you might
skip breakfast.
Some of the most classic visualizations within personal informatics are bar plots,
line graphs, pie charts and other visualizations which are also common in ex-

MODIFIED BAR
PLOT

46

Cognitive Interfaces

Figure 5.3: Calendar heat map showing Dow Jones index value of each day as
colored cells, arranged into rows of weeks and blocks of months 2 .
cel. These can easily be modified like Moves1 , that tracks the whereabouts and
amount of walking, running, and biking. Moves consists of colored circles representing different physical activities (walking, running, biking), see Figure 5.2.
The size of the circles increases with increasing activity, and works as a summary
of the current day’s activity. Below them is a more detailed view of the history,
a stream (of stacked bars) with colors corresponding to the activities and the
length indicates the duration. The bars are divided by small maps, indicating
time spent in one place, and the description of the place (from Google Maps api).
This app is fairly simple and can provide the user with a one-glance overview of
the day’s activity. However, it is not very helpful as a guide or tool to change behavior, since it has no threshold indicating the effect of the physical activity and
even though they collect data from millions of users, there is no crowd-sourcing
allowing the user to compare himself with others.
HEAT MAPS &
SPIRALS

If we expect there to be a sequential pattern in the data, this could become more
clear with a heat map, aligning the data by hour, day or week to spot patterns. In
Figure 5.3 Dow Jones data is displayed as a heat map2 : the values are visualized
by the colored cells. Each cell represents a day, which is arranged into columns,
by week, month and year. The data is constructed into a calendar heat map. Here
we see high fluctuations in the data in October and November of 2008, likewise
in February and March of 2009, whereas the index is more moderate the rest of
the time. This view is especially helpful when the focus is on both extreme high
and low values. Recently, the heat map has been modified into a spiral pattern,
creating a more seamless and continuous flow [Larsen et al., 2013], with no break
between the last and first hours of the days or between Saturday and Sunday.

DATA AS
NETWORK

In a few cases we see examples of time series visualized as a network. Baur
1 moves.com
2 bl.ocks.org/mbostock/4063318

5.1 Visualization

47

Figure 5.4: Visualization of Last.fm history as a network and with integration
of calendar events and photo stream.

created an exploratory visualization of the streaming history from Last.fm
[Baur et al., 2010]. The data are a sequence of music tracks, where some tracks
are in the history and are transformed into a network. This is a simple centralized
network, where each node represents the same music track. This is a fairly simple
network, and does not provide us with much more information other than how
many times a track was played over a period of time. However this could easily
be transformed into more complex networks by connecting tracks by the same
artist, genre etc.
The classic approach for exploratory interfaces is the ability to select, filter, reconfigure, elaborate and connect data. In this example, some of this is done
by applying different colors according to the genres, and changing the size of
the nodes to illustrate which are being played the most and by applying different
temporal filters, such as dates, and time of day. What is more interesting though,
is how calendar events and photo streams (discrete events) are used to get another level of personal data (Figure 5.4). By adding these, the tool is transformed
from an exploratory tool, which might get tedious with time, to a more emotional
system for reliving old events, through music and pictures. The user can then
choose to see photos from a period as a slide show while the most popular music
tracks of that period are being played.
These three systems illustrate how data in time series can be visualized in different

ZOOMING,
SELECTING &
FILTERING

48

Cognitive Interfaces

ways. They also illustrate different levels of detail and interaction, from the
spare time line of Moves - providing an overview and a simple history, to the
Last.fm visualization - allowing the user to explore past behavior by applying
filters and selecting tracks, artists, genres etc. This lets the user specify the
content according to his needs.

5.2

Infer Meaning

BASELINE &
METHAPHORS

In order to gain insight based on the data visualization, one of the key elements
is a reference point or a baseline. As Huang et al. mention in their review of
visualization studies "Making comparisons is a fundamental way to gain insights
from data"[Huang et al., 2015]. In other words, a single number does not give
much meaning in itself. E.g. knowing that you have burned 1900 calories in a
day, does not give you much insight. However, metaphors can help translate the
numbers into commodities such as seven burgers or walking a distance of 20km,
hence it becomes easier to understand. The number can also be compared to
prior data - a personal baseline; an average across days, weeks, months, years;
or to the average of others. However knowing which baselines to choose will
depend on the goals, context of use, and on personal preferences. E.g. should a
household’s energy consumption be compared to other families of the same size,
or households of the same demographic context? Thus designing visualizations
with the flexibility to change baseline depending on goal and preferences can
accommodate useful insights.

THRESHOLD

Just as baselines are important, thresholds are equally important. Are seven
burgers a lot or not? Some thresholds are universal (such as a bank balance
below 0 being unwanted), but most are individual. The threshold might depend
on height, weight, gender, age, or amount of activity. Thresholds can thereby
help the user to analyze and understand the data. Common is their ability to
convey a message easily and quickly. This is important, if the user needs to
act upon it. Thresholds are not static, they can just as easily vary with the
behavioral patterns. Thus the amount of sleep needed might depend on the lack
of sleep previously or on the amount of physical activity. Thresholds are easy to
act upon, because they transform data into binary results or decisions.

5.3 Actionable Feedback

5.3

49

Actionable Feedback

When it comes to behavior change initiated by the personal informatics system,
this is often referred to as persuasive design, which nudges people towards a
certain behavior (whether they are consciously aware of it or not). Recently, there
has been a lot of discussion about whether personal informatics systems should be
persuasive or not, and if so, to what degree 3 . The critique of personal informatics
is often that it is one-size fits all solutions: Often with fixed thresholds, such as
a good night’s sleep is 7-9 hours; or standard suggestions such as do not drink
coffee after 7pm; or standard goals, such as TripIt’s4 leader board, showing who
is traveling the most in the user’s network. However, this is misleading if the user
is trying to reduce his traveling due to environmental impact or spending more
time with his family [Munson, 2012].
This can of course be solved, by making it possible to establishing personal goals.
Thus systems should accommodate personal goals, which could be distinctly opposite. But it is not only important that the user can set his own goals. The
system should also suggest goals which are realistic and which can be adjusted if
they are too ambitious. Systems like Micoach5 have different training programs
according to the user’s goals (run 5km, 10km etc), and to the user’s physical
fitness, however if the user fails to follow the program, there is no possibility for
adjusting it, only to start a new one. Thus being able to set appropriate goals is
essential for the user to succeed, and will increase their motivation and control.
Likewise thresholds and suggestions should be personalized, through learning algorithms or crowd sourced data such as Last.fm’s6 music suggestions. Having
data on users’ preferences might lead to better suggestions.

PERSONALIZED
GOALS

By modifying and personalizing the system, we make systems less persuasive and
instead make them provide actionable feedback. This can be done by creating
scenarios. Scenarios are especially useful if the effect of an action is not visible
right away, but has more long-term effects, or when data points show discouraging
results, despite the user’s effort to change behavior. This is typical for behavioral
changes which require persistence and a long time before results are visible. This
often leads to lack of motivation. In some cases this could be helped by not
focusing on single data points but on more global trends or by presenting different
scenarios and their trade-offs, showing the user different ways of obtaining the

SCENARIOS &
TRADE-OFFS

3 quantifiedself.com/2012/03/personal-informatics-in-practice-reflection-and-persuasion-inpersonal-informatics/
4 tripit.com
5 http://micoach.adidas.com/
6 last.fm

50

Cognitive Interfaces

Figure 5.5: A beacon placed in the fridge, sensing when a smartphone is near,
makes Carrot Hunger context aware, prompting the user to enter
the food he is about to consume.

goals. For example, if the user wants to lower his energy consumption, the system
could show what the effect of changing half of the bulbs to energy saving ones
would be, and compare that to the effect of changing them all. Or it could show
what the effect of lowering the temperature on the thermostats would be. Having
multiple scenarios can help the user evaluate the trade-off from different behavior,
and make decisions based on that.

SEQUENCES &
ROUTINES

To provide actionable feedback, and encourage action or even change behavior,certain elements should be considered. The system should first of all provide
feedback in the right situation - when the choice/action is being made, and should
fit into the physical environment and context of use. We see running applications that are excellent at giving feedback on one’s pace while running, however
they are less successful at getting the user out running. This requires the system
to obtain information which contributes to understanding the contextual and sequential circumstances: This could be calendar events, GPS locations, or weather
information, e.g. if the user only runs in the morning and only on days when it
is not raining. The system could use weather data and calendar events to figure
out which mornings would be most suited for running. And with knowledge of
the sequence of actions required to perform an activity or task, the system could
suggest setting an earlier alarm, giving extra time for running in the morning,

5.3 Actionable Feedback

51

and suggest having the running gear ready next to the bed. By accommodating
and adjusting to the user’s routines, the likelihood of success is increased.
An example of a context-aware system is the Carrot hunger7 . Like many other
systems that track calories, it lets the user enter what food is consumed. However,
it can also be hooked up to a beacon that senses the proximity of a connected
smart-phone. By placing the beacon inside the fridge, it will recognize the smartphone once the fridge opens, see Figure 5.5. This triggers a notification, telling
the user to input the food he is about to consume. A clever detail when the
phone is within range of the beacon is that a little Carrot Hunger icon appears in
the bottom left corner of the lock screen, which enables fast entry to the system,
similar to the camera icon function on many phones. Both the notification and
the icon on the lock screen are examples of micro-interactions. It also has a
bar-code scanner that the user can use instead of typing in the food, which will
make the interaction quicker, which is essential if the user needs to input this
multiple times a day. Thus if the visualization is to lead to action, it should
provide in-the-moment data for real-time awareness with respect to the context
(place, behavior, routine etc).
In the next chapter we apply individual baselines and thresholds to neurofeedback
interfaces, and examine the tight coupling between these interface elements and
the temporal aspects of the feedback loop.

7 meetcarrot.com/hunger/

CONTEXT
AWARE
SYSTEMS

52

5.4

Cognitive Interfaces

Summary

Providing feedback is the basic component of any personal informatics system.
The feedback is typically based on data that have been analyzed and presented,
often by a visualization. Since most personal informatics are a type of time-series
data, we saw how different types of visualization could be created from simple
Excel charts to illustrate relative amounts of activity by circles or a history of
events by stacked bars. We saw also how visualizations could underline sequential
patterns of acute high or low values from heat maps.
If the data are not only visualized but also translated into something meaningful,
it becomes easier to reflect upon them and gain insights. This could be done
by the use of personalized baselines and thresholds. Furthermore, the feedback
can be actionable by allowing personalized goals, or by presenting scenarios and
trade-offs of alternative ways of achieving goals. It could consider the user’s
individual routines and sequences of actions that the system should be part of.
And it should be context aware - knowing where and when in a user’s sequence
of actions to provide feedback.

Chapter

6
Neurofeedback Training

Feedback can be provided at many different levels and in different ways. Feedback
is often a confirmation or declaration of an action. We often see them in relation
to micro-interactions as a confirmation or indication of a rule being triggered when switching off the sound on a phone, a bell icon with a cross will appear
and the phone might vibrate as feedback to the user. In personal informatics,
feedback is often presented in relation to a baseline or a threshold, basically a
change in condition. Feedback can be provided on different temporal levels, from
real-time feedback, provided from milliseconds, seconds or minutes of data, to
feedback illustrating tendencies from minutes or hours, such as heartbeat when
exercising, or from days, weeks and months, such as sleep patterns. Feedback can
be described as active when given in real-time, for example a coach monitoring
a person’s running pace and telling him to speed up or slow down. It can also
be passive information available on demand, such as the summary of a run,
showing the distance covered. In other words, the feedback is a clarification of
the behavioral output, which is fed back as input to the user on which the user
can reflect and choose to act.
We will explore how feedback can be used to infer meaning and can affect how
we adjust behavior by considering the temporal aspects of data. Likewise we will
examine the effect of the outcome due to feedback, see Figure 6.1.

FEEDBACK

54

Neurofeedback Training

Figure 6.1: We view the feedback loop in relation to neurofeedback training,
while focusing on the effect of the interface, and on providing feedback at multiple temporal levels.

6.1

Neurofeedback

We look at the feedback in the context of neurofeedback training. Neurofeedback training is a method for self-regulating neural activity based on real-time
feedback of the brain activity. Neurofeedback training is primarily used as part
of treatment for various psychological disorders, but is also used in attempts to
enhance cognitive ability such as concentration or focus and has slowly become
part of the personal informatics segment.
NEUROFEEDBACK & THE
FEEDBACK
LOOP

In traditional neurofeedback systems, the scope would involve decisions on which
brain waves should be trained and from which cortical areas. This is of course
closely coupled with the disorder that is being treated or the cognitive ability
one wishes to train. Then comes the data collection, where the cortical activity
is monitored, followed by the analysis & visualization where the data are baseline corrected. This is of great importance when dealing with EEG because the
amount and oscillation of brain activity is highly individual. Thus it is always the
relative cortical activity changes which are visualized. Next is the inferring meaning, this however has often been neglected in traditional neurofeedback interfaces.
It has been assumed that users can regulate their brain activity regardless of how
it is presented. Based on the feedback from the interface, the user can respond
by changing mental strategy or trying to achieve a certain state of mind in the

6.1 Neurofeedback

55

Figure 6.2: A traditional neurofeedback interface, with a single square providing real-time feedback of the neural activity by changing colors from
blue to grey and red when activity is below, equal and above baseline, respectively

next adjust behavior step. Depending on the user’s ability to regulate his brain
activity and the amount of training, the subsequent outcome can be seen in the
brain activity and in addition is often measured by a cognitive test or behavioral evaluation based on a response. The loop between data collection, analysis
& visualization and adjust behavior is completed in milliseconds, creating the
real-time feedback loop between brain activity and visualization.
With the temporal resolution of the EEG brain scanning neurofeedback training
can provide instant feedback. However, by repeating this loop multiple times, the
reflection might span across seconds, minutes or hours during which the user can
explore different mental strategies to optimize his training, by receiving real-time
feedback on the cortical activity. Due to the brevity of the loop before the change
in brain activity is fed back as input, the way of providing the feedback becomes
extremely important.

6.1.1

Previous Neurofeedback Interfaces

Many traditional training interfaces provide feedback, indicating whether the
activity has increased, decreased or is similar to baseline. The typical feedback
is visual, audio or both. Previous experimental setups have typically used bar
diagrams or colored squares indicating high or low brain activity. These interfaces
provide feedback based on a personal baseline, but provide no other affordances
to guide the user.

56

ASSOCIATIVE
FEEDBACK

PERSONAL
INFORMATICS
SYSTEM

Neurofeedback Training

Rather than providing a visual stimulus, other neurofeedback systems such as
that of Egner et al. have used audio feedback to increase the alpha/theta ratio
[Egner et al., 2004]. A background sound resembling either a ’babbling brook’ or
’ocean waves’ was used to indicate a relative increase of alpha and theta activity respectively. These sounds would create associations which some might find
supportive to the mental training. Additionally, a high-pitched or low-pitched
gong sound would be executed when the activity exceeded a pre-set threshold
of alpha and theta, respectively. The subjects aimed to increase the amount of
theta sound representation, whereas the gong sound would indicate reaching a
’significant’ activity level, representing succeeding or failing.
Recently a commercial neurofeedback system, Muse1 , was developed - a personal
informatics system to "enhance your meditation". Muse consists of only four electrodes used to measure brain activity with the goal of helping the user maintain
his focus for a longer period of time. Muse provides real-time feedback by two
audio-streams. The task is for the user to close his eyes and count his breaths.
When the user is focused on counting, the volume of the waves will increase. However if the user’s mind starts wandering, the sound of the wind in the audio track
will increase. If the user manages to keep focused for 20 consecutive seconds, the
sound of birds will appear, which serves both as a threshold and a motivational
feedback. This way of providing feedback is similar to that of Egner et al., with
audio tracks providing real-time feedback on increased or decreased brain activity
and a threshold sound: The ’babbling brook’ and the threshold gong have been
replaced with the sound of blowing wind and birds. The chosen audio streams of
waves and wind are not coincidental, since these have often been associated with
meditation. While Muse provides real-time feedback by audio streams, it is only
after the training session that the history of past activity is visualized as a graph
chart. Though the sounds of waves and blowing wind may help the user get into
a relaxed state, it is only the audio thresholds that serve as affordances in both
Muse and Egner’s neurofeedback interface.

6.1.2

Feedback on Multiple Temporal Levels

We wanted to design an interface that not only defined thresholds but presented
them within a spatiotemporal context that could contribute to the user’s reflection on his mental state. Thus we created a real-time history. Our interface
consisted of multiple squares. Each square would represent a real-time feedback
of the brain activity, with the square changing colors from black (merging into
1 choosemuse.com

6.1 Neurofeedback

57

Figure 6.3: The intensity of the brain activity alters the color of the squares
from black, then blue-scaled to orange-scaled when the brain activity is below, above and more than three times the baseline, respectively

the black background), to blue and orange. Activity below baseline would be
represented with a black square, which would not be visible on the black background. If activity increased above baseline, the square would be dark blue, and
become lighter with greater increase. If the increase had a magnitude of three
times the baseline (which was determined in a pilot study), the square would
become yellow towards orange with a even greater increase, see Figure 6.3. The
color of the squares changed continuously according to the attained brain activity
calculated from a running mean of 2 seconds within a sliding window. In this
way, we stabilized the fluctuating pattern of brain activity. After one second,
the square would freeze in the current color and a new square would appear and
change color depending on the brain activity. Thus the square would create a
history of past activity on which the user could reflect. Thus the feedback would
now consist of data from multiple seconds. The squares were also binned into
15-second bins, starting from the left towards right. Thus the number of visible
squares within each bin would represent the number of times the brain activity
was above baseline and their color would represent the intensity. Our hypothesis
was that the history would make it easier for the users to try out different mental strategies and compare them across seconds and minutes. The result of the
interfaces is a more continuous and smooth representation of the data with the
intention of accommodating the mental state of the users.

6.1.3

Neurofeedback Experiment

To see whether this threshold and history had a measurable effect on the neurofeedback training, we tested it against the traditional interface, Appendix C
[Jensen et al., 2013]. The traditional interface was similar to that of Zoefel et
al. with a colored square, representing the brain activity [Zoefel et al., 2011]. If
activity was below baseline the square would turn blue. If it was equal to baseline
it would be grey and if it was above baseline it would turn red, see see Figure 6.2.

58

Neurofeedback Training

Figure 6.4: The neurofeedback interface provides feedback on real-time brain
activity. With an increase in activity translated into small colored
squares arranged in columns representing 15 seconds of the training.
By displaying the columns side by side, the activity of the 5-minute
training is represented as a horizontal time line.

The color changed with the sampling rate of 128Hz providing a feedback every
125 ms. This meant that the square would flicker if the brain activity fluctuated.
The two interfaces were tested on two different groups, aiming to increase their
upper alpha activity (approximately from 10-12Hz). A group of 11 participants,
including 5 females, trained using the traditional interface. The other group
of 13, including 6 females, trained using our interface. Non of the participants
had any experience with neurofeedback training. The experiment was conducted
with a 16-channel mobile neuro-headset, Emotiv. The live feedback was provided
from the two occipital channels, O1 and O2, which cover the visual center, where
high levels of alpha power can be monitored. Each interface was tested on five
consecutive days from Monday to Friday and each day started with a baseline
recording, which would be used as the baseline for the following five training
sessions of each five minutes. At the end of the day a second baseline recording
was made, to see if the training had any effect on the baseline.
RESULTS

The results showed a significant increase of upper alpha amplitudes for participants training on our interface, though this effect was not transferred to a change
in baseline, see Figure 6.5. This indicates that the interfaces had a helping effect
during the training, but not on the overall baseline, suggesting that it was easier

6.1 Neurofeedback

59

Figure 6.5: The difference in the participants’ ability to increase upper alpha
brain activity when using the classic red/blue interfaces (plotted
in red) or using our multi-square interface(plotted in green). The
participants using our interfaces increased their brain activity significantly compared to those using the classic interface. Those who
were not able to increase their brain activity are referred to as nonresponders2 (plotted in blue). The bars indicate the standard error
of the mean.

60

Neurofeedback Training

for the participants to reflect on their brain activity and reach the required mental state using our interface. But it had no effect on their general brain activity in
the longer term. In cases where neurofeedback is used as treatment for children
with Attention Deficit Hyperactivity Disorder (ADHD), this would mean that
the subjects might have an easier time entering a focused state of mind, when
asked to. However, they would not naturally become more focused when not
intended.

Whether the effect is due to the properties of the visual components or the temporal aspects of the representation is unknown. Statements from the participants
revealed that those using our interface would compare different mental strategies,
which was enabled by the temporal feedback history. It might be that the sliding
time window of our feedback interfaces was crucial, in its ability to create a stabilized visualization of the brain activity. This was supported by other statements
revealing that the gradient color change in the feedback could be perceived as
more smooth and continuous, compared to the disruptive flickering of the traditional interface. This suggests that the feedback should, if possible, support the
mental state which is required for the training, whether it is a relaxed peaceful
mind or a focused and sharp mind, or something completely different. Also, feedback provided in more continuous flow might help sustain the mental state, and
help the user feel more in control of his brain activity.

These results indicate that not only does the interface influence our response, but
the time window of the feedback is crucial in our ability to act upon our insights.
Thus with neurofeedback, the feedback loop is very short, providing real-time
information which requires instant reflection. If the user is unable to reflect, the
feedback can be provided from a longer time period, a history. However with if
the feedback only resembles past behavior, it becomes less actionable. This is
just like the fact that increasing one’s pace while running is easier with real-time
feedback, compared to viewing past data on one’s pace and increasing it next
time one goes running.

In traditional neurofeedback interfaces, there has been limited or no focus on how
the feedback was provided. This indicates that there has been little thought on
how to infer meaning from the data, which could guide the user to the acquired
state of mind. It also illustrates that the temporal level of feedback can contribute
to the user’s feeling of control.

6.2 Interface Components

(a) New York City Skyline

61

3

(b) Our neurofeedback interface

Figure 6.6: Our interface could create associations to the blinking lights of a
skyline. While the blurring filter applied to the picture, may create a dreaming effect, allowing the user to project himself into the
atmosphere.

6.2

Interface Components

Whether the neurofeedback is provided by sounds or colors, we suggest that
these audio and visual components in combination can create associations and
atmospheres which can help to pull the user into the wanted state of mind.

6.2.1

Visual Feedback Components

A simplified characterization of the visual feedback can be reduced to the following components:
• geometric primitives (connected segments)
• color (discrete, gradients)
• size (proximity, scale-ability)
• movement (horizontal, vertical)
• composition (spatial distribution)
In our interface we experience a combination of these components, where small
squares, geometric primitives, are composed around vertical stacks and a horizontal line, while changing colors from dark blue to more yellow and orange tomes.

62

Neurofeedback Training

This could be associated with a skyline of rising towers with blinking lights in
the horizon as shown in Figure 6.63 .

6.2.2

Audio Feedback Components

Similarly the auditory feedback can be constructed from the following components:
• pitch (low, high)
• volume (soft, loud)
• timbre (dark, light)
• duration (short, long)
• rhythm (temporal distribution)
In Muse, the audio interface consists of audio streams of blowing wind and ocean
waves which is a combination of continuous and rhythmic changes in the frequency spectrum. The feedback is provided by a volume change in the audio
streams, and by having short high-pitched bird tweets sounding. In this case,
ocean waves, blowing wind, and bird sounds will naturally lead the user to an
imaginary beach. The rolling sound of the waves can be associated with the inhaling and exhaling of the user, thereby accommodating the task of focusing on
breathing.

6.2.3

Haptic Feedback Components

In line with the audio and visual components, we can further expand with haptic
components:
• frequency
• amplitude (intensity)
• duration (short, long)
3 ispynyc.wordpress.com/2011/11/21/the-skyline-lights

6.2 Interface Components

63

Figure 6.7: The Bouba-Kiki effect illustrates the associative coupling between
the visual shapes and the shape of the mouth when pronounced:
Which of these shapes resembles Bouba and which resembles Kiki?
• rhythm (temporal distribution; overlapping, inter-burst duration)
• composition (spatial distribution)
Consequently, when designing new neurofeedback interfaces, our focus should not
only be on how to visualize an increase or decrease of brain activity, but should
consider how combinations of components might affect the user’s imagination,
associations, or trigger related memories, supporting entering the wanted state
of mind.

6.2.4

Multi-Modal Compositions

Above we see all the components separated. However, by combining these it
is possible to create feedback which not only provides the necessary actionable
information, but which can also support the interaction at other levels. Some of
these components are inherently connected. We see this tight coupling between
visual, audio, and haptic components in the Bouba–Kiki effect.
The Bouba–Kiki effect was first discovered in 1947 by Köhler, who found that
people associated certain visual stimuli with the nonsense words, Maluma and
Takete. An even more pronounced effect was found in 2001 with the words Bouba

BOUBA-KIKI
EFFECT

64

Neurofeedback Training

and Kiki, showing that 95% of the participants connected the round shape with
Bouba and the pointy shape with Kiki [Ramachandran and Hubbard, 2001], see
Figure 6.7. This effect has been seen across cultural boundaries, in pre-lexical
toddlers [Maurer et al., 2006] and for populations with no written language
[Bremner et al., 2013]. It has been suggested that the coupling is due to associations between the visual shape and articulatory characteristics of the vowels
when pronounced [Ramachandran and Hubbard, 2001]. In the study by Fryer,
the visual stimuli were transformed with 2D and 3D representations and a similar
effect was found between the haptic stimuli and the auditive [Fryer et al., 2014].
However, for participants with visual impairment, the effect was significantly
lower, suggesting that the effect is tightly coupled with visual imagery, and that
visual imagery plays an active role in cross-modal integration. This tight coupling
between haptics and visual imagery might be because both sensory modalities
provide geometric information.
We will investigate the effect of different haptic components in creation of perceived continuous motion and how haptic patterns of different modulations are
perceived as pleasant, unpleasant, calm and energetic.

6.3 Summary

6.3

65

Summary

With neurofeedback training we see the tight coupling between temporal aspects
of the analysis and we see how the visualization can provide feedback ot different
temporal levels, from milliseconds to minutes. By the use of a sliding window in
the analysis, it is possible to stabilize the visualization of brain activity, which
might provide the user with a feeling of control. The history of past activity enables the user reflect on and compare past mental strategies. The neurofeedback
training also illustrated how the visualization and being able to infer meaning
from it influences the outcome, the ability to access a successful state of mind.
This kind of monitoring and training of mind is now moving from the scientific and
clinical settings to the consumer arena. Disregarding the effect of the feedback
and inferring meaning from the data can no longer be tolerated. Thus focus
should not only be on how to visualize an increase or decrease of brain activity,
but should consider how combinations of visual, audio, or haptic components
might create associations and support the user in entering the desired state of
mind.

Chapter

7
Haptic Interfaces

The previous chapter demonstrated the effect of combining different audio and
visual components into helpful feedback. In this chapter, I focus on how haptic
feedback can be used to infer meaning by providing distinguishable patterns,
which is commonly used as a notification for the user to adjust behavior, see Figure 7.1. Haptic feedback is of special interest because of its direct skin contact,
which makes it personal and intimate. This makes it an obvious candidate for
several personal informatics systems providing real-time feedback. Within personal informatics systems, it has only been applied to a few systems, for example
Spire1 , which provides a haptic notification if the user has been sedentary for too
long, indicating that the user should stand up or change position. With actuators becoming an integrated part of smart-watch interaction, we will probably
see even more efforts towards design of haptic feedback, which not only sends a
message, but which also is connected to tactile associations of touch.
Physical contact with objects and people is a natural part of how we interact and
communicate with our environment. In contrast to our other sensory modalities
(vision, taste etc.), haptic interaction is responsive: when touching something
you will be touched in return. This is referred to as active (touching) and passive
1 spire.io

ACTIVE &
PASSIVE TOUCH

68

Haptic Interfaces

Figure 7.1: Taking a closer look at the stages inferring meaning and adjusting
behavior, in relation to haptic feedback - including how haptic feedback can be used as notifications for adjusting behavior, or how it
can create associations that support other types of feedback, such
as visualizations.

(being touched) touch. The responsive interaction enables us to refine and adjust
our motor activation - enabling us to lift heavy items, gently interact with fragile
items, or caress the skin of a loved one. It might also be the reason why touching
something often results in a strong personal experience. Thus when we experience
haptic feedback from phones or smart-watches this can easily feel invasive, if not
done in the right way.
The strong effect of haptic interaction can be illustrated by looking at our brains
- the primary somatosensory cortex and the primary motor cortex (see Figure
7.2) [Reisberg, 1997]. Here we see how somatosensory and motor cortices are
mapped to different body regions, illustrating the areas devoted to processing and
integrating information. Large parts of these areas are mapped to the hands and
mouth input and output, which makes sense from a primitive survival perspective,
in which our hands play a significant role in both defending us from danger
and hunting or gathering food. The pictogram in Figure 7.2. also illustrates a
hierarchical range, from a precision grip demanding high coordination of finger
movements, to grasping a heavy object using the whole hand.
The haptic sensory information comes from receptors located in our skin (mechanore-

69

Figure 7.2: A pictogram mapping areas of the somatosensory and motor cortex devoted to processing information and executing movements
of the different body regions. Inspired by Reisbergs illustration in
[Reisberg, 1997]

ceptors and thermoreceptors) as well as receptors in muscles, spindles and tendons
(proprioceptors)[Carbon and Jakesch, 2013], thus research on haptic interaction
has been divided into two areas:
Tactile feedback is the stimulation of nerves in the skin, including feeling heat
and texture
Force feedback describes interactions that lead to activation of muscles and
tendons, which would be the feeling of a force (such as weight) or reaction
force (such as hardness)
From these receptors we are able to get information about geometric (size, shape,
etc.) and material properties (texture, hardness, temperature). The geometric
properties are also provided by our visual system and can therefore confirm our
haptic perception. However the material properties, such as quality assessments,
can provide additional information. Thus, if we can not rely on our visual system
to distinguish whether a doorknob is made of metal, plastic or wood, we can get
information about the material properties from touching the handle. Thus haptic feedback can both strengthen the visual information in regards to geometric
information, and provide additional information on material properties.

70

Haptic Interfaces

In the following sections I will provide state-of-the-art examples of haptic feedback, which is divided into two types of haptic feedback:

Direct haptic feedback describes a haptic stimulation which is invented and
often serves to provide information, such as navigation or notifications.
Indirect haptic feedback mimics a real haptic sensation which comes from
interacting with the physical world, this could be plucking a guitar string,
a tap on the shoulder etc. and is often accommodated by either audio or
visual feedback.

7.1
DICRECT
HAPTIC
FEEDBACK

Direct Haptic Feedback

Despite the strong and personal effect haptic feedback can have, it has only been
applied in a few applications. The early research on haptic feedback focused on
applications aiding blind or visually impaired people such as needles on fingers
providing direction indications for navigation [Ertan et al., 1998]. The braille
tactile writing system for visually impaired is a good example of how fine tuned
our tactile perception is at our finger tips and of the resolution in which vwe can
distinguish tactile patterns. However most of the direct haptic feedback we see
today is not attached to our finger tips but comes from phones in our pockets or
clips attached to our waistline. The feedback we receive is primarily notifications
of incoming calls, text, calendar event, etc. With the recent Apple Watch, haptic
feedback is now constantly available on the user’s wrist. Because of its invasive
effect, the watch needs to be configured as a filter, allowing only urgent information to pass through. The advantage of haptic feedback is that it is invisible to the
environment. Thus in situations where it would be inappropriate to interact with
a smartphone, either because it would be impolite or disturbing, the user can rely
on interaction with a smart-watch. Besides from the normal notification function,
the haptic feedback is also used to confirm micro-interactions when navigating
with the watch. Since most haptic feedback is used to provide information, the
primary focus has been on creating distinguishable haptic patterns.
There are different methods for creating haptic patterns, including vibrotactile
actuators, electrodes, shape memory alloys, electro-mechanical actuators etc. I
will focus on vibrotactile stimulation, since this has been available in mobile
phones for decades and is the most prominent type of haptic feedback in other
personal informatics devices.

7.1 Direct Haptic Feedback

71

Figure 7.3: Tactile associations to haptic modulations.
The Apple watch consists of only one actuator, thus the patterns can only be
designed from modification of the following components:

SETUP WITH
ONE ACTUATOR

• frequency
• amplitude (intensity)
• burst duration (short, long)
By modifying these, the vibrations can feel like a subtle tap or a hard poke, or
perhaps even the ripple effect of water rings.
However, with more actuators, the patterns could also be modified by:
• rhythm (temporal distribution: overlapping, inter-burst duration, direction)
• composition (spatial distribution: close together, far apart)
• spatial direction

By modifying these a continuous motion from an overlap of vibration from actuators could be created. This feeling could be translated into the stroke of a hand.
The pattern could also be based on successive bursts by having an inter-burst
interval, making actuators vibrate separately, one after another. The could be
perceived as a step-wise motion, like a hopping rabbit, see Figure 7.3.
We describe patterns as being based on the following three modulations:
Simultaneous where all actuators are vibrating simultaneously.

SETUP WITH
MULTIPLE
ACTUATORS

72

Haptic Interfaces

Figure 7.4: The rhythm and perceived stimulation of three haptic modulations
- simultaneous, continuous, and successive - is illustrated.
Continuous where there is an overlap of vibration, before the first actuator
stops vibrating the next will start.
Successive where there is a time period in between two actuators’ vibrations,
an inter-burst duration.

7.1.1

Perceiving Continuous Motion

Though these patterns seem straightforward, the understanding of how each parameter contributes to the perceived sensation and our translation to known
tactile interactions is limited. Thus attempts to understand continuous motion,
includes motion created from three different patterns: continuous (also referred to
as saltation), amplitude modulation and a hybrid pattern [Yatani and Truong, 2009].
Also studies on the effect of frequency, burst duration [Israr and Poupyrev, 2011b],
intensity and body site [Israr and Poupyrev, 2011a, Cholewiak and Collins, 2000]
have been examined.
It may be possible to create haptic feedback which could assist in enhancing an
emotional experience in scenarios such as long-distance relationships. Therefor, in
the hope of overcoming the lack of physical contact when communicating through
standard interpersonal communication systems such as Skype2 , or in a virtual
environment such as SecondLife3 , we investigated the control space of creating
perceived continuous motions which could be associated with a caressing stroke,
Appendix D [Eid et al., 2015]. We investigated the upper and lower thresholds
of perceived continuous motion on the forearm, by altering the stimulus onset
asynchrony (SOA). The SOA is defined as the time between onset of vibrations
coming from two actuators. If SOA is shorter than the vibration (burst duration),
2 skype.com
3 secondlife.com

7.1 Direct Haptic Feedback

73

Figure 7.5: The upper (dashed lines) and lower thresholds (solid lines) for creating perceived continuous patterns, with actuators at different distances and with different burst durations.

there will be an overlap of vibration from the actuators, see Figure 7.4. Is SOA
is greater than the burst duration, there will be a period of time between the
two vibrations. The lower threshold is where the vibrations are perceived as
coming from both actuators simultaneously rather than a continuous motion.
The upper threshold is where the vibration is perceived as coming from two
successive actuators, one at a time.

We found the thresholds for five different spatial distributions by placing actuators at 4, 8, 12, 16, and 20cm apart. At each spatial distribution the thresholds
were found for three different burst durations, 120, 180, and 240ms. The results
in Figure 7.5 show the lower and upper thresholds of the SOA, indicating that
the area in between is perceived as a continuous motion. Thus we found that the
relationship between inter-bust duration and the burst duration was linear at all
distances except for the lower thresholds at the shortest distance (4 cm). This
could be because the actuators at 4 cm are placed too closely together for the
subject to distinguish between simultaneous and continuous motion. Thus, this
might be an indication of the resolution at which we can distinguish patterns on
our forearm.

RESULTS

74

Haptic Interfaces

Pattern
1
2
3
4
5
6
7
8

Modulation
Simultaneous
Simultaneous
Continuous
Continuous
Continuous
Successive
Successive
Successive

Feature
Intensity
Intensity
Intensity
Intensity
Intensity
Intensity
Intensity
Intensity

+ Speed
+ Speed
+ Direction
+ Speed
+ Direction

Table 7.1: Eight different haptic patterns based on the three modulations: simultaneous, continuous, successive.

7.1.2

Designing Haptic Patterns

Although most studies examining preferences towards vibro-tactile feedback report preferences towards more continuous stimulation, this might not be the most
suitable haptic feedback in all cases. When implementing haptic stimulation in
a wrist-worn alarm clock, the stimulation should not only be pleasant, but also
attract attention. In this case, a soothing continuous motion might not draw
enough attention upon itself to cause the user to wake up. Instead it might become integrated into the user’s sleep. However, using an arousing and aggressive
stimulation could result in an unpleasant awakening that might lead to a dislike of
the system. Thus we examined both the subjective emotional ratings of different
haptic patterns as well as the response time, how fast 12 participants’ attention
shifted towards the different haptic patterns, Appendix E [Jensen et al., 2015].
The emotional ratings are based on the classic 9-point valence and arousal scales.
The valence level indicates how pleasant or unpleasant the haptic pattern is,
whereas the arousal level indicates how calm or exiting the stimulus is. The
patterns’ ability to attract attention was measured with a dual attention task.
The dual attention task consisted of two tasks performed at once. The first task
was a visual identification task, where the participant had to identify the correct
target among different distractor stimuli. While the user did this test, a haptic
pattern would start. Once the participant perceived the haptic stimulation, he
was to abandon the visual task and respond to the haptic stimuli by pressing a
button. This response time would indicate how quickly the participant’s attention
shifted to the haptic pattern.
HAPTIC
PATTERNS

We created eight different patterns based on the three modulations (simultaneous,

7.1 Direct Haptic Feedback

75

Figure 7.6: The subjective valence ratings (pleasant-unpleasant) of eight different haptic patterns, show a preference towards patterns based on
the continuous modulation. Results are shown as box plots with
median, upper and lower quartiles, min and max values, while diamonds represent the means.

continuous, and successive). The first two (pattern 1 and 2) were based on the
simultaneous modulations. The next three (pattern 3, 4, and 5) were based on
the continuous modulation. The last three (pattern 6, 7, and 8) were based
on the successive modulation. All of the eight patterns increased in amplitude
(intensity), starting from not being perceivable and slowly increasing until the
user perceived it and responded to it. Pattern 1, 3, and 6 were only modulated
by this intensity increase. Pattern 2, 4, and 7 were in addition changed in speed,
from slow to fast vibrations. This was done by decreasing the SOA interval,
decreasing the inter-burst interval in pattern 2 and 7, and by increasing the
overlap in pattern 4. The last two patterns, 5 and 8 changed in direction (from
left to right and right to left) in addition to the increase in intensity.
Starting with the valence rating, the three patterns based on continuous modulation were rated as the most pleasant, and the two based on simultaneous modulation as the least pleasant, see Figure 7.6. However, we also obtained greater error
bars for the simultaneous patterns, indicating that some participants found these
pleasant whereas others did not. The arousal rating in Figures 7.7 shows a less
clear picture, with no distinguishable difference between the patterns. However,
the error bars reveal that some, but not all participants, found the continuous
patterns more calm compared to the others. The response time shows a much
faster response to the continuous modulations compared to any of the others, see

RESULTS

76

Haptic Interfaces

Figure 7.7: The subjective arousal ratings (calm-exciting) show great variability for some patterns. Results are shown as box plots with median,
upper and lower quartiles, min and max values, while diamonds
represent the means.

Figure 7.8: The response times of the eight different patterns show significantly
faster responses to patterns based on continuous modulations and
successive modulations compared to those based on simultaneous
modulations. Results are shown as box plots with median, upper
and lower quartiles, min and max values, diamonds representing
the mean, and circles representing an outlier.

7.2 Indirect Haptic Feedback

77

Figure 7.8, indicating that the continuous modulation of these patterns might be
the easiest to perceive or that these patterns confirmed the participants’ suspicion, once even a slight vibration was perceived. Thus, it would seem natural to
choose a pattern based on a continuous modulation for the haptic feedback of a
wrist-worn alarm clock.

7.2

Indirect Haptic Feedback

From a focus on providing information, haptic feedback has been applied to enhance user experience and simulate real haptic interactions. In 1997, Nintendo
introduced a game controller providing feedback, the Rumble Pack for Nintendo
64. Depending on the game, the controller would vibrate when shooting with a
machine gun or when a race car was pushed into the verge on the side of the road.
This type of feedback is more indirect and has typically served as an extension of
the gaming experience. This has also been implemented for other entertainment
scenarios, such as seats in movie theaters etc. Attempts have been made to create
haptic feedback as an extension to emotional and interpersonal communication,
in the hope of providing a physical connection for long distance relationships.
Examples of these are everything from small gesture-based sensors to large devices and jackets simulating hugs [Arafsha et al., 2012, Tsetserukou et al., 2009].
However, these products for haptic interpersonal communications have not become successful as a common commercial product, perhaps because mimicking
real social interaction is much more difficult compared to the crude shaking of
a machine gun. The most convincing attempt is perhaps Babybe, which is a
device used in health care that simulates physical contact between mothers and
their premature babies4 . Babybe consists of a sensor recording the mother’s chest
movements and heartbeat, which are transmitted to a cushion on which the infant
lies, raising and lowering the cushion.
The Apple Watch also provides indirect feedback, however currently only in one
application. This is the case, with transmitting the user’s heartbeat, recorded
from a sensor in the watch, to another user, while enhancing the tactile feedback
with the visual outline of a heart pumping. This visual feedback is crucial in
order for the receiver to associate the sudden haptic stimulation with the sender’s
heartbeat, instead of just a rhythm of taps. This is a good example of how hard
it is to provide an affective haptic feedback and why it is often accompanied
with either visual or audio feedback. Concurrently with Apple Watch’s growing
4 babybemedical.com

INDIRECT
HAPTIC
FEEDBACK

78

Haptic Interfaces

market, we are likely to see many more of these haptic micro-interactions being
used to boost and expand traditional interaction and feedback patterns.

7.3 Summary

7.3

79

Summary

The recently developed sensor-packed wearable devices are obvious candidates for
future personal informatics systems. This implies there will be an increasing focus
on how to create haptic feedback, which can be used not only to infer meaning
by providing distinguishable patterns, but also to create patterns associated with
real tactile touch, which produce personal emotional experiences.
In this chapter we examined the how the human sensory abilities define an upper and lower threshold for creating a perceived continuous motion. We also
illustrated how having multiple actuators enables further design components,
which widens the possibilities for creating more advanced haptic patterns. We
also showed how an experiment revealed that haptic patterns based on continuous modulations were considered more pleasant than simple simultaneous bursts.
This suggests that these could be associated with affective natural interaction
patterns such as stroking and caressing.

Chapter

8

Perspectives

This thesis has covered several aspects of cognitive interfaces for personal informatics feedback. First we looked at high-level aspects of design processes such
as scope and outcome. We also looked at stages of activities, from data collection
to infer meaning and adjust behavior, which are a fundamental part of personal
informatics systems aiming to obtain self-knowledge. Furthermore, peeling off a
layer, we examined ways of inferring meaning from data, suggesting use of baselines, thresholds, context awareness, scenarios, etc. to enable the user to reflect
and make feedback actionable. Finally, we looked at separating interfaces into a
low-level view of visual, audio and haptic feedback components, and how these
can be combined to create supportive feedback. Thus we can see this as a hierarchy going from high-level aspects of scope and outcome to low-level feedback
components. This last chapter will summarize the significant contributions of
this thesis and discuss its implications and how we move on from here.

82

Perspectives

Figure 8.1: User Story Mapping as part of an iterative design process, helping
to scope the product by testing prototypes, thereby evaluating and
redefining the scope of the product.

8.1

Integration of Design Principles

With many personal informatics systems being driven by the ability to collect
large amounts of data, and with no scoping of the systems, they are likely to
provide meaningless data, which are hard to reflect on and gain insight from. Or
as a Quantified Self’er describes: "It’s not that we lack the information, we are
virtually drowning in it. The obstacle is that we don’t have the proper tools to interpret the significance of our data" [Choe et al., 2014] Although this Quantified
Self’er might be looking for the ultimate analytical tool, the quote does capture
a common problem within personal informatics systems: Focusing on collecting
and analyzing data, rather than supporting the users’ motivations, needs, and
goals [Ohlin et al., 2015].
Thus we present a framework, The feedback loop, which incorporates these highlevel aspects scope and outcome that are essential in lean design processes to help
define the minimal viable product. Scoping a system requires translation of the
of the users’ motivations, needs, and goals into sequence of activities and tasks
within each of the stages, which are decoupled and prioritized as in Figure 8.1.
One way of scoping a personal informatics systems is by the use of personality
traits. Personality traits can provide insight into users’ motivations and prefer-

8.2 Creating Supportive Interfaces

83

ences, both in terms of how actionable or exploratory the system should be, and
in terms of the content’s level of detail and how it is delivered. Thus, personality
traits can be used to scope the high-level motivation, needs and goals as well as
the tangible activities and tasks. Thus the design process can be seen as a way
of transitioning between high-level scope and outcome, and the middle layer of
sequences of activities and tasks.
An example of the importance of scoping personal informatics systems in relation
to intended high-level outcome can be seen with neurofeedback interfaces.

8.2

Creating Supportive Interfaces

Previous neurofeedback studies have merely viewed the actual feedback as a way
of informing the user of his current brain activity level - above, below or equal to
baseline [Vernon et al., 2003, Zoefel et al., 2011]. To the best of my knowledge,
no previous studies on neurofeedback have suggested that the way of providing
feedback could influence the results of the training, until now.
The traditional interfaces have typically consisted of simple geometric primitives
such as a square or bar, indicating high or low brain activity by altering components such as color, size or movement. All of which could be viewed as a binary
information feedback: red vs. blue, big vs. small, high vs. low. In contrast,
our interface would provide a smoothly changing color gradient, which based on
the underlying moving window for sampling the data would stabilize the visualization of brain activity. The statements from participants also suggested that
this smooth gradient provided the user with an enhanced feeling of control. In
addition, the spatio-temporal layout of gradient squares generates a history of
past activity, enabling the user to reflect and reuse previous successful attempts
to enhance the alpha activity.
By comparing our interface with a traditional interface, we found an effect during
training, but not on the overall baseline. This suggests that it was easier to increase and maintain an increased alpha activity. However, we could not replicate
earlier results indicating that the neurofeedback training would alter the individual alpha baselines [Zoefel et al., 2011]. Whether the training effect was due to
color-indicated threshold, the temporal aspects, or other properties of the visual
components is unknown. But one thing is clear: We need to consider feedback
which is supportive of the activities that underpin the desired outcome. In this
case it should support and guide the user to attain the desired state of mind -

84

Perspectives

whether it is a relaxed, peaceful state of mind; a focused, sharp state of mind; or
something completely different.
Thus neurofeedback interfaces are a great example of how we need to consider
high-level needs, motivations and goals when we design personal informatics systems. It illustrates the tight the coupling between all the stages of the loop: data
collection, analysis & visualization, infer meaning, and adjust behavior.

8.2.1

Towards Multi-Modal Interface Components

The interfaces described in this thesis all consist of a combination of components:
visual, audio and haptic such as geometric primitives, colors, pitch, volume, and
many more. The components could be smooth, gradual color change, as in our
neurofeedback interface; the smooth fading volume changes between ocean waves
and blowing wind in Muse; or continuous or rhythmic haptic patterns from vibrating actuators.
Perhaps by providing feedback in a continuous manner rather than the binary
high-low indication of brain activity, we can accommodate the sequence of increasing and decreasing brain activity and thereby the progress of training. An
analysis of the effect of individual components was undertaken in the case of the
haptic interface experiments. While we may be able to isolate each individual
component, as in the experiment identifying how the haptic parameters of intensity and rhythm are perceived in isolation, we in reality often combine modalities
as exemplified by the cross modal Bouba-Kiki effect: coupling sound with visual
shapes.

8.3

Moving Science into the Wild

In the experiment examining the upper and lower thresholds of perceived continuous motion, we isolated the haptic feedback to consist of only three haptic
components: rhythm by changing the SOA; burst duration for 120ms, 180ms
to 240 ms; and composition by changing the distance between the actuators.
Though we did find the upper and lower thresholds, this is unlikely to provide
associations towards affective human interaction such as a caressing hand. This
illustrates how isolation of components might lead to specifications of designing
haptic feedback, but it does not contribute much to the full user experience. For

8.3 Moving Science into the Wild

85

that we need to consider multi-modal feedback, such as the combination of visual
and haptic feedback of heart beats from Apple Watch.
The same constraints are valid when it comes to investigating emotional responses
using brain signals: First, by removing eye movements and blinks, whose power
are magnitudes stronger than the brain activity. Secondly, by separating emotional responses in EEG into relevant time windows. Still, in order to get a
measurable difference of how we perceive pleasant or unpleasant images, we not
only have to move beyond electrode-based analysis by employing a range of machine learning approaches, we also have to use extreme visual stimuli consisting
of mutilated bodies or erotic scenes. But these extreme images are rarely encountered in an everyday context, and therefor unlikely to be used for personal
informatics systems.
Thus we remain far from being able to monitor these underlying emotional parameters outside controlled lab conditions and we are still unable to continuously
collect and analyze such signals ’in the wild’ in relation to personal informatics
systems. We may therefore need to examine alternative ways of extracting individual responses: from other physiological measures, such as eye-tracking, skin
conductance or heart rate monitoring; or from behavioral patterns. Or maybe we
can use the general insights of users’ motivations and preferences from personality traits and build on these with the individual physiological and behavioral
measurements, which could easily be retrieved from smart-phones and smartwatches.

Chapter

9
Conclusion

This thesis should be viewed as a response to the challenges of designing personal informatics systems, where people seek to gain knowledge of them-selves
by tracking various aspects of their lives, ranging from physiological measurements to behavioral patterns. In particular, this thesis aims to provide a new
personal informatics framework that incorporates lean agile design principles,
thereby focusing on scoping high-level goals, activities and tasks to create minimal viable products. The created framework stretches from high-level aspects of
users’ motivation, needs, and goals to guidelines for creating cognitive interfaces,
seeking to infer meaning from data, and even provide actionable feedback.
With neurofeedback interfaces, I showed design of these interfaces can have an
effect on the training outcome. This suggests that the temporal aspects of the
feedback appear to have an effect. However, further research on this area is
needed. I described how interfaces can be constructed from different visual,
audio and haptic components and argue that by combining these we can hope to
create interfaces which not only provide meaningful feedback, but also enhance
the user experience and support the underlying interaction.
Finally, I hope that this thesis will mark a break with the existing focus on data
collection or data analysis, which has characterized many personal informatics

88

Conclusion

systems [Ohlin et al., 2015], and rather focus on supporting the users’ motivations, needs, and goals.

Appendix

A

Smartphones as pocketable
labs: Visions for mobile brain
imaging and neurofeedback

Arkadiusz Stopczynski, Carsten Stahlhut, Michael Kai Petersen, Jakob Eg Larsen,
Camilla Birgitte Falk Jensen, Marieta Georgieva Ivanova, Tobias S Andersen, and Lars Kai Hansen. Smartphones as pocketable labs: Visions for mobile
brain imaging and neurofeedback. International journal of psychophysiology,
91(1):54–66, 2014.

International Journal of Psychophysiology 91 (2014) 54–66

Contents lists available at ScienceDirect

International Journal of Psychophysiology
journal homepage: www.elsevier.com/locate/ijpsycho

Smartphones as pocketable labs: Visions for mobile brain imaging
and neurofeedback
Arkadiusz Stopczynski ⁎, Carsten Stahlhut, Michael Kai Petersen, Jakob Eg Larsen,
Camilla Falk Jensen, Marieta Georgieva Ivanova, Tobias S. Andersen, Lars Kai Hansen
Technical University of Denmark, Department of Applied Mathematics and Computer Science, Section for Cognitive Systems, Building 303B, DK-2800 Kgs. Lyngby, Denmark

a r t i c l e

i n f o

Article history:
Received 10 May 2013
Received in revised form 24 July 2013
Accepted 12 August 2013
Available online 29 August 2013
Keywords:
Mobile sensor
Real-time
EEG
Neuroimaging
Source reconstruction
Brain monitoring
Brain-computer interface

a b s t r a c t
Mobile brain imaging solutions, such as the Smartphone Brain Scanner, which combines low cost wireless
EEG sensors with open source software for real-time neuroimaging, may transform neuroscience experimental
paradigms. Normally subject to the physical constraints in labs, neuroscience experimental paradigms can
be transformed into dynamic environments allowing for the capturing of brain signals in everyday contexts.
Using smartphones or tablets to access text or images may enable experimental design capable of tracing
emotional responses when shopping or consuming media, incorporating sensorimotor responses reﬂecting
our actions into brain machine interfaces, and facilitating neurofeedback training over extended periods.
Even though the quality of consumer neuroheadsets is still lower than laboratory equipment and susceptible
to environmental noise, we show that mobile neuroimaging solutions, like the Smartphone Brain Scanner,
complemented by 3D reconstruction or source separation techniques may support a range of neuroimaging
applications and thus become a valuable addition to high-end neuroimaging solutions.
© 2013 Elsevier B.V. All rights reserved.

1. Introduction
Only recently have wireless neuroheadsets, capable of capturing
changing electrical potentials from brain activity through electrodes
placed on the scalp using Electroencephalography (EEG), made mobile
brain imaging a reality. The emergence of low-cost EEG sensors and
the increasing computational power of smartphones may transform
neuroimaging from constrained laboratory settings to experimental
paradigms, allowing us to model mental state in an everyday context.
This presents a paradigm shift, making it possible to design new types
of experiments that characterize brain states during natural interaction
over extended periods of time. Until recently most neuroimaging experiments have been performed with subjects who are at rest, under
the assumption that the brain responses being measured will not be
inﬂuenced by subjects sitting or laying down. However, this may be
inaccurate, as animal studies using mice indicate that neurons in the
visual cortex double their visually evoked ﬁring rates if they run on
a treadmill rather than stand still (Niel and Stryker, 2010). Since the
discovery of parietal–frontal circuits of mirror neurones, which ﬁre
both when we grasp an object and when we observe others doing
the same (Pellegrino et al., 1992; Gallese et al., 1996), the sensorimotor
system can no longer be considered as only involved with motion.
⁎ Corresponding author. Tel.: +45 45253899.
E-mail addresses: arks@dtu.dk (A. Stopczynski), csta@dtu.dk (C. Stahlhut), mkai@dtu.dk
(M.K. Petersen), jaeg@dtu.dk (J.E. Larsen), cbfj@dtu.dk (C.F. Jensen), mgiv@dtu.dk
(M.G. Ivanova), toban@dtu.dk (T.S. Andersen), lkai@dtu.dk (L.K. Hansen).
0167-8760/$ – see front matter © 2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.ijpsycho.2013.08.007

Consequently, these mechanisms should rather be understood as
forming an integral part of cognition, allowing us to generalize the
goals of actions based on motor representations in the brain
(Rizzolati and Sinigaglia, 2010).
While there is already signiﬁcant literature concerned with dynamic
brain states during natural complex stimuli in conventional laboratory
experiments (see e.g., Hasson et al., 2004; Bartels and Zeki, 2004;
Dmochowski et al., 2012), there has been a growing call to design
studies that relax the constraints of the lab and widen the focus to
map out how we perceive our surroundings under naturalistic conditions (Makeig et al., 2009). For example, natural motion has been
incorporated into laboratory experiments using tools such as the
MoBi Lab Matlab plugin (2009) in order to correlate motion capture
data of moving limbs with the brain responses being triggered
(Gramann et al., 2011). Even adding a few degrees of freedom may
provide an understanding of how cortical responses differ by simply
changing posture (Slobounov et al., 2008), either by measuring how
theta brainwave activity is attenuated in sleepy subjects once they
stand up (Caldwell et al., 2003), or by analyzing the modulation in
spectral power within alpha and beta brainwaves appearing when
one foot hits the ground and the other foot is lifted, as subjects are
no longer transﬁxed on a chair in front of a computer screen (Gwin
et al., 2011). This provides a foundation for extending standard
EEG paradigms, such as the P300 event-related potential, to measure how we consciously perceive visual objects when participants
are no longer required to sit motionless but are able to walk on a
belt during the experiment (Gramann et al., 2010). It also makes it

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

possible to eventually move a P300 experiment outside the lab,
as has recently been demonstrated by Debener and colleagues (2012)
by combining the wireless hardware from a consumer neuroheadset1
with standard EEG cap electrodes2 and using a laptop to record the
cortical responses, thus providing a portable lab which can be stored
in a backpack and easily carried by the subjects participating in the
experiment.
Taking the idea of bringing EEG into the wild one step further, the
Smartphone Brain Scanner (SBS2) open-source software project (http://
github.com/SmartphoneBrainScanner) introduced in Stopczynski et al.
(2011, 2013), makes it possible to build brain imaging applications
for real-time 3D source reconstruction or neurofeedback training. By
combining a wireless EEG cap with an Android smartphone or tablet,
the SBS2 allows for presenting time-locked audiovisual stimuli such as
text, images, or video, and it allows for capturing elicited neuroimaging
responses on the mobile device, thereby transforming low-cost consumer
hardware into a pocketable brain imaging lab. As the Smartphone Brain
Scanner project potentially allows for designing novel types of brain imaging paradigms, we have initially validated the SBS2 framework in three
experiments related to BCI motor control, embodied semantics, and
neurofeeedback interfaces in order to illustrate the feasibility of capturing
mental state in a mobile context. In the following sections we brieﬂy
review existing mobile EEG sensors, outline the architectural design
of the Smartphone Brain Scanner system for real-time 3D reconstruction,
describe aspects of source separation and spatial ﬁltering in relation to
mobile brain imaging, and give examples of applications built on top of
the open-source software framework for mobile Android devices related
to imagined ﬁnger tapping, emotional responses to text, and design of
neurofeedback interfaces (Fig. 1).

2. Mobile EEG acquisition
A wide range of prototype electrode designs, suitable for mobile
neuroimaging, are currently under development, based on MEMS
microelectromechanical systems utilizing spring-loaded dry contact
pins or hard carbon nanotubes that press against the scalp (Rufﬁni
et al., 2008). For long-term EEG measurement without gel, another
option is electrodes made from soft foam covered with conductive
fabric (Lin et al., 2011), or new types of non-contact high input
impedance sensors capable of capturing EEG signals on the basis of
capacitive coupling (Chi et al., 2012), even when resting on top of
several layers of hair. In contrast to gel-based EEG electrodes, dry
contacts need no skin preparation, and can therefore more easily
be utilized for neuroimaging as participants are able to put on a
neuroheadset without any assistance. However, even though pin
or nanotube contacts easily penetrate the hair and therefore offer
more possibilities for placement than conductive foam-based sensors attached to the skin of the forehead, a spring-like setup may
still be susceptible to noise when users move. Capacitive sensors
provide an alternative for unobtrusive physiological monitoring,
but require an integrated ultra-high impedance front-end for noncontact biopotential sensing (Chi et al., 2011). So-called Ear-EEG is
a promising technology for long-term EEG data collection, offering
improved comfort and esthetics (Looney et al., 2012). Benchmarks
of prototype capacitive non-contact and mechanical sensors in an
experiment related to decoding a steady state visual evoked potential in the 8–13 Hz frequency band showed only little signal degradation when compared to standard gel-based Ag/AgCl electrodes
(Chi et al., 2012), showing that these novel sensors may, in
longer term, provide the increased usability that may assure the
transformation of neuroimaging from ﬁxed laboratory setups to an
everyday mobile context.
1
2

http://www.emotiv.com.
http://easycap.de/easycap.

55

Fig. 1. SBS2 mobile EEG recording with real-time 3D source reconstruction, on an
Android smartphone connected wirelessly to an Easycap 16 electrode setup based
on Emotiv hardware.

Among existing commercial solutions, the ThinkGear module manufactured by NeuroSky3 provides the foundation for several EEG consumer products which integrate a single dry electrode along with a reference
and a ground attached to a headband. It provides A/D conversion
and ampliﬁcation of one EEG channel, is capable of capturing brain
wave patterns in the 3–100 Hz frequency range, and records at 512 Hz
sampling rate. Even a single-channel EEG setup, using a passive dry
electrode, such as the NeuroSky, positioned at the forehead and a reference (typically an earlobe), may allow for measuring mental concentration and drowsiness by assessing the relative distribution of brainwave
frequencies (Yasui, 2009). More comfortable neuroheadsets using conductive Ni/Cu covered polymer foam, such as Mindo4, measure brain
activity from the forehead on three EEG electrodes plus a reference
channel attached to the earlobe. Integrating analog to digital conversion
at 256 Hz sampling rate for acquisition of bandpass ﬁltered signals in
the 0.5–50 Hz range, the neuroheadset offers 23 h of battery life and
wireless Bluetooth communication, and has been demonstrated in
BCI brain machine interfaces used in games based on controlling the
power of alpha brainwave activity (Liao et al., 2012). Other consumer
neuroheadsets such as the Emotiv EEG, provide both wireless communication via a USB dongle and analog to digital conversion of 16 EEG channels (including reference and ground) at 128 Hz sampling rate while
using moist felt-tipped sensors which press against the scalp with a simple spring-like design. Originally designed as a mental game controller
capable of tracing emotional responses and facial expressions, the majority of electrodes are placed over the frontal cortex and have no midline positions (AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8, AF4
with P3/P4 used as reference and ground). However, as mentioned earlier, Debener and colleagues (2012) recently demonstrated that it is
possible to merge the wireless hardware from the Emotiv neuroheadset
with high quality, conductive, gel-based electrodes in a standard EEG
cap. Repackaging the electronics and battery into a small box
(49 mm × 44 mm × 25 mm) which can be attached to the EEG
cap and rewired through a connector plug to 16 sintered Ag/AgCl
ring electrodes can occur, thus providing a fully customizable montage which allows the electrodes to be freely placed in the EEG
cap according to the 10–20 international system (in the present

3
4

http://www.neurosky.com/Products/ThinkGearAM.aspx.
http://www.mindo.com.tw/en/index.php.

56

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

experiment Fpz, F3, Fz, F4, C3, Cz, C4, TP9, TP10, P3, Pz, P4, O1, O2
with AFz/FCz used as reference and ground).
We have tested both the original Emotiv neuroheadset as well as
the modiﬁed EEG cap setup in connection with the Smartphone Brain
Scanner open-source software project in the experimental designs
outlined below.

inverse methods implemented in the Smartphone Brain Scanner will be
given later.

4. Methods
4.1. Brain computer interface based on imagined ﬁnger tapping

3. Software framework: Smartphone Brain Scanner
The Smartphone Brain Scanner (SBS2) is a software platform
for building research and end-user oriented multi-platform EEG applications. The focus of the framework is on mobile devices (smartphones,
tablets) and on consumer-grade (low-density and low-cost) mobile
neurosystems. SBS2 is freely available under an MIT License at https://
github.com/SmartphoneBrainScanner. Additional technical details about
the framework can be found in Stopczynski et al. (2013).
The framework is divided into three layers: low-level data acquisition, data processing, and applications. The ﬁrst two layers constitute
the core of the system and include common elements used by various
applications. The architecture is outlined in Fig. 2.
3.1. Key features
With focus on the mobile devices, SBS2 is a multi-platform
framework. The underlying technology – Qt – is an extension of
C++ programming language and is currently supported on the
main desktop operating systems (Linux, OSX, Windows) as well as
mobile devices (Android, BB10, partially iOS) (see http://qt.digia.
com/Product/Supported-Platforms/).
The additional acquisition and processing modules can be created as
C++ classes and integrated directly with the core of the framework.
The framework supports building real-time applications; data can be
recorded for subsequent ofﬂine analysis, most of the implemented
data processing blocks aim to provide real-time functionality for working with the EEG signal. The applications developed with SBS2 are applications in the full sense, as they can be installed on desktop and mobile
devices, can be started by the user in the usual way, and can be distributed via regular channels, such as repositories and application stores.
The most demanding data processing block is the real-time source
reconstruction aimed at producing 3D images as demonstrated in
Fig. 4. Source reconstruction is carried out using Bayesian formulation
of either the widely used Minimum-norm method (MN) (Hämäläinen
and Ilmoniemi, 1994) or the low resolution electromagnetic tomography
(LORETA) (Pascual-Marqui et al., 1994). Further description about the

Fig. 2. Layered architecture of the SBS2 framework. Data from connected EEG hardware is
extracted by speciﬁc adapters, processed, and used by the applications.

One of the arguably most widely used paradigms of the brain computer interface literature is a task in which a subject is instructed to
select between two or more different imagined movements (MüllerGerking et al., 1999; Babiloni et al., 2000; Dornhege et al., 2004;
Blankertz et al., 2006). Mental imagery is the basis of many BCI systems, originally conceived to assist patients with severe disabilities
to communicate by ‘thought’. The rationale is that the patient, while
having problems carrying out the actual movements, may still be able
to plan the movement and thereby produce a stable motor-related
brain activity, which can be used as an input to the computer/machine.
In this contribution we replicate a classical experiment with imagined
ﬁnger tapping (left vs. right) inspired by Blankertz et al. (2006). The
setup consisted of a set of three different images with instructions,
Relax, Left, and Right. In order to minimize the effect of eye movements,
the subject was instructed to focus on the center of the screen, where
the instructions also appeared (3.5″ display size, 800 × 480 pixel resolution, at a distance of 0.5 m) (Fig. 3).
The instructions Left and Right appeared in random order with an
equal probability. A total of 200 trials were conducted for a single subject.
We selected 3.5 s duration for the ‘active’ instruction (Left or Right) and
1.75–2.25 s randomly selected for the Relax task, similar to Blankertz
et al. (2006). The main motivation for random duration of the Relax
task was to minimize the effect of the subject anticipating and starting
the task prior to the instruction. The experiment was conducted with
an Emotiv EEG neuroheadset transmitting wirelessly to a Nokia N900
smartphone. To illustrate the potential for performing such a study in a
completely mobile context, all stimulus delivery and data recording
were carried out using the SBS2. Analysis and post-processing and
decoding were conducted off-line using standard analysis tools. In particular, we applied a common spatial pattern (CSP) approach (MüllerGerking et al., 1999) to extract spatial ﬁlters which would maximize
the variance for one class, while minimizing the variance of the other
class and vice versa. A quadratic Bayesian classiﬁer for decoding was
applied on features transformed as in Müller-Gerking et al. (1999).

Fig. 3. SBS2 mobile neuroimaging apps for neurofeedback training, presentation of
experimental stimuli, and real-time 3D source reconstruction, running on Android mobile
devices via a wireless connection to an Emotiv or Easycap EEG setup.

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

57

assumed as prior. From a Bayesian perspective the LORETA method is
formulated as
 

Nt

−1
p XjVÞ ¼ ∏ N xt Fvt ; β INc

ð1Þ

 

Nt

−1 T
pðVÞ ¼ ∏ N vt 0; α L L

ð2Þ

t¼1

t¼1

Fig. 4. SBS2 app for presentation of visual stimuli and mobile EEG recording, using an Android
tablet connected wirelessly to an Easycap 16 electrode setup based on Emotiv hardware.

in which β denotes the precision of the noise (inverse variance), α
the precision parameter of the sources, and L the spatial coherence
between the sources V. As the MN method assumes no spatial coherence
between neighboring sources, the spatial coherence matrix becomes an
identity matrix, L = I. In contrast, for LORETA this spatial coherence
matrix typically takes the form of a graph Laplacian, implementing geometrical neighborhood driven smoothness. Given the likelihood, p(X|V),
and prior distribution, p(V), of the current sources, the most likely
source distribution can be obtained by maximizing the posterior distribution over the sources as
Nt
 
p VjXÞ ¼ ∏ N vt μ t ; Σv Þ
t¼1

4.2. Source reconstruction and source separation
Compared to standard EEG laboratory setups, mobile neuroimaging is
extremely susceptible to noise, as the ability to move around simultaneously introduces artifacts into the neuroimaging data induced by the
EEG sensors, as well as originating from motion-related muscle activity.
Likewise, mobile neuroimaging is much more exposed to environmental
noise than experiments taking place under controlled conditions in
a shielded laboratory. Combining sensor and source features, however,
has been shown to improve classiﬁcation in brain-computer interfaces
(Ahn et al., 2012), even though these paradigms often involve activation
of sensorimotor circuits where the location of sources is already quite
well known. There might be an even larger potential by integrating source
information for decoding complex brain states involving a range of different cognitive tasks. In particular, spectral analysis of changes in power
may offer additional information on activity within speciﬁc brainwave bands, which, based on the frequency, determines whether it
reﬂects local or more distributed cortical ﬁeld potentials. We therefore
suggest that incorporating prior knowledge on what constitutes braingenerated signals may overall enhance the feasibility of performing experiments using mobile neuroimaging solutions (see also Besserve
et al., 2011).
One approach to localize the actual brain activity in EEG is to tackle
the inverse problem of retrieving the distribution of underlying sources
from a scalp map by using a forward head model to estimate the projection weights which are captured by the electrodes. The problem is, however, severely ill-posed, as typically tens of EEG electrodes will capture
volume conducted brain activities which may have been generated by
tens of thousands of equivalent dipoles representing post-synaptic
activity within macrocolumns of the cortex (Hämäläinen and
Ilmoniemi, 1994; Pascual-Marqui et al., 1994; Baillet et al., 2001).
A regularization that reduces the number of solutions is therefore applied,
using methods such as low resolution electromagnetic tomography
(LORETA), which assumes both the activity of neighboring neurons is
synchronized and their orientation and strength can be modeled as
point sources in a 3D grid reﬂecting ‘blurred-localized’ images of maximal
activity (Pascual-Marqui et al., 1994). With F∈RNc ñNv representing the
forward model relating the Nv cortical current sources, V∈RNv ñNt , to the
Nc measured scalp electrodes, X∈RNc ñNt , the forward problem for a
set of time points (Nt) is given by, X = FV + E, when the noise
contribution E is assumed additive. The Minimum-norm method
(MN) (Hämäläinen and Ilmoniemi, 1994) and LORETA methods can
be represented as a single method, with MN as a special case of
LORETA, namely, when no spatial coherence of neighboring sources is

Σv ¼ α
−1

Σx

−1

¼α

vt ¼ α

INv −α

−1

−1

T

−1

T

T

T

F Σx Fα

FL L F þ β

F Σx x t :

−1

−1

INc

ð3Þ
ð4Þ

The hyper-parameters α and β are optimized on-line using a standard Expectation–Maximization (EM) approach (Bishop, 2006).
Rather than aiming to solve the inverse problem of determining the
‘what’ from ‘where’ of brain activity, an alternative approach is to apply
methods based on higher-order statistics such as independent component analysis (ICA) (Comon, 1994). This allows to separate individual
processes (‘what’) when they stand out as temporally independent in
the native, spatially overlapping scalp representation (Makeig et al.,
1996).
The ability of ICA to identify temporally independent events also
allows for enhanced detection and automatic removal of artifacts
(Delorme et al.). Eye blinks manifest themselves as low 1–3 Hz as well
as higher frequency activity, which translates into stereotypical ICA
scalp maps consisting of a single frontal dipole (Delorme et al., 2007).
When comparing this method against a regression approach using an
electrooculogram (EOG) eye movement correction procedure (EMCP)
to remove eye blink artifacts, ICA turns out to yield almost perfect
correction (Hoffmann and Falkenstein, 2008). Also, other kinds of muscle activity stand out distinctly in the corresponding scalp maps. Overall,
applying ICA as a preprocessing step improves artifact detection compared to analysis based on the raw EEG data (Delorme et al., 2007).
With particular relevance to mobile neuroimaging, it has been demonstrated that independent component-based gait-artifact removal
makes it possible to capture neural correlates in standard EEG
experiments, even when walking or running (Gwin et al., 2010).
EEG experiments have traditionally focused on analysis of eventrelated time-domain waveform deﬂections and frequency-domain
perturbations in power, but neither of these approaches fully captures
the underlying brain dynamics when averaging data over multiple
trials, or ignoring phase resetting that contributes to the ERP (Makeig
et al., 2004). When ﬁrst applying ICA to the EEG data, the event-related
time series waveforms come to represent independent components generated by temporally independent, physiologically decoupled local ﬁeld
potentials, and their corresponding scalp maps that resemble dipolar
projections of the underlying sources (Delorme et al., 2012). This
indicates that ICA may be used for more than denoising, e.g., it can

58

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

be used to ﬁnd the modes of event-related changes in power, as the
independent components framed by the dimensions of frequency,
power, and phase consistency across trials. Even when electrodes are
accurately placed, the recorded potentials may still vary due to individual
differences in cortical surface and volume conduction. ICA may also here
provide a common framework for comparison of the underlying brain
activity in EEG data, regardless of the actual electrode positions. We
thus compared ICA of the EEG data retrieved from both the Emotiv
neuroheadset containing no central electrodes and the Easycap
EEG montage including midline electrodes. In particular, we used the
retrieved scalp maps and activation time series, as well as eventrelated changes in power spectra, to perform a statistical group comparison across experimental conditions and trials. As a preprocessing step,
we reduced dimensionality based on principal component analysis
(PCA) and subsequently applied K-means clustering to the independent
components, in order to identify common patterns of brain activity
across the two different mobile EEG setups (Delorme et al., 2011).
4.3. Visual stimulus to investigate emotional responses
Over the past decades, neuroimaging studies have established that
language is grounded in sensorimotor areas of the brain; highly related
neuronal circuits seem involved whether we literally pick up a ball or
in a phrase refer to grasping an idea (Pulvermüller and Fadiga, 2010).
Exploring whether such brain activation can be detected using a mobile
EEG setup, the SBS2 framework was used to display the stimulus
consisting of a subset of action verbs related to emotional expressions,
face, and hand motion as used in a recent fMRI experiment (Moseley
et al., 2011). The framework was also used to record the EEG signal for
subsequent ofﬂine data analysis.
Two mobile 16 channel EEG setups were compared; the low cost
Emotiv neuroheadset using saline sensors positioned laterally at AF3,
F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8 and AF4 (P3/P4 used
as Common Mode Sense (CMS) reference/Driven Right Leg (DRL) feedback) — versus a standard electrolyte gel-based EEG cap (Easycap,
Germany) similar to what has previously been used for mobile P300
experiments (Debener et al., 2012) enabling an EEG setup including
central and midline Ag/AcCl ring electrodes positioned at Fpz, F3, Fz,
F4, C3, Cz, C4, TP9, TP10, P3, Pz, P4, O1 and O2 (AFz/FCz used as CMS
reference/DRL feedback) according to the international 10–20 system.
A single subject pilot study was performed to compare the Emotiv and
Easycap EEG setups based on 2 × 10 trials, each consisting of 3 × 20
action verbs presented in a randomized sequence on the smartphone
display (Nokia N900). Each verb was exposed for 1000 ms in a large
white font on black background (3.5″ display size, 800 × 480 pixel resolution) at a distance of 0.5 m, preceded by a ﬁxation cross 500 ms prestimuli, and followed by 1000 ms post-stimuli black screen. Using
the EEGLAB plug-in for MATLAB (MathWorks, USA), epoched EEG data
was extracted ofﬂine (−500 ms to 1000 ms) and baseline corrected
(−500 ms to 0 ms). As some of the recorded potentials are induced by
muscle activity, we rejected abnormal data epochs by specifying that
the spectrum should not deviate from baseline by ±50 dB at 0–2 Hz
and manually removed eye blinks (Delorme et al., 2011). To facilitate a
comparison between the different electrode placements used in the
two experiments, we applied the extended Infomax algorithm to linearly
project the EEG data recorded from individual electrodes onto a space of
basis vectors, which were temporally independent from each other, using
independent component analysis ICA to estimate the scalp maps and
time courses of individual neural sources (Delorme et al., 2007).
To assess the degree to which the Emotiv neuroheadset and the
Easycap EEG setups capture common patterns in brain activity despite
their differences in electrode montage, rather than simply measuring
event-related responses from the 14 electrodes, we applied ICA in a
single-subject study to retrieve 14 independent components from each
of the 2 × 10 trials, related to the Emotiv and Easycap experiments,
respectively: we analyzed 2 × 10 × 14 independent components

generated from the time-locked responses to reading 3 × 20 emotion,
face, and hand-related action verbs in each trial. Retrieving the ICA components enabled us to initially compare the event related responses
across the 3 action verb conditions, which in turn enabled us to identify
similar independent sources within trials using the EEGLAB studyset
functionality. Secondly, to identify common patterns of brain activity
both within and across the Emotiv and Easycap experiments, the
EEGLAB studyset functionality and MATLAB Statistical Toolbox were
used to cluster the 2 × 10 × 14, in total 280 ICs based on scalp maps,
power spectra, and amplitude time series. After initially applying ICA
for artifact rejection in each trial, the 280 ICA weights were recomputed
as a basis for a statistical analysis using the EEGLAB studyset functionality
(Delorme et al., 2011), where the dimensionality of the feature space
was reduced to N = 10 by applying PCA principal component analysis
(Jolliffe, 2002). The pre-clustering function PCA compresses the multivariate EEG features into a smaller number of mutually uncorrelated
scalp projections, and computes a vector for each component to deﬁne
normalized distances in a subspace representing the largest covariances
in the ICA-weighted data. This means that the vectors contain the 10
highest PCA components for the ICA-weighted time series responses,
scalp maps, and power, related to the three conditions. Next, the Kmeans algorithm (K = 10) was applied to cluster common ICA components within the 10 trials (σ = 3), related to the Emotiv neuroheadset
(Fig. 13) and the Easycap EEG setup (Fig. 14), respectively. Comparing
functionally equivalent groups of ICs makes it possible to assess whether
they resemble recurring neural sources retrieved from multiple sessions,
and to determine if the clustered ICs remain shared across the two different experimental EEG setups (Fig. 5).
4.4. Mobile interfaces for neurofeedback
In contrast to personal informatics apps, neurofeedback interfaces
require the user to interact in real time with audiovisual representations of EEG data in an attempt to control the ongoing brain activity.
Neurofeedback experiments aiming to increase power in the upper
alpha range have been shown to improve cognitive performance in several studies (Hanslmayr et al., 2005; Zoefel et al., 2011). While there
is often a peak in individual alpha brainwave power around 10 Hz,
neurofeedback training makes it possible to control and shift the activity
towards the upper alpha range of 12 Hz. In relation to neurofeedback, an
ability to consciously control alpha brainwave oscillations, which as a
gating mechanism appears to be involved in selective attention (Foxe
and Snyder, 2011), might thus potentially help explain the previously
reported training effects on cognitive performance. Likewise, an association between higher alpha frequency and good memory performance
has previously been shown (Klimesch, 1999) (Fig. 6).
However, designs for neurofeedback interfaces are often conceptualized with little attention to how the actual feedback of audiovisual elements might affect the user's ability to control brain activity. Normally,
User Experience (UX) design of graphical interfaces involves initial
modeling of user needs and selection of design patterns for organizing
content and navigational layout reﬂecting gestalt principles. This may
FPZ
F7

AF3
F3

AF4
F4

F8

F3

FZ

F4

FC6

FC5
T7

C3

T8
TP 9

P7

P8
O1

O2

CZ

P3
O1

PZ

C4
P4

TP 10

O2

Fig. 5. Electrode locations for two mobile 16 channel EEG setups; the Emotiv neuroheadset
using saline sensors positioned laterally (left), versus a standard gel-based Easycap EEG
montage including central and midline positions (right).

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

59

Fig. 6. SBS2 neurofeedback training app running on an Android tablet, where a blue to
orange shift in color horizontally over time represents an increase in upper alpha power.

subsequently be translated into frameworks for interaction ranging from
scrollable timeline lists to multilayered contextual map metaphors
(Tidwell, 2011). Neurofeedback applications on the other hand have
typically concentrated on mapping EGG amplitude values directly onto
audiovisual components. For example, sounds of ocean waves or
high- or low-pitched gongs (Egner et al., 2004; Hinterberger et al.,
2004) would map to visual designs based on vertical scales and
squares of changing colors (Zoefel et al., 2011; Neumann et al.,
2003; Vernon et al., 2003). When targeted towards children, these
elements have been incorporated into more complex scenarios built
around airplanes, a 3D car racing environment, or a pole-vaulting
cartoon mouse (Gevensleben et al., 2009; Heinrich et al., 2007). In summary, these designs may be understood as based on contrasting combinations of the following audiovisual components (Jensen et al., 2013):
•
•
•
•
•

pitch (low, high)
volume (soft, loud)
timbre (dark, light)
duration (short, long)
rhythm (temporal distribution)

•
•
•
•
•

geometric primitives (connected segments)
color (discrete, gradients)
size (proximity, scalability)
movement (horizontal, vertical)
composition (spatial distribution).

To explore the inﬂuence that such components might have on the
efﬁcacy of neurofeedback training, we tested two different interfaces
developed for the SBS2. We conducted an experiment with 25 subjects
aiming at increasing their upper alpha frequency band (Jensen et al.,
2012). The neurofeedback experiment consisted of two iterations, testing the two different interfaces. In the ﬁrst iteration, 12 healthy subjects (7 males and 5 females) with an average age of 23.6 ± 1.9 did
neurofeedback training on a replication of an existing interface
(Zoefel et al., 2011). This interface indicated brain activity based
on only two components (color gradients framed by a square primitive). In the second iteration another 13 healthy subjects (7 male
and 6 female) with an average age of 26.6 ± 5.5 performed
neurofeedback training using an interface developed on basis of
the common features extracted from the ﬁrst group of subjects.
The second interface combined four components (scaled down

Fig. 7. (a) In the interface from the ﬁrst version of the neurofeedback application, the
current brain activity is visualized by the square changing colors; blue indicating activity
below, gray — equal to, and red — above baseline. (b) The interface of the neurofeedback
application from the second iteration is shown with additional illustrations describing
how it is constructed: The 5-minute timeline shown in the bottom illustrates how the
training session is divided into columns of 15 s. Within a column squares would appear
ﬁrst (0–3 s) in a window in the lower part of the screen, then (4–7 s) in windows above
and below the ﬁrst window and lastly (8–11 s and 12–15 s) in windows above the ﬁrst
two windows. The windows are shown in the rightmost column which was not shown
during the experiments. The encircled column illustrates how the user can easily compare
the ability to increase brain activity in different time intervals.

color gradients framed by square primitives spatially distributed
horizontally and vertically).
The EEG signal from all of Emotiv's 16 electrodes was recorded and
the real-time feedback was constructed from O1 and O2. Additionally,
an ofﬂine re-referencing of P3 and P4 with the frontal electrodes AF3,
AF4, F3 and F4 allowed for P3 and P4 to be included in the later data
processing, thus covering a larger area of the relevant cortical area.
The power of the brain activity was calculated using Fast Fourier
Transformation.
Both iterations consisted of ﬁve sessions during one week from
Monday to Friday. Each session started and ended with a 5-minute
baseline recording measuring the average brain activity during a simple
task. In between the baseline recordings ﬁve 5-minute training sessions
were conducted. After each session, we gathered qualitative data on the
thought patterns of the subjects leading to an increase of alpha brain activity based on informal interviews. Each subject received a total of 25
training recordings and 10 baseline recordings.

60

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

Fig. 8. The colors of the squares indicate the intensity of the brain activity.

The interface used in the ﬁrst iteration was similar to the one used in
a study by Zoefel et al. (2011), where the feedback consisted of a square
of changing colors gradually from blue, gray to red. Respectively each
color represented real-time amplitudes below, equal to baseline, or
above baseline, respectively, see Fig. 7a. The subjects were instructed
to make the square turn red. For the baseline recording a similar interface setup was used but with random color changes, making the visual
stimuli similar to those of the training recordings and therefore more
compatible. The subjects were asked to count the number of times the
square turned red. This would ensure a similar cognitive task across
the subjects while recording the baseline, thereby making these recordings comparable.
The feedback interface used in the second iteration consisted of
small squares being generated once a second, if the alpha amplitudes
exceeded the baseline. Over a 15 second interval the squares (maximum 15 squares) were assembled into a column, after which a new
column of squares was incrementally generated along a horizontal
axis. At the end of the 5 minute training recording, the interface
would consist of 20 columns of squares, see Fig. 7b. Thus the interface
not only showed the current amplitudes, but also the previous, allowing
the user to easily compare methods for increasing the amplitudes. The
squares not only indicated when the amplitudes exceed baseline, but
also the degree of increase by a change in color, ranging from dark
blue to orange (see Fig. 8). The degree of increase was calculated
from a running mean creating a smooth color ﬂow. The subjects were
instructed to create as many squares as possible and preferably with
yellow and orange colors. For the baseline recording a similar interface
was used, although with squares appearing randomly in the columns
and with random color. The subjects were asked to count the yellow
and orange squares.
All subjects of both iterations were asked to keep their eyes open for
as long as possible, and avoid muscle movements, jaw movements, and
swallowing during all recordings to limit artifacts.

Fig. 9. Mean accuracy of left and right imagined ﬁnger tapping classiﬁcation for a single
subject. Mean accuracy is based on 200 splits in training and test data. Classiﬁcation is
based on CSP and a quadratic Bayes classiﬁer focusing on bandpass ﬁltered data
8–32 Hz in interval 0.75–2.00 s after onset.

5. Results and discussion
In this section we present the results of the experiments, validating
the performance of the software, platforms used, and EEG hardware.
5.1. Brain computer interface based on imagined ﬁnger tapping
In order to validate the applicability of the platform in decoding
imagined left and right ﬁnger tapping, the EEG data was bandpass
ﬁltered (8–32 Hz) and we used the data in the interval 0.75–2.00 s
after stimuli onset as input to the common spatial pattern (CSP) algorithm (Müller-Gerking et al., 1999). One important parameter in the
CSP algorithm to be controlled is the number of spatial ﬁlters. To determine the number of spatial ﬁlters we applied cross-validation and examined the performance (accuracy of classiﬁer) as a function of the training
size (number of trials used for training), see Fig. 9. The classiﬁer was
trained on a balanced set of trials (i.e. equal number of left and right
trials), which was carried out 200 times for each training set size.
Fig. 9 indicates that we need more than a single spatial ﬁlter (m N 1).
When m = 2, for example, two spatial ﬁlters are used to maximize
the variance for class 1 while minimizing the variance for class 2 and
an additional two spatial ﬁlters are used to minimize the variance for
class 1 while maximizing the variance for class 2. It is interesting that
only a few spatial ﬁlters are required to obtain an accuracy close
to 60%. We also note that performance is increasing as a function of
samples, hence, even better performance can be expected if more samples are collected.
5.1.1. Source reconstruction and source separation
For further validation we applied standard statistical evaluation
for signiﬁcance and correction for multiple comparisons. Thus, we
performed a Monte Carlo permutation test (Maris and Oostenveld,
2007) to check for signiﬁcant electrode differences between left
and right ﬁnger tapping. Fig. 10 demonstrates a scalp map of the
effect of the averaged response based on left ﬁnger tapping minus
averaged response based on right ﬁnger tapping. The signiﬁcant
channels at given time intervals are highlighted in accordance
with the Monte Carlo permutation test conducted using Fieldtrip
(Oostenveld et al., 2011). Both positive and negative effects are
detected as signiﬁcant with Monte Carlo p-values of 0.012 and 0.001,
respectively. A set of 1000 random permutations were performed.
Inspecting Fig. 10 reveals signiﬁcant differences over the left and the
right hemisphere and more importantly the electrodes contributing to
the signiﬁcant difference between left and right imagined ﬁnger tapping
are electrodes located close to the premotor area. Thus, it seems that
these electrodes are taking over the often reported electrodes C3 and
C4 as the main drivers, as C3 and C4 are not present in the Emotiv EEG
sensor conﬁguration.
To examine the ability to perform reliable 3D EEG imaging based
on the data acquired using an Emotiv neuroheadset, source reconstruction was carried out on the bandpass-ﬁltered imagined ﬁnger tapping
data (8–32 Hz) also used for the classiﬁcation task and in the nonparametric statistical test. Fig. 11 illustrates the mean power of the
difference between left and right imagined ﬁnger tapping in the interval
0.75–2.00 s post-stimuli. Premotor areas are typically involved in executing the task and in differentiating a left from right imagined
movement. This is also the case here to some extent with minor

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

61

Fig. 10. Monte Carlo permutation test for signiﬁcant difference between averaged left imagined ﬁnger tapping response and averaged right imagined tapping. Electrodes located close to
the premotor region are detected as signiﬁcant in the time interval 0.9–2.1 s after stimuli.

discrimination in the premotor areas and more pronounced discrimination in the more frontally located areas. Note the polarity
of the power difference map, with the left hemisphere indicating
a positive difference and the right hemisphere indicating a negative
contribution. During the imagined ﬁnger tapping part, the contralateral
premotor/motor regions desynchronize (resulting in a decrease in
power within the speciﬁc frequency range) while the ipsilateral
premotor/motor regions ﬁrst desynchronize shortly and right after synchronize (meaning increased power within the frequency range). The
main explanation for the displacement more frontally found in Fig. 11,

is the uneven distribution of sensors for the Emotiv EPOC system, with
most of the sensors positioned frontally.
However, large proportions of the occipital and temporal areas are
also found to be active by the reconstruction. These apparent visual
and temporal source activation differences may, however, be explained
by the fact that re-referencing to an average channel is performed prior
to source estimation. Since the distribution of the sensor locations
is highly unevenly distributed, with the majority placed frontally,
re-referencing data with a strong frontal activation (e.g. eye blink/
movement) to an average reference channel will map part of the

Fig. 11. Source reconstruction of mean difference power map between left and right imagined ﬁnger tapping.

62

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

Fig. 12. Source reconstruction of mean difference power map between left and right imagined ﬁnger tapping. Emotiv EEG data corrected by removal of ICA component associated with eye
movement.

frontal activity to the temporal and occipital electrodes. To further
test this hypothesis, we investigated the inﬂuence of artifacts caused
by eye motion on the source reconstruction estimates by removing
an eye-related ICA component. Indeed, the removal of the eye movement component seems to improve the source estimates signiﬁcantly,
as demonstrated in Fig. 13. The operating regions (frontal areas and
slightly pre-motor regions) are more highly visible in this power difference map between the averaged left minus right imagined ﬁnger tapping conditions. Similarly, as in Fig. 11, the sources are displaced more
frontally than typically, and this can be explained by the sensor positioning offered by the Emotiv EPOC system. The source reconstruction
was performed ofﬂine to ensure a fair comparison with and without
removal of the ICA component related to eye movement. The ICA
decomposition was performed using the extended Infomax algorithm
supported by EEGLAB (Fig. 12).

polarity of ICs into account when comparing the two studies, the clustered scalp maps in both experiments suggest left lateralized prefrontal
as well as parietal activations in language areas, which integrate motor
and semantic aspects connected through the dorsal and ventral streams
in the brain (Rolheiser et al., 2011; Axer et al., 2012).
This is in line with results obtained in a recent MRI experiment
(Moseley et al., 2011) using the same verbs as in the present EEG
study, indicating that premotor neural circuits are activated when passively reading verbs related to face and hand motion and when seeing
emotional expressions. Mobile neuroimaging could potentially extend
our ability to explore such action-based links between actual motion
and emotion in an everyday context, which might in turn reﬂect imitation of gestures or facial expressions involving mirror neuron circuits in
the brain, possibly providing a foundation for higher level feelings of
empathy and theory of mind.

5.2. Visual stimulus to investigate emotional responses

5.3. Mobile interfaces for neurofeedback

Within the Emotiv data, 2 × 18 ICs have been clustered in 10 out
of 10 trials, indicating that these independent components are consistently activated across all trials (Fig. 13). Similarly, in the Easycap
data, 23 ICs have been clustered within 3 standard deviations of the
K-means centroids in 10 out of 10 trials, while 9 ICs have been grouped
in 7 out of 10 trials, conﬁrming that temporally independent activations
are also grouped across trials in this study (Fig. 14). Taking the relative

All signal processing of the data for the Neurofeedback experiment
was done off-line using the EEGLAB (Delorme and Makeig, 2004)
plug-in for MATLAB.
Since the alpha frequency band has shown to vary depending on age,
possible neurological diseases, and memory performance (Klimesch,
1999), the upper alpha frequency band had to be determined for
each individual. By identifying the peak in the power spectrum, the

Fig. 13. Single-subject EEG neuroimaging study with Emotiv 16 channel neuroheadset: PCA dimensionality reduction and K-means clustering (K = 10, σ = 3) of 140 IC scalp maps,
activation time series and event-related changes in power spectra based on 10 trials, each consisting of reading 3 × 20 emotion (blue), face (green) and hand (pink) related action verbs.

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

63

Fig. 14. Single-subject EEG neuroimaging study with Easycap 16 channel EEG setup: PCA dimensionality reduction and K-means clustering (K = 10, σ = 3) of 140 IC scalp maps, activation time series and event-related changes in power spectra based on 10 trials, each consisting of reading 3 × 20 emotion (blue), face (green) and hand (pink) related action verbs.

individual alpha peak (IAF), the upper alpha frequency band was set as
a band of 2 Hz above IAF (from IAF to IAF + 2Hz). Thus the individuals'
upper alpha frequency band were determined from the ﬁrst baseline
recording of every session, and the mean amplitude was calculated for
all baseline and training recordings. Two subjects (1 male and 1 female)
from the ﬁrst iteration of the experiment did not complete all training
sessions, and were therefore excluded from further analysis.
In addition, it has repeatedly been reported that some subjects,
usually called non-responders, are unable to change amplitudes of the
brain frequencies signiﬁcantly (Zoefel et al., 2011; Gevensleben et al.,
2009; Fuchs et al., 2003; Lubar et al., 1995). Subjects who did not
show a signiﬁcant increase in the upper alpha frequencies when comparing the very ﬁrst baseline (baseline 1 in session 1) with the training
recordings from Friday (session 5) were considered non-responders. As
a result, 3 subjects (2 female, 1 male) from the ﬁrst iteration and another 3 subjects (2 male, 1 female) from the second iteration were considered non-responders. These left 7 subjects in the ﬁrst iteration (5 male,
2 female) and 10 subjects in the second iteration (5 male, 5 female)
remain for statistical analysis.
The individuals' EEG results from the baseline- and training recordings were normalized in respect to the ﬁrst baseline Monday (session
1), thereby showing the ability to increase upper alpha (UA) amplitudes
in relation to the ﬁrst baseline in percentage. The results obtained
over the week (Monday to Friday) have been plotted in Fig. 15. Each

line represents a subject's ability to increase UA amplitudes: The red
lines represent subjects from the ﬁrst iteration, the black lines represent
subjects from the second iteration and the bold lines represent the nonresponders. From the graph it is clear that some subjects are more capable of increasing their UA amplitudes and increase above 400%, whereas
others experience a decrease (usually the non-responders). In addition,
the subjects who get the highest increase are mainly those who use the
second iteration interface. However, the variance in the subject ability
to increase their UA is also greater.
These results suggest that the ability to control neural activity is very
individual and that the interface should be supportive of the individual's
strategies.
Following the approach of Zoefel et al. (2011), we ﬁtted regression
lines to the individual UA amplitudes as a function of session number
(1–35) and used a one-sample, one-sided t-test to test whether they
were signiﬁcantly greater than zero, which they were in both iterations
(p b 0.05 and p b 0.03 for the ﬁrst and second iterations respectively).
We also compared the regression lines between the iterations using
a two-sample, two-sided t-test and found no signiﬁcant difference
(p N 0.70). This result, in itself, could indicate that the two types of feedback are equally effective.
This approach does not, however, separate the effect of training (a
lasting increase in UA amplitude) from the feedback effect (an immediate increase in UA amplitude during feedback). To isolate the training

Fig. 15. The individuals' upper alpha (UA) amplitude in percentage of the UA amplitude from the ﬁst baseline recording on the ﬁrst day (Monday). Subjects from ﬁrst and second iterations
as well as the non-responders from both iterations are plotted in red, black and gray, respectively. The average of these groups is marked by bold lines in corresponding colors. All baselineand training recordings have been plotted in sequence across the week, e.g. Monday will show results from the ﬁrst baseline, then the 5 training recording, ending with the second baseline,
giving a total of 7 points of each day. The results illustrate a large variance in the individual subjects ability to increase UA amplitudes, where subjects from the second iteration were
capable of reaching a greater increase, although some subjects were unable to achieve any increase at all (referred to as non-responders).

64

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

effect, we again follow the approach of Zoefel et al. who quantiﬁed the
training effect as the difference between UA amplitude during the ﬁrst
baseline recording in the ﬁrst session and the ﬁrst baseline recording
in the last session and tested for an increase with a one-sample onesided t-test. Using this approach we found a signiﬁcant effect in the
ﬁrst iteration (p b 0.002) but not in the second iteration (p N 0.14).
This result indicates that the interface used in the ﬁrst iteration was
more effective for neurofeedback training.
In addition to this, we are also interested in isolating the feedback
effect, which we quantify as the difference between the mean UA amplitude across feedback recordings and the mean UA amplitude across
the ﬁrst and last baseline recording for each session. We compare the
feedback effect from the two iterations using a repeated-measures
ANOVA with session number as within subject factor and iteration
as between groups factor. We found a signiﬁcant effect of iteration
(F(1,15) = 11.85, p b 0.005) but no signiﬁcant effect of session number
or the interaction between session and iteration. Based on the lack of
effect of the session number, we averaged the feedback effect across
session number and subjects within an iteration and found that
the mean feedback effect was 0.17 for the ﬁrst iteration and 0.67 for
the second iteration. This result indicates that the interface used in the
second iteration was more effective for inducing an immediate increase
in UA amplitude.
That the feedback effect was higher in the second group without
a corresponding increase in the training effect suggests that the magnitude of UA amplitude during feedback does not completely determine
the training effect. This could be due to a ceiling effect, so that UA amplitude during training has no effect above a certain level. Alternatively,
it could also mean that the two groups used different strategies for
increasing UA amplitude during training and that although the second
group's strategy was more efﬁcient for increasing UA amplitude during
feedback, it did not increase the training effect. Such strategic differences could be facilitated by the difference in the visual feedback stimulus. In the ﬁrst group participants needed to constantly look at the
feedback stimulus to get feedback, whereas the second group could
look elsewhere intermittently and return their gaze to the feedback
stimulus only when they wished to learn about their performance.
This could change the UA amplitude during feedback without increasing
the training effect as could the mere physical differences in the feedback
signals.
In summary, our neurofeedback study conﬁrms the ﬁndings of Zoefel
et al. (2011), provides new insights into the effects of the type of feedback provided, and conﬁrms that neurofeedback training is possible
with a mobile setup based on the Smartphone Brain Scanner.
6. Further perspectives
6.1. Hardware
Current consumer-grade and research-oriented mobile EEG systems
are only the ﬁrst iteration of the hardware. We predict two major directions of the development.
On one hand, the high-density systems will become mobile,
pushing for the best possible quality of the acquired signal in naturalistic conditions. The development of these systems will not be
primarily focused on making them unobtrusive, fashionable, or consumer-operated. From the spectrum of the features offered by the
new EEG hardware, these systems will focus on mobility, portability,
and low-cost. They will be used in the more or less classical experiments, controlled and initiated by the researchers.
On the other hand, more consumer-oriented devices will emerge.
They will be ﬁtted for particular use-cases, which will allow to make
them smaller, concealed, and user-friendly. Such sensors will not necessarily be seen as EEG devices, but rather as cognitive state monitoring
devices, and in addition to the EEG signal, they may include other
electro-physiological signals, such as EMG, ECG, and skin conductance.

Still, the data available from a large number of such systems bought
and worn by the consumers for their particular function, may offer
an unparalleled opportunity for understanding human brain and cognitive states. Gaining privacy-preserving access to, and analyzing noisy,
not-at-all or poorly annotated data originating from brain state, from
hundreds or thousands of subjects and collected over days, weeks,
or months can become one of the grand challenges for cognitive neuroscience in the next few years.
The development of neuroheadsets and sensors accompanies
the development of mobile devices, smartphones and tablets, allowing
for personal hubs for interconnected, wearable devices. The increasing
processing power and low-energy protocols (e.g., Bluetooth 4.0, NFC)
turn our personal space into a busy network of devices (phones,
Bluetooth headsets, smart watches, glasses, hearing aids etc.). EEG
sensors, even if equipped with a single electrode, can ﬁt naturally in
such systems, as long as they can provide certain well-deﬁned value
for the user.
6.2. Software
The evolution of the software will be closely coupled with the usecases of the hardware solutions. For the research-focused high-density
mobile hardware, the minimal requirement of data collection and
possibly transmission on mobile devices can be easily satisﬁed with
simple software. In such cases, the already existing frameworks, such
as EEGLAB, can utilize signiﬁcant processing power of desktop or even
server systems, and can even be used for data processing and transmitting the extracted features back to the user.
For more consumer-oriented sensors, real-time applications, possibly operating directly on mobile devices without server connection,
need to be developed. The Smartphone Brain Scanner is the ﬁrst framework that enables such development; pushing the limits of what can
be done in terms of creating user value by enabling novel EEG applications. As the mobile devices performing the processing grow more powerful, more complex algorithms can be enabled to compensate for noise
and low density of the systems.
6.3. Experiments
The vast majority of studies of neural and cognitive functions have
so far been set in the laboratory, where the subject is severely restrained
in movement, isolated from the surrounding world, and is required
to carry out the same limited task repeatedly. This is an impoverished
environment that we normally live in and are optimized to function
in; it totally ignores human agency.
Taking EEG out of the laboratory and into the natural world will
allow us to move beyond these constraints. Measuring the EEG of a freely moving subject will allow us to characterize the neural activity of
many important functions. With wearable EEG we can study natural
motion such as walking and complex composite motion. We can also
study the many cognitive tasks that we constantly perform in their
full complexity. Examples include preference-based choice as we select
given consumer goods over others, the constant updating of working
memory throughout our daily work, and the use of speech in natural
social interactions. Measuring the EEG of subjects in rich natural environments will allow us to characterize the neural function of the perceptual systems when they are met with rich multimodal stimuli in
which attention is constantly needed to select the relevant stimuli and
ﬁlter out irrelevant stimuli.
The complexity and variability of data collected in the natural environment will be tremendous compared to the data collected in the laboratory. In order to derive anything meaningful from it, the amount
of necessary data will be equally tremendous. Wearable EEG offers an
immediate solution as hours, days, even weeks of data can be collected
outside of the laboratory; something which is completely unrealistic in
lab-based experimentation.

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

7. Conclusions
Mobile brain imaging, here realized as an EEG system, offers huge
promise for many research areas. Here we show our initial work with
the Smartphone Brain Scanner framework, which can record, analyze,
and 3D real-time visualize EEG signals directly on a mobile device,
using low-cost, consumer-grade neuroheadsets. The signal obtained
in the studies, although of low dimensionality (14 channels) and noisy,
can still be successfully used for multiple classical neuroscience applications, including brain-computer interfaces (BCIs), analysis of high-level
brain activity, and neurofeedback. The features of the presented system
make it possible to use in domains such as cognitive psychology, medical
applications, social science research, as well as for “self monitoring” as
promoted by the Quantiﬁed Self community5.
As the presented framework runs on mobile devices, including
tablets and smartphones, it can be coupled with other embedded sensors in a natural way. In this sense, EEG serves as an extension of the
sensing capabilities of the already existing devices, and can be used
in an integrated way with the other collected data (e.g. location, social
interactions, activity level).
We argue that the presented framework enables a wide variety of
experiments, and the initial set of these presented in this paper serves
as a validation and showcase of the versatility of the framework and
general approach. It is now clear that we are at the stage where hardware is powerful and inexpensive enough to be used for mobile brain
imaging, while at the same time available algorithms can handle noisy
data, allowing us to recover signiﬁcant information.
The approach to user-oriented and mobile EEG does not end with
the notion of researchers using the mobile devices and consumergrade neuroheadsets to collect the data from the subjects. We can easily
imagine that the systems will eventually be able to deliver interesting
data to the public, giving them incentive to invest in their own hardware. In this shift, the data would be collected and uploaded by the participants themselves, distributing the cost and difﬁculty of running
experiments. This presents yet more challenges, such as data quality
control and privacy. On the other hand, it does give a promise of
extremely large datasets created for large populations and over long
periods of time, for only little costs.
Acknowledgment
This work is supported in part by the Danish Lundbeck Foundation
through CIMBI — Center for Integrated Molecular Brain Imaging (LKH,
CS) and a Postdoc grant for CS, and by the H. C. Ørsted Foundation.
We thank the reviewers for many useful comments and suggestions.
References
Ahn, M., Hong, J.H., Jun, S.C., 2012. Feasibility of approaches combining sensor and source
features in brain-computer interface. J. Neurosci. Methods 204, 168–178.
Axer, H., Klingner, C.M., Prescher, A., 2012. Fiber anatomy of dorsal and ventral language
streams. Brain Lang. http://dx.doi.org/10.1016/j.bandl.2012.04.015.
Babiloni, F., Cincotti, F., Lazzarini, L., Millan, J., Mourino, J., Varsta, M., Heikkonen, J., Bianchi, L.,
Marciani, M., 2000. Linear classiﬁcation of low-resolution EEG patterns produced by
imagined hand movements. IEEE Trans. Rehabil. Eng. 8, 186–188.
Baillet, S., Mosher, J., Leahy, R., 2001. Electromagnetic brain mapping. IEEE Signal Proc.
Mag. 18, 14–30.
Bartels, A., Zeki, S., 2004. Functional brain mapping during free viewing of natural scenes.
Hum. Brain Mapp. 21, 75–85.
Besserve, M., Martinerie, J., Garnero, L., 2011. Improving quantiﬁcation of functional
networks with EEG inverse problem: evidence from a decoding point of view.
Neuroimage 55, 1536–1547.
Bishop, C.M., 2006. Pattern Recognition and Machine Learning. Springer, NY 10013 (USA).
Blankertz, B., Dornhege, G., Krauledat, M., Muller, K., Kunzmann, V., Losch, F., Curio, G.,
2006. The Berlin brain-computer interface: EEG-based communication without
subject training. IEEE Trans. Neural Syst. Rehabil. Eng. 14, 147–152.
Caldwell, J.A., Przinko, B., Caldwell, J.L., 2003. Body posture affects electroencephalographic
activity and psychomotor vigilance task performance in sleep-deprived subjects. Clin.
Neurophysiol. 114, 23–31.
5

http://quantiﬁedself.com/.

65

Chi, Y.M., Maier, C., Cauwenberghs, G., 2011. Integrated ultra-high impedance front-end
for noncontact biopotential sensing. Biomedical Circuits and Systems Conference,
2011. BioCAS 2011, Xplore, IEEE.
Chi, Y., Wang, Y.-T., Wang, Y., Maier, C., Jung, T.-P., Cauwenberghs, G., 2012. Dry and
noncontact EEG sensors for mobile brain-computer interfaces. IEEE Trans. Neural
Syst. Rehabil. Eng. 20, 228–235.
Comon, P., 1994. Independent component analysis, a new concept? Signal Process. 36,
287–314.
Debener, S., Minow, F., Emkes, R., Gandras, K., Vos, M., 2012. How about taking a low-cost,
small, and wireless EEG for a walk? Psychophysiology 49.11, 1617–1621.
Delorme, A., Makeig, S., 2004. EEGLAB: an open source toolbox for analysis of single-trial
EEG dynamics including independent component analysis. J. Neurosci. Methods 134,
9–21.
Delorme, A., Sejnowski, T.J., Makeig, S., 2007. Enhanced detection of artifacts from EEG
data using higher-order statistics and independent component analysis. Neuroimage
34.
Delorme, A., Mullen, T., Kothe, C., Acar, Z., Bigdely-Shamlo, N., Vankov, A., Makeig, S., 2011.
EEGLAB, SIFT, NFT, BCILAB, and ERICA: new tools for advanced EEG processing.
Comput. Intell. Neurosci. 2011, 10.
Delorme, A., Palmer, J., Onton, J., Oostenveld, R., Makeig, S., 2012. Independent EEG
sources are dipolar. PLoS One 7. http://dx.doi.org/10.1371/journal.pone.0030135.
Delorme, A., Makeig, S., Sejnowski, T., 2001. Automatic artifact rejection for EEG data using
high-order statistics and independent component analysis. International workshop
on ICA (San Diego, CA).
Dmochowski, J.P., Sajda, P., Dias, J., Parra, L.C., 2012. Correlated components of ongoing
EEG point to emotionally laden attention—a possible marker of engagement? Front.
Hum. Neurosci. 6.
Dornhege, G., Blankertz, B., Curio, G., Müller, K., 2004. Boosting bit rates in non-invasive
EEG single-trial classiﬁcations by feature combination and multi-class paradigms.
IEEE Trans. Biomed. Eng. 51 (6), 993–1002.
Egner, T., Zech, T.F., Gruzelier, J.H., 2004. The effects of neurofeedback training on the spectral topography of the electroencephalogram. Clin. Neurophysiol. 115, 2452–2460.
Foxe, J.J., Snyder, A.C., 2011. The role of alpha-band brain oscillations as a sensory
suppression mechanism during selective attention. Front. Psychol. 2, 1–13.
Fuchs, T., Birbaumer, N., Lutzenberger, W., Gruzelier, J.H., Kaiser, J., 2003. Neurofeedback
treatment for attention-deﬁcit/hyperactivity disorder in children: a comparison
with methylphenidate. Appl. Psychophysiol. Biofeedback 28, 1–12.
Gallese, V., Fadiga, L., Fogassi, L., Rizzolati, G., 1996. Action recognition in the premotor
cortex. Brain 119, 593–609.
Gevensleben, H., Holl, B., Albrecht, B., Vogel, C., Schlamp, D., Kratz, O., Studer, P.,
Rothenberger, A., Moll, G.H., Heinrich, H., 2009. Is neurofeedback an efﬁcacious treatment for ADHD? A randomised controlled clinical trial. J. Child Psychol. Psychiatry 50,
780–789.
Gramann, K., Gwin, J.T., Bigdely-Shamlo, N., Ferris, D.P., Makeig, S., 2010. Visual evoked
responses during standing and walking. Front. Hum. Neurosci. http://dx.doi.org/
10.3389/fnhum.2010.00202.
Gramann, K., Gwin, J., Ferris, D., Oie, K., Jung, T., Lin, C., Liao, L., Makeig, S., 2011. Cognition in
action: imaging brain/body dynamics in mobile humans. Rev. Neurosci. 22, 593–608.
Gwin, J.T., Gramann, K., Makeig, S., Ferris, D.P., 2010. Removal of movement artifact from
high-density EEG recorded during walking and running. J. Neurophysiol. http://
dx.doi.org/10.1152/jn.00105.2010.
Gwin, J.T., Gramann, K., Makeig, S., Ferris, D.P., 2011. Electrocortical activity is coupled to
gait cycle phase during treadmill walking. Neuroimage 54.
Hämäläinen, M., Ilmoniemi, R., 1994. Interpreting magnetic ﬁelds of the brain: minimum
norm estimates. Med. Biol. Eng. Comput. 32, 35–42.
Hanslmayr, S., Sauseng, P., Doppelmayr, M., Schaubus, M., Klimesch, W., 2005. Increasing
individual upper alpha power by neurofeedback improves cognitive performance in
human subjects. Appl. Psychophysiol. Biofeedback 30. http://dx.doi.org/10.1007/
s10484-005-2169-8.
Hasson, U., Nir, Y., Levy, I., Fuhrmann, G., Malach, R., 2004. Intersubject synchronization of
cortical activity during natural vision. Science 303, 1634–1640.
Heinrich, H., Gevensleben, H., Strehl, U., 2007. Annotation: neurofeedback — train your
brain to train behaviour. J. Child Psychol. Psychiatry 48, 3–16.
Hinterberger, T., Neumann, N., Pharn, M., Kübler, A., Grether, A., Hofmayer, N., Wilhelm,
B., Flor, H., Birbaumer, N., 2004. A multimodal brain-based feedback and communication system. Exp. Brain Res. 154, 521–526.
Hoffmann, S., Falkenstein, M., 2008. The correction of eye blink artefacts in the EEG: a
comparison of two prominent methods. PLoS One 3. http://dx.doi.org/10.1371/
journal.pone.0003004.
Jensen, C., Birgitte, F., et al., 2012. Training your brain on a tablet. 34th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC).
Jensen, C., Birgitte, F., et al., 2013. Spatio temporal media components for neurofeedback.
2013 IEEE International Conference on Multimedia and Expo (ICME).
Jolliffe, I.T., 2002. Principal Component Analysis. Springer Series in Statistics, 1986. Springer.
Klimesch, W., 1999. EEG alpha and theta oscillations reﬂect cognitive and memory performance: a review and analysis. Brain Res. Brain Res. Rev. 29, 169–195.
Liao, L.-d., Chen, C.-Y., Wang, I.-J., Chen, S.-F., Li, S.-Y., Chen, B.-W., Chang, J.-Y., Lin, C.-T.,
2012. Gaming control using a wearable and wireless EEG based brain computer interface device with novel dry foam based sensors. J. Neuroeng. Rehabil. 9 (http://www.
jneuroengrehab.com/content/9/1/5).
Lin, C.-T., Liao, L.-d., Liu, Y.-H., Wang, I.-J., Lin, B.-S., Chang, J.-Y., 2011. Novel dry polymer
foam electrodes for long-term EEG measurement. IEEE Trans. Biomed. Eng. 58,
1200–1207.
Looney, D., Kidmose, P., Park, C., Ungstrup, M., Rank, M.L., Rosenkranz, K., Mandic, D.P.,
2012. The in-the-ear recording concept: user-centered and wearable brain monitoring.
IEEE Pulse 3, 32–42.

66

A. Stopczynski et al. / International Journal of Psychophysiology 91 (2014) 54–66

Lubar, J.F., Swartwood, M.O., Swartwood, J.N., O'Donnell, P.H., 1995. Evaluation of the
effectiveness of EEG neurofeedback training for ADHD in a clinical setting as measured by changes in T.O.V.A. scores, behavioral ratings, and WISC-R performance.
Biofeedback Self Regul. 20, 83–99.
Makeig, S., Bell, A.J., Jung, T.-P., Sejnowski, T.J., 1996. Independent component analysis of
electroencephalographic data. Adv. Neural Inf. Process. Syst. 8, 145–151.
Makeig, S., Debener, S., Onton, J., Delorme, A., 2004. Mining event-related brain dynamics.
Trends Cogn. Sci. 8, 204–210.
Makeig, S., Gramann, K., Jung, T., Sejnowski, T., Poizner, H., 2009. Linking brain, mind and
behavior. Int. J. Psychophysiol. 73, 95–100.
Maris, E., Oostenveld, R., 2007. Nonparametric statistical testing of EEG-and MEG-data.
J. Neurosci. Methods 164, 177–190.
MoBi Lab Matlab plugin, 2009. http://sccn.ucsd.edu/wiki/MoBI_Lab (Accessed: 2013-02-27).
Moseley, R., Carota, F., Hauk, O., Mohr, B., Pulvermüller, F., 2011. A role for the motor system in binding abstract emotional meaning. Cereb. Cortex 1–14. http://dx.doi.org/
10.1093/cercor/bhr238.
Müller-Gerking, J., Pfurtscheller, G., Flyvbjerg, H., 1999. Designing optimal spatial ﬁlters
for single-trial EEG classiﬁcation in a movement task. Clin. Neurophysiol. 110,
787–798.
Neumann, N., Kübler, A., Kaiser, J., Hinterberger, T., Birbaumer, N., 2003. Conscious
perception of brain states: mental strategies for brain-computer communication.
Neuropsychologia 41, 1028–1036.
Niel, C.M., Stryker, M.P., 2010. Modulation of visual responses by behavioral state in
mouse visual cortex. Neuron 65, 472–479.
Oostenveld, R., Fries, P., Maris, E., Schoffelen, J., 2011. Fieldtrip: open source software for advanced analysis of MEG, EEG, and invasive electrophysiological data. Comput. Intell.
Neurosci. 2011, 1.
Pascual-Marqui, R.D., Michel, C.M., Lehmann, D., 1994. Low resolution electromagnetic
tomography: a new method for localizing electrical activity in the brain. Int.
J. Psychophysiol. 18, 49–65.

Pellegrino, G.d., Fadiga, L., Fogassi, L., Gallese, V., Rizzolati, G., 1992. Understanding motor
events: a neurophysiological study. Exp. Brain Res. 91, 176–180.
Pulvermüller, F., Fadiga, L., 2010. Active perception: sensorimotor circuits as a cortical
basis for language. Nat. Neurosci. 11, 351–360.
Rizzolati, G., Sinigaglia, C., 2010. The functional role of the parieto-frontal mirror circuit:
interpretations and misinterpretations. Nature 11, 264–274.
Rolheiser, T., Stamatakis, E.A., Tyler, L.k., 2011. Dynamic processing in the human language
system: synergy between the arcuate fascicle and extreme capsule. J. Neurosci. 31,
16949–16957.
Rufﬁni, G., Dunne, S., Fuentermilla, L., Grau, C., Farrés, E., Marco-Pallarés, J., Watts,
P.J.P., Silva, S.R.P., 2008. First human trials of a dry electrophysiology sensor
using a carbon nanotube array interface. Sensors Actuators A Phys. 144,
275–279.
Slobounov, S., Hallett, M., Cao, C., Newell, K., 2008. Modulation of cortical activity as a
result of voluntary postural sway direction: an EEG study. Neurosci. Lett. 442. http://
dx.doi.org/10.1016/j.neulet.2008.07.021.
Stopczynski, A., Larsen, J.E., Stahlhut, C., Petersen, M.K., Hansen, L.K., 2011. A smartphone
interface for a wireless EEG headset with real-time 3D reconstruction. Affective
Computing and Intelligent Interaction. Springer, pp. 317–318.
Stopczynski, A., Stahlhut, C., Larsen, J.E., Petersen, M.K., Hansen, L.K., 2013. The smartphone
brain scanner: a mobile real-time neuroimaging system. http://arxiv.org/abs/1304.0357.
Tidwell, J., 2011. Designing Interfaces — Patterns for Effective Interaction Design. O' Reilly.
Vernon, D., Egner, T., Cooper, N., Compton, T., Neilands, C., Sheri, A., Gruzelier, J.H., 2003.
The effect of training distinct neurofeedback protocols on aspects of cognitive performance. Int. J. Psychophysiol. 47, 75–85.
Yasui, Y., 2009. A brainwave signal measurement and data processing technique for daily
life applications. J. Physiol. Anthropol. 28, 145–150.
Zoefel, B., Huster, R.J., Herrmann, C.S., 2011. Neurofeedback training of the upper
alpha frequency band in EEG improves cognitive performance. Neuroimage 54,
1427–1431.

Appendix

B

Emotional responses as
independent components in
EEG stimulation

Camilla Birgitte Falk Jensen, Michael Kai Petersen, and Jakob Eg Larsen.
Emotional responses as independent components in EEG. In Cognitive Information Processing (CIP), 2014 4th International Workshop on, pages 1–6. IEEE,
2014.

4TH INTERNATIONAL WORKSHOP ON COGNITIVE INFORMATION PROCESSING, MAY 26–28, 2014, COPENHAGEN, DENMARK

EMOTIONAL RESPONSES AS INDEPENDENT COMPONENTS IN EEG
Camilla B. Falk Jensen, Michael Kai Petersen, Jakob Eg Larsen
Technical University of Denmark
Cognitive Systems
DTU Compute, Building 321
DK-2800 Kgs. Lyngby
ABSTRACT
With neuroimaging studies showing promising results for
discrimination of affective responses, the perspectives of applying these to create more personalised interfaces that adapt
to our preferences in real-time seems within reach. Additionally the emergence of wireless electroencephalograph (EEG)
neuroheadsets and smartphone brainscanners widens the possibilities for this to be used in mobile settings on a consumer
level. However the neural signatures of emotional responses
are characterized by small voltage changes that would be
highly susceptible to noise if captured in a mobile context.
Hypothesizing that retrieval of emotional responses in mobile
usage scenarios could be enhanced through spatial filtering,
we compare a standard EEG electrode-based analysis against
an approach based on independent component analysis (ICA).
By clustering scalp maps and time series responses we identify neural signatures that are differentially modulated when
passively viewing neutral, pleasant and unpleasant images.
While early responses can be detected from the raw EEG
signal, we identify multiple early and late ICA components
that are modulated by emotional content. We propose that
similar approaches to spatial filtering might allow us to retrieve more robust signals in real-life mobile usage scenarios,
and potentially facilitate design of cognitive interfaces that
adapt the selection of media to our emotional responses.
Index Terms— EEG, ICA, Affective Computing, Affective Response
1. INTRODUCTION
Many of the neuroimaging experiments that use electroencephalograph (EEG) when measuring affective responses
from pleasant or unpleasant images have identified two main
components; early posterior negativity (EPN) emerging before 300ms and triggering a relative negative peak in the EEG
amplitude [1], and a late positive potential (LPP) emerging
after 300ms [2]. Recent studies indicate that several eventrelated potential (ERP) components may be influenced as
early as 50-80ms by emotional words and faces [3]. After
978-1-4799-3696-0/14/$31.00 c 2014 IEEE

that an N1 component at 130ms, characterized by a parietal
negativity, is modulated by emotional versus neutral content,
followed by the EPN occipital negativity described above.
The early negativities are thought to reflect an increased allocation of attentional resources elicited in response to what
is initially perceived as salient in contrast to neutral. Subsequently, after 300ms, an initial P3 parietal component,
followed by an occipital positive deflection corresponding to
the LPP described above, and a later central positivity (LCP)
appear likewise to be modulated by emotional content. These
three positive components (P3, LPP and LCP) characterized by shifting scalp topologies might be related to memory
encoding and further semantic processing of the emotional
content [4].
The promising results for distinguishing emotional responses lead to the perspective of applying these to create
more personalised content in real-time [5, 6, 7]. Additionally the emergence of wireless electroencephalograph (EEG)
neuroheadsets and smartphone brainscanners [8] widens the
possibilities for this to be used on consumer level. However, even when recording EEG under ideal conditions in a
lab, emotional ERP responses are defined by only slight µV
changes within single electrodes that would remain highly
susceptible to noise in a mobile context. An example of this
can be seen in a recent EEG experiment using an auditory
oddball paradigm indicating that the P300 amplitudes might
be reduced by 35% when walking compared to sitting on
a chair [9]. In contrast, a similar mobile EEG experiment
recording P300 responses while standing, walking and running showed no difference in amplitudes [10] when applying
spatial filtering based on independent component analysis
ICA [11]. Although the rows in the matrix of EEG data
are initially defined by voltage differences measured over
time between each electrode and the reference channel, they
are based on ICA transformed into temporally independent
events that are spatially filtered from the channel data. While
aspects of volume conduction within the brain are not taken
into consideration, the ICA decomposition frequently results
in components that resemble neuroanatomical projections of
dipoles, which in turn reflect synchronous activity of local

field potentials projected throughout the scalp [12].
Hypothesizing that retrieval of affective responses (ERPs)
might be enhanced through spatial filtering, we compare a
standard EEG electrode-based analysis against an approach
based on independent component analysis (ICA). In the subsequent sections we outline the experimental methods, derived results and discuss our findings.
2. METHODS
In this study we used pictures from two studies, both showing successful discrimination of affective stimuli. One, based
on 96 stimuli from the International Affective Picture System
(IAPS) [13] consisting of 32 pleasant (mean valence/arousal
7.0/5.5), 32 neutral (mean valence/arousal 4.9/3.4), and 32
unpleasant (mean valence/ arousal 2.4/5.9) pictures1 . The
other study [14] was also based on stimuli from IAPS, consisting of 22 pleasant (mean valence/arousal 5.3/5.2), 22 neutral (mean valence/arousal 5.2/ 5.2) and 22 unpleasant (mean
valence/arousal 5.0/ 5.3)2 .
The stimuli were presented in a random order for 6 seconds in blocks of 5 pictures. Between blocks, the participant was presented with a pause screen and was instructed to
press enter when he was ready to continue the experiment. A
scrambled image was generated by randomising the stimuli
to achieve a mean luminosity equivalent to that found across
all pictures. The scrambled picture was displayed 3 seconds
before stimuli presentation, while the baseline was recorded.
After the stimuli presentation, a grey screen was presented for
3 seconds.
The subjects (four right-handed males) were seated in a
darkened, sound isolated room, at 50 cm distance from the
screen. All stimuli were presented in colour on a 19 inch
ViewSony GF90 screen. Prior to the experiment, the subjects
were instructed to avoid excessive blinking during the baseline and picture presentation, and to avoid muscle and jaw
tension and head movements.
EEG data was collected from 64 active AgCl electrodes
(placed according to the international 10-20 system) recorded
on a BioSemi ActiveTwo system at 512 Hz sampling frequency, while the impedance of each electrode was kept
1 IAPS stimuli numbers; pleasant: 2208, 2250, 2260, 2501, 2560, 2650,
4611, 4617, 4640, 4653, 4659, 4666, 4687, 4694, 5621, 8041, 8080, 8090,
8116, 8120, 8161, 8180, 8200, 8280, 8300, 8320, 8330, 8370, 8380, 8400,
8420, 8465; neutral: 2020, 2190, 2200, 2210, 2214, 2215, 2220, 2221, 2235,
2240, 2270, 2272, 2278, 2383, 2393, 2410, 2441, 2491, 2493, 2514, 2579,
2745.1, 2749, 2752, 2810, 2850, 2870, 2890, 3210, 5455, 7550, 9210; unpleasant: 2120, 2205, 2520, 2590, 2691, 2730, 2750, 2800, 3015, 3030,
3053, 3100, 3170, 3180, 3181, 3400, 3500, 3530, 3550, 6210, 6211, 6212,
6821, 6834, 6838, 9041, 9250, 9254, 9341, 9405, 9800, 9921
2 IAPS stimuli numbers; pleasant: 1660, 4656, 4534, 7460, 4680, 5300,
4520, 8460, 4641, 7430, 2208, 8210, 8501, 8080, 8185, 8370, 5910, 2340,
2057, 1710, 1750, 2050; neutral: 5972, 4230, 4004, 4000, 6930, 2661, 8060,
5920, 7010, 6150, 7217, 7002, 7050, 7006, 4800, 8260, 5731, 1121, 8311,
7600, 4598, 4571; unpleasant: 3130, 3100, 2800, 9570, 3261, 9810, 3400,
9500, 9560, 6260, 3160, 6610, 6561, 9520, 1274, 5971, 1052, 9102, 4302,
4770, 1930, 3250

below 50 kΩ. The data was referenced to two mastoid electrodes. After recording, the data was processed using the
EEGLAB plug-in for MATLAB (MathWorks, USA). The
data was filtered offline from 1 to 30 Hz and down sampled
to 128 Hz. Epochs of 3000ms (1000ms pre- and 2000ms post
onset) were obtained for each stimulus from the continuously
recorded EEG. The mean voltage of a 1000ms segment preceding picture onset was subtracted as the baseline. Then,
extreme noise from movement (muscle activity) or sensor
artefacts was controlled by rejecting epochs with large amplitudes and thresholding the data on all electrodes (except the
frontal electrodes) to +/- 150 µ V. This resulted in rejection of
30.3% of the epoch for one subject in contrast to rejection of
4.1% for the other subjects. Thus one subject was excluded
from further analysis. In the next step, statistically deviating sensors were replaced with spherical interpolation based
on the remaining sensors. Independent component analysis
(ICA) was used to find distinct source activity across electrodes, such as non-brain components from eye and muscle
activity. Vertical and horizontal eye movements were automatically removed using EyeCatch based on a library of over
half a million independent component scalp maps [15].
Traditionally emotional responses have been analysed
from ERPs by assessing the time windows of the relevant
components (P1, N1, P2, ENP, P300, LLP, slow wave, etc.)
from a Global Field Power plot. This was either followed
by inspection of individual sensor’s ERPs in order to access
which sensors should be included in the statistical analysis,
or by relying on sensor locations from previous studies for
the statistical analysis.
Thus we assessed the relevant time windows from a
Global Field Power plot, and used sensor locations identified
from scalp maps of relevant time windows for the statistical analysis. The statistical analysis consisted of one-way
repeated anova tests which compared how a within-subjects
experimental group performed in the three experimental conditions (neutral, pleasant and unpleasant). Thereby we compared whether the individual average of selected electrodes
for each condition differed significantly from the aggregate
mean across the experimental conditions.
Furthermore, some studies use methods for topographic
analysis such as Global Map Dissimilarity [16], principal
component analysis [4, 17] or minimum norm analysis [2].
Similarly we used machine learning methods to identify
common patterns of brain activity across the subjects, by
combining PCA, ICA and K-mean clustering, following the
standard EEGlab procedure [19].
We decomposed the EEG data into ICA scalp maps and
time series components, which were grouped by initially
reducing the dimensionality of the feature space to N=10
based on principal component analysis PCA [18], which as
a pre-clustering function computes a vector for each component to define normalized distances in a subspace representing the largest covariances within scalp maps and time

series changes in power. Subsequently we applied a K-means
algorithm (K=10) to cluster similar ICA components and
separate outliers that remain more than three standard deviations removed from any cluster centroid, thereby following
the standard EEGlab procedure [19].
3. RESULTS
The Global Field Power (plotted in Fig. 1) is the squared electrical activity over all sensors, trials, and subjects according
to the procedure of [2]. The following relevant time windows
are drawn based on the Global Field Power: P1 (75-125ms),
EPN (130-225ms), P3 (250-315ms) and LPP (325-525ms).

The scalp maps in Fig. 2 reveal sensors contributing to
the deviating activity shown in Global Field Power. In the
time window of P1, the activity is centered around the centro parietal area (Cz, CPz, CFz) and part of the occipital right
hemisphere (POz, PO4, Oz, O2 and PO8). Although there
is little consistency in which electrodes are examined across
studies, both [20, 21] examine sensors covering the occipital
lobe of an early effect. Thus sensors covering the centro parietal area and the right occipital hemisphere were examined
for an effect in P1 by an average EPR and this was followed
by a statistical anova analysis. Sensors in the temporal occipital area (P7, P8, POz, PO7, and PO8), corresponding to the
sensors used in [22, 21] were analysed for an effect within the
EPN time window. Likewise the sensors covering the centro
parietal (Cz, CPz, CFz) and the temporal parietal area (PO3,
PO4 PO7, PO8, P7 and P8) corresponding to the sensors used
in [22, 21] and the sensors Pz, P1, P2, P5, P6, PO3, PO4,
PO7, and PO8 corresponding to the sensors used in [20] were
analysed in regard to P3 and LPP respectively.
From the paired one-way anova test of each average ERP,
the only significant effect was found in the EPN time window,
see Fig. 3.

Fig. 1. The averaged global field power plotted for the three
conditions, neutral (green), pleasant (red), unpleasant (blue)
By plotting the activity of sensors in the corresponding
time windows, we used the following scalp plots to identify
the sensors of interest.

Fig. 3. An average ERP of channels covering the temporal occipital lobe (P7, P8, POz, PO7, and PO8) shows the three conditions pleasant (red), unpleasant (blue) and neutral (green).
A statistical one-way anova on each channel marks the significant areas (α=0.01) of the ERPs in grey.

Fig. 2. Scalp maps for the time periods P1, EPN, P3 and LPP
for each of the conditions (unpleasant, neutral and pleasant).

This showed a number of short significant time windows
at 133-164ms, 180-188ms and 211-219ms. The first significant time window distinguishes the neutral condition from
those elicited by pleasant pictures, whereas the amplitudes
of negative pictures are significantly lower in the second and
third windows. These findings are consistent with results
from [21], showing a significant effect for pleasant stimuli versus neutral and unpleasant in the early time window
(150-300ms).
From the 20 clustered ICA components, three clusters are
described in this paper (cluster 6, 10, and 12). These three
were shared among all subjects (indicating that these components could include more general processing). They were
also based on a relatively large amount of ICs and all of them

showed a significant difference between one or more conditions. Among these, only one cluster (cluster 6) showed a significant difference of conditions across both early (<300ms)
and late (>300ms) time windows from a unpaired balanced
one-way anova test. This suggests that cluster 6 represents
neural activity contributing to both early and late emotional
responses. The time course activations corresponding to the
averaged ICA scalp maps in clusters 10 and 12 represent only
short intervals, with a significant effect of condition limited
to late responses.
In Fig. 4, 5, and 6, the averaged ICA scalp maps and
corresponding ERP of clusters 6, 10 and 12 are shown.

Fig. 6. Cluster 12, based on PCA dimensionality reduction
and K-means clustering (K=10,σ=3) of 89 ICA scalp maps
found within all of the 12 sessions, with corresponding ERPs
for pleasant (red), unpleasant (blue) and neutral (green) images. Significant intervals for differentiating between the
emotions are indicated in grey.
in the early time window (133-148ms).
4. DISCUSSION

Fig. 4. Cluster 6, based on PCA dimensionality reduction and
K-means clustering (K=10,σ=3) of 97 ICA scalp maps found
within all of the 12 sessions, with corresponding ERPs for
pleasant (red), unpleasant (blue) and neutral (green) images.
Significant intervals for differentiating between the emotions
are indicated in grey.

Fig. 5. Cluster 10, based on PCA dimensionality reduction
and K-means clustering (K=10,σ=3) of 69 ICA scalp maps
found within all of the 12 sessions, with corresponding ERPs
for pleasant (red), unpleasant (blue) and neutral (green) images. Significant intervals for differentiating between the
emotions are indicated in grey.
From the ERPs we see a significant effect (α=0.01) of
conditions in short windows of both early (94-177ms and
133-148ms), medial (312-328ms) and late (445-484ms, 500570ms, 688-766ms, 797-828ms, 836-898ms and 922-953ms)
temporal responses. Within these time windows the significant effect is mainly between pleasant and neutral/unpleasant,
while we only see a significant effect of unpleasant pictures

Our retrieval of affective ERP components based on a standard EEG analysis shows that we can differentiate pleasant
from unpleasant and neutral images within three intervals
133-164ms, 180-188ms and 211-219ms. Even though this
corresponds well to earlier reports on N1 and EPN [4, 21],
we see no significant effect in the more pronounced P300 or
LPP from the basic ERP analysis. Although this could be
due to the low number of subjects (making it harder to reach
a significant level), it might also indicate that the later components are more sensitive to individual processing. Thus,
using P300 or LPP as a marker for real time classification of
emotions might require extensive training of the classifier to
accommodate the individual differences in late EPR oscillations. In contrast, using an earlier low-level marker such as
EPN has proved to be sensitive to physical stimulus factors
and indexes early sensory processing within the visual cortex.
However, a spatial filtering based on clustering ICA scalp
maps and time series indicates that we can identify multiple
early and late responses that are modulated by emotional
content. Here, cluster 6 based on 97 ICA scalp maps shows
an early difference at 130ms between pleasant compared to
unpleasant as well as neutral content. In later time windows
it additionally shows a difference between pleasant versus
unpleasant/neutral content at 300ms, 450-600ms and 700950ms. The cluster is characterized by a topology indicating
a right lateralized activation in the extrastriate visual cortex
and a polarity shift in the time series changes in power around
300ms. We found similar polarity shifts after 300ms in clusters 10 and 12 in our experiment, but here the significant
differences between emotional and neutral content were limited to short intervals between 450 and 900ms after picture
onset. However in clusters 10 and 12, based on 69 and 89

ICA scalp maps respectively, the corresponding time course
polarity shifts around 300ms resemble the early negativities
and late positivities reported earlier. In our experiments the
averaged ICA scalp topologies of local field potentials also
resembled these posterior early negativity and late positivity scalp distributions identified previously in affective ERP
responses [23].
Our results are in line with the findings in a recent
temporal-spatial PCA analysis of emotional ERP responses
[4]. Here, the analysis establishes that the overlapping latencies within early negativities and late positivities represent
distinct ERP components that reflect consecutive stages of
neural processing [4]. Within that study, virtual epochs were
extracted as described by the time course factor loadings for
pleasant, unpleasant and neutral images, indicating that a reduced set of principal components is modulated by emotional
content corresponding to the N1, EPN, P3 and LPP time windows. In our study, we also applied PCA as a preprocessing
step before clustering ICA scalp maps and their corresponding ERPs, and similarly found that the significant intervals
for differentiating between the pleasant and unpleasant images are within an early (<300ms) time window. In contrast,
components emerging later (>300ms) rather indicated the
difference between pleasant versus unpleasant and neutral,
as has previously been reported [17]. This confirms that the
early emotional ERP responses primarily capture the polarity
of valence, that is, whether the images represent something
pleasant or unpleasant. However, the later ERP responses
incorporate complementary aspects of arousal that characterize the intensity of the emotional involvement relative to a
neutral state [16].
In order to capture these components in even more noisy
environments, like when accessing audiovisual media in reallife usage scenarios, other types of spatial filtering might be
required to retrieve robust signals from EEG data, as exemplified by the real-time 3D source reconstruction used in the
Smartphone Brain Scanner open-source software project [24],
[8]. This would enable us to gain a more thorough understanding of the consecutive steps in affective responses, ranging from allocation of attentional resources and memory encoding to aspects of semantic processing. This in turn could
potentially lead to incorporating these components into next
generation cognitive interfaces capable of adapting the selection of content to our emotional responses.

5. REFERENCES
[1] Harald T Schupp, Markus Junghöfer, Almut I Weike,
and Alfons O Hamm, “The selective processing of
briefly presented affective pictures: an ERP analysis.,”
Psychophysiology, vol. 41, no. 3, pp. 441–9, May 2004.
[2] Andreas Keil, Margaret M Bradley, Olaf Hauk, Brigitte
Rockstroh, Thomas Elbert, and Peter J Lang, “Largescale neural correlates of affective picture processing.,”
Psychophysiology, vol. 39, no. 5, pp. 641–9, Sept. 2002.
[3] Isabel Taake, Fern Jaspers-Fayer, and Mario Liotti,
“Early frontal responses elicited by physical threat
words in an emotional stroop task: modulation by anxiety sensitivity,” Biological Psychology, vol. 81, pp. 48–
57, 2009.
[4] Dan Foti, Greg Hajcak, and Joseph Dien, “Differentiating neural responses to emotional pictures: Evidence
from temporal-spatial PCA,” Psychophysiology, vol. 46,
no. 3, pp. 521–530, May 2009.
[5] Lindsay Brown, Bernard Grundlehner, and Julien Penders, “Towards wireless emotional valence detection
from EEG.,” Conference proceedings : ... Annual
International Conference of the IEEE Engineering in
Medicine and Biology Society. IEEE Engineering in
Medicine and Biology Society. Conference, vol. 2011,
pp. 2188–91, Jan. 2011.
[6] Sander Koelstra, Ashkan Yazdani, Mohammad Soleymani, and C Mühl, “Single trial classification of EEG
and peripheral physiological signals for recognition of
emotions induced by music videos,” Brain informatics,
pp. 89–100, 2010.
[7] Christian Andreas Kothe, Scott Makeig, and Julie Anne
Onton, “Emotion Recognition from EEG during SelfPaced Emotional Imagery,” 2013 Humaine Association
Conference on Affective Computing and Intelligent Interaction, pp. 855–858, Sept. 2013.
[8] Arkadiusz Stopczynski, Carsten Stahlhut, Michael Kai
Petersen, Jakob Eg Larsen, Camilla Falk Jensen, Marieta Georgieva Ivanova, Tobias S Andersen, and Lars Kai
Hansen, “Smartphones as pocketable labs: Visions
for mobile brain imaging and neurofeedback.,” International journal of psychophysiology : official journal
of the International Organization of Psychophysiology,
vol. 91, no. 1, pp. 54–66, Aug. 2013.
[9] Maarten De Vos, Katharina Gandras, and Stefan
Debener, “Towards a truly mobile auditory braincomputer interface: exploring the p300 to take away,”
International Journal of Psychophysiology, vol. 91, pp.
46–53, 2014.

[10] Klaus Gramann, Joseph T. Gwin, Daniel P. Ferris,
Kelvin Oie, Tzyy-Ping Jung, Chin-Teng Lin, Lun-De
Liao, and Scott Makeig, “Cognition in action: imaging brain/body dynamics in mobile humans,” Reviews
in the Neurosciences, vol. 22, no. 6, pp. 593–608, 2011.
[11] Pierre Comon, “Independent component analysis, a new
concept?,” Signal processing, vol. 36, pp. 287–314,
1994.
[12] Arnaud Delorme and Scott Makeig, “EEGLAB: an open
source toolbox for analysis of single-trial EEG dynamics including independent component analysis.,” Journal of neuroscience methods, vol. 134, no. 1, pp. 9–21,
Mar. 2004.
[13] Margaret M Bradley and Peter J Lang, “The International Affective Picture System (IAPS) in the study of
emotion and attention,” Oxford University Press, , no.
Handbook of Emotion Elicitaton and Assessment, pp.
29–46, 2007.
[14] Jeff T Larsen, Catherine J Norris, and John T Cacioppo,
“Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator
supercilii.,” Psychophysiology, vol. 40, no. 5, pp. 776–
85, Sept. 2003.
[15] Nima Bigdely-Shamlo, Ken Kreutz-Delgado, Christian
Kothe, and Scott Makeig, “EyeCatch: Data-mining over
half a million EEG independent components to construct a fully-automated eye-component detector.,” Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology
Society. IEEE Engineering in Medicine and Biology Society. Conference, vol. 2013, pp. 5845–8, Jan. 2013.
[16] Lorena R R Gianotti, Pascal L Faber, Matthias Schuler,
Roberto D Pascual-Marqui, Kieko Kochi, and Dietrich
Lehmann, “First valence, then arousal: the temporal
dynamics of brain electric activity evoked by emotional
stimuli.,” Brain topography, vol. 20, no. 3, pp. 143–56,
Jan. 2008.
[17] Luis Carretié, José a Hinojosa, Sara López-Martı́n, and
Manuel Tapia, “An electrophysiological study on the
interaction between emotional content and spatial frequency of visual stimuli.,” Neuropsychologia, vol. 45,
no. 6, pp. 1187–95, Mar. 2007.
[18] Ian T. Jolliffe, Principal Component Analysis, Springer
Series in Statistics. Springer, 1986 (2002).
[19] Arnaud Delorme, Tim Mullen, Christian Kothe,
Zeynep Akalin Acar, Nima Bigdely-Shamlo, Andrey
Vankov, and Scott Makeig, “Eeglab, sift, nft, bcilab

and erica: new tools for advanced eeg processing,” Computational Intelligence and Neuroscience, vol.
doi:10.1155/2011/130714, pp. 1–12, 2011.
[20] Johanna Kissler, Cornelia Herbert, Irene Winkler, and
Markus Junghofer, “Emotion and attention in visual
word processing: an ERP study.,” Biological psychology, vol. 80, no. 1, pp. 75–83, Jan. 2009.
[21] Andrea De Cesarei and Maurizio Codispoti, “When
does size not matter? Effects of stimulus size on affective modulation.,” Psychophysiology, vol. 43, no. 2, pp.
207–15, Mar. 2006.
[22] Graham G Scott, Patrick J O’Donnell, Hartmut
Leuthold, and Sara C Sereno, “Early emotion word processing: evidence from event-related potentials.,” Biological psychology, vol. 80, no. 1, pp. 95–104, Jan.
2009.
[23] Jonas K Olofsson, Steven Nordin, Henrique Sequeira,
and John Polich, “Affective picture processing: an integrative review of ERP findings.,” Biological psychology,
vol. 77, no. 3, pp. 247–65, Mar. 2008.
[24] Arkadiusz Stopczynski, Carsten Stahlhut, Jakob Eg
Larsen, Michael Kai Petersen, and Lars Kai Hansen,
“The smartphone brain scanner: a portable real-time
neuroimaging system,” PLoS ONE, vol. 9, no. 2, pp.
1–10, 2014.

Appendix

C

Spatio temporal media
components for neurofeedback

Camilla Birgitte Falk Jensen, Michael Kai Petersen, Jakob Eg Larsen, Arkadiusz Stopczynski, Carsten Stahlhut, Marieta Georgieva Ivanova, Tobias Andersen, and Lars Kai Hansen. Spatio temporal media components for neurofeedback.
In Multimedia and Expo Workshops (ICMEW), 2013 IEEE International Conference on, pages 1–6. IEEE, 2013.

SPATIO TEMPORAL MEDIA COMPONENTS FOR NEUROFEEDBACK
Camilla B. Falk Jensen, Michael Kai Petersen, Jakob Eg Larsen, Arkadiuzs Stopczynski
Carsten Stahlhut, Marieta Giovani Ivanova, Tobias Andersen, Lars Kai Hansen
Technical University of Denmark
DTU Compute
DK-2800 Kgs. Lyngby
{cbfj, mkai, jaeg, arks, csta, mgiv, toban, lkai}@dtu.dk
ABSTRACT
A class of Brain Computer Interfaces (BCI) involves interfaces for neurofeedback training, where a user can learn
to self-regulate brain activity based on real-time feedback.
These particular interfaces are constructed from audio-visual
components and temporal settings, which appear to have a
strong influence on the ability to control brain activity. Therefore, identifying the different interface components and exploring their individual effects might be key for constructing new interfaces that support more efficient neurofeedback
training. We discuss experiments involving two different
designs of neurofeedback interfaces and suggest further research to clarify the influence of different audiovisual components and temporal settings on neurofeedback effect.
Index Terms— Interfaces design, BCI, Neurofeedback
Training, Audiovisual Components, User Experience
1. INTRODUCTION
Over the last couple of years the number of personal informatics apps has skyrocketed, allowing smartphone users to;
track their sleeping patterns by combining remembrance of
espressos past with built-in motion sensors 1 ; upload photos of their lunch for peer approval of the number of calories
consumed 2 ; or share their exercise progress when running
3
accompanied by minutely detailed monitoring of heartbeat
and respiration rates 4 . When it comes to personal informatics apps for monitoring mental state, mobile interfaces have
largely been limited to subjective measurements of mood with
e.g. MoodPanda 5 or measuring brain agility with standard
cognitive tasks using e.g. Quantified Mind6 . However the recently launched smartphone brainscanner open-source project
1 sleepcycle.com.

2 https://itunes.apple.com/en/app/meal-snap-caloriecounting/id425203142?mt=8
3 endomondo.com.
4 http://www.zephyr-technology.com/products/bioharness-3/
5 moodpanda.com.
6 qualified-mind.com

7

[1], enables continuous monitoring of cortical activity in an
everyday context. The smartphone brainscanner combines a
wireless EEG neuroheadset with a smartphone which allows
for real-time 3D brain imaging [2] as well as altering brain
activity on the basis of neurofeedback. With neurofeedback
training, the individual can learn to increase or decrease the
activity of particular brainwave frequencies on the basis of
real-time feedback.
These types of brain machine apps will require novel approaches to the design of multimedia interfaces, as the user is
no longer simply passively accessing a history of self-tracking
events. Instead the user is actively carrying out a training session by generating data in real time by interacting with audiovisual representations of brainwave activity using a mobile
device.
In the following sections we outline existing neurofeedback training paradigms, and extract the underlying audiovisual components from the interfaces. We propose a framework to describe these interface components as part of a feedback loop, serving as a basis for designing neurofeedback
interfaces. This work serves as a first step towards understanding the effect of interface components on neurofeedback
training. Likewise we conduct an experiment with two different interfaces, illustrating an effect on upper alpha activity
depending on the feedback interface.
2. TRAINING FREQUENCY BANDS
Training neural activity by neurofeedback training has been
applied on various frequency bands. E.g. decreasing theta
(4-7Hz) and increasing beta (15-18 Hz) and cortical sensorymotor rhythms (12-15 Hz) amplitudes have been applied as
treatment for children with Attention Deficit Hyperactivity
Disorder (ADHD) [3]. Increasing the theta-to-alpha ratio
has shown an increase in artistic performance among musicians [4]. When it comes to alpha band activity, this has
recently been associated with a more basic cognitive process [5].
7 https://github.com/SmartphoneBrainScanner

An increase of alpha brainwave (8-12Hz) activity is typically associated with inhibition of neural activity, and is
referred to as event-related synchronisation. Event-related
alpha synchronization may block information processing in
task-irrelevant areas of the brain like the occipital cortex when
closing our eyes [5]. By inhibiting task-irrelevant activity, the
decrease in alpha activity improves the signal-to-noise ratio.
In this sense inhibition is an active process for information
processing [5]. Regarding the upper alpha band (10-12 Hz), it
has been shown that amplitude/power increases as a function
of memory load during the retention period in a memory test.
This probably reflects the effort of keeping a growing number of items in short-term memory [6]. In contrast, a decrease
in alpha activity (referred to as event-related desynchronisation) in the upper alpha band (10-12Hz) is typically observed
during actual retrieval of semantic information. The magnitude reflects cortical activation [7], meaning that excitation of
neurons increases the more well integrated the information is
[8]. Considering the distribution of brainwave frequencies, an
alpha mean frequency of 10 Hz can be interpreted as the center around which the neighboring brainwave bands of delta,
theta, beta and gamma constitute the harmonics at 2.5, 5, 20
and 40 Hz, respectively. Such a phase-coupling of, e.g. alpha and beta brainwave activity, is often observed, but if the
power is shifted towards the upper 12.4 Hz or lower 8.4 Hz
range, the alpha activity becomes maximally decoupled as the
frequency ratios between brainwave bands approach an irrational number and oscillations no longer synchronize [9] [5].
Thus neurofeedback experiments aiming to increase
power in the upper alpha range represent a decoupling of alpha activity from the surrounding frequency bands. Therefore
any improvement in cognitive performance could reflect an
association between higher alpha frequency and good memory performance which has previously been shown [8]. However, when it comes to the design of neurofeedback interfaces, they are often conceptualised with little attention to
how the actual feedback of audiovisual elements might affect
the user’s ability to control brain activity.
3. AUDIOVISUAL COMPONENTS
Irrespective of whether the training aims at increasing or decreasing one or multiple types of brain waves, it has commonly been represented by a simple scaling of visual components; like the height of a bar and a color change, or audio components; like high and low pitch tones and a volume
change. The most common auditive component used for auditive or audiovisual feedback is the pitch of a sound [10, 11, 4].
A short high-pitched or low-pitched sound indicates that the
increase or decrease in brain activity has reached a threshold.
Thus the pitch often represents a successful or failed trial. Although pitch is the most common auditive component used in
neurofeedback training, other more continuous sounds could
be used with components such as rhythm and volume as indi-

Fig. 1: In this interface gray indicates brain activity equal to
an average activity (baseline), and gradually turns blue when
activity decreases and red when activity increases.
cators.
In visual interfaces there is more variety in the use of
components. Some studies use simple interfaces where e.g.
a square changes color gradually from blue to gray and red
indicating activity below, equal to and above an average baseline activity [12] (see Figure 1). This gradual color change
from blue to red can be associated with cold and warm temperatures. Other interfaces indicate increases in activity by
altering the size of a graphical element, such as the height
of a bar [13], which resembles a vertical scale like a tuning
bar. Another method is moving an object on a vertical or horizontal axis [14], resembling a high or low activity. These
interfaces all consist of geometric primitives (e.g. a square),
altered by another component such as color, size or the spacial
distribution within the screen.
Besides representing brain activity, components might be
added to create an atmosphere or simply to make the training more interesting. An example of this is the neurofeedback study of Egner et al. [4] using an auditive feedback to
increase the alpha/theta ratio. A background sound resembling either a ’babbling brook’ or ’ocean waves’ was used
to indicate a relative increase of alpha and theta activity respectively. Additionally, a high-pitched or low-pitched gong
sound would be executed when the activity exceeded a pre-set
threshold of alpha and theta, respectively. The subjects aimed
to increase the amount of theta sound representation. Thus
the background sound would not only indicate the relative activity but also contribute to achieving a mental state, whereas
the gong sound would primarily indicate reaching a ’significant’ activity level; a ’success’/’fail’ scenario. Another example is the visual interface used for neurofeedback training
of children with ADHD in Heinrich et al. [15]. Here, the aim
is to make a famous German cartoon mouse do a pole-vault.
This is achieved when a threshold is met; indicated by a pole
changing color from white to blue or red to show a relative
decrease or increase, respectively. In this interface, the color
change of the pole is the main indicator of the brain activity, much like the interface with a blue/gray/red square mentioned above. However, the cartoon mouse creates a ’story’
supporting the training, and might encourage imagination or
trigger-related memories.

Thus these game-like interfaces are basically constructed
from a combination of the same simple components and substitute generic primitives with more game-like features such
as airplanes [15] or pacmans [3]. 3D games have also been
developed, where high or low activity is represented by e.g.
the speed of a racing car or a dancing robot [16]. These 3D
interfaces can be described as a combination of multiple visual components. A car speeding up is created by changing
the size and spatial distribution of the surrounding elements at
a faster frequency. These game-like interfaces are commonly
used in clinics for treatment of e.g. ADHD. Whereas the more
primitive interfaces typically occur in scientific settings.
Summing up, we have identified the following audiovisual components, which in combination can describe any interface. All of these change with the EEG sampling frequency
and the screen update frequency.
Auditive components include:
• pitch (low, high)
• volume (soft, loud)
• timbre (dark, light)
• duration (short, long)
• rhythm (temporal distribution)
Visual components include:
• geometric primitives (connected segments)
• color (discrete, gradients)
• size (proximity, scalability)
• movement (horizontal, vertical)
• composition (spatial distribution)
Consequently, when designing new neurofeedback interfaces, focus should not only be on how to visualize an increase or decrease of brain activity, but should consider how
components (and combinations of these) might affect the
user’s imagination, or trigger-related memories, etc. Furthermore, no studies have examined which components are in reality causing the greatest effect.
4. TEMPORAL ASPECTS OF NEUROFEEDBACK
The importance of these interface components is due to the
real-time feedback, which creates a tight coupling between
the visual interface and the brain activity. To understand this
relationship and aspects of real-time feedback, we illustrate
how a neurofeedback application is constructed, see Figure 2.
Neurofeedback applications consist of four stages, which
constitute the feedback loop:
Preparation deciding which brainwaves to train and from
which area
EEG data acquisition collecting data on cortical activity

Fig. 2: An illustration of the stages represented in neurofeedback applications, with a loop connecting the data acquisition, interface and response stage. Varying the time span may
cause a measurable effect, from changes in plasticity of neural
networks [17], to behavioral and neuro-psychological [18].
Interface visualizing the relative cortical activity (increase
or decrease) enabling reflection
Response changing behavior or mental strategy
Effect all of which might lead to a long-term effect on cognitive, physiological or behavioral measures.
The loop between data collection, visualization and response is completed in milliseconds, creating the real-time
feedback loop between brain activity and visualization. Since
changes in brain activity are monitored at the level of milliseconds, it allows the user to instantly change mental states
on the basis of real-time feedback. Thus this time span should
be considered when developing neurofeedback applications.
Repeating this loop several times creates a continuous representation of brain activity during the time span of seconds,
minutes or hours. Within this time span the user can explore
the effect of different strategies of thoughts by receiving and
reflecting on the information provided by the instant real-time
feedback.
5. TESTING TWO NEUROFEEDBACK INTERFACES
To explore the influence components might have on the efficacy of neurofeedback training, we test two different interfaces; one indicating brain activity based on only a color component (a blue/gray/red square) and the second interface using
multiple components as indicators.
5.1. Experimental Design
We conducted an experiment with 25 subjects aiming at increasing their upper alpha frequency band (10-12Hz) using
one of the two different neurofeedback interfaces. The first
group of 12 healthy subjects (7 males and 5 females) with
an age average of 23.6 ±1.9 did neurofeedback training on a
replication of an existing interface [12]. The interface is similar to that mentioned earlier, consisting of a single square,
with the color being the only component changing. The color

Fig. 3: The second neurofeedback interface where small
squares colored from blue to red that build up columns along
a horizontal temporal plane represent the user’s ability to increase upper alpha brainwave activity.

Fig. 4: The color of the squares indicates the intensity of the
brain activity
changed gradually from blue to gray and red when the brain
activity was below, equal to and above baseline (an average
relaxed brain activity), respectively (see Figure 1). The subjects were instructed to make the square turn red. During the
baseline recording, the subjects were presented with an interface similar to the training interface, however the input values
were randomly generated. The subjects were instructed to
only focus on counting the number of times the square turned
red. This simple cognitive task would help prevent their mind
to wander off, and would make the recording more compatible across subjects. In addition they were told to avoid any
muscle tension and jaw clinching as well as to reduce eye
blinks to a minimum during all recordings.
The second group of 13 healthy subjects (7 male and 6
female) with an age average of 26.6 ± 5.5 did neurofeedback training using an interface consisting of multiple parameters. A pattern of small squares was generated once
a second, if the alpha activity exceeded the baseline activity. These squares created a column of squares, incrementally
generated along a horizontal axis. This formed a virtual timeline of columns illustrating the brain activity over a 5-minute
training session, where the color of the squares changed continuously according to the attained power measure calculated
from a running mean of 2 seconds (see Figure 3). A small
increase above baseline resulted in a dark blue square, which
turned lighter and gradually changed from yellow to orange,
reflecting the magnitude of the attained power measured using EEG (see Figure 4). Thus this interface consisted of the

following components: a composition of geometrical primitives (formed as squares), changing colors gradually within a
color range (see Figure 4) and the spatial distribution along
a horizontal and vertical axis. The subjects were instructed
to generate as many squares as possible within the columns
and preferably make them turn yellow or orange. During the
baseline recording the subjects were presented with an interface similar to the training interface, however the input values were randomly generated. The subjects were instructed
to only focus on counting the number of times the squares
turned orange. This simple cognitive task would help prevent
their mind from wandering off, and would make the recording
more compatible across subjects. In addition they were told
to avoid any muscle tension and jaw clinching, as well as to
reduce eye blinks to a minimum during all recordings.
The training of both groups consisted of five sessions during one week from Monday to Friday. Each session started
and ended with a 5-minute baseline recording, and in between
five, 5-minute training sessions were conducted. Each subject received a total of 25 training recordings and 10 baseline
recordings.
To record the brain activity the Smartphone brainscanner
was used, transmitting EEG signals to a USB-dongle connected to a tablet. The setup had a sampling frequency of 128
Hz with feedback every 125 ms, giving the user an impression of live feedback. The Emotiv neuroheadset used in the
experiment consisted of 16 electrodes, including 2 reference
electrodes (P3/P4). The remaining electrodes were positioned
at AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8,
AF4 according to the international 10-20 system. The live
feedback was constructed from O1 and O2.
5.2. Data Analysis
For the analysis we were interested in cortical information
from the occipital lobe covered by O3, O4, P3 and P4. Since
P3 and P4 were reference electrodes, the signal could be
reconstructed by re-referencing. Thus the signal from the
frontal electrodes AF3, AF4, F3 and F4 (furthest away from
the area of interest) was averaged and referenced to P3 and
P4, allowing P3 and P4 to be included in the data processing.
This and all the following data processing was done offline
using EEGLAB [19]. The raw signal from the electrodes was
transformed into a power spectrum using Fast Fourier Transformation.
With the alpha frequency band varying with age, possible
neurological diseases and memory performance [8], the upper
alpha frequency band had to be determined for each individual. By identifying the peak in the power spectrum (the individual alpha peak (IAF)), the upper alpha frequency band was
set as a band of 2 Hz above IAF (from IAF to IAF+2Hz). The
upper alpha frequency band was determined for each subject
by the first baseline recording of each session. The mean amplitude of the upper alpha band was calculated for all baseline

Fig. 5: A comparison of the first interface (plotted in red) and
second interface (plotted in green) shows a significant difference in the users’ ability to increase upper alpha brain activity
when using the second interface for training. In contrast nonresponders experience a slight decrease. The bars indicate the
standard error of the mean.

and training recordings using a fast Fourier transformation.
However two subjects (1 male and 1 female) from the first
iteration of the experiment did not complete all the training
sessions, and were therefore excluded from further analysis.
In addition it has repeatedly been reported that some subjects,
referred to as non-responders, are unable to change amplitudes of the brain frequencies significantly [3, 14, 20, 12].
The reasons for this have not been determined, but might be
due to pure physiological reasons [14] such as the thickness
of the skull. Thus subjects who did not show a significant increase between the very first baseline (baseline 1 in session
1) and the training recordings from the last session (Friday)
were considered non-responders. As a result, 3 subjects (2
female, 1 male) from the first iteration and another 3 subjects
(2 male, 1 female) from the second iteration were considered
non-responders and where excluded from the statistical analysis.
Each subject’s EEG results from all recordings was normalized in respect to the first baseline Monday (session 1),
thereby showing the ability to increase UA amplitudes in relation to the first baseline in percentage. The average results
of the two groups have been plotted in Figure 5. The results
show a steady increase in the baselines (gray dots) for both
groups, which is similar to the results from Zoefel et al. [12].
In contrast the non-responders (blue) experience a slight decrease in baseline activity. Furthermore the plot shows a clear
increase in brain activity during the training recordings of
subjects of the second iteration (green) compared to the subjects of the first iteration (red).
From the results above a paired, one-tailed t-test was con-

ducted for both the first and the second group, testing the
effect of training (difference between the very first baseline
and the very last baseline) on the responders. Both interfaces showed a significant increase from the first baseline of
the first session (Monday) to the first baseline of the last session (Friday), with t(6)=4.46, p=0.002 and t(9)=3.47, p=0.003
for the first and the second group, respectively. To compare
the results of the two groups using different interfaces, we
conducted 2, two-sample t-tests (assuming unequal variance);
one testing the difference in training effect; the other testing
the difference in feedback effect. The tests showed no significant difference between the two interfaces in training effect
(t(14)=0.66, p=0.522. In contrast the tests did show a significantly greater effect of feedback (the difference between
the average baseline and average training recording across all
sessions) for the second group (t(14)=2.70, p=0.020).
This suggests that neurofeedback interfaces should not
only relay the attained power in specific brain wave frequency
bands, but also take into consideration how different interface
components affect the user interaction. Thus we suggest investigating the efficiency of the individual visual components
in order to combine them into supportive interfaces.
6. CONCLUSION
In contrast to many traditional media interfaces, the audiovisual components and the temporal aspects of neurofeedback interfaces have a direct influence on training abilities.
Thus, considering how the user responds to the audiovisual
representation (the user experience) should be included when
designing neurofeedback interfaces. This calls for more research on the influence of specific audiovisual components
and the temporal aspect. This could be done by either constructing a thorough bottom-up analysis of the individual
components; or by a top-down meta-analysis of previous
studies, exploring best-practice combinations of components,
which have led to successful neurofeedback training.
7. ACKNOWLEDGMENTS
This work is supported in part by Danish Lundbeck Foundation through Center for Integrated Molecular Brain Imaging
(CIMBI).
8. REFERENCES
[1] Jakob Eg Larsen, Arkadiusz Stopczynski, Carsten
Stahlhut, Michael Kai Petersen, and Lars Kai Hansen,
“A Cross-Platform Smartphone Brain Scanner,” in
CHI’12, Austin, Texas, USA, 2012.
[2] Carsten Stahlhut, Hagai T Attias, Arkadiusz Stopczynski, Michael K Petersen, Jakob E Larsen, and Lars K
Hansen, “An Evaluation of EEG Scanner s Dependence

on the Imaging Technique , Forward Model Computation Method , and Array Dimensionality,” Accepted at
EMBC’2012 conference in San Diego, 2012.
[3] Thomas Fuchs, Niels Birbaumer, Werner Lutzenberger,
John H Gruzelier, and Jochen Kaiser, “Neurofeedback
treatment for attention-deficit/hyperactivity disorder in
children: a comparison with methylphenidate.,” Applied
psychophysiology and biofeedback, vol. 28, no. 1, pp. 1–
12, Mar. 2003.
[4] Tobias Egner, T F Zech, and J H Gruzelier, “The effects of neurofeedback training on the spectral topography of the electroencephalogram.,” Clinical neurophysiology : official journal of the International Federation
of Clinical Neurophysiology, vol. 115, no. 11, pp. 2452–
60, Nov. 2004.
[5] Wolfgang Klimesh, “Alpha band oscillations attention
and controlled access to stored information,” Trends in
Cognitive Sciences, vol. 16, no. 12, pp. 608–617, 2012.
[6] Ole Jensen, Jack Gelfand, John Kounios, and John E.
Lisman, “Oscillations in the alpha band (9-12hz) increase with memory load during retention in a short term
memory task,” Cerebral Cortex, pp. 877–882, August
2002.
[7] Gert Pfurtscheller and FH Lopes da Silva, “Eventrelated eeg/meg synchronization and desynchronization:
basic principles,” Clinical neurophysiology, vol. 110,
no. 11, pp. 1842–1857, 1999.
[8] W Klimesch, “EEG alpha and theta oscillations reflect
cognitive and memory performance: a review and analysis.,” Brain research. Brain research reviews, vol. 29,
no. 2-3, pp. 169–95, Apr. 1999.
[9] Belinda Pletzer, Hubert Kerschbaum, and Wolfgang
Klimesh, “When frequencies never synchronize: the
golden mean an the resting eeg,” Brain Research, vol.
1335, no. 91-102, 2010.
[10] Thilo Hinterberger, Nicola Neumann, Mirko Pham, Andrea Kübler, Anke Grether, Nadine Hofmayer, Barbara
Wilhelm, Herta Flor, and Niels Birbaumer, “A multimodal brain-based feedback and communication system.,” Experimental brain research. Experimentelle
Hirnforschung. Expérimentation cérébrale, vol. 154,
no. 4, pp. 521–6, Feb. 2004.
[11] Nicola Neumann, Andrea Kübler, Jochen Kaiser, Thilo
Hinterberger, and Niels Birbaumer, “Conscious perception of brain states: mental strategies for brain-computer
communication.,” Neuropsychologia, vol. 41, no. 8, pp.
1028–36, Jan. 2003.

[12] Benedikt Zoefel, René J Huster, and Christoph S Herrmann, “Neurofeedback training of the upper alpha frequency band in EEG improves cognitive performance.,”
NeuroImage, vol. 54, no. 2, pp. 1427–31, Jan. 2011.
[13] David Vernon, Tobias Egner, Nick Cooper, Theresa
Compton, Claire Neilands, Amna Sheri, and John
Gruzelier, “The effect of training distinct neurofeedback
protocols on aspects of cognitive performance.,” International journal of psychophysiology : official journal
of the International Organization of Psychophysiology,
vol. 47, no. 1, pp. 75–85, Jan. 2003.
[14] Holger Gevensleben, Birgit Holl, Björn Albrecht, Claudia Vogel, Dieter Schlamp, Oliver Kratz, Petra Studer,
Aribert Rothenberger, Gunther H Moll, and Hartmut
Heinrich, “Is neurofeedback an efficacious treatment for
ADHD? A randomised controlled clinical trial.,” Journal of child psychology and psychiatry, and allied disciplines, vol. 50, no. 7, pp. 780–9, July 2009.
[15] Hartmut Heinrich, Holger Gevensleben, and Ute Strehl,
“Annotation: neurofeedback - train your brain to train
behaviour.,” Journal of child psychology and psychiatry, and allied disciplines, vol. 48, no. 1, pp. 3–16, Jan.
2007.
[16] Qiang Wang, Olga Sourina, and Minh Khoa Nguyen,
“Fractal dimension based neurofeedback in serious
games,” The Visual Computer, vol. 27, no. 4, pp. 299–
309, Feb. 2011.
[17] Tomas Ros, Jean Théberge, Paul a. Frewen, Rosemarie Kluetsch, Maria Densmore, Vince D. Calhoun,
and Ruth a. Lanius, “Mind over chatter: Plastic upregulation of the fMRI salience network directly after
EEG neurofeedback,” NeuroImage, Sept. 2012.
[18] Hartmut Heinrich and Holger Gevensleben,
“Training of slow cortical potentials in attentiondeficit/hyperactivity disorder: evidence for positive
behavioral and neurophysiological effects,” Biological
Psychiatry, pp. 11–14, 2004.
[19] Arnaud Delorme and Scott Makeig, “Eeglab: an open
source toolbox for analysis of single-trial eeg dynamics
including independent component analysis,” Journal of
Neuroscience Methods, vol. 134, no. 1, pp. 9–21, 2004.
[20] J F Lubar, M O Swartwood, J N Swartwood, and P H
O’Donnell, “Evaluation of the effectiveness of EEG
neurofeedback training for ADHD in a clinical setting
as measured by changes in T.O.V.A. scores, behavioral
ratings, and WISC-R performance.,” Biofeedback and
self-regulation, vol. 20, no. 1, pp. 83–99, Mar. 1995.

Appendix

D

SOA thresholds for the
perception of
discrete/continuous tactile
stimulation

Mohamad Eid, Georgios Korres, and Camilla Birgitte Falk Jensen. Soa
thresholds for the perception of discrete/continuous tactile stimulation.
In
PQuality of Multimedia Experience (QoMEX), 2015 Seventh International Workshop on. IEEE, 2015.

SOA Thresholds for the Perception of
Discrete/Continuous Tactile Stimulation
Mohamad Eid and Georgios Korres

Camilla Birgitte Falk Jensen

Applied Interactive Multimedia Research Lab (AIMlab)
Division of Engineering
New York University Abu Dhabi
Abu Dhabi, United Arab Emirates
E-mail: {george.korres, mohamad.eid}@nyu.edu

Cognitve Systems
Technical University of Denmark,
2800 Kgs. Lyngby, Denmark
E-mail: cbfj@dtu.dk.

Abstract— In this paper we present an experiment to measure
the upper and lower thresholds of the Stimulus Onset
Asynchrony (SOA) for continuous/discrete apparent haptic
motion. We focus on three stimulation parameters: the burst
duration, the SOA time, and the distance between successive
actuation points. The experimental setup is based on a set of six
(6) vibrotactile actuators to investigate effects of the distance
between successive actuation points (over the range 4 cm to 20
cm) on the respective SOA thresholds. We found that as the burst
duration increases, subjects detected the simultaneous-discrete
boundary at decreasing SOA. Furthermore, we found that the
larger the inter-actuator distance, the more linear the
relationship between the burst duration and SOA. Finally, the
large range between lower and upper thresholds for SOA can be
utilized to create continuous movement stimulation on the skin at
“varying speeds”. The results are discussed in reference to
designing a tactile interface for providing continuous haptic
motion with a desired speed of continuous tactile stimulation.
Keywords— Funneling Illusion; Vibrotactile Feedback; Tactile
Interface Design, Haptics Technologies.

I.

INTRODUCTION

An increased demand for computer-mediated interpersonal
interaction has spurred research to improve the sense of touch
using haptic devices (both kinesthetic and tactile) [1]. Tactile
sensation provides a very powerful means of communication in
our daily interactions, in particular to learn about the physical
characteristics of the ambient world. For instance, it enables us
to explore surfaces and textures, and in several cases develop
personal and emotional connections with objects and
individuals we touch. Furthermore, tactile stimulation can be
used as a substitute modality for interaction with people with
visual impairment or blindness [33].
The spectrum for vibrotactile stimulation applications is
wide, and ranges from training and education [17] to health
care [18-19], social interaction [20], and gaming and
entertainment [21-22]. For instance, the authors in [23] provide
the musicians with vibrotactile feedback about their bowing
and posture using vibration motors that are positioned on their
arms and torso. A ubiquitous wearable vibrotactile belt is
proposed to support a novel application of teaching participants
choreographed dance [17]. In the medical field, a
vibrating tactile probe is presented [18] that can be used to
distinguish among different materials. The sensing capabilities

could be exploited in various biomedical applications, such as
catheterism and surgical resection of tumors.
Tactile devices are primarily composed of an array of
actuators where multiple actuation technologies can be utilized,
such as vibrotactile motors, electrodes, piezoelectric ceramics,
pneumatic tubes, shape memory alloys, electromechanical
actuators (voice coils). In general, tactile arrays are of low
resolution due to the size of the actuators and the coarse
sensitivity of two-point limen on the skin [8]. Therefore, in
order to provide a more apparent and subtle information with
low spatial resolution devices, such as continuous sensation,
researchers investigated the display of detailed shapes using
human sensory illusions [9-10].
Various sensations can be induced via tactile stimulation,
such as hardness and softness, roughness of a surface,
vibration, and warmth and cold [2]. Furthermore, there is no
doubt that complex emotional sensations such as the
pleasantness are associated with tactile sensations [3]. These
complex sensations would be derived from the dynamic
spatial-temporal patterns of tactile stimulations.
Funneling [4] and saltation [5] are well-known illusory
feedback techniques associated with tactile stimulation.
Funneling refers to stimulating the skin at two distinct points
with different amplitudes and eliciting tactile sensation in the
somewhere in between [4]. On the other hand, saltation
involves stimulating the skin at two locations with proper
actuation overlap time to give a perceived stimulation in
between. Varying the amplitude (funneling) and/or time
interval (saltation) between two adjoining actuators to give
continuous tactile stimulation has been a subject for research
for a few years [6, 7].
The authors in [35] demonstrated that the variables
producing robust apparent tactile motion are the stimuli
duration and the stimulus onset asynchrony (SOA). The SOA is
defined as the time between onsets of subsequent actuations
[35]. In this paper, we develop a wearable tactile display using
low-resolution vibrotactile actuators to study the upper and
lower thresholds of the SOA based on a known human sensory
phenomenon called the “funneling illusion” [11]. The paper
presents an experimental study to determine the upper and
lower thresholds of the SOA to achieve continuous/discrete
tactile movement using discrete actuators. We investigate the
effects of spatial and temporal attributes to obtain a smooth,

continuous movement sensation. Moving tactile sensation is a
common and effective way to communicate, express, alert and
direct user’s attention [12].
The remainder of the paper is organized as follows: In
section II we present an overview of the related studies on
human tactile stimulation. Section III introduces the
psychophysical experiment setup, apparatus, procedure, and
implementation details for the tactile armband device. In
section IV, we present our results and a discussion about the
interesting findings. Finally, in section V we summarize the
paper findings and provide perspectives for future work.
II.

RELATED WORK

The concept of tactile stimulation has been around for
decades, since Geldard [13] in the 1950s studied the ability of
the skin to make temporal and spatial discriminations close to
those achieved by eyes and ears, and highlighted the
underutilization of tactile channel for presenting information.
More recently, there has been an increasing interest in
developing tactile displays in multimodal interfaces (such as
[14-16]).
Research in developing high-resolution tactile displays has
an immense historic background. Early studies reported that
the intensity of tactile stimulation can be varied linearly and
logarithmically to provide continuous vibrotactile motion [24].
Tan et al. developed a 3x3 vibrotactile array to study the
impact of a sensory illusion called “sensory saltation” to be
used as a haptic navigation system to convey directional
information on the back of a human body [25]. Tactile display
devices may also be used to represent text or images on the
skin with high granularity [26]. However these devices are still
in their infancy due to the poor and limited resolution of tactile
stimulation, as well as their high cost, maintenance, bulkiness
and power requirements.
Researchers are conducting psychophysical experiments in
order to investigate the spatial/temporal resolution of
vibrotactile stimuli at various parts of the human body (such as
the forearm [27], the back [28], and the neck [29]). For
instance, the authors in [27] evaluated 13 test items of display
methods, namely stationary and moving vibrotactile stimuli.
Both display methods show best localization accuracy in the
vicinity of the joints (elbow and wrist), followed by the
locations of the actuators themselves.

[14]. In one experiment, they measured the range of SOA for
straight-line apparent motion on the skin in relation to variation
in frequency, duration and direction of stimulation [25]. Based
on the measurements, a model that related the perceived
illusory motions to the stimulation parameters was developed.
The work was extended in [28] to vary frequency, intensity,
duration, and body site.
Designing tactile displays based on apparent tactile motion
is challenged by the fact that there is insufficient understanding
of the parameter space where the motion exists. Previous
studies have focused on identifying variables that control the
illusion by demonstrating various control values [34], and the
design of control algorithms that are based on psychological
modeling of apparent tactile motion [28]. In this paper, our
goal is to extend the knowledge about such parameters by
measuring the lower and upper thresholds of SOA timing for
continuous/discrete apparent tactile motion.
III.

METHOD

A. Participants
Ten voluntary participants, 5 female and 5 male, took part
in the experiment (mean age 27.4 years, SD = 6.97). All the
participants had a normal sense of touch by their own report
and were naïve with respect to tactile stimulation display
devices.
B. Hardware Platform and Software Interface
A hardware platform was developed to stimulate a series of
six vibrating points on the skin of the forearm using
vibrotactile actuators. The Vibrotactile Actuators are controlled
by a microprocessor that receives actuation patterns from the
computer and generates actuation signals that control a driving
circuit, which eventually feeds the vibrotactile motors with
appropriate current/voltage. The actuators can be driven
separately, simultaneously or in sequence. A snapshot of the
experimental setup is shown in Fig. 1.

Researchers have also focused on the perceptual illusions of
vibrotactile movement such as the research presented in [1516, 30]. For instance, Bonanni et al. [31] used a static vibration
method to provide affectionate touch via illusory vibrotactile
movement. Patterns of movement that resemble the act of
smoothing on the skin were created and tested.
Vibrotactile movement was explored by comparing three
different presentation methods, namely saltation, amplitude
modulation, and a hybrid condition [32]. Results demonstrated
that modulation method was rated significantly more
continuous and pleasant, and less arousing compared to both
the saltation and the hybrid methods. Another effort to measure
the control parameter space of apparent haptic motion using a
variety of stimulation attributes and body sites is presented in

Fig. 1 Experimental setup.

The armband providing the vibrotactile stimulus was
constructed with six actuators (Pico Vibe 310-177, Precision
Microdrives) vibrating with a frequency of 700 rpm at the
minimum intensity 0.25g, and 1400 rpm at the maximum
intensity of 1.75g. They were placed at a distance of 4 cm from
center to center of the actuators. A snapshot of the actuators
configuration in the armband device is shown in Fig. 2.

handed, the actuators were place around the left wrist. While
the experiments took place the subjects were listening to pink
noise in order to mask any noise from actuators.
The experiment condition was tested in two threshold
cases: (1) Lower threshold of SOA: the onset threshold
between the total simultaneity and apparent movement of
stimulation points, and (2) Upper threshold of SOA: the onset
threshold between the total discreteness of stimulation points
and apparent movement of stimulation points.
We utilized a one-interval, two-alternatives forced-choice
(1I-2AFC) paradigm combined with one-up one-down adaptive
procedure to determine the upper and lower thresholds of SOA,
as used in [28, 14].

Fig 2: Armband device configuration.
C. Stimuli
The stimuli were designed to be generally favorable for the
vibrotactile movement on the forearm region. To keep the
length of the experiment at a reasonable level, five different
burst sequence patterns were used: 11, 101, 1001, 10001, and
100001 (shown in Fig. 3), which are equivalent to physical
distances of 4 cm, 8 cm, 12 cm, 16 cm, and 20 cm respectively
between successive actuation points. Furthermore, three burst
actuation durations, 120ms, 180ms, and 240ms, were used to
design the stimuli. All fifteen possible combinations of these
two parameters were used to stimulate the participants’
forearm. The movement of the stimuli traveled between the
elbow and the wrist.

For the runs determining the upper threshold of SOA, the
start value of SOA was selected large enough so that the
participant clearly feel independent stimulation points. In every
trial participant was asked if s/he could feel individual
“discrete” actuators. They responded by pressing a button
marked “yes” or “no” using the keyboard. A new trial started
immediately after the response. For every “yes” response the
SOA value decreased and for every “no” response the SOA
value increased for the subsequent trial. Similarly, for the runs
determining the lower threshold of SOA, the start value of
SOA was selected small enough such that the participant feels
simultaneous stimulations (no sense of directionality of the
stimulation). In every trial they were asked if they felt actuators
“simultaneous”. For every “yes” response the SOA increased
and for every “no” the SOA decreased for the subsequent trial.
The SOA value was changed initially by 16 ms and then by
4 ms after the first two reversals. A reversal occurred when the
participant's response changed from “yes” to “no”, or vice
versa. The experimental run terminated after six reversals at the
4 ms step-size. Each run typically took 10 trials, which lasted
about 10-15 minutes. Participants sat comfortably on the chair
facing towards the computer screen displaying experimental
protocol.
E. Data Analysis
The average SOA threshold was computed by taking the
mean value of the last five reversals of each run. Repeated
measures Analysis of Variance (ANOVA) tests were utilized to
determine significant effects (α = 0.05) of test conditions.
IV.

Fig 3: Visual representation of the Stimulation patterns used in
the study (distance center-to-center is 4 cm).
D. Procedure
The tactile armband device was attached to the dorsal side
of the forearm with the elastic straps, so that the center of the
first actuator was 4 cm from the wrist. The actuators were
placed on the non-dominant hand, thus if the subject was right

RESULTS

Three parameters are considered in this study: the burst
duration, the burst overlap time, and the actuators distance
(distance between actuation points). We aim at exploring the
relationship between these three parameters for the sake of
generating continuous tactile stimulation, and find the upper
and lower thresholds for the SOA values.
A. Upper Threshold for SOA
The grand average of the upper threshold for the SOA time
against the burst duration, for various actuators distances, is
shown in Fig. 4. From the plot in Fig. 4 we clearly see a linear
correlation between the SOA timings and the burst overlap
time, with an exception at 4 cm actuators distance, which might

indicated that these actuators are place too closely together for
the subject to discriminate the continuous tactile stimulation.
4#cm#

8#cm#

12#cm#

16#cm#

20#cm#

70#

4#cm#

8#cm#

12#cm#

16#cm#

20#cm#

140#

50#

30#
100#

120#

140#

160#

180#

200#

220#

240#

Burst'dura"on'(ms)'

Fig. 4: Upper threshold for the SOA against burst time.
Fig. 5 depicts the SOA time in milliseconds against interactuator distance for the upper threshold for three different
burst duration values (120 ms, 180 ms, and 240 ms). The error
bars of Fig. 5 represent the standard variation thresholds
whereas the trends line shows the linear interpolation trends
against the distance between successive actuation points. Fig. 5
shows clearly that the further the distance between successive
stimulation points (4 to 20 cm), the more linear is the
relationship between the burst duration and the SOA. Note that
the interpolation line shown in Fig. 5 represents the upper
thresholds of the linearized distance/SOA relationships for the
three types of burst duration. The interpolation demonstrates
that a linear relationship between the SOA and the interactuator distance produces a perceivable tactile apparent
motion. Therefore, we suggest that these equations sufficiently
describe the upper threshold for the inter-actuator
distance/SOA relationship for continuous tactile stimulation.
120#burst#dura1on#

180#burst#dura1on#

240#burst#dura1on#

Linear#(120#burst#dura1on)#

Linear#(180#burst#dura1on)#

Linear#(240#burst#dura1on)#

S"mulus'Onset'Asynchrony'(SOA)'(ms)'

S"mulus'Onset'Asynchrony'(SOA)'(ms)'

90#

B. Lower Threshold for SOA
The grand average of the lower threshold for the SOA time
against the burst duration is shown in Fig. 6. Fig. 6 clearly
shows a linear correlation between the burst duration and the
burst overlap time, except around the 4 cm distance between
successive actuation points. Again, the reason might be
because these actuators are place too closely together for the
subject to discriminate the continuous tactile stimulation.

130#
120#
110#
100#
90#
80#
70#
60#
50#
40#
100#

120#

140#

160#

180#

200#

220#

240#

Burst'dura"on'(ms)'

Fig. 6: Lower threshold for the SOA against burst time.
Fig. 7 presents the SOA time against the inter-actuator
distance, for the lower threshold at 120 ms, 180 ms, and 240
ms burst duration values. Fig. 7 also shows the standard
deviations and trends for the SOA time. Fig. 7 shows clearly
that the further the distance between successive stimulation
points (4 to 20 cm), the more linear is the relationship between
the burst duration and the overlap time. Note that the
interpolation line shown in Fig. 7 represents the lower
threshold of the linearized inter-actuator distance/SOA
relationships for the three values of burst duration.
240"burst"dura9on"

180"burst"dura9on"

120"burst"dura9on"

Linear"(240"burst"dura9on)"

Linear"(180"burst"dura9on)"

Linear"(120"burst"dura9on)"

140"

85#

S"mulus'Onset'Asynchrony'(SOA)'(ms)'

130"

S"mulus'Onset'Asynchrony'(SOA)'(ms)'

80#

75#

70#

65#

60#

120"

y"="$1.4684x"+"130.09"
110"

100"

y"="$1.0732x"+"104.74"
90"

80"

55#
70"

y"="$1.6245x"+"89.961"

50#
60"
45#
50"
4"
40#
4#

6#

8#

10#

12#

14#

16#

18#

20#

Inter5actuator'distance'(cm)'

Fig. 5: Upper threshold for the SOA against inter-actuator
distance.

6"

8"

10"

12"

14"

16"

18"

20"

Inter5actuator'distance'(cm)'

Fig. 7: Lower threshold for the SOA against inter-actuation
distance.
The interpolation line in Fig. 7 demonstrates that a linear
relationship between the SOA and the inter-actuator distance
produces a perceivable tactile apparent motion. Therefore, we

suggest that these equations sufficiently describe the lower
threshold for the distance-overlap relationship for continuous
tactile stimulation.
C. Discussion
A thorough one-way ANOVA analysis suggested that the
burst duration was a significant factor for both the upper and
lower thresholds of SOA [F(2,12) = 196, p<0.01 for upper
threshold and F(2,12) = 69, p<0.01 for lower threshold]. As the
burst duration increased, both the upper- and lower-thresholds
also increased. However, the SOA range between upper- and
lower-thresholds was greater for larger burst durations,
indicating broader margins for creating continuous apparent
tactile motion on the skin.
In the present experiment one-way ANOVA analysis has
also shown that the upper and lower thresholds for the SOA
were not significantly affected by the distance between
successive actuation points [F(4,10)=0.044, p=0.995]. This
suggests that by placing the actuators anywhere between 4 cm
and 20 cm apart, continuous movement stimulation can still be
generated on the skin – with the exception of measurements
taken at 4 cm distance.
The one-way ANOVA analysis for comparison between the
upper and lower thresholds of the SOA suggested significant
differences between upper and lower SOA thresholds
[F(1,26)=6.11, p<0.05].
All configurations of stimulation patterns are shown to
generate continuous stimulation with corresponding
upper/lower thresholds. Furthermore, within upper/lower
overlap time interval, continuous stimulation can be produced
at various speed of stimulation. This implies that the designer
for the tactile display device may simulate various ‘speeds’ of
tactile stimulation according to the application requirements.
For instance, a bullet tactile effect would be rendered as a fast
continuous stimulation whereas an insect walking across the
human skin would be simulated using slow continuous
stimulation.
Fig. 2 indicates that the upper threshold (with exception of
the 4 cm pattern) is almost independent of the placement of the
actuators. However this is not the case for the lower threshold,
where we see a lower threshold for the 4 cm and 8 cm patterns
compared to the other patterns. This implies that the lower
threshold for a continuous motion can be lower as actuators are
placed closer together. Consequently, bringing the actuators
closer to each other has resulted in a wider SOA space between
the lower- and upper-thresholds, while maintaining a nonlinear relationship between the burst overlap and the burst
duration.
The final threshold (average of upper and lower) for the
SOA marks the value below which the stimuli are perceived to
be simultaneous, and above which the stimuli are perceived to
be discrete. Values of SOA at the average threshold and
upwards can be used to control the speed of movement of the
stimulus.

It is also worth highlighting that the 4 cm configuration (for
both upper and lower thresholds) is characterized by a nonlinear behavior, probably because the actuators are placed too
close to each other for the user to discriminate spatial
differences.
A similar experiment by Israr et al. [28] compares the upper
and lower thresholds of patterns created from three actuators
with distance of 2.35 inches (app. 6 cm) and 4.7 inches
between actuators with the burst durations 240 ms. Israr et al.
found a significant difference of spacing with larger variance of
the upper threshold. If we look at the threshold values, the
upper and lower threshold is quite similar when the burst
duration is 120 ms for both experiments. However this is not
the case when we look at the results for burst durations of 240
ms: here our results show significantly higher values for both
lower and upper threshold. The reason for this is unknown, but
it could indicate that stimulation of the actuators from our
experiments and that of Israr et al., feels different when the
burst duration is increased. We therefore suggest a further
investigation of the perception of continuous motion using
different actuators.
V.

CONCLUSION AND FUTURE WORK

The motivation of this work has been to derive
specifications for the design and development of tactile
stimulation interfaces based on psychophysical experiments.
The long-term goal is to study tactile stimulation for affective
communication. The key contribution of this paper was to
measure the SOA space for continuous tactile stimulation
(upper- and lower-threshold SOA values). We also investigated
and reported the effects of burst duration, burst overlap time,
and the distance between successive actuation points on
generating continuous tactile stimulation.
Our immediate future work is to expand our work to
various parts of the human body such as shoulder, back, or
neck (the current study is focused on the forearm). We also
would like to study further for correlations between continuous
tactile stimulation and emotional developments. This would
enable several applications in social media and gaming by
enabling users to communicate emotions over the Web.
Finally, we plan to develop an authoring tool through which
users may create their own tactile stimuli and use the tactile
device to display the corresponding stimulus.
REFERENCES
[1]

[2]
[3]

[4]

Eid, M.; Jongeun Cha; El Saddik, A., "Admux: An Adaptive Multiplexer
for Haptic–Audio–Visual Data Communication," IEEE Transactions on
Instrumentation and Measurement, vol.60, no.1, pp.21,31, Jan. 2011.
D. Katz and L. Krueger. The world of touch. Lawrence Erlbaum
Associates, Inc, 1989.
K. Takahashi, H. Mitsuhashi, K. Murata, S. Norieda, K. Watanabe,
"Feelings of animacy and pleasantness from tactile stimulation: Effect of
stimulus frequency and stimulated body part", 2011 IEEE International
Conference on Systems, Man, and Cybernetics (SMC), pp.3292-3297,
2011.
Lugo ZR1, Rodriguez J, Lechner A, Ortner R, Gantner IS, Laureys
S, Noirhomme Q, Guger C., “A vibrotactile p300-based brain-computer
interface for consciousness detection and communication”, Clinical EEG
and neuroscience: official journal of the EEG and Clinical Neuroscience
Society, 2014.

[5]

[6]

[7]

[8]
[9]
[10]

[11]

[12]

[13]

[14]
[15]

[16]
[17]

[18]

[19]

[20]

Wollman I, Fritz C, Poitevineau J., “Influence of vibrotactile feedback
on some perceptual features of violins”, The Journal of the Acoustical
Society of America, 136(2):910-21, 2014.
J. Raisamo, R. Raisamo, and V. Surakka, “Evaluating the Effect of
Temporal Parameters for Vibrotactile Saltatory Patterns,” Proc. ACM
Int’l Conf. Multimodal Interfaces, pp. 319-326, 2009.
L. Bonanni, C. Lieberman, J. Vaucelle, and O. Zuckerman, “TapTap: A
Haptic Wearable for Asynchronous Distributed Touch Therapy,” Proc.
24th Ann. ACM SIGCHI Conf. Human Factors Computing Systems, pp.
580-585, 2006.
D. S. Alles. Information transmission by phantom sensations. IEEE
Transactions on Man-Machine Systems, 11(1), 85–91, 1970.
F.A. Geldard and C.E. Sherrick. The Cutaneous rabbit: A perceptual
Illusion. Science, 178-179, 1972.
J. Lee; Y.. Kim; E. Ahn; Kim, G.J., "Applying “out of the body”
funneling and saltation to interaction with virtual and augmented
objects," 2012 IEEE VR Workshop on Perceptual Illusions in Virtual
Environments (PIVE), , pp.7,9, 4-6 March 2012.
A. Barghout, J. Cha; A. El-Saddik, J. Kammerl, E. Steinbach, "Spatial
resolution of vibrotactile perception on the human forearm when
exploiting funneling illusion," IEEE International Workshop on Haptic
Audio visual Environments and Games, 2009. pp.19,23, 7-8 Nov. 2009.
V. G. Chouvardas, A. N. Miliou, and M. K. Hatalis. Tactile display
applications: A state of the art survey. pages 290–303, Ohrid,
Macedonia, November 2005.
Y. Mizukami and H. Sawada. Tactile information transmission by
apparent movement phenomenon using shape-memory alloy device.
International Journal on Disability and Human Development, 5(3):277–
284, July 2006.
H. Z. Tan, R. Gray, J. J. Young, and R. Traylor. A haptic back display
for attentional and directional cueing. Haptics-e, 3(1), June 2003.
G. V. Bekesy. Funneling in the nervous system and its role in loudness
and sensation intensity on the skin. The Journal of the Acoustical
Society of America, 30(5):399–412, May 1958.
G. K. Essick, "Factors affecting direction discrimination of moving
tactile stimuli," in Advances in Psychology, vol. 127, 1998, pp. 1-54.
Rosenthal, J.; Edwards, N.; Villanueva, D.; Krishna, S.; McDaniel, T.;
Panchanathan, S., "Design, Implementation, and Case Study of a
Pragmatic Vibrotactile Belt," IEEE Transactions on Instrumentation and
Measurement, vol.60, no.1, pp.114,125, Jan. 2011.
Brunetto, P.; Fortuna, L.; Giannone, P.; Graziani, S.; Pagano, F., "A
Resonant Vibrating Tactile Probe for Biomedical Applications Based on
IPMC," IEEE Transactions on Instrumentation and Measurement, ,
vol.59, no.5, pp.1453,1462, May 2010.
Karime, A.; Al-Osman, H.; Alja'am, J.M.; Gueaieb, W.; El Saddik, A.,
"Tele-Wobble: A Telerehabilitation Wobble Board for Lower Extremity
Therapy," IEEE Transactions on Instrumentation and Measurement,
vol.61, no.7, pp.1816,1824, July 2012.
S.K.A. Hossain, A.S.M.M. Rahman, A. El Saddik, "Measurements of
Multimodal Approach to Haptic Interaction in Second Life Interpersonal
Communication System," IEEE Transactions on Instrumentation and
Measurement, vol.60, no.11, pp.3547,3558, Nov. 2011.

[21] J. Kammerl, R.G. Chaudhari, E. Steinbach, “Combining contact models
with perceptual data reduction for efﬁcient haptic data communication in
networked VEs”, IEEE Transactions on Instrumentation &
Measurement, vol. 60(1), pp. 57-68, 2011.
[22] Lenay, C.; Tixier, M.; Le Bihan, G.; Aubert, D.; Mara, J., "[D87]
Remote tactile interaction," IEEE Haptics Symposium (HAPTICS),
pp.1,1, 23-26 Feb. 2014.
[23] van der Linden, J.; Schoonderwaldt, E.; Bird, J.; Johnson, R.,
"MusicJacket—Combining Motion Capture and Vibrotactile Feedback
to Teach Violin Bowing," IEEE Transactions on Instrumentation and
Measurement, vol.60, no.1, pp.104,113, Jan. 2011.
[24] Sakurai, T.; Shinoda, H., "Sharp tactile line presentation array using
edge stimulation method," Haptics Symposium (HAPTICS), 2014
IEEE , vol., no., pp.271,275, 23-26 Feb. 2014.
[25] Ali Israr and Ivan Poupyrev, “Tactile brush: drawing on skin with a
tactile grid display”, In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems (CHI '11), pp. 2019-2028, 2011.
[26] P. Lemmens, F. Crompvoets, D. Brokken, J. van den Eerenbeemd, and
G.-J. de Vries, “A Body-Conforming Tactile Jacket to Enrich Movie
Viewing,” Proc. Third Joint EuroHaptics Conf. and Symp. Haptic
Interfaces for Virtual Environment and Teleoperator Systems
(WorldHaptics’09), pp. 7-12, 2009.
[27] Raisamo, J.; Raisamo, R.; Surakka, V., "Comparison of Saltation,
Amplitude Modulation, and a Hybrid Method of Vibrotactile
Stimulation," IEEE Transactions on Haptics, vol.6, no.4, pp. 517-521,
Oct.-Dec. 2013.
[28] Israr, A; Poupyrev, I, "Control space of apparent haptic
motion," IEEE World Haptics Conference (WHC), pp.457-462, 21-24
June 2011.
[29] Yanagida, Y.; Kakita, M.; Lindeman, R.W.; Kume, Y.; Tetsutani,
Nobuji, "Vibrotactile letter reading using a low-resolution tactor
array," 12th International Symposium on Haptic Interfaces for Virtual
Environment and Teleoperator Systems, pp.400,406, 2004.
[30] F.A. Geldard, "Adventures in tactile literacy." American Psychologist
12, no. 3, 1957
[31] L.M. Brown, S.A. Brewster, and H.C. Purchase, “Multidimensional
Tactons for Non-Visual Information Display in Mobile Devices,” Proc.
Eighth Int’l Conf. Human-Computer Interaction with Mobile Devices
and Services (MobileHCI ’06), pp. 231-238, 2006.
[32] K. Yatani and K.N. Truong, “SemFeel: A User Interface with Semantic
Tactile Feedback for Mobile Touch-Screen Devices,” Proc. 22nd Ann.
ACM Symp. User Interface Software and Technology, pp. 111-120,
2009.
[33] Ahmed, A.A.; Yasin, M.A.A.; Babiker, S.F., "Tactile web navigator
device for blind and visually impaired people," 2011 IEEE Jordan
Conference on Applied Electrical Engineering and Computing
Technologies (AEECT), pp.1-5, 6-8 Dec. 2011.
[34] M. Niwa, Y. Yanagida, H. Noma, K. Hosaka, and Y. Kume,
"Vibrotactile apparent movement by DC motors and voice-coil tactors,"
in ICAT 2004, 2004, pp. 126-131.
[35] C. E. Sherrick and R. Rogers, "Apparent Haptic Movement," Perception
and Psychophysics, vol. 1, pp. 175-180, 1966.

Appendix

E

Vibrotactile alarm system for
reducing sleep inertia

Camilla Birgitte Falk Jensen, Georgios Korres, Carsten Bartsch, and Mohamad Eid. Vibrotactile alarm system for reducing sleep inertia. Affective
Computing, IEEE Transactions on, 2015. Submitted for publication.

IEEE TRANSACTIONS ON AFFECTIVE COMPUTING, MANUSCRIPT ID

1

Vibrotactile Alarm System For Reducing
Sleep Inertia
Camilla Birgitte Falk Jensen, Georgios Korres, Carsten Bartsch, Mohamad Eid
Abstract— There has been a vast development of personal informatics devices combining sleep monitoring with alarm
systems, in order to schedule the alarm to reduce sleep inertia. Many of these systems implement algorithms based on
psychophysiological measurements such as heart rate, body movement or brain signal. In contrast to these devices, we present
a wireless armband alarm system, named Aegis that uses vibrotactile stimulation as a silent alarm to wake up the user and
thereby minimizing sleep inertia. Furthermore, the utilization of haptic modality as an alarm eliminates noise disturbance of
roommates, spouses or family members sharing the same sleeping space. The paper investigates the emotional ratings and
level of attention towards different haptic patterns, in order to choose a haptic pattern that complements a pleasant awakening.
Performance evaluation of the proposed solution has successfully demonstrated the ability of Aegis to identify various sleep
stages (awake, REM and non-REM). The results from the attention task and the subjective valence rating suggest that the
vibrotactile stimulation should be based on the continuous modulation, since this not only is very perceivable but also highly
rated with positive attention.
Index Terms—H.5.1.d Evaluation/methodology, H.5.2.g Haptic I/O, J.8.h Health care, J.9.d Pervasive computing, J.4.b
Psychology, H.5.2.q User-centered design.

—————————— ! ——————————

1 INTRODUCTION

A

CCORDING to the National Sleep Foundation (NSF),
sleep is a physical and mental resting state that is
essential for a person’s health and wellbeing [1]. Irritability, moodiness, daily sleepiness, and disinhibition are
some of the first signs a person experiences from lack of
sleep [2]. Furthermore, the degradation in sleep quality is
associated with long-term health consequences such as
chronic medical conditions such as diabetes, high blood
pressure, and heart disease, among others [3].

1.1 Sleep Phases
Researchers have identified four different stages of
sleep, known as N1, N2, N3 and N4; cycling over and
over throughout the night [4]. For simplicity, we classify
sleep into awake, REM (Rapid Eye Movement) and nonREM phases. Non-REM phases include N1 (conscious
awareness of the surroundings slowly disappears within
20 minutes after sleep onset) and N2 (increased appearance of sleep spindles and complete withdrawal from
external awareness). Adults spend at least 50% of total
sleep time in light sleep (non-REM).
REM phases include N3 and N4, commonly summa————————————————

• C.B.F. Jensen is with the Applied Interactive Multimedia Research Lab
(AIMlab) at New York University Abu Dhabi, Abu Dhabi, United Arab
Emirates. On leave from Cognitve Systems at Technical University of
Denmark, 2800 Kgs. Lyngby, Denmark E-mail: cbfj@dtu.dk.
• G. Korres is with the Applied Interactive Multimedia Research Lab (AIMlab) at New York University Abu Dhabi, Abu Dhabi, United Arab Emirates. E-mail: george.korres@nyu.edu
• C. Bartsch is with the Applied Interactive Multimedia Research (AIMlab)
at New York University Abu Dhabi, Abu Dhabi, United Arab Emirates. Email: carsten.bartsch@nyu.edu
• M. Eid is with the Applied Interactive Multimedia Research Lab (AIMlab)
at New York University Abu Dhabi, Abu Dhabi, United Arab Emirates. Email: mohamad.eid@nyu.edu.
xxxx-xxxx/0x/$xx.00 © 200x IEEE

rized as slow wave sleep (SWS) or deep sleep [5]. Together, N1 and N2 are known as light sleep, characterized by
the lowest arousal threshold (level of stimulation needed
to wake up an individual while in a particular stage) of all
sleep stages.
The REM phase is identified by rapid eye movement
and intense atony of skeletal muscles. Abruptly waking
someone during the REM phase can cause sleep paralysis,
which is defined as the sudden experience of an inability
to move combined with terrifying visions to which one is
unable to react due to this paralysis. This phenomenon
occurs due to the particular biological attributes seen in
REM sleep: complete muscle paralysis, dreaming, and
irregular breathing and heart rate [2]. A sudden snap
back to consciousness often results in the brain’s recognition of these sensations as panic, suffocation, and visual
hallucinations.
Immediately after REM, however, a period of light
sleep returns, and the body has completed one entire
sleep cycle. Therefore, waking during light sleep limits
sleep inertia effectively. An individual moves through
several sleep cycles of approximately 90 minutes in which
non-REM and REM alternate [6].

1.2 Methods For Sleep Monitoring
Nowadays there exist several methods to measure the
quality of sleep and identify sleep phases, such as the
polysomnography (PSG) procedure [7], self-rated questionnaires instruments such as Pittsburgh Sleep Quality
Index (PSQI) [8], and lately using biosensors [9]. Although PSG provides accurate monitoring and assessment
of sleep quality, it is highly expensive, intrusive, and requires specialized centers [7]. PSQI is commonly used,
Published by the IEEE Computer Society

2

however, its subjective nature makes it less reliable [8].
Sleep has also been monitored using sophisticated biosensor equipment, including the electroencephalograph
(EEG) to capture brain signal, electrooculogram (EOG) to
read eye movements, electrocardiograph (ECG) for capturing heart signal, and electromyogram (EMG) to measure muscles activities [10]. More technologies have been
developed such as measuring respiration and wrist/body
movement using ECG sensor, accelerometer or event audio and video. With the improvement in mobile sensor
and sensing technologies, it is now possible to monitor
sleep ubiquitously.
Sensory technologies are used nowadays to continuously and automatically detect sleep cycles and provide a
quantitative means to measure the quality of sleep [11,
12]. A common and simple technique uses actimeter/actigraph [13, 14]; a watch shaped accelerometer
worn on the wrist to measure the user activity during
sleep. Other researchers have used HRV and respiratory
signals (captured using an ECG sensor) to detect sleep
fragmentations (number of sleep micro-arousals) and detect sleep cycles [15, 16].
Several sleep-tracking applications (mostly mobile
phone applications) and related devices already exist in
the market, such as Sleep Cycle alarm clock [17], SOMNOwatch™ plus EEG 6 device [47] and SleepMiner [18].
For instance, SleepMiner is an Android-based smart
phone application that predicts the quality of sleep based
on daily contexts [18]. Several features such as daily activity, living environment and social activity are extracted
from mobile phone data, and then a machine-learning
algorithm is proposed to measure the sleep quality. In the
performance evaluation, we use the SOMNOwatch™
plus EEG 6 device as a reference device due to its high
repulation as a robust and reliable commercial system for
detecting sleep stages.

1.3 Adaptive Alarm Systems
In contrast to regular alarm clocks, an adaptive alarm
clock chooses an optimal time to wake up the user using
contextual knowledge (such as calendar information,
sleep quality, and psychophysiology). Most alarm systems use auditory modality to display the alarm signal.
Sony has recently patented an alarm pillow with electrodes on the surface to come in contact with the head to
read brain wave signals [7]. The system analyzes the collected signals to determine when the user goes into the
REM or non-REM stages and turns on a buzzer attached
to the pillow as soon as the person gets out of the deep
sleep. The alarm pillow may not provide continuous
sleep monitoring since contact between the sleeper and
the pillow is not guaranteed.
An adaptive alarm clock was developed in [19] where
the clock predetermines in what state the observed user
will be at the time of supposed alarm-firing, and adjusts
that instant to a more favourable one such as when the
user is in light sleep. A webcam is used to measure
movements and estimate the quality of sleep. However a

IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID

webcam may compromise the sleeper confidentiality.
Several projects are available to download and run on
various mobile device platforms such as Nokia, Apple
iPhone, Windows Mobile, etc. HappyWakeUp application
[20] is available to Nokia and iPhone mobile platforms
that wake the sleeper during shallow sleep (non-REM
stage). The application detects user movements in a bed
using the microphone of the mobile phone. Macjek Drejak
Labs developed a sleep cycle mobile application that uses
embedded accelerometers that are equipped with modern
smart phones [21]. The user makes different movements
in bed during different sleep phases, which is used to
detect sleep cycles. Similar prototypes are also available
such as the Zeo Personal Sleep Coach [22], EASYWAKEme [23], and wakeNsmile [24].
Aegis system, introduced in our previous work [25],
utilizes acceleration data to measure a movement index
and define the firing time for the alarm, and uses haptic
modality (vibrotactile feedback) for displaying the alarm
signal where four (4) vibrotactile motors vibrate simultaneously, with constatnt vibration intensity, to stimulate
tactile sensation. In this paper, we study various tactile
stimulation patterns (such as simultaneous, successive
and continuous) that result in minimized sleep inertia.
The remainder of the paper is organized as follows: In
section 2 we present the software architecture and hardware implementation of the Aegis system and details of
the sleep stage extraction algorithm. Section 3 introduces
the vibrotactile stimulation literature and defines stimulation patterns to be examined in this study. In section 4, we
present the experimental setup, performance analysis and
discuss our findings. Finally, in section 5 we summarize
the paper and provide perspectives for future work.

2 AEGIS DESIGN AND IMPLEMENTATION
Aegis utilizes the accelerometer embedded in a armband
to determine the optimal times to wake up the sleeper.
When the user is ready to go to sleep, he/she puts on the
armband device and sets an interval for the alarm time.
During sleep, the Aegis system records the nighttime
movements of the user and analyzes them using the body
movement index algorithm presented in [46] to detect
sleep stage. The system searches for a point – within the
time interval provided by the user – where the user is in
non-REM sleep and provides vibrotactile stimulation to
wake him/her up so as to minimize sleep inertia.
Fig. 1 shows an overview of the proposed Aegis system. The user’s movements are captured by the armband
device and sent to the sleep management center that is
hosted on a mobile device (or a nearby computing device)
via Zigbee technology. The sleep management center processes the collected data and identifies sleep phases
(awake, REM and non-REM), and sets a vibrotactile
alarm. Furthermore, the collected data are stored and
may be streamed to a third party (such as a family member or a therapist) using the Data Center module. In the
following a brief introduction to Aegis components is
given.

AUTHOR ET AL.: TITLE

3

The preprocessing phase is adopted from the work
presented in [48]. The raw three-axis acceleration data are
converted first into the SI units (!/! ! ) by a calibration
procedure. The three signals (!! , !! , !! ) are then passed
through the following phases:
(i) Low Pass Filter: A second order low-pass filter with a
cutoff frequency of 18 Hz is applied to cancel out high
frequency noise. The transfer function of this filter is:

Fig. 1. Software architecture for the Aegis system.

2.1 Input Interfaces Modules
The Input Interfaces Module reads psychophysiological
data about the user, along with the context, and passes
the collected data to the Sleep Management Center. The
components of the Input Interfaces Module are described
briefly here:
• Motion Sensors: Hand movement is tracked using an
accelerometer to identify deep and shallow sleep so
that the alarm can be set accordingly. Other motion
sensors may be added to the Input Interfaces Module to achieve a higher accuracy of detecting sleeping phases.
• Context: The context component collects information
related to the place, time, and circumstances around
the sleeping user. Examples of contextual information include, but not limited to, location, ambient
noise and light conditions.
2.2 Adaptive Alarm
!The Adaptive Alarm module is the heart of the !Aegis system. It includes the Sleep Stage Extraction component and
the Alarm Response component that defines the vibrotactile stimulation pattern that should be used to wakeup the
sleeper. !
2.2.1 Sleep Stage Extraction
The Sleep Stage Extraction component analyzes body
movement from the accelerometer to classify sleep stages
into awake, REM and non-REM. Signal artifacts, caused
mostly by sleeper movements that may severely deteriorate the accelerometer readings and thus the motion estimation, are removed via a preprocessing phase. The sleep
stage extraction flowchart is shown in Fig. 2.

Fig. 2. Sleep stage extraction flowchart.

!(!) =

0.0625+0.125!−1 +0.0625!−2
1+1.3!−1 −!0.5!

−2

,!

(1)

(ii) Signal Derivation and Aggregation: A second order derivative operation is applied to remove baseline wander
and gravity components. The transfer function for the
derivative operation is shown in equation (2). Next, the
three axis-acceleration signals were combined into a single (axis-independent) signal by calculating the absolute
sum.

!(!) =

1
!2

2

[1 − !−1 ] !

(2)

(iii) Feature Extraction: Before extracting features, two separate integration operations are applied over two seconds
(!! ) and four seconds (!! ) window. The two signals have
different response characteristics to different periods of
movement activity. Feature extraction was based on the
calculation of the body movement index [46]. For each 30second epoch (!) a movement index !(!) was calculated
using equation (3). The average over a window of 18 subsequent epochs of the body movement index is used for
each epoch.

! ! =!

!

2

(!2 /!4 ) !

(3)

(iv) Classification and Evaluation: Naïve Bayes classifiers
[49] are used to distinguish awake, REM and non-REM
sleep using the body movement index feature. Evaluation
was performed by recording data with the SOMNOwatch™ plus EEG 6 device [REFx]. Two healthy subjects (one male and one female) were recorded during
seven consecutive nights. Aegis system was placed at the
subject’s arm, while reference hypnograms were collected
using the SOMNOwatch™ plus EEG 6 device.
The reference data is also divided into 30-second
epochs from the accelerometer signals, and was preprocessed with the DOMINOlight software (SOMNOmedics
GmbH). The body movement index feature was used together with the ground truth information recorded from
the SOMNOwatch™ plus EEG 6 device to train the Naïve
Bayes classifiers extract sleep stage (awake, REM and
non-REM). A snapshot comparing results we got using
Aegis system and the SOMNOwatch™ plus EEG 6 device
for detecting awake, REM and non-REM over 8 hours of
sleep is shown in Fig. 3.
A total of 12550 epochs were collected from the two
subjects and used for training and evaluation, 18.2% of
those epochs were REM-epochs. Results for classification
of REM, non-REM and Awake phases are given in Table I

4

IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID

Fig. 3. Example hypnogram with the different sleep stages (Awake, REM and non-REM) over around 8 hours of sleep.

for each subject and over a course of seven days. The classifier has an average accuracy of 94.24% compared to the
reference SOMNOwatch™ plus EEG 6 measurements.
TABLE 1
CLASSIFICATION ACCURACY (AWAKE, REM AND NON-REM)

Days
1
2
3
4
5
6
7
Average
Stddev

Subject 1 (male)

Subject 2 (female)

Error

SleepTime

Error

SleepTime

6.06 %
5.69%
5.22%
5.81%
6.12%
5.10%
5.26%
5.67%
0.42%

7.45 hrs
8.03 hrs
7.82 hrs
7.45 hrs
6.81 hrs
6.65 hrs
7.88 hrs
7.37 hrs
0.53 hrs

6.48 %
6.21%
5.62%
5.12%
5.90%
6.01%
5.80%
5.84%
0.37%

7.90 hrs
7.69 hrs
7.21 hrs
7.68 hrs
7.10 hrs
7.32 hrs
7.22 hrs
7.40 hrs
0.25 hrs

2.2.2 Alarm Response
The Alarm Response component is responsible for determining, based on the corresponding sleep stage, the vibrotactile stimulation pattern that must be applied to
wakeup the sleeper. There are three types of vibrotactile
stimulation patterns that we are exploring: simultaneous
stimulation where all actuators vibrate at the same time,
successive stimulation where actuators vibrating separately one after the other, and continuous stimulation
created from an overlap of vibration from different actuators. Fig. 4 explains graphically the differences between
the three types of stimulation patterns.

real-time data as well as previously captured data (historical data), as needed.
• Server: The server is a facility that is !capable of
streaming data stored in the Data Center to a third
party via a Web Service Architecture or a cloud
computing architecture (this component is not implemented in the current prototype).
• Data Records: The Data Records component is a database that saves data about the sleeping behavior of
the user as well as the history of sleep patterns. For
instance, the Data Records component includes the
time-stamped movement information and a snapshot of the context at every phase of the sleeping cycle. The database tables the following information:
o The time at which the user set his/her alarm
o The time Aegis chose to wake him/her up
o Duration of sleep
o Location of sleep
o Ages of cohabitants
o Age of user
o Chronotype architecture for the Aegis system
• Sleep Profile: The Sleep Profile stores the user’s personal information including age and gender, preferences as for sleeping context (ambient noise, light
condition, room temperature, etc.).

2.4 Aegis System Implementation
!The prototype implementation, as shown in Fig. 2, !is
composed of an accelerometer to measure user’s hand
movements, a microprocessor to read acceleration data,
implement the sleeping management logic, activate the
vibrotactile motors, and communicate sleep-related data
to a remote server, and a battery.
The wristband device is composed of the following components:
• ADXL-335 accelerometer
• Arduino Mini Pro
• XBee transceiver module
• Six vibrotactile actuators
• DeadOn DS3235 RTC (Real Time Clock)
• Power cell and 3.7 V Lithium battery and voltage
regulator

3 VIBROTACTILE STIMULATION
Fig. 4. The burst pattern and how this is perceived for three different
modulations; simultaneous, continuous and successive.

2.3 Data Center
!The Data Center is a repository to store data related to the
Aegis system and a streaming server to share the collected data with a third party. The streaming server delivers

The concept of tactile stimulation has been around for
decades, since Geldard [27] in the 1950s studied the temporal and spatial aspects of tactile discrimination on skin
and wrote: “for some kinds of messages the skin offers a
valuable supplement to ears and eyes”. Early research on
haptic stimulation has focused on applications aiding
blind or visually impaired people [28], but later develop-

AUTHOR ET AL.: TITLE

ments included entertainment and gaming [29], mobile
and touchscreen interaction [30], emotional and interpersonal communication [31, 32, 33], and health care (such as
physical rehabilitation) [34].
In relation to these novel application areas, there has
been a growing interest in the subjective responses to different haptic patterns. The haptic pattern can be described
from the following modulations; simultaneous, continuous and successive stimulation (as shown in Fig. 4). Simultaneous stimulation comprises all actuators vibrating at
the same time. This is known to cause an illusion of sensation displacement, know as the Funnel illusion [35]. Continuous motion can be created from an overlap of vibration from different actuators. Actuators vibrating separately create the successive stimulation.
These modulations can be modified by changing the
frequency, amplitude (intensity), Burst Duration (BD),
Inter-Burst Interval (IBI), spatial distribution and direction, which result in different haptic patterns.
Many studies have compared the subjective ratings of
different haptic patterns using the classic 9-point valence
and arousal scale (commonly used to measure emotions
[36]). The valence level indicates how positive or negative
a stimulus is, whereas the arousal level indicates how
calm or exiting the stimulus is. An example of how emotions can be distributed into these two scales is shown in
Russell’s model [37] shown in Fig.5. In addition some
studies compares different patterns on additional scales
such as cognitive scales of continuity [38], smoothness
[39], strength and rhythm [40], or speed [39].
Due to the multiple possibilities for modulating haptic
patterns, many studies choose to keep some parameters
fixed, while concentrating on the effects of others, which
often results in different varying results. However many
studies on affective haptic report continuous motion being perceived as pleasant, whereas simultaneous stimuli
were rated more unpleasant [41, 39]. Other studies focus
more on how the different parameters contribute or affect
the continuous feeling [29, 38, 42].
Raisamo et al. [41] found a correlation between continuous motion being perceived as pleasant, whereas simultaneous stimuli were rated more unpleasant. While others
examined the effect of frequency, amplitude, duration,

5

direction and body site on continuity and subjective preferences [29, 38], Rahal et al. showed an effect of gender,
limb size and intensity [42].
Although most studies on affective haptic report preferences towards more continuous stimulation, this might
not be the most suitable haptic stimulation when used as
an alarm clock. In this case a soothing continuous motion
might not draw enough attention upon itself to cause the
user to wake up. Instead it might become integrated into
the user’s sleep or even dream. However using a very
arousing and aggressive stimulation could result in an
unpleasant awakening that might lead to a dislike of the
product. Thus we are interested in examining not only the
subjective emotional ratings of haptic patterns but also
how fast the subject’s attention is shifted to the different
haptic patterns.
The majority of attention research has focused on single sensory modalities, such as vision, audition, touch
and even olfaction and gustation. However in everyday
life we commonly operate across different sensory modalities to facilitate the selection of relevant information. The
classic “cocktail party problem”, where we direct our auditory attention to one particular voice in order to have a
conversation in a noisy environment is actually misleading: we often rely on many other sensory modalities as
visual information from lip-movements, facial expressions, and gestures. In addition we often ignore irrelevant
sensory inputs, such as the feel of one’s clothes (tactile),
the smell of someone’s perfume (olfactory), and perhaps
even the taste of one’s drink (gustatory).
Attention can be divided into endogenous and exogenous attention: The endogenous attention describes the
voluntarily direction of attention to a particular point,
such as attending to one person at a cocktail party. In contrast, exogenous or involuntary attention is the reflexive
shifts of attention to unexpected or uninformative event,
such as someone calling your name at a cocktail party, or
if a fly suddenly lands on your arm [43].
However it is difficult to say what our attention is directed towards when we sleep, since this might depend
on the sleep phase and the individual. Hypothesizing that
our attention is not necessarily directed towards tactile
inputs, we are interested in measuring the exogenous
attention towards different haptic stimuli in a multimodal
scenario.

4 PERFORMANCE EVALUATION

Fig. 5. Russell’s model [37] illustrates how the valence and arousal
scale can be used to describe different emotions.

This section introduces the design, setup, and evaluation
of an experiment investigating 1) the effectiveness of vibrotactile stimulation to draw the user’s attention and 2)
the emotional responses to different stimulation patterns.
Thus we created a dual-task paradigm, where the subject
is performing two tasks at once, thereby forced to divide
her attention between the two tasks [44, 45]. The paradigm consists of a haptic detection task (where the user
has to respond when detecting a haptic stimulus) and a
visual identification task (where the subject has to identify the correct target among different distractor stimuli).

6

IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID

4.1 Vibrotactile Armband Device
The tactile armband device is composed of 6 vibrotactile
actuators that are aligned 4 cm apart along the arm. The
armband device is capable of producing three types of
tactile stimulation: simultaneous stimulation, successive
stimulation and continuous stimulation. The three modes
are explained in equation (4). A demonstration of the continuous stimulation algorithm is shown in Fig.6.
! = ! ∗ !!,!

(4)

! = !0!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"#$%&'()*$!!!"#$%&'"#()!
!ℎ!"!!! ! < !1!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"#$%#&"&'!!"#$%&'"#() !!
! > !1!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"##$!!%&$!!"#$%&'"#()
Where T is the burst duration, τ is the inter-burst interval
and α is the mode factor. Simultaneous stimulation involves stimulating the motors at the same time (α=0 or
τ=0) to produce the highest sense of vibration possible
where intensity of vibration can also be controlled. Successive stimulation has one motor stimulated at a time
(α>1 or τ>T); there is no inter-burst stimulation. Continuous stimulation is based on the funneling illusion concept
[35] and produces apparent tactile motion along the armband surface.

Fig. 6. Tactile stimulation algorithm.

The stimulation intensity is controlled by adjusting the
duty cyle of the Pulse Width Modulation (PWM) signal
that feeds the vibrotactile actuators. Increasing the duty
cycle of the PWM signal would increase the effective
voltage applied to the actuator and thus the vibration
intensity. The change in the intensity of vibration is linear
over time, and is described by equation (5).

! = !!"# ∓ !!!"# − ! !!"# ∗

!!"#$%&"
!""

,!

(5)

where Imin = 0.25g, Imax = 1.25g, and StepSize = +10
when intensity increase and StepSize = -10 when intensity
decreases.

4.2 Experimental Setup
The wristband providing the vibrotactile stimulus was

constructed with six actuators (Pico Vibe 310-177, Precision Microdrives) vibrating with a frequency of 700 rpm
at the minimum intensity 0.25g, and 1400 rpm at the maximum intensity 1.75g. They were placed at a distance of
4 cm from center to center of the actuators. We created 8
patterns based on the three modulations described earlier:
simultaneous, continuous and successive. A summary of
the stimulation patterns is shown in Table 2.
Similar to other alarm clocks with increasing volume,
our patterns increase in amplitude (intensity) as time
passes. This was chosen partially to examine how easily
the different patterns were perceived and in attempt to
make a smooth waking. Thus all of the patterns started
from an intensity of 0.25g, which is almost not perceivable and ended after 35 to 40 seconds with an intensity of
1.75g, which is easily perceived. The intensity increased
linearly with time.
Three of the patterns, one from each modulation type,
were altered by this intensity increase: the simultaneous
stimulation has burst duration of 100 ms and an interburst interval of 300 ms. The relatively short burst duration was chosen due to the effect of multiple actuators
vibrating simultaneously, which intensifies the stimulation. The continuous stimulation was characterized by a
burst duration of 300 ms and an overlap of 120 ms (corresponding to 40% of the burst duration). The successive
stimulation comprised of a 150 ms burst duration and an
inter-burst interval of 150 ms.
In an attempt to avoid the bias that the stimulus
would become integrated in the user’s sleep, the next patterns were created with either increasing speed or changing direction to increase variations.
Three patterns (one for each modulation) were, in addition to the intensity increase, altered by an increase in
velocity. This implied a change in the inter-burst interval
from 500 ms to 100 ms for the simultaneous modulation.
As for the continuous stimulation, this resulted in a
change in burst duration from 500 ms to 100 ms, while the
overlap changed from 200 ms to 40 ms. Furthermore, the
successive stimulation increased in velocity by a change
in burst duration from 250 ms to 50 ms and in inter-burst
interval from 250 ms to 50 ms.
The last two patterns are based on continuous and
successive modulation and vary in intensity and change
in direction, starting with vibration of the first to the sixth
actuator, and then in reversed order.
The experiment was conducted on 12 participants, all
of them were students or employees of NYU Abu Dhabi,
5 were female and 7 males. Two of the male participants
were left-handed all others were right-handed. The average age of the participants was 29.2 years ranging from 19
to 41 years. A snapshot of this experimental setup is
shown in Fig 7.
The actuators were placed on the non-dominant hand,
thus if the subject was right handed, the actuators were
place around the left wrist. While the experiments took
place the subjects were listening to pink noise in order to
mask any noise from actuators.

AUTHOR ET AL.: TITLE

7

TABLE 2
OVERVIEW OF THE DIFFERENT PATTERNS
Pattern
Modulation
1
Simultaneous
2
Simultaneous

Feature
Intensity increase
Intensity + velocity increase

3
4

Continuous
Continuous

Intensity increase
Intensity + velocity increase

5

Continuous

Intensity increase + direction change

6
7

Successive
Successive

Intensity increase
Intensity + velocity increase

8

Successive

Intensity increase + direction change

Attributes
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=0
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=0,
timeStepSize=0.1sec
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=0.5
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=0.5,
timeStepSize=0.1sec
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=0.5,
timeStepSize=0.1sec
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=3
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=3,
timeStepSize=0.1sec
Imax=1.25g, Imin=0.25g, StepSize=0.1g, Duration=2 sec, α=3,
timeStepSize=0.1sec

Fig. 7. The experimental setup.

The experiment consisted of two parts; 1) the dual-task
paradigm followed by 2) subjective emotional ratings of
the haptic patterns. The dual-task paradigm combined a
visual (conjunction) search task [39] with a simple haptic
detection task. In the visual search task, the subject
searched for a target, a red plus sign “+”, among distractors that share two visual properties, color and orientation, green and red letters “x” and green plus signs “+”.
An example with the target present is shown in Fig. 8. If
the target was identified, the user responded by pressing
“c” for cross, but if there is no target, the user should
press “n” for no cross. In 40 % of the trials the user would
also be presented with a haptic stimulus, and should respond to this by pressing space. To minimize anticipation
effects of simultaneous changes of visual stimuli and appearance of haptic stimuli, the haptic stimulus would
start randomly within the first 3 seconds from the beginning of the trial. Each haptic pattern started with an intensity of 0.2g, which is almost not perceivable. The intensity then increased over time until it reached 1.6g. Meanwhile the step size for increase in the intensity was the
same across patterns. All 8 patterns were presented 3
times in random order. The experiment stopped as soon
as the user detected all 8 haptic patterns, three times.
Before the experiment, the user was instructed to first
of all respond to the haptic stimuli as fast as possible. After which he should respond correctly to the visual search

Fig. 8. The visual search task, where the subject is seeking to identify a red cross (+).

Fig. 9. The 9-point arousal scale used to rate how arousing the haptic simulation was.

task, and lastly doing this as fast as possible. The user
would be notified whether his/her response was correct
or incorrect after each trial, so that he/she could adjust
his/her strategy. In addition the subject was told to place
his/her right and left index fingers on the “n” and “c”
keys respectively. With these keys place at equal distances to the space key and by instructing the subject to use
index finger of the dominant hand to press the space, we
hoped to minimize effects on the response time.
In the second part of the experiment the user was presented a haptic pattern and was asked to rate this on a
9-point valence and arousal scale (see Fig. 9). The user

8

IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID

could repeat the stimulus by pressing “r”. Before the experiment, the valence and arousal scale was explained to
the user and three practice trials were performed, so the
subject was familiarized with the scale. The stimulus presented in the practice trials was not used in the actual
experiment. One subject did not complete the second part
of the experiment and was therefore excluded from the
analysis of the emotional ratings.

4.3 Analysis and Results
The average response time of the three trials for each haptic pattern and for every subject was calculated. The data
is plotted in a standard box plot (see Fig. 10); showing the
mean as black diamonds and the median, upper and lower quartile in boxes while the whiskers represent the maximum and minimum values. The numbers on the x-axes
correspond to the pattern numbers in Table 2. The outliers are marked as circles, all of which came from the same
subject. Indicating that this subject had a different threshold for tactile stimulation. A larger response time for the
patterns based on the simultaneous modulation is clearly
observed from Fig. 10, which is supported by the following statistical tests.
Conducting one-way ANOVA analysis, both with and
without outliers, showed no influence of outliers on the
results. Thus we included the outliers in our analysis, and
the results showed a significant effect of the patterns
F(7,88)=25.61, p<0.001.
To test for interaction effects two two-way ANOVA
tests were conducted. One was testing two modulations
(continuous and successive) with three features (intensity,
intensity+velocity, intensity+direction). The other was
testing three modulations (simultaneous, continuous, and
successive) with two features (intensity and intensity+velocity). The first ANOVA test showed only a significant effect of modulation F(1,71)=42.249, p<0.001, suggesting that perception of patterns based on the continuous
pattern are significantly different from patterns based on
the successive pattern. The second ANOVA showed
likewise a significant effect of modulation F(2,71)=80.33,
p<0.001, however it also revealed an interaction effect
between modulations and features F(2,71)=3.79, p=0.028.
To examine the interaction effect, a permutation test with

Fig. 10. The response time of 8 different haptic patterns, with diamond representing the mean and circles the outliers (consisting data
from only one subject).

Fig. 11. The subjective valence (negative-positive) ratings of 8 different haptic stimulations, with diamond representing the mean.

Fig. 12. The subjective arousal (calm-energetic) ratings of 8 different
haptic stimuli, with diamond representing the mean.

paired t-test (and 10000 reputations) and a Bonferroni
correction of the significance level (α=0.05/15) is conducted. The results revealed significant difference with
p=<0.007 on all levels except from the tests where modulation was similar and features were different. This indicates that the features have different effects depending on
the modulation, however not as significant as with the
effect of feature.
The results of the emotional ratings on valence and
arousal are shown in the boxplots below, Fig. 11 and Fig.
12. The boxes represent the median, upper and lower
quartile; the black diamonds represents the mean; and the
whiskers represent the maximum and minimum values.
The valence data shows a large individual difference
amongst subjects, which in most cases are larger compared to the arousal ratings.
By conducting a repeated one-way ANOVA test we
found a significant difference between the valence ratings
of the patterns with F(7,81)=7.39, p<0.001, however we
found no significant difference for the arousal ratings. To
examine the valence data further, two two-way ANOVA
tests were conducted: one testing two modulations (continuous and successive) for three features (intensity, intensity+velocity, intensity+direction), the other testing the
three modulations (simultaneous, continuous, and successive) with two features (intensity and intensity+velocity). The first ANOVA showed no significant
effect of modulation and feature. There was no significant

AUTHOR ET AL.: TITLE

difference between any of the patterns based on the continuous or successive patterns. The second ANOVA
showed only a significant effect of modulations
F(2,65)=20.29, p<0.001. Post hoc t-test with Bonferroni
correction (α=0.05/3) revealed patterns based on the simultaneous modulations were significantly different from
those based on both continuous (t=6.16, p<0.001) and successive (t=3.96, p<0.001).
To sum up, the results on response time showed a
clear effect of modulations. With the subjects responding
quickest to the continuous patterns and slowest to the
simultaneous. However the slow reaction times for the
simultaneous patterns (especially pattern 2) might be explained by the long inter-burst interval (of 500ms). However it is also surprising to see how fast and similar the
response times of the continuous patterns (pattern 3, 4,
and 5) across subjects are.
The results from the emotional ratings show greater
differences amongst subjects, compared to the response
times, suggesting that there might be personal preferences towards different haptic patterns. Similar to earlier
reports on valence ratings, we also see higher ratings towards patterns with more continuous motions, suggesting that these are preferred over discrete motions. However we do not see any significant effect of the arousal
data and no correlations between arousal and valence
data.
Therefore, in order to create an effective haptic alarm,
that is not only effective but also smooth and pleasant for
waking up a person, our results suggest to preferably
utilizing the continuous modulation.

5 CONCLUSION AND DISCUSSION
This paper presents a novel clock alarm system named
Aegis – a smart wireless wristband arm system for sleep
management and reducing sleep inertia. An experimental
study was conducted to investigate vibrotactile patterns
as silent clock alarm response to wake a user in a smooth
and pleasant manner. The results from the attention task
and the subjective valence rating suggest that the haptic
alarm for the Aegis system should be based on the continuous modulation, since this not only is very perceivable but also rated as more positive.
However, as the prototype evolves, further improvements can be made to the system. The hardware could be
optimized in terms of size (for example, the circuit should
move to a smaller microprocessor board for even more
compact assembly), and sensory data may be saved in
internal non-volatile memory. A mobile device application may also be developed to provide a convenient interface to configure the system. In addition more usability
studies could be performed over longer time intervals to
further analyze the system and derive a more robust and
personalized performance. One can imagine that the haptic patterns could provide the user with more information
than the alarm onset. E.g. different patterns could indicate
how close the systems alarm onset is to the users preset
alarm time, thereby indicating how fast one needs to get
ready. This is similar to Lylykanga’s examination of pat-

9

terns representing information on motion (e.g. decelerate,
accelerate or keep speed constant) [39].
For future research it would be interesting to test the
effectiveness of the different patterns in real waking-up
scenarios and whether the haptic patterns could also be
used to influence a users sleep phase, e.g. moving them
from a deep sleep to light sleep before waking the user. In
this case it might be useful to create patterns based on the
simultaneous modulation, which are not that easily perceived.

REFERENCES
[1] A. Aubrey, MS. NPR News, Washington. NPR.org. NPR, 3 Aug.
2009.
[2] S. Bohm, “Sleep and Chronotype in Adolescents.” Diss. Institut
fur Medizinische Psychologie, Electronic Theses of LMU Munich. 2013.
[3] A. Rechtschaffen and A. Kales, “A Manual of Standardized
Terminology, Techniques and Scoring System For Sleep Stages
of Human Subjects”,US Dept of Health, Education, and Welfare; National Institutes of Health, 1968.
[4] National Institute of Health, “Sleep: A Dynamic Activity.” National Institute of Neurological Disorders and Stroke (NINDS).
2007.
[5] S. F. Dingfelder, “To Sleep, Perchance to Twitch”, Monitor on
Psychology 37.1, 2006.
[6] T. Salmi and L. Leinonen, “Automatic analysis of sleep records
with static charge sensitive bed”, Electroencephalogr. Clin.
Neurophysiol., vol. 64, no. 1, pp. 84–87, Jul. 1986.
[7] K. E. Bloch, “Polysomnography: A systematic review,” Technol.
Health Care, vol. 5, no. 4, pp. 285–305, Oct. 1997.
[8] D. J. Buysse, C. F. Reynolds, III, T. H. Monk, S. R. Berman, and
D. J. Kupfer, “The pittsburgh sleep quality index: A new
instrument for psychiatric practice and research,” Psych. Res.,
vol. 28, no. 2, pp. 193–213, May 1989.
[9] M. Thompson, L. Thompson, “The biofeedback book: An
introduction to basic concepts in applied psychophysiology”,
Wheat Ridge, CO: Association for Applied Psychophysiology
and Biofeedback, 2003.
[10] A. Casson, D. Yates, S. Smith, J. Duncan, and E. RodriguezVillegas, “Wearable electroencephalography,” Engineering in
Medicine and Biology Magazine, IEEE, vol. 29, no. 3, pp. 44–56,
2010.
[11] D. J. Buysse, C. F. Reynolds, T. H. Monk, S. R. Berman, and D. J.
Kupfer. The pittsburgh sleep quality index: a new instrument
for psychiatric practice and research. Psychiatry Research,
28(2):193–213, 1989.
[12] M. T. Smith and S. T. Wegener, “Measures of sleep: The
Insomnia Severity Index, Medical Outcomes Study (MOS) Sleep
Scale, Pittsburgh Sleep Diary (PSD), and Pittsburgh Sleep
Quality Index (PSQI),” Arthritis & Rheumatism, vol. 49, no. S5,
p. S184-S196, Oct. 2003.
[13] T. Salmi and L. Leinonen, “Automatic analysis of sleep records
with static charge sensitive bed”, Electroencephalogr. Clin.
Neurophysiol., vol. 64, no. 1, pp. 84–87, Jul. 1986.!
[14] A. Sadeh, K. M. Sharkey, and M. A. Carskadon, “Activity-based
sleep–wake identification: An empirical test of methodological
issues,” Sleep, vol. 17, no. 3, pp. 201– 207, Apr. 1994.
[15] A. M. Bianchi, M. O. Mendez, S. Cerutti, “Processing of signals
recorded through smart devices: sleep-quality assessment.”
IEEE Trans Inf Technol Biomed 2010; 14: 741-7.!
[16] Real-Time Sleep Quality Assessment Using Single- Lead and
Multi-Stage SVM Classifier.
[17] !http://www.sleepcycle.com/!
[18] Y. Bai, B. Xu, Y. Ma, G. Sun, and Y. Zhao. 2012. Will you have a
good sleep tonight?: sleep quality prediction with mobile
phone. In Proceedings of the 7th International Conference on
Body Area Networks (BodyNets '12). ICST (Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering), ICST, Brussels, Belgium, Belgium, 124-130.
[19] J. Kuijsten, “Adaptive Alarm Clock Using Movement Detection
to Differentiate Sleep Phases”, Eindhoven University of Technology, March 2010.
[20] HappyWakeUp smart alarm clock application for Nokia mobile

10

phones (2010), http://www.happywakeup.com/en/.
[21] Sleep Cycle mobile application,
http://www.mdlabs.se/sleepcycle/
[22] Chen, Zhenyu, et al. "Unobtrusive sleep monitoring using
smartphones."Pervasive Computing Technologies for Healthcare
(PervasiveHealth), 2013 7th International Conference on. IEEE,
2013.
[23] http://www.easywake.me/!
[24] O. Krejcar, J. Jirka, “Proactive User Adaptive Application for
Pleasant Wakeup”, Intelligent Information and Database Systems, Lecture Notes in Computer Science Volume 6592, 2011,
pp 472-481.
[25] L. Duong, M. Andargie, J. Chen, N. Giakoumidis, M. Eid, “Aegis: A Biofeedback Adaptive Alarm System Using Vibrotactile
Feedback”, IEEE International Instrumentation and Measurement Technology Conference (I2MTC), 2014.
[26] F.A. Geldard, "Adventures in tactile literacy." American Psychologist 12, no. 3, 1957
[27] !Y. Bellik and D. Burger. Multimodal interfaces: New solutions
to the problem of computer accessibility for the blind. In Proc.
CHI’94, pages 24–28, 1994.
[28] A. Israr and I. Poupyrev, “Control space of apparent haptic
motion,” 2011 IEEE World Haptics Conf., pp. 457– 462, Jun.
2011.
[29] L. M. Brown, S. A. Brewster, and H. C. Purchase. "A first
investigation into the effectiveness of tactons." In Eurohaptics
Conference, 2005 and Symposium on Haptic Interfaces for
Virtual Environment and Teleoperator Systems, 2005. World
Haptics 2005. First Joint, pp. 167-176. IEEE, 2005.
[30] D. Tsetserukou, et al. "Affective haptics in emotional
communication." Affective Computing and Intelligent
Interaction and Workshops, 2009. ACII 2009. 3rd International
Conference on. IEEE, 2009.
[31] M. Eid, A. El Issawi, A. El Saddik, “Slingshot 3D: A
synchronous haptic-audio-video game”, Multimedia Tools and
Applications, November 2012.
[32] G. Huisman et al. "The TaSST: Tactile sleeve for social
touch." World Haptics Conference (WHC), 2013. IEEE, 2013.
[33] P. Kapur, S. Premakumar, S. A. Jax, L. J. Buxbaum, A.M. Dawson, K.J. Kuchenbecker, "Vibrotactile feedback system for intuitive upper-limb rehabilitation," EuroHaptics, pp.621,622, 2009.
[34] G. Bekesy, “Funneling in the nervous system and its role in
loudness and sensation intensity on the skin,” J. Acoust. Soc.
Am., vol. 30, no. 3, 1958.!
[35] M. Bradley and P. Lang, “Measuring emotion: the selfassessment manikin and the semantic differential,” J. Behav.
Ther. Exp. phychiatry, vol. 25, no. I, 1994.
[36] J.A. Russell, "A circumplex model of affect." Journal of personality and social psychology 39, no. 6 (1980): 1161.
[37] R. W. Cholewiak and A.A. Collins, “The generation of
vibrotactile patterns on a linear array: influences of body site,
time, and presentation mode.,” Percept. Psychophys., vol. 62, no.
6, pp. 1220–35, Aug. 2000.
[38] J. Lylykangas, V. Surakka, J. Rantala, J. Raisamo, R. Raisamo,
and E. Tuulari, “Vibrotactile Information for Intuitive Speed
Regulation,” pp. 112–119, 2009.
[39] H. Seifi and K. E. MacLean, “A first look at individuals’
affective ratings of vibrations,” 2013 World Haptics Conf., pp.
605–610, Apr. 2013.
[40] J. Raisamo, R. Raisamo, and V. Surakka, “Comparison of
Saltation, Amplitude Modulation, and a Hybrid Method of
Vibrotactile Stimulation,” vol. 6, no. 4, pp. 517–521, 2013.
[41] L. Rahal, J. Cha, and A. El Saddik, “Continuous tactile
perception for vibrotactile displays,” 2009 IEEE Int. Work. Robot.
Sensors Environ., pp. 86–91, Nov. 2009.
[42] C. Spence, “Multisensory attention and tactile informationprocessing.,” Behav. Brain Res., vol. 135, no. 1–2, pp. 57–64, Sep.
2002.
[43] T. Kristjánsson and T. Þorvaldsson, “Divided multimodal
attention: Sensory trace and context coding strategies in
spatially congruent auditory and visual presentation,” 2013.
[44] A.-M. Bonnel and E. R. Hafter, “Divided attention between
simultaneous auditory and visual signals.,” Percept. Psychophys.,
vol. 60, no. 2, pp. 179–90, Feb. 1998.
[45] T. Watanabe and K. Watanabe, “Noncontact method for sleep
stage estimation.” IEEE transactions on bio-medical
engineering, vol. 51, no. 10, pp. 1735–1748, 2004.
[46] Rechtschaffen, A.; Kales, A. A Manual of Standardized
Terminology, Techniques and Scoring System for Sleep Stages

IEEE TRANSACTIONS ON JOURNAL NAME, MANUSCRIPT ID

of Human Subjects; US Department of Health, Education, and
Welfare: Bethesda, MD, USA, 1968.
[47] SOMNOwatch plus EEG 6, Medical Devices for Sleep
Diagnostics and Therapy,
http://somnomedics.eu/products/extended-screening/,
accessed February 10, 2015.
[48] Gradl, Stefan, Leutheuser, Heike, Kugler, Patrick, Biermann,
Teresa, Kreil, Sebastian, Kornhuber, Johannes, Bergner,
Matthias, Eskofier, Björn, “Somnography using unobtrusive
motion sensors and Android-based mobile phones”, 35th
Annual International Conference of the Engineering in
Medicine and Biology Society (EMBC), pp. 1182-1185, 2013.
[49] Cuiping Leng; Shuangcheng Wang; Hui Wang, "Learning
Naive Bayes Classifiers with Incomplete Data," International
Conference on Artificial Intelligence and Computational
Intelligence, vol.4, no., pp.350,353, 2009.
C.B.F. Jensen has a M.Sc. in Engineering (2012); and currently
doing a Ph.D. at the Technical University of Denmark, Cognitive
Systems. Publications include several conference contributions and
a journal paper in International Journal of Physchophysiology. C.B.F.
Jensen is doing research on mobile interaction and interface design,
with focus on creating adaptive and cognitive interfaces supporting
personal informatics. Her work includes interfaces for self-regulation
of brainwaves using mobile equipment, with the aim of supporting
the interaction between human and computer.
G. Korres studied Applied Mathematics in the School of Science &
Engineering at the University of Crete. Korrea was involved for several years with the development of educational software and hardware regarding educational robotics. The last three years he has
also dealt with industrial automation (mainly in the field of recycling
industry). His research interests focus on development of new sensors and actuators as well as the use of these in human computer
interaction.
C. Bartsch received his M.S. (Dipl.-Ing.) in Electrical Engineering
from the FAU Erlangen-Nürnberg in Germany. He worked for the
Fraunhofer IIS in Erlangen (Germany) with focus on digital signal
processing. He created software for receivers and test equipment for
digital radio broadcasting. At IK4-Ikerlan in Mondragon (Spain) he
developed hardware and software for a variety of industrial and European projects. His current work is in the field of human-computerinteraction and haptic technologies.
M. Eid received the PhD in Electrical and Computer Engineering
from the University of Ottawa, Canada, in 2010. He is currently an
assistant professor of practice of electrical engineering in the engineering division at New York University Abu Dhabi (NYUAD). He
was previously a teaching and research associate at the University
of Ottawa (June 2008-April 2012). He is the co-author of the book:
“Haptics Technologies: Bringing Touch to Multimedia”, Springers
2011, the co-chair of the 3rd International IEEE Workshop on Multimedia Services and Technologies for E-health (MUST-EH 2013),
and has been involved in the organization of the Haptic-Audio-Visual
Environment and Gaming (HAVE) workshop for the years 2007,
2008, 2009, 2010, and 2013. His academic interests include Multimedia haptics, with emphasis on affective haptics, tangible human
computer interaction, and instrumentations (sensors and actuators).

Bibliography

[Arafsha et al., 2012] Arafsha, F., Alam, K. M., and El Saddik, A. (2012). Emojacket: Consumer centric wearable affective jacket to enhance emotional immersion. In Innovations in Information Technology (IIT), 2012 International
Conference on, pages 350–355. IEEE.
[Baumer et al., 2014] Baumer, E. P., Khovanskaya, V., Matthews, M., Reynolds,
L., Schwanda Sosik, V., and Gay, G. (2014). Reviewing reflection: On the use
of reflection in interactive system design. In Proceedings of the 2014 conference
on Designing interactive systems, pages 93–102. ACM.
[Baur et al., 2010] Baur, D., Seiffert, F., Sedlmair, M., and Boring, S. (2010).
The streams of our lives: Visualizing listening histories in context. Visualization and Computer Graphics, IEEE Transactions on, 16(6):1119–1128.
[Bremner et al., 2013] Bremner, A. J., Caparos, S., Davidoff, J., de Fockert, J.,
Linnell, K. J., and Spence, C. (2013). “bouba” and “kiki” in namibia? a remote
culture make similar shape–sound matches, but different shape–taste matches
to westerners. Cognition, 126(2):165–172.
[Carbon and Jakesch, 2013] Carbon, C.-C. and Jakesch, M. (2013). A model for
haptic aesthetic processing and its implications for design. Proceedings of the
IEEE, 101(9):2123–2133.
[Choe et al., 2014] Choe, E. K., Lee, N. B., Lee, B., Pratt, W., and Kientz, J. A.
(2014). Understanding quantified-selfers’ practices in collecting and exploring
personal data. In Proceedings of the 32nd annual ACM conference on Human
factors in computing systems, pages 1143–1152. ACM.

140

BIBLIOGRAPHY

[Cholewiak and Collins, 2000] Cholewiak, R. W. and Collins, A. A. (2000). The
generation of vibrotactile patterns on a linear array: Influences of body site,
time, and presentation mode. Perception & Psychophysics, 62(6):1220–1235.
[Damasio, 2008] Damasio, A. (2008). Descartes’ error: Emotion, reason and the
human brain. Random House.
[De Cesarei and Codispoti, 2006] De Cesarei, A. and Codispoti, M. (2006).
When does size not matter? effects of stimulus size on affective modulation.
Psychophysiology, 43(2):207–215.
[Delorme et al., 2011] Delorme, A., Mullen, T., Kothe, C., Acar, Z. A., BigdelyShamlo, N., Vankov, A., and Makeig, S. (2011). Eeglab, sift, nft, bcilab, and
erica: new tools for advanced eeg processing. Computational intelligence and
neuroscience, 2011:10.
[Egner et al., 2004] Egner, T., Zech, T., and Gruzelier, J. H. (2004). The effects
of neurofeedback training on the spectral topography of the electroencephalogram. Clinical Neurophysiology, 115(11):2452–2460.
[Eid et al., 2015] Eid, M., Korres, G., and Jensen, C. B. F. (2015). Soa thresholds
for the perception of discrete/continuous tactile stimulation. In PQuality of
Multimedia Experience (QoMEX), 2015 Seventh International Workshop on.
IEEE.
[Ertan et al., 1998] Ertan, S., Lee, C., Willets, A., Tan, H., and Pentland, A.
(1998). A wearable haptic navigation guidance system. In Wearable Computers,
1998. Digest of Papers. Second International Symposium on, pages 164–165.
IEEE.
[Foti et al., 2009] Foti, D., Hajcak, G., and Dien, J. (2009). Differentiating neural responses to emotional pictures: evidence from temporal-spatial pca. Psychophysiology, 46(3):521–530.
[Fryer et al., 2014] Fryer, L., Freeman, J., and Pring, L. (2014). Touching words
is not enough: How visual experience influences haptic–auditory associations
in the “bouba–kiki” effect. Cognition, 132(2):164–173.
[Gianotti et al., 2008] Gianotti, L. R., Faber, P. L., Schuler, M., Pascual-Marqui,
R. D., Kochi, K., and Lehmann, D. (2008). First valence, then arousal: the
temporal dynamics of brain electric activity evoked by emotional stimuli. Brain
topography, 20(3):143–156.
[Golbeck et al., 2011] Golbeck, J., Robles, C., Edmondson, M., and Turner, K.
(2011). Predicting personality from twitter. In Privacy, Security, Risk and

BIBLIOGRAPHY

141

Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social
Computing (SocialCom), 2011 IEEE Third International Conference on, pages
149–156. IEEE.
[Huang et al., 2015] Huang, D., Tory, M., Aseniero, B. A., Bartram, L., Bateman, S., Carpendale, S., Tang, A., and Woodbury, R. (2015). Personal visualization and personal visual analytics. Visualization and Computer Graphics,
IEEE Transactions on, 21(3):420–433.
[Israr and Poupyrev, 2011a] Israr, A. and Poupyrev, I. (2011a). Control space
of apparent haptic motion. In World Haptics Conference (WHC), 2011 IEEE,
pages 457–462. IEEE.
[Israr and Poupyrev, 2011b] Israr, A. and Poupyrev, I. (2011b). Tactile brush:
drawing on skin with a tactile grid display. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, pages 2019–2028. ACM.
[Jensen et al., 2015] Jensen, C. B. F., Korres, G., Bartsch, C., and Eid, M.
(2015). Vibrotactile alarm system for reducing sleep inertia. Affective Computing, IEEE Transactions on.
[Jensen et al., 2014] Jensen, C. B. F., Petersen, M. K., and Larsen, J. E. (2014).
Emotional responses as independent components in eeg. In Cognitive Information Processing (CIP), 2014 4th International Workshop on, pages 1–6. IEEE.
[Jensen et al., 2013] Jensen, C. B. F., Petersen, M. K., Larsen, J. E., Stopczynski, A., Stahlhut, C., Ivanova, M. G., Andersen, T., and Hansen, L. K. (2013).
Spatio temporal media components for neurofeedback. In Multimedia and Expo
Workshops (ICMEW), 2013 IEEE International Conference on, pages 1–6.
IEEE.
[Karanam et al., 2014] Karanam, Y., Alotaibi, H., Filko, L., Makhsoom, E.,
Kaser, L., and Voida, S. (2014). Motivational affordances and personality
types in personal informatics. In Proceedings of the 2014 ACM International
Joint Conference on Pervasive and Ubiquitous Computing: Adjunct Publication, pages 79–82. ACM.
[Kim and Paulos, 2010] Kim, S. and Paulos, E. (2010). Inair: sharing indoor
air quality measurements and visualizations. In Proceedings of the SIGCHI
Conference on Human Factors in Computing Systems, pages 1861–1870. ACM.
[Lang and Bradley, 2007] Lang, P. and Bradley, M. M. (2007). The international
affective picture system (iaps) in the study of emotion and attention. Handbook
of emotion elicitation and assessment, 29.

142

BIBLIOGRAPHY

[Lang and Bradley, 2010] Lang, P. J. and Bradley, M. M. (2010). Emotion and
the motivational brain. Biological psychology, 84(3):437–450.
[Lang et al., 1997] Lang, P. J., Bradley, M. M., and Cuthbert, B. N. (1997).
International affective picture system (iaps): Technical manual and affective
ratings. NIMH Center for the Study of Emotion and Attention, pages 39–58.
[Larsen et al., 2013] Larsen, J. E., Cuttone, A., and Jørgensen, S. L. (2013). Qs
spiral: Visualizing periodic quantified self data. In CHI 2013 Workshop on
Personal Informatics in the Wild: Hacking Habits for Health & Happiness.
[Larsen et al., 2003] Larsen, J. T., Norris, C. J., and Cacioppo, J. T. (2003).
Effects of positive and negative affect on electromyographic activity over zygomaticus major and corrugator supercilii. Psychophysiology, 40(5):776–785.
[Li et al., 2010] Li, I., Dey, A., and Forlizzi, J. (2010). A stage-based model of
personal informatics systems. In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, pages 557–566. ACM.
[Li et al., 2011] Li, I., Dey, A. K., and Forlizzi, J. (2011). Understanding my data,
myself: supporting self-reflection with ubicomp technologies. In Proceedings
of the 13th international conference on Ubiquitous computing, pages 405–414.
ACM.
[Maurer et al., 2006] Maurer, D., Pathman, T., and Mondloch, C. J. (2006). The
shape of boubas: sound–shape correspondences in toddlers and adults. Developmental science, 9(3):316–322.
[McCrae and Costa, 2003] McCrae, R. R. and Costa, P. T. (2003). Personality
in adulthood: A five-factor theory perspective. Guilford Press.
[Moore, 2002] Moore, G. A. (2002). Crossing the chasm. Capstone.
[Munson, 2012] Munson, S. (2012). Mindfulness, reflection, and persuasion in
personal informatics.
[Nass and Lee, 2000] Nass, C. and Lee, K. M. (2000). Does computer-generated
speech manifest personality? an experimental test of similarity-attraction. In
Proceedings of the SIGCHI conference on Human Factors in Computing Systems, pages 329–336. ACM.
[Ohlin et al., 2015] Ohlin, F., Olsson, C. M., and Davidsson, P. (2015). Analyzing
the design space of personal informatics: A state-of-practice based classification
of existing tools. In Universal Access in Human-Computer Interaction. Access
to Today’s Technologies, pages 85–97. Springer.

BIBLIOGRAPHY

143

[Patton and Economy, 2014] Patton, J. and Economy, P. (2014). User Story
Mapping: Discover the Whole Story, Build the Right Product. " O’Reilly Media,
Inc.".
[Powers, 1973] Powers, W. T. (1973). Behavior: The control of perception. Aldine
Chicago.
[Ramachandran and Hubbard, 2001] Ramachandran, V. S. and Hubbard, E. M.
(2001). Synaesthesia–a window into perception, thought and language. Journal
of consciousness studies, 8(12):3–34.
[Reisberg, 1997] Reisberg, D. (1997). Cognition: Exploring the science of the
mind. WW Norton & Co.
[Ries, 2011] Ries, E. (2011). The lean startup: How today’s entrepreneurs use
continuous innovation to create radically successful businesses. Random House
LLC.
[Saffer, 2013] Saffer, D. (2013). Microinteractions: designing with details. "
O’Reilly Media, Inc.".
[Schön, 1983] Schön, D. A. (1983). The reflective practitioner: How professionals
think in action, volume 5126. Basic books.
[Stopczynski et al., 2014] Stopczynski, A., Stahlhut, C., Petersen, M. K., Larsen,
J. E., Jensen, C. F., Ivanova, M. G., Andersen, T. S., and Hansen, L. K.
(2014). Smartphones as pocketable labs: Visions for mobile brain imaging and
neurofeedback. International journal of psychophysiology, 91(1):54–66.
[Tsetserukou et al., 2009] Tsetserukou, D., Neviarouskaya, A., Prendinger, H.,
Kawakami, N., and Tachi, S. (2009). Affective haptics in emotional communication. In 2009 3rd International Conference on Affective Computing and
Intelligent Interaction and Workshops.
[Vernon et al., 2003] Vernon, D., Egner, T., Cooper, N., Compton, T., Neilands,
C., Sheri, A., and Gruzelier, J. (2003). The effect of training distinct neurofeedback protocols on aspects of cognitive performance. International journal
of psychophysiology, 47(1):75–85.
[Yatani and Truong, 2009] Yatani, K. and Truong, K. N. (2009). Semfeel: a user
interface with semantic tactile feedback for mobile touch-screen devices. In
Proceedings of the 22nd annual ACM symposium on User interface software
and technology, pages 111–120. ACM.

144

BIBLIOGRAPHY

[Zoefel et al., 2011] Zoefel, B., Huster, R. J., and Herrmann, C. S. (2011). Neurofeedback training of the upper alpha frequency band in eeg improves cognitive
performance. Neuroimage, 54(2):1427–1431.

