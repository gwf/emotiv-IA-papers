Towards Multimodal, Multi-party, and Social
Brain-Computer Interfacing
Anton Nijholt
University of Twente, Human Media Interaction
P.O. Box 217, 7500 AE Enschede, The Netherlands
anijholt@cs.utwente.nl

Abstract. In this paper we identify developments that have led to the current
interest from computer scientists in Brain-Computer Interfacing (BCI). Nondisabled users have become a target group for BCI applications. Non-disabled
users can not be treated as patients. They are free to move and use their hands
during the interaction with an application. Therefore BCI should be integrated in a
multimodal approach. Games are an important research area since shortcomings
of BCI can be translated into challenges in multimodal cooperative, competitive,
social and casual games.
Keywords: Brain-Computer Interfacing, Human-Computer Interaction, Multimodal interaction, Games.

1

Introduction

Until recently Brain-Computer Interfacing (BCI) was only considered to be useful
for applications that were intended for disabled users. BCI has been used for
restoring the communication and mobility of disabled people through applications
such as spellers, browsers and wheelchair controls. BCI research aimed almost solely
at improving the life of ALS (Amyotrophic Lateral Sclerosis) patients, Parkinson
patients, or other kinds of patients in need of interaction with their environment.
When there is no alternative, people accept that training is necessary, that they need
help to get connected to a computing device, that performance is far from perfect,
that they need to concentrate fully on the task at hand, and that any disturbance of
this concentration will lead to a breakdown in control. BCI was not part of Computer
Science (CS) and Human-Computer Interaction (HCI), let alone a modality that
could be used in combination with other modalities to control applications for nondisabled users.

2

Towards BCI for Various Target Populations

In this paper we survey and summarize developments in BCI and HCI that made it
possible that BCI research has become an accepted topic in HCI research.
A. Camurri, C. Costa, and G. Volpe (Eds.): INTETAIN 2011, LNICST 78, pp. 12–17, 2012.
© Institute for Computer Sciences, Social Informatics and Telecommunications Engineering 2012

Towards Multimodal, Multi-party, and Social Brain-Computer Interfacing

13

Ambient Intelligence. Ambient Intelligence (AmI), pervasive computing, ubiquitous
computing and 'disappearing computers' are names that have been introduced to
describe the research domains in which we assume that we live or will live in sensorequipped environments, that the sensors will be embedded, that they will have local
intelligence, and that the information they collect and process can be distributed to
other intelligent sensors and computing devices. Obviously, there are already sensorequipped environments, but, as long as their design is tuned to rather specialized
applications, they will certainly not achieve their full potential. In AmI environments,
sensors can be used to detect and interpret human behavior and activities, to anticipate
certain activities or desires in order to provide real-time support, and to allow explicit
control of the environment by its inhabitants by providing feedback and appropriate
actions on commands of the inhabitants. These views have led to an increase in
attention for sensors in general, including sensors that allow us to issue commands,
for example for games and domestic applications, through BCI devices and systems.
HCI and Physiological Measurements. There has always been interest in using
(neuro-) physiological measurements to learn about the cognitive load associated with
performing certain tasks using a particular interface. Hence, in this case we are not
talking about real-time support of the user. We are talking about providing the designer
of the interface with information about how users use the interface and about how
users experience the interface. Measuring cognitive load is of course the standard
example of what interface designers are interested in. This kind of information is
meant to re-think, to re-design, and to re-implement the interface in order that it should
perform better for a particular user or group of users. However, in recent years many
more methods have become available to measure experience. Computer vision, speech
analysis, and eye tracking are among them, and this has led to a boost of interest,
methods and devices, including BCI devices, that not only measure user experience for
redesign and performing tasks more efficiently, but also look at 'tasks' that do not
necessarily require efficiency but rather aim at providing positive experiences such as
fun, game experience, relaxation, and edutainment. And, moreover, use the
information that is sensed in real time to adapt the interface, the task (e.g., the game
level) and the interaction modalities to user and context.
BCI Paradigms. The possibility of having sensors (cameras, microphones, accelerators,
pressure sensors, proximity sensors, physiological sensors, etc.) that gather knowledge
about user characteristics and user activities and behavior is, from an HCI point of view,
very useful. It allows us to provide better support to the user of a smart environment.
Looking at the possibility of gathering as much information from a user as can be
sensed has become a research aim. Brain activity can be sensed. So, why not use it?
Knowledge has become available about activity and its appearance in specific regions of
the brain. Therefore, various BCI paradigms could be introduced. Activity can be
evoked by presenting external stimuli (visual, auditory, tactile), which means that
choices can be presented to a user. Brain activity can be measured while a user performs
a certain task and the results can be used to adapt task and interface to the user. Certain
brain activity is related to a user making errors, a user noticing something irregular, or a

14

A. Nijholt

user noticing an event that was anticipated. But there can also be internally evoked
activity: elicited by imagining a movement or consciously performing a mental activity.
These paradigms allow the mapping of brain activity to implicit or explicit control
activity for particular applications.
Towards Unobtrusive Sensing. Measuring brain activity, whether it is caused by
internally evoked mental activity or by external activity, requires sensors. For brain
research it is no problem to ask a person to perform a certain physical or mental task
in a situation that is not necessarily a 'daily life' situation. Rather complicated and
expensive devices, e.g. an fMRI scanner, can be used. Fortunately, there are other
ways to record brain activity, but unfortunately, they are less precise and only allow a
limited number of applications. However, activity related to the BCI paradigms
mentioned above can be measured, not yet unobtrusive, but slowly getting there.
The standard way of measuring brain activity in BCI applications is to use a BCI
electrodes cap that is on the head of the user and is connected to a computer. The
computer can be embedded in an application device, for example a wheelchair, but it
can also be a standard PC. This EEG (ElectroEncephaloGraphy) method has up to
256 electrodes integrated in a cap and placed directly on the head. Their positions on
the head make it possible to gain information about the electrical activity and,
importantly, the function of this activity. But there is not always a need for an EEG
cap with lots of electrodes when we look at applications. Apart from being expensive,
it has the disadvantage that users are physically connected to the computer, and time
is needed to position the electrodes, apply conductive gel and clean-up afterwards.
However, in recent years research groups and companies have developed EEG
devices that use so-called dry-cap technology (or 'dry electrodes') and they are
exploring the possibility of having a wireless connection between device and
computer, so that the user can move more freely. And sometimes, depending on the
application, rather than 256 electrodes it is sufficient to have a device with two, eight
or sixteen electrodes, allowing companies to develop fancy and portable headsets that
can be used for game or domestic applications and in which other sensors, such as
accelerators measuring head movements, can be integrated. Companies that are
exploring the market for portable headsets for BCI applications can now be identified.

3

BCI, Computer Science, and Human-Computer Interaction

We mentioned the main reasons why BCI research has become visible for the CS and
HCI community and why we are now seeing attempts to integrate this research into
HCI research in general, research on multimodal interaction, research on using
(neuro)physical information aiming at improving interface design, and on real-time
adaptation of the interface to the user, and on having BCI as an added modality to
control devices and facilitate communication.
When BCI became visible for the CS community one could expect that both startup companies and R&D departments of large ICT companies would try to exploit and
investigate the commercial possibilities of this new technology. That has indeed
happened. New companies such as Emotiv and NeuroSky, just to mention the most

Towards Multimodal, Multi-party, and Social Brain-Computer Interfacing

15

influential, and companies such as IBM and Microsoft, have become active in this
field. Rather than aiming at medical applications they look at the much bigger market
of non-disabled and healthy persons. Consumer products are being offered, but until
now these are mainly games, toys, and gadgets. This is not bad, the game market is a
billion dollar market and still growing. But clearly, companies also see possibilities to
introduce BCI into domestic and professional environments where an added modality
to interact with an application will make the interaction more safe or precise.

4

Integrating BCI in Human-Computer Interaction

It is useful to be more explicit about what BCI can offer to the HCI community. Any
interface to an application can profit from knowing and learning as much as possible
of the persons that are using it. Knowing about activity in particular regions of the
human brain provides information that cannot be obtained from other sensors and that
can complement that information. The information can be used to inform the interface
about the mental and affective state of the user and that knowledge can be used to
provide more adequate feedback and also to adapt the interface and the application to
a particular user. In recent years this type of BCI has been called passive BCI. It is the
system that decides how to use the information. There is no attempt by the user to
control the system by consciously ‘playing’ with this brain activity.
The second reason that BCI is interesting for HCI is similar to the reason that BCI
has been exploited for disabled persons: a user can use his or her brain activity as an
input modality, maybe in combination with other modalities, to directly control an
interface and its application. By performing certain mental tasks the associated brain
activity in various regions of the brain can be distinguished and mapped onto
commands that control the application. Applications include, among other things,
navigating in a virtual world, controlling a robot, or cursor and menu control.
Clearly, it is preferable that this mapping is ‘natural’ or ‘intuitive’. Hence, the mental
task that has to be performed should be related to the task that has to be performed in
the real world or in the graphical user interface. This type of BCI has been called
active BCI. A nice example is imagined movement. Brain activity related to
imagining a movement (whether it is the tongue, a finger, or a limb) can be
distinguished from other activity and can be used to steer a robot around, to control a
menu and a cursor, or to have an avatar perform a movement in a virtual world.
Other types of BCI have been introduced or have been included in the definitions
of active and passive BCI. For example, there are visual, auditory or tactile evoked
potentials. That is, events can be designed in an application to evoke distinguishable
brain activity. This allows the user to make clear to the system in which of the
available alternatives he or she is interested just by paying attention to it. A possible
name for this type of BCI is reactive BCI. Of course, evoked potentials can also
happen and be measured when a user is in a situation where he or she has to perform a
routine task where once in a while an interesting event happens and this is noticed by
the BCI because of a change in brain activity in a particular brain region. Again, an
implicit command from the user to the application can be issued if this particular
brain activity is detected and the application is ready to accept this command.

16

A. Nijholt

There are no clear-cut distinctions between the different types of BCI we have
mentioned. For example, we can measure an affective state or rather changes in an
affective state in order to adapt the interface to these changes. But it is also possible to
design applications where the user is expected to change his or her affective state in
order to issue a command. For example, in a game situation we can have a natural
change, caused by events happening in the game world, from a user being relaxed to a
user being stressed, or the other way around. But issuing commands in such a
situation by affective state changes that are consciously aimed at by a gamer can work
as well. In the latter situation a gamer has, for example, to decide to become
aggressive or to become relaxed knowing what the effect will be in the game world.
We mentioned ways in which brain activity can be used to issue commands or to
adapt the interface, the interaction or the particular task a user is expected to perform.
A final observation that can be made is that there can be designed or naturally
occurring stimuli (physical, visual, auditory, tactile, olfactory) that help the user to
perform a mental task or to make a transition from one affective state to another.

5

Multimodal and Hybrid Brain-Computer Interfacing

In HCI and CS research environments physical or mental disabilities of users are
hardly taken into consideration. This is quite a contrast with a BCI application that
aims at providing an ALS patient with a communication device. These patients are
'locked-in', their brain is functioning, but there is no way that brain activity can
control the patient’s muscles and movements. It also means that measuring the brain
activity related to what a patient wants to communicate will not be interfered with by
brain activity evoked by the actual movements of the patient, including involuntary
movements such as eye blinks. Clearly, if we want to design BCI applications for
non-disabled users we cannot assume that users will not move and therefore we
should be able to distinguish brain activity that is meant to control a device from brain
activity that has other causes. Similarly, we need to be able to distinguish brain
activity that we want to use to adapt the interface to a particular mental state of the
user from brain activity that is caused by intended or performed movements. These
are quite complicated issues and they lead to quite complicated research issues.
These issues are addressed in hybrid BCI research [4] and in multimodal
interaction research [2], where BCI is one of the modalities whose role in the
interaction is supported by other modalities or whose role provides support to the
other modalities. That is, information obtained from measuring brain activity during
interaction can help to reduce ambiguity that is still there after analyzing the role of
the more traditional modalities. On the other hand, it can also be the case that brain
activity plays the leading role and that the other modalities are there to support and
complement the information that can be extracted from the brain activity and that is
meant to control devices or activities in a (virtual) environment or that is meant to
inform the environment about how to adapt to a particular user, including the user's
preferences, cognitive load, and affective mental states. Designing 3D game
environments that include BCI as an interaction modality allows us to experiment
with multimodal (including BCI) interaction modalities, without necessarily being
bothered or limited in creativity by questions about robustness and efficiency [2,5].

Towards Multimodal, Multi-party, and Social Brain-Computer Interfacing

6

17

Multi-party and Social Brain-Computer Interfacing

Nevertheless, robustness is an issue and will probably remain an issue for a long time.
Having robust BCI also means that we have the possibility to control and to decrease
robustness in order to introduce challenges in game and entertainment situations. But
mainly robustness is necessary in noisy environments, environments with lots of
distractions, and environments that require the user to spend his or her attention to
non-BCI related issues. However, these issues are being addressed in current BCI
research. To mention some of the situations that are being addressed: walking in a city
environment while using BCI, measuring and distinguishing mental activity while
driving a car, playing pinball with motor imagery, playing World of Warcraft with
BCI control, or playing a BCI version of Pong with a friend while others are watching
and commenting. All this has been done and certainly not in an unsatisfactory way.
Clearly, when we look at nowadays video games, then most interest goes to
competitive and cooperative games. There are others involved, sometimes in a
mediated manner, sometimes physically present as in LAN parties with lots of verbal
and nonverbal interactions or in a situation comparable to a traditional board game.
We are investigating the possibilities of such cooperative and competitive multimodal
and multi-party social games that use BCI [3]. A next step, in which we look at
mediating brain activity from one person to another is much further away, but
scientific discussion about it is becoming possible [1]. That will lead us to a social
media change from FaceBook to BrainBook.
Acknowledgments. We gratefully acknowledge the support of the BrainGain Smart
Mix Programme of the Netherlands Ministry of Economic Affairs and the Ministry of
Education, Culture and Science.

References
1. Chorost, M.: World Wide Mind: The Coming Integration of Humanity, Machines, and the
Internet. Free Press, New York (2011)
2. Gürkök, H., Nijholt, A.: Brain-computer interfaces for multimodal interaction: a survey and
principles. International Journal of Human-Computer Interaction (2011) (to appear)
3. Obbink, M., Gürkök, H., Plass-Oude Bos, D., Hakvoort, G., Poel, M., Nijholt, A.: Social
Interaction in a Cooperative Brain-computer Interface Game. In: Camurri, A., Costa, C.,
Volpe, G. (eds.) INTETAIN 2011. LNICST, vol. 78, pp. 179–188. Springer, Heidelberg
(2011)
4. Pfurtscheller, G., Allison, B.Z., Brunner, C., Bauernfeind, G., Solis-Escalante, T., Scherer,
R., Zander, T.O., Mueller-Putz, G., Neuper, C., Birbaumer, N.: The hybrid BCI. Frontiers in
NeuroScience 2, article 3, 1–11 (2010)
5. Tan, D., Nijholt, A. (eds.): Brain-Computer Interfaces: Applying our Minds to HumanComputer Interaction. Human-Computer Interaction Series. Springer, London (2010)

