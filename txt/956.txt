Simulating Auditory Hallucinations in a Video Game:
Three Prototype Mechanisms
Jonathan Weinel

Stuart Cunningham

Music and Sound Knowledge Group (MaSK)
Aalborg Universitet
Musikkens Plads 1
9000 Aalborg
+45 99 40 31 32

Affective Audio
Glyndŵr University
Plas Coch Campus
Mold Road, Wrexham, LL11 2AW, North Wales, UK
+44(0)1978 293583

weinel@hum.aau.dk

s.cunningham@glyndwr.ac.uk

ABSTRACT
In previous work the authors have proposed the concept of ‘ASC
Simulations’: including audio-visual installations and experiences,
as well as interactive video game systems, which simulate altered
states of consciousness (ASCs) such as dreams and hallucinations.
Building on the discussion of the authors’ previous paper, where a
large-scale qualitative study explored the changes to auditory
perception that users of various intoxicating substances report,
here the authors present three prototype audio mechanisms for
simulating hallucinations in a video game. These were designed
in the Unity video game engine as an early proof-of-concept. The
first mechanism simulates ‘selective auditory attention’ to
different sound sources, by attenuating the amplitude of
unattended sources. The second simulates ‘enhanced sounds’, by
adjusting perceived brightness through filtering.
The third
simulates ‘spatial disruptions’ to perception, by dislocating sound
sources from their virtual acoustic origin in 3D-space, causing
them to move in oscillations around a central location. In terms of
programming structure, these mechanisms are designed using
scripts that are attached to the collection of assets that make up the
player character, and in future developments of this type of work
we foresee a more advanced, standardised interface that models
the senses, emotions and state of consciousness of player avatars.

Categories and Subject Descriptors
• Applied computing~Sound and music computing • Humancentered computing~Sound-based input / output • Software and
its engineering~Interactive games

General Terms
Design, Experimentation.

Keywords
Sound Design, Altered States of Consciousness, Video Games,
Auditory Hallucinations, Game Sound.

1. INTRODUCTION
In previous work we have discussed the concept of Altered States
of Consciousness (ASCs), in the context of video games [3, 16,
17, 19].
ASCs may include states of dream, delirium,
hallucination, meditation or trance (among others), as were
discussed in early work by Tart [13]. As capabilities for
representing subjective states in first and third-person point-ofview (POV) computer games have increased, we have begun to
see representations of ASCs in video games. For example, Far
Cry 3 (2012) and Grand Theft Auto V (2013) are two examples of
recent games that explicitly use modifications and time-based

modulations of graphics and sound in order to reflect states of
hallucinations, which occur when the game character consumes
intoxicating substances such as psychedelic drugs. Along similar
lines, Demarque and Lima [5] have explored horror games that
portray auditory hallucinations. Just as games provide improved
realism through the use of photorealistic graphics and spatial
sound, the possibility has emerged that these ‘unreal’ experiences
can be portrayed with greater levels of sophistication and
accuracy than every before. The nature of 3D computer game
worlds, in particular, offers the possibilities of not only adjusting
the way in which a sound is represented in a game, but affords the
possibility of spatial audio manipulations through binaural and
multi-channel speaker configurations, that are now a standard
feature in such games. This offers the opportunity to bring the
experience of an auditory hallucination much closer to being
‘inside’ or ‘around’ the head of a listener in a more convincing
and immersive fashion.
It is an assumption of our research that by seeking to design ASCs
in video games based on research, rather than relying on clichés or
intuitive design approaches, that we can provide simulations of
ASCs in video games that have improved accuracy. Towards this
end, in a previous study [18] we carried out a large-scale
qualitative study of nearly 2000 experience reports, gathered from
the internet site erowid.org, which has a large database of usersubmitted experience reports regarding the effects of various
intoxicating substances. From these studies we devised a system
of categorisation to be used as a basis for sound design. We also
created a number of proof-of-concept audio files based on these,
which were related to descriptions of auditory hallucinations from
the study. Here we extend this work by using these descriptions
as a basis for three prototype mechanisms in a video game demo,
which we created using the game engine Unity. This paper begins
by summarising the basis for these mechanisms from the previous
study, before outlining the mechanisms themselves, as we
implemented them in Unity. We then offer some concluding
comments, outlining further challenges and possible applications
for this research.

2. AUDITORY HALLUCINATIONS
2.1 Overview
Auditory hallucinations are experiences of sound that occur
without acoustic stimulation that arises in a real environment
external to the individual. They may occur due to intoxication
through various substances such as psychedelic drugs, or via
neurological conditions such as schizophrenia. In the latter cases,
‘auditory verbal hallucinations’ (AVH) are a common
phenomenon, during which the individual hears voices that arise

internally, with no external acoustic basis.
Although in
schizophrenia, AVH are the most widely discussed form of
auditory hallucination, and may be the most common [15], other
types of sound can also be experienced. For example, ‘non-verbal
auditory hallucinations’ (NVAH) are also reported, which may
include noise-based sounds [8]. Other types, such as music, are
also known to occur [9]. In our previous study of experiences of
intoxication, in which we looked at a large number of self-reports
(focusing especially on the effects of psychedelic drugs),
hallucinated noises were actually more commonly reported than
voices [18].

2.2 System of Categorisation
From our previous study [18] we suggested a continuum of
auditory hallucinations ranging from unaltered sounds that arise
‘externally’ from the environment, to wholly hallucinated sounds
that originate ‘internally’ within the brain (Figure 1). According
to this continuum, ‘normal’ perceptions are those that arise via an
external environment, and are not abnormally altered in their
perception due to intoxication. ‘Enhanced’ sounds are those that
arise externally, but which are perceived as either more, or less,
enjoyable, interesting, detailed or intricate than one would
normally expect. ‘Distorted’ sounds similarly arise via the
external environment, but are more dramatically altered due to
intoxication; for example, individuals may describe hearing
sounds that are heard as if processed by filters or effects, such as
distortion or echo. Lastly, ‘hallucinated’ sounds are those that
arise internally, with no acoustic basis in the environment.
Sounds in this category can be subdivided into different types,
including hallucinated music, noise, voices, or silence.

we can use as a basis to provide real-time audio mechanisms
using computer audio techniques. This approach provides more
tangible and focused outcomes when we begin to evaluate
technological approaches since they serve as a baseline, which can
later be assessed, rather than making abstract interventions based
upon the continuum of auditory hallucinations. In this paper we
explore three such mechanisms, which we shall first illustrate with
reference to the text statements acquired previously.

2.3.1 Selective Auditory Attention
The first of our ‘features’ is ‘selective auditory attention’.
Selective auditory attention is a well-established concept in
cognitive psychology. In early work by Broadbent [2], using
‘dichotic listening tests’, selective auditory attention was shown,
and provided the basis for his proposition of a ‘filter theory of
attention’. This theory proposes that attention directs incoming
auditory inputs, promoting attended sources for higher-level
processing, while others are ‘filtered out’ and do not reach
awareness. Subsequent studies such as Treisman [14] developed
this theory, proposing that unattended sources are ‘attenuated’,
and are passed for very limited higher level processing only.
Although selective auditory attention is a phenomenon
experienced in normal waking consciousness, under ASCs the
reports of intoxicated individuals seem to suggest that the
operation of attention is significantly altered. In some cases,
individuals seem more susceptible to complete absorption in a
particular entity, for instance. Excerpts from the text statements
obtained in our previous work illustrate this point:
“During this time I noticed a lot of things that I don't normally
pay much attention to, in a serious way. Every single movement
of my fingers was palpable, and the vibrations my vocal chords
made were so amazingly easy to feel and control...”
“My attention was constantly scanning my surroundings, and so
background noises became much more apparent.”
Alternatively, however, in some cases individuals reported a
perceived broadening of their attention; that is, they claimed to be
able to pay attention to more things simultaneously. For example:

Figure 1: Proposed continuum of auditory hallucinations.
We placed these sounds on a continuum since, at each progressive
stage, we may consider that internal perception (which is present
in all experiences of sound), plays an increasingly dominant role
in defining the subjective experience of sound. This ‘internal
perception’ encompasses various cognitive systems, including
those of attention, memory, and inputs from other sensory
modalities such as vision; the latter of which are commonly
associated with synaesthesia [4], and have also been demonstrated
through phenomena such as the McGurk effect [10]. The
concepts of ‘internal’ and ‘external’ sensory inputs here are
derived from Hobson’s [6] work, and his ‘Activation, Input,
Modulation’ concept of consciousness.

2.3 Specific Features
While our system outlines the main types of auditory
hallucination, our previous study [18] also assembled a large body
of text statements from individuals describing specific
experiences of auditory hallucination. From these it is possible to
extract statements describing particular kinds of phenomena that

“Listening to 'The Crystal Method'… their music sounds even
more amazing than ever. Their music style sometimes has many
'layers' to it, and it feels like my (high-quality) headphones are
playing 5 songs at once, and I can enjoy each of them with my
undivided attention.”
“I am not hearing with my (attention-limited) ears any longer, but
with my whole perception. I can hear a dog barking echoing from
the wall over my right shoulder from the window to my left, a
truck downshifting on [the] street a block and a half away, hear
the dampening effect of the warmer, moister air next to the grass
of the lawn as a tangible presence.”
These reports seem to suggest that the narrowing or widening of
auditory attention may be a feature of hallucinations, and indeed,
this is consistent with classic accounts such as Huxley’s The
Doors of Perception [7], in which he memorably describes
becoming absorbed in the creases of his trousers. In this paper,
we focus especially on the idea of intoxicated states causing a
narrowing of auditory perception, following Huxley’s account,
though further development of this work might also look at ways
to represent widening of attention.

2.3.2 Enhanced Sounds
As noted in our system of categorisation, experiences of
‘enhanced’ sound during intoxication are common; indeed, this is
the most typical change to the experience of sound that occurs due
to intoxication. The following quotes are indicative of the type of
change that occurs, which we may reasonably attribute to
intoxication:
“My attention then shifted to the music. It suddenly got very loud.
The band sounded so good I had no idea how I had missed it
before.”
“Music was also amazing, like a wall of sound, with some
colours.”
“Music was very effective to hear and she could feel the sound as
if it were currents of air moving in the room.”
“The music that was on sounded absolutely amazing. It was
shocking! How on earth could they make music that sounded so
good?! I wondered if those specific sound waves happened to be
tuned to trigger euphoria in the human brain.”
Although individuals often report music being more interesting,
enjoyable or meaningful, in some accounts the opposite occurs,
and sound seems to be more foreboding or a source of anxiety:
“The music seemed to become metallic and foreboding”
“Noises from neighbours and a tooting on the street made me
paranoid…”
Here then, we seem to find a further typical variation in
perception, where sound and music can be more, or less,
interesting, detailed, or enjoyable, in a way we can attribute
specifically to the intoxicated state of the individual. It is also
interesting to note that enhancements that are reported can relate
to the intensity of sound sources and also to the timbral or
frequency content of the stimuli.

2.3.3 Spatial Disruption
Lastly, another phenomena that we noted in some reports, is that
of ‘spatial disruption’. In these cases, the individual may perceive
sounds that seem as though they are more ‘close’ or ‘distant’ than
usual, or otherwise perceive changes related to the spatial
experience of sound, as incongruous with the acoustic properties
of the environment in which the individual is situated. The
following quotations illustrate this effect:
“My hearing began to get messed up.
direction sounds were coming from.”

I couldn’t tell which

“My perception of space was terribly skewed. The wind, the
noise, the movement through space is overwhelming.”
“A car would be about four hundred yards or so down the street,
but I could hear them as if they were about to hit me… It was very
confusing to see a car very far away but hear it as if it were so
close.”

.

“The sounds in the room of my companions talking quietly around
me were echoing, as if I were in a stainless steel vessel, but I was
unable to understand what they were saying.”
As with the other features, we may note that the specific effects
seem to vary between individuals and instances of intoxication,
yet ‘spatial disruption’ seems to be another common theme, and
one that we will draw upon in this paper. As it is spatial, this type
of phenomenon broadly incorporates temporal and phase
characteristics, coupled with intensity and/or frequency-based
elements. Although it is possible that confusion or mismatches
over sound localisation can occur even during ‘normal’ states (for
instance, due to acoustic reflections), the scenarios described in
the reports suggest that the ASC are the key factor.
The features outlined in this section are by no means exhaustive,
as our text statements also illustrate various other features, such as
experiences of synaesthesia; for instance, experiences in which
sounds and music trigger corresponding visual hallucinations.
These other features will remain beyond the scope of this paper,
though could also be explored in future work. Here however, we
shall focus on the three features outlined in this section, and
consider these as a basis for the design of real-time audio systems
in a game engine.

3. THREE PROTOTYPE MECHANISMS
Having identified three features of auditory hallucination, we can
begin to devise specific mechanisms for generating these sounds
using real-time audio systems within video games. The benefit of
doing so should be to allow improved accuracy in the simulation
of these effects for first-person POV video games, and also to
allow them to be created more easily by having automated
systems that produce these effects, removing the need for sound
designers to generate each of these sonic experiences ‘from
scratch’.
In order to provide an early prototype of such a system, a simple
demo game was created in the video game engine Unity, using
basic geometric shapes. The game consists of a grassy plane that
forms the ‘land’, a skybox, and a ‘player’ asset that allows
navigation in first-person POV (Figure 2). Situated on the plane
are several coloured boxes (cubes) that represent localised
acoustic sources in the 3D world. These are assigned looping 3D
sounds with distance attenuation (i.e. sounds that change in
amplitude based on their relative distance from the player), such
as synthesizer tones utilising a range of frequencies. In an actual
game, these might be replaced with any sound emitting sources
such as machines, radios, streams or game characters; here
however, since we are only interested in modelling processes,
coloured boxes that indicate the location of the sound sources are
sufficient. This demo game provides a basis for the proof-ofconcept audio mechanisms that we shall now describe.

Figure 2: Screenshot from the prototype Unity game. The
coloured boxes provide simple sound sources for testing out
the auditory mechanisms. The power-meters in the top-left
corner of the user interface indicate levels of ‘Attention’ to a
particular sound source, and ‘Enhancement Level’,
respectively.

spectrum. As indicated in Figure 4, increasing the ‘enhancement’
slider mixes between the three versions, producing ostensibly
brighter sounds in the fully enhanced position, and duller sounds
in the lowest position of the parameter.

3.1 Selective Auditory Attention
As noted previously ‘selective auditory attention’ involves the
filtering or attenuation of sound sources in subjective perception.
In our prototype game we model these as follows:
The attention of the player is taken based on which object the
game character is looking at. This is calculated based on the
central point of the player’s view (or ‘crosshair’), which casts a
ray intersecting with whatever object the camera is pointing at.
While the player is paying attention (i.e. looking at) one of the
sound sources (represented by the coloured blocks), a value for
attention increases (Figure 3). This is represented in the demo
with a simple power-meter. As this value increases, the other
sound sources are attenuated using an envelope function, causing
them to fade out. This produces the effect that when the player
attends a particular sound source, all others are reduced,
representing subjective attention to the one sound source only.
Looking away from that sound source reduces ‘attention’, and all
other sounds fade back into hearing. Similarly, looking at a
different sound source causes the sound for the new one to be
heard while others are faded out. In this way we provide a simple
model of selective auditory attention.

Figure 4: Diagram showing how the ‘enhanced sounds’
function changes the amplitude of sound sources. As the
enhancement meter increases, the source material mixes
between ‘dull’, ‘medium’, and ‘bright’ versions of the source
material.

3.3 Spatial Disruption
The phenomenon of ‘spatial disruption’ in experiences of auditory
hallucinations describes the experience of sounds that seem to
emerge from a location other than where the individual would
expect based on the physical position of the acoustic source.
Our Unity demo models this spatial disruption using sine
operators, which allow us to variably modulate the distance of the
sound source from its origin in time. This approach was
conceived as a way to retain an approximate sense of where sound
sources are located with regards to their associated objects,
preserving some salient features of the acoustic scene, while also
causing disruption. As indicated in Figure 5, this causes
oscillations of the location of the sounds in 3D space, while the
actual object (the box that you can see) stays in a fixed location.
In a version of the prototype, additional spheres were attached to
the moving sound sources, allowing us to observe and verify their
path around the associated object.

Figure 3: Diagram showing the ‘selective auditory attention’
function. As the player pays attention (looks at) a sound
source, all others fade out, so that only the object of attention
is heard.

3.2 Enhanced Sounds
Following our discussion, ‘enhanced sounds’ occur due to
intoxication, and may cause experiences of sound to seem more
bright, intricate, detailed, enjoyable or interesting than usual.
In our proof-of-concept game, we modelled this by implementing
a value for ‘enhancement’. This value is indicated visually with a
power-meter graphical display. Changing the value of the bar
causes the sound sources to cross-fade between different versions
of the same sound loops, which have been prepared in advance
with different equalisation and filtering processes. Three versions
of each sound were prepared providing ‘dull’, ‘medium’ and
‘bright’ versions of the same sound, which are produced using a
graphical equaliser and filters in a digital audio workstation
(DAW). The ‘dull version’ subtracts high frequencies reducing
the frequency range of the source material; the ‘medium’ version
is close the original sound; and the ‘bright version’ boosts
frequencies, creating an artificially enhanced sense of the sound

Figure 5: Diagram showing ‘spatial disruption’. The sound
source moves in an oscillating ‘flower’ pattern around the
central point where the associated object is located in 3Dspace.

4. STRUCTURAL DESIGN
The concept of ‘ASC Simulations’ that the prototype game is
based on, presumes that eventually game engines could provide
pre-designed packages for modelling the subjective conscious
states of avatars, offering parameterised features that modulate
graphics and sounds based on the conscious state of the game
characters with minimum effort. For example, as discussed in

previous work [17], we could provide an ‘input’ parameter that
modulates between internal and external sources of sensory
information. Similarly, the emotional states of a video game
character could be defined according to valence and arousal
models of emotion such as Russell’s ‘circumplex model of affect’
[12].
Following this view, our Unity prototype places the scripts for the
respective auditory mechanisms as features within a group entitled
‘ASC Engine’, which is located within the group of objects that
make up the first-person POV game character (this larger group
also includes assets such as the camera that represents the vision
of the game character). We argue that this is the most logical
place to locate the code, and indeed, eventually we could conceive
of an ‘ASC Engine’ group of scripts being attached to multiple
game characters as a way to modulate their functionality based on
different states of consciousness. Figure 6 illustrates the structural
location of the scripts in Unity.

Figure 6: Diagram indicating structural design in Unity. The ‘ASC
Engine’ provides the prototype features, and is located a subgroup
attached to the main ‘first person controller’ group.

We propose that this structural approach is essential in order to
make the principles we begin to outline in this paper scalable.
Though the demo project we provide here has only a few assets, it
would be straight-forwards to add many more, and achieve similar
effects by adjusting the perceptual parameters of the player avatar,
within the ‘ASC engine’. This would allow the mechanisms
outlined here to be applied to much larger and more complex
scenes that have a greater number of sound sources. Critically,
this is not the type of approach used in other examples we have
looked at, which tend to trigger sounds of auditory hallucination
when a player enters a specific location (e.g. walking down a dark
corridor). The avatar-centred approach that we propose here
offers improved scalability for a variety of situations. We propose
that modeling features of subjective perception for avatars,
through general parameters, will be an essential part of sound
design for first-person POV media such as games in the near
future.

5. INITIAL OBSERVATIONS
This project is a very early prototype, and for a further
development we would aim to provide a formal evaluation of the
auditory mechanisms based on a group of participants through a
user experience study, potentially using a repertory grid or Likert
evaluation process, as described in our earlier evaluation of ASC
in games [3]. Such mechanisms would provide useful insight into
the experience that users have of the auditory design mechanisms
and how they can be evaluated. A challenge in doing so is in
ensuring the users have a suitable benchmark or set of metrics that
they can use to evaluate ASC experiences, since they may never
have experienced these in real life. This presents a potential
barrier to the validity of any validation study. To help mitigate for

this, our work in [3] created a series of scales for ASC sound,
synthesised from group norms that capture the broad sensory
evocations. Similarly, providing users with a series of words or
descriptions, such as those exemplars in Section 2 of this paper,
and asking participants to match and weight those to aspects to
their experience of the simulation, would be another approach for
evaluation. Similarly, more traditional observational studies of
users playing through the simulation, as in computer game user
experience research [11], offer potential. Nonetheless, here we
offer some initial observations on the system.
Considering first the ‘selective auditory attention’ mechanism, the
manner in which we predict the attention of the user is an obvious
limitation. For now, the mechanism of ray-casting based on the
direction the player camera is pointing is adequate to illustrate the
concept, but there is evidently room for improvement. An
advantage of the current method is that it would translate well to
virtual reality (VR) applications, where a limited field of vision
necessitates turning the head towards objects to observe them, and
thus may give a useful rough indication of attention. It could also
be developed further by using eye-tracking technology, which
may provide more accurate predictions of attention that do not
require turning of the player’s viewpoint. However, this still
presumes a relationship between sight and attention that is not an
accurate reflection of how attention actually occurs in the real
world, since we know that auditory attention is not necessary
related to what we are looking at. More sophisticated techniques
could perhaps be devised based on biofeedback; for example,
using Auditory Event-Related Potentials (ERPs) it is possible to
register auditory attention using Electroencephalography (EEG),
and this has been demonstrated effectively with consumer-grade
EEG headsets such as the Emotiv EPOC, which is sold as a lowcost biofeedback device for gaming [1]. Such equipment might
then provide a way to register the attention of the player to
different sound sources in a gaming environment.
With regards to the ‘enhanced sounds’ mechanism, clearly a
functional limitation is the need to pre-process sound sources.
This would be very time consuming for a full game, and would
require 3x the data for the audio assets, which could present
problems as audio files are among the largest assets of a game
already, and storage constraints remain an important concern for
developers. A more practical solution would be the real-time
processing of sound sources, and this could easily be achieved
using modern game engines; it is a limitation of the ‘free’ version
of Unity that we created the demo with, that meant we did not
have access to such features. However, full-featured engines
would be able to undertake such processing and variation without
significant processing overheads and there is the potential to make
this part of the design process simpler for the sound designer by
including such steps in the manipulation of game audio events
being designed in middleware packages, such as WWise or
FMOD.
Lastly, the ‘spatial disruption’ feature was too dramatic and
incoherent to be of practical use in its current form. We were
using the in-built Doppler effect of Unity, which at the time did
not offer a good quality spatial impression. With improved spatial
algorithms, or by fine-tuning the Doppler shift parameters, this
could be a much more interesting effect. For example, it is easy
to image that this could provide an effective way to generate
interesting psychosis effects in VR horror games, which
disorientate the player through sound. Most likely for such
purposes, slower oscillations would be preferable than the ones
used here. Also, other patterns of disruption could be used, which

move sound along various Lissajous curves, or other more
randomised, non-periodic, paths of movement. Additionally,
other acoustic properties of reverberation or echo could be
modulated, allowing these to make spaces seem larger or more
compact at times, imitating the ‘drink me’ distortions described in
Lewis Carroll’s Alice’s Adventures in Wonderland.

6. DISCUSSION
The three prototype mechanisms for simulating auditory
hallucinations discussed here represent an early foray into a novel
area where more research is needed. Based on the data from our
previous research, we have suggested three possible mechanisms
for representing specific features of auditory hallucinations.
These were chosen selectively, and eventually we would like to
see these form part of a larger, comprehensive system for
modelling conscious states and ASCs of game characters. Such
systems will allow more accurate modelling of the cognition of
game characters and avatars that are controlled by a player. The
systems discussed here require further development, and in due
course we would also aim to undertake a formal evaluation of
them using empirical methods.
Work in this area can be associated with a growing field of
activity where VR is being used to create experiences that
represent or induce ASCs, either by inducing heightened states of
fear as entertainment (e.g. Affected the Manor, 2016; and The
Hospital: Allison’s Diary, 2016, on Samsung Gear VR), or by
producing relaxing or meditation experiences (e.g. Zen Zone,
2016; and Guided Meditation VR, 2016, also on Gear VR).
Research is needed to explore how such experiences can be
designed effectively; to demonstrate what effects they have on
their users; and to ensure that they are used in ethically sound
ways. If we can do this, the potential is there to alter
consciousness ethically in range of ways that may be entertaining,
and even therapeutic for the user, depending on the aims of the
software and its successful realisation.

7. REFERENCES
[1] Badcock, N.A., Mousikou, P., Mahajan, Y., Lissa, P., Thie,
J., and McArthur, G. 2013. EEG gaming system for
measuring research quality auditory ERPs. PeerJ 1:e38; doi:
10.7717/peerj.3
[2] Broadbent, D. 1958. Perception and Communication. New
York: Pergamon Press.

[6] Hobson, A.J. 2002. The Dream Drugstore: Chemically
Altered States of Consciousness. MIT Press.
[7] Huxley, A. 1954-1956. The Doors of Perception and Heaven
and Hell. Reprint, London: Flamingo, 1994.
[8] Jones, S.M. Trauer, T. Mackinnon, A. Sims, E. Thomas, N.,
and Copolov, D.L. 2012. A New Phenomenological Survey
of Auditory Hallucinations: Evidence for Subtypes and
Implications for Theory and Practice. Schizophrenia Bulletin,
40(1):231-235.
[9] Kumar, S. Sedley, W. Barnes, G.R. Teki, S. Friston, K.J. and
Griffiths, T.D. 2014. A Brain Basis for Musical
Hallucinations. Cortex, 52:56-97.
[10] McGurk, H. and MacDonald, J. 1976. Hearing Lips and
Seeing Voices. Nature, 263:746-748.
[11] Moreno-Ger, P., Torrente, J., Hsieh, Y.G. and Lester, W.T.,
2012. Usability testing for serious games: Making informed
design decisions with user data. Adv. in Hum.-Comp. Int.
2012, Article 4.
[12] Russell, J. 1980. A Circumplex Model of Affect. Journal of
Personality and Social Psychology, 39(6): 1161–1178.
[13] Tart, C.T. 1969. Altered States of Consciousness: A Book of
Readings. New York: John Wiley & Sons.
[14] Treisman, A.M. 1960. Contextual Cues in Selective
Listening. Quarterly Journal of Experimental Psychology,
12(4):242-248. doi: 10.1080/17470216008416732
[15] Wayne, W.U. 2012. Explaining Schizophrenia: Auditory V
erbal Hallucination and Self-Monitoring. Mind & Language
27(1):87.
[16] Weinel, J. 2010. Quake Delirium: Remixing Psychedelic
Video Games. Sonic Ideas/Ideas Sonicas, 3(2):22-29.
[17] Weinel, J. 2013. Quake Delirium Revisited: Systems for
Video Game ASC Hallucinations. Proceedings of the Fifth
International Conference on Internet Technologies &
Applications 2013, Glyndŵr University, Wales, 249-255.
[18] Weinel, J. Cunningham, S. Griffiths, D. 2014. Sound
Through The Rabbit Hole: Sound Design Based On Reports
of Auditory Hallucination. ACM Proceedings of Audio
Mostly 2014, Aalborg University, Denmark. doi:
10.1145/2636879.2636883

[3] Cunningham, S., Weinel, J., and Picking, R. 2016. In-game
Intoxication: Demonstrating the Evaluation of the Audio
Experience of Games with a focus on Altered States of
Consciousness, in Garcia-Ruiz, M. (ed.) Games User
Research: A Case Study Approach. Boca Raton, FL: CRC
Press/Taylor & Francis, 97-118.

[19] Weinel, J. Cunningham, S. Roberts, N. Griffiths, D. Roberts,
S. 2015. Quake Delirium EEG: A Pilot Study Regarding
Biofeedback-Driven Visual Effects in a Computer Game.
IEEE Proceedings of the Sixth International Conference on
Internet Technologies and Applications 2015, Glyndwr
University, North Wales, 335-338. doi:
10.1109/ITechA.2015.7317420

[4] Cytowic, R.E. 1989. Synaesthesia: A Union of the Senses.
New York: Springer-Verlag.

8. APPENDIX

[5] Demarque, T.C. and Lima, E.S. 2013. Auditory
Hallucination: Audiological Perspective for Horror Games.
SBC – Proceedings of SBGames 2013, São Paulo, Brazil.

http://www.jonweinel.com/media/ASC_Sim_Supporting.zip

The source code, and several demonstration videos of the project
discussed, are available in the supporting zip file available here:
th

[Accessed: 7 June 2017]

