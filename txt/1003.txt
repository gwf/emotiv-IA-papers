Implicit Engagement Detection for
Interactive Museums Using
Brain-Computer Interfaces
Yomna Abdelrahman
University of Stuttgart
Institute for Visualization and
Interactive Systems
yomna.abdelrahman@vis.unistuttgart.de

Markus Funk
University of Stuttgart
Institute for Visualization and
Interactive Systems
markus.funk@vis.uni-stuttgart.de

Mariam Hassib
University of Stuttgart
Institute for Visualization and
Interactive Systems
mariam.hassib@vis.unistuttgart.de

Albrecht Schmidt
University of Stuttgart
Institute for Visualization and
Interactive Systems
albrecht.schmidt@vis.unistuttgart.de

Maria Guinea Marquez
University of Stuttgart
Institute for Visualization and
Interactive Systems
m.guinea10@gmail.com

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author. Copyright is held by the owner/author(s).
MobileHCI’15 Adjunct,, August 25–28, 2015, Copenhagen, Denmark.
ACM 978-1-4503-3653-6/15/08.
http://dx.doi.org/10.1145/2786567.2793709

Abstract
A rich museum experience is one that is engaging, educating and enjoyable to the visitors, such experiences can
only be achieved by personalizing and enriching the museum experience according to the visitor’s state. Neural
signals from the brain can provide information about the affective and cognitive state of the person implicitly. With the
rise of commercial Brain-Computer Interface devices, this
technology can be utilized in extracting information to adapt
various experiences to the state of the person. We propose
a concept and preliminary study which uses brain signals
from commercial grade Brain-Computer Interface (BCI) devices to implicitly detect museum visitors’ engagement in
the exhibited objects. Our concept and output of the study
envision an experience where real time feedback based on
visitors engagement is provided and the whole museum experience is tailored to each visitor’s taste. In future work,
we aim to gain external validity by testing our prototype in a
museum setting.

Author Keywords
EEG; BCI; Museum Guide;User Defined Exhibitions;Interaction
in Museums;Interest classification; Wearable Computing.

ACM Classification Keywords
H.5.2 [Information Interfaces and Presentation]: User Interfaces.Graphical User Interfaces.

Introduction
Museums exhibit large number of objects, one of the main
challenges is to design and set the order of which the objects are organized especially in large museums of multiple
rooms. Usually curators organize exhibits either by date,
storyline, theme, etc.. However, the setting might not fit all
visitors due to the diversity of interest and preferences. HCI
Researchers tackled this by introducing an explicit interactive personalized dimension to the experience [3, 15]. On
the other hand, this type of interaction could also have negative impacts [8, 9].
One sensing modality that can be used to implicitly capture
the interest of museum visitors would be using physiological
signals, particularly neural signals. Using the brain as a way
to implicitly understand human emotions can be used in
building personalized and interactive experiences. A BrainComputer Interface (BCI) is able to capture signals from
the brain elicited by neurons that are fired when an area
of the brain is active. BCIs have been recently available as
commercial and portable devices and have been used in
the non-medical field with non-disabled users. Commercial BCIs have succeeded as passive BCI systems which
use the neural signals in a biofeedback loop to gain insight
about the cognitive and emotional state of users and adapt
systems accordingly. For example, in personalizing computer games by adapting the content and difficulty depending on the player’s state of mind [22], or in helping people
and children sustain their focus and hence retain more information while learning [2, 10] based on brain signal measurements of engagement.
In this paper we present the results of a preliminary controlled lab study which aims to detect visual engagement of
museum visitors using commercial BCI. We develop a user
independent visual engagement index which correlates with

subjective measures of engagement provided by users. We
also discuss our concept and the next steps of our work
towards building personalized museum tours based on implicit visual engagement detection.

Related Work
In the following, we discuss prior work in two different areas
(1) tools and techniques aimed to enhance and personalize
the museum visit, and (2) the usage of commercial BCI to
extract and detect users engagement.
Interactive Museum Experience
Human computer interaction researchers and curators are
examining ways to enrich and enhance the experience of
museums and exhibits. By deploying various tools and
technologies, an interactive dimension has been added
to objects in museums. Grammenos et al. [7] explored having a touch enabled surface, where a digital catalog is presented to the visitors and they can browse through it for the
objects in the exhibition. They also developed a wall sized
display, where visitors can view additional content using
a physical magnifying glass. Additionally [6] proposed the
system PaperView where regular paper sheets are placed
and additional information based on the context is projected
on the paper sheet. Schneegass et al. [17] introduced the
concept of Point. Explore. Learn. aiming to use more natural interaction techniques to provide an exploratory interaction with exhibited objects. Moreover they developed the
interactive torch, where an interactive device in the form of
a torch is used by visitors to explore additional information.
Further work included a post museum visit interactive dimension. The addition of RFID tags to enable visitors to tag
and interact with exhibits in a museum [9, 12] adding a post
visit personalization experience, where the visitor can revisit
the objects later on a personalized website.

Hornecker [8] investigated the use of multi-touch table for
interaction in museums and further explored the gain of
knowledge using an interactive installation. However, further studies reported the lack of self-explanatory technology
augmentation and that technologies are not fully understood by visitors [9]. In this work we aim to exploit more intuitive and implicit interaction on which we rely on the brain
signals as bases of engagement detection to introduce interactive exhibitions.
Using Brain Signals to Detect Engagement
Brain signals provide a lot of information about the cognitive and emotional state of humans. There are many ways
to measure brain signals using invasive and non-invasive
techniques. Non-invasive techniques which are more suitable for HCI applications such as functional near infra-red
spectroscopy (fNIRS) and electroencephalography (EEG)
have recently become more available to the masses and
affordable with the emergence of commercial devices (e.g.
Neurosky 1 , Emotiv 2 ).
EEG devices measure brain signals by placing electrodes
on certain locations on the scalp which measure changes in
electrical potential as neurons in the brain’s cerebral cortex
are fired [20]. The collected signals fall into five different
frequency bands which have been extensively studied and
are proven to provide insight into a person’s cognitive states
such as attention/engagement and relaxation [19].
In 1995, Pope et. al defined the following formula relying on
three of the frequency bands which correlate EEG signals
with task engagement[14]:

E=
1

β
α+θ

Neurosky: http://www.neurosky.com/
2
Emotiv: http://www.emotiv.com/

The formula uses the Alpha (α) band (7-13 Hz) associated
with relaxation, the Beta (β ) band (13-30 Hz) associated
with attentiveness and focus, and finally the Theta (θ ) band
(4-7 Hz) associated with dreaminess and creativity.Szafir
et.al utilized the engagement formula and the high temporal resolution of EEG for detecting engagement in realtime and correlating it to external stimuli. The work uses
the Neurosky Mindset commercial BCI to design adaptive
agents that monitor the attention levels of students and detecting attention drops. Behavioral techniques were employed by robotics agents to regain students’ attention [19].
Similarily, Huang et. al also used the engagement formula
and designed the FOCUS system in which the Emotiv
EPOC was used to monitor a child’s reading engagement
and provide BCI training tasks contextually to attempt to elevate attention and engagement levels [10]. Andujar and
Gilbert proposed a proof of concept investigating the ability
to retain more information by incrementing physiological engagement using the Emotiv EPOC [2]. They proposed measuring engagement in three different ways: using Emotiv
proprietry algorithms, using subjective measures (surveys)
and using the engagement formula [2].
Other researchers investigated the efficiency of the Emotiv
EPOC commercial EEG device in detecting states of attention and relaxation [5, 18]. The proprietary values provided
by the EPOC for engagement, short-term and long-term excitement were used in an intelligent tutoring system (ITS)
student model during different scenarios prompting high
and low cognitive loads [5]. Previous work using the EPOC
to successfully detect states of attention using a reduced
number of electrodes was done by Yaomanee et. al in[21].

Concept
In our work we assess the feasibility of using neural data
in creating personalized and interactive museum experi-

ences based on the visitors’ preferences. Prior research
defined a full cultural heritage experience, be it a museum
or an art gallery, as one in which visitors experience entertainment, immersion, education and pleasure [4, 13]. As
human physiology can provide a lot of implicit cues about
person’s state, physiological computing systems can monitor the museum experience in real time where the cognitive and emotional state of the museum visitor is passively
sensed in real-time forming a bio-feedback loop where the
experience is personalized in various ways in a process of
’adaptive curation’.
The visitor experience in a museum is mainly shaped by
his/her behavior based on his/her interest and engagement
in the exhibited items. The conceptual model for interest
states that a visitor’s interest is affected by emotional and
cognitive aspects. Karrain et. al operationalized the interest
model and defined the cognitive component of interest as
the activation of the pre-frontal cortex of the brain captured
using EEG signals.
We propose a museum experience which utilizes brain signals acquired by commercially available BCI systems to
sense the museum visitors’ engagement in exhibits and
provides real-time feedback to the visitor with suggestions
to personalize their experience. EEG signals being highly
user dependent, require prior training to truly be able to detect engagement of each visitor. Our concept, depicted in
Figure 1, suggests that museum visitors entering a museum
would first be presented a selection of photographs showing various exhibited items covering a range of topics from
this museum. The EEG signals collected would then be
used to calibrate and train the engagement detection model
for this particular visitor. The visitor will then start his/her
museum tour, upon standing at each exhibit the EEG signals will be labeled, filtered using popular EEG signal pro-

Engagement

EEG Signals

Calibration/Training

Noise/Artefact
Removal

Personalized
Feedback

Engagement
Detection

Figure 1: Concept of Implicit Engagement Detection in Museums

cessing algorithms, and the engagement index for the exhibit will be computed in real time. The visitor will then receive cues on his/her mobile phone recommending other
exhibited items that match the engagement levels detected
previously to personalize his/her experience. At the end of
the visitors’ tour they will be provided with a summary of
their engagement throughout the whole museum visit making museum experiences rich, enjoyable and personal.

Data Acquisition & BCI Engagement Detection
Pilot Study
In order to assess the feasibility of our concept and answer
the question "Can commercial grade BCI be able to detect
engagement in museum settings?" we conducted a first
pilot study in a controlled setting. We recruited 10 participants(with an average age of 25.1 years, SD=2.77) using

university mailing lists. All participants were students in different majors.
We chose the EEG-based commercial BCI system Emotiv
EPOC BCI which has 14 electrodes from electrode locations based on the 10-20 international electrode positioning
system and provides access to the raw data. The EPOC
provides wireless connectivity to connect to the PC, which
makes it portable and suitable to providing a better user
experience than other more complex BCI devices. Several research studies, such the ones done by Yaomanee
et al. [21], and Huang et al. [10], prove that the electrodes
placed on the frontal and occipital lobe perform a good approach to obtain cognitive and visual information from the
EEG data. According to this, electrodes AF3, AF4 (frontal)
and 01, 02 (occipital) were selected for our study.

Figure 2: Participant viewing pictures in stationary setup

Ten different pictures covering ten different topics that can
be found in museums (History, Travel, Birds, Animals, Plants,
Egyptology, Technology, Architecture, Automotive, Roman)
were chosen. We simulated viewing objects in a museum
through displaying pictures of these topics on a 30” screen,
with an informative title on each picture. Participants were
seated approximately 1m away from the screen as shown
in figure 2. All participants were not familiar with the shown
pictures and the displaying order of the pictures was randomized. The setup took between 10-15 minutes and the
complete study took approximately 30 minutes per participant.

relaxation EEG at the beginning of each session where participants were instructed to relax by closing their eyes. Each
picture was then shown for 20 seconds, followed by 10 seconds in which the participants were asked to rate their engagement subjectively, and finally a relaxation phase of 20
seconds by showing a black screen. The process is depicted in figure 3.

We used a repeated measures design with two levels of
feedback for engagement in the shown topic: implicitly using the EPOC and explicitly by asking users about their
interest level in the displayed image using a 7-point likert
scale. Participants were seated in a controlled environment
and fitted with the EPOC 2. To have a baseline for comparison of EEG signals, we first recorded 60 seconds of

The Emotiv includes a frequency pass-band filter from 0,2
to 45 Hz as well as two digital notch filters at 50 and 60 Hz
in order to avoid interferences from electrical devices. However, further noise and artefact removal was necessary. A
Finite Impulse Response (FIR) high-pass filter was used
for rejecting the frequency components lower than 1 Hz.
EOG artifacts, generated from eye movements/blinking,

Figure 3: Pictures viewing process

were automatically removed using the Second Order Blind
Inference (SOBI) algorithm which previously showed high
performance among the existing algorithms for removing
eye movement and blink artifacts from EEG data [11]. After
the noise and artefact removal, the power spectrum of Alpha, Beta and Theta frequency bands were extracted to be
able to calculate the engagement as per formula 1.
Visual Engagement Detector Score
Following artefact removal from the EEG baseline and pictures segments, the Engagement Index (EI) for each segment was calculated via formula 1. According to the circumplex model of affect [16], engagement and interest are
associated with positive valence and arousal. There have
been many studies correlating hemispheric asymmetry
with emotions [1, 23]. Hence, we calculated the assymetric engagement (AE) where the difference between the right
hemisphere electrodes (AF4, O2) engagement indices and
the left hemisphere electrodes (AF3, O1) was used.

AEOccipital =EIO2 -EIO1
AEF rontal =EIAF 4 -EIAF 3
Next we aimed to weigh the visual information extracted
from occipital lobe electrodes against the frontal lobe electrodes. We used the following equation to calculate the engagement score (ES):

ESpic,relax = a · AEOccipital + b · AEF rontal (b = 1 − a)
It was concluded that the best result were obtained when
a = 0.85 and thus, b = 1 − a = 0.15. This results that
85% of the visual interest comes from visual information,
whereas the thinking initiation represent the other 15%.
This relationship is supported by other research studies,
based in EEG data extracted from the occipital lobe (O1
and O2 electrodes) to create a cognitive system [19]. The

Figure 4: Linear Correlation between the Likert Scale of all the
Participants and the Visual Engagement Score

Euclidean distance between ESpic and the baseline relaxation ESrelax was the calculated. Finally, we defined the
Visual Engagement Detector Score (VEDS) as follows:


V EDS =

If ESpic > ESrelax , +Distance
If ESpic < ESrelax , −Distance

Results
Pearson product-moment correlation was used to compute the correlation between Likert scale data with the
computed Visual Engagement Detector Score per user
(r(98) = 0.256, p < 0.01). Figure 4 depicts the linear correlation between the Likert scale scores and the engagement
scores of all participants (user independent). This found
correlation, although low, shows promising results in using
BCI to detect visual engagement.

Conclusion and Future Work
Since this is preliminary work in progress, we plan to explore the limitations in more depth. As a following step we
need to validate our VEDS scoring system with a larger
number of subjects and with a wider variety of images.
We would also repeat the experiment on different days to

ensure internal validity as the emotional state of a person
would change on different days and their EEG signals may
be different due to external conditions. A further step would
be evaluating the engagement detection algorithm in real
time and more realistic settings. We aim to increase the external validity of system by conducting a second study in
which participants walk in a real environment simulating a
museum setting. This will introduce motion artefacts and
more challenges, which we aim to address. Furthermore,
we plan to consider the challenges of deploying a BCI in a
museum environment regarding curators and visitors. The
final vision of our project would be to realize the concept
(figure 1) where a mobile application will be implemented
in which the real time feedback will be provided to the museum visitor.
Detecting visitor engagement implicitly using brain signals
with commercially available devices will pave the road towards creating richer museum experiences in the future.
High engagement levels can for example trigger more interaction in the form of projecting more content on certain
exhibits. Personalized guided tours suggesting similar exhibits depending on engagement levels. Finally, implicit engagement detection from large numbers of museum visitors
can help museum curators organizing and adapting the museum to suit their visitors’ tastes.

Acknowledgements
The research leading to these results has partly received
funding from the European Union Seventh Framework
Programme ([FP7/2007-2013]) under grant agreement no
600851, grant agreement no 323849, and the German Research Foundation within the SimTech Cluster of Excellence (EXC 310/1).

REFERENCES
1. John JB Allen and John P Kline. 2004. Frontal EEG
asymmetry, emotion, and psychopathology: the first,
and the next 25 years. Biological psychology 67, 1
(2004), 1–5.
2. Marvin Andujar and Juan E Gilbert. 2013. Let’s learn!:
enhancing user’s engagement levels through passive
brain-computer interfaces. In CHI’13 Extended
Abstracts on Human Factors in Computing Systems.
ACM, 703–708.
3. Jonathan P Bowen and Silvia Filippini-Fantoni. 2004.
Personalization and the web from a museum
perspective. In Museums and the Web, Vol. 4.
4. Carmen De Rojas and Carmen Camarero. 2008.
Visitors’ experience, mood and satisfaction in a
heritage context: Evidence from an interpretation
center. Tourism Management 29, 3 (2008), 525–537.
5. Benjamin Goldberg, Keith W Brawner, and Heather K
Holden. 2012. Efficacy of Measuring Engagement
during Computer-Based Training with Low-Cost
Electroencephalogram (EEG) Sensor Outputs. In Proc.
HFES’12. 198–202.
6. Dimitris Grammenos, Damien Michel, Xenophon
Zabulis, and Antonis A Argyros. 2011. PaperView:
augmenting physical surfaces with location-aware
digital information. In Proc. TEI’11. ACM, 57–60.
7. D Grammenos, X Zabulis, D Michel, P Padeleris, T
Sarmis, G Georgalis, P Koutlemanis, K Tzevanidis, AA
Argyros, M Sifakis, and others. 2013. A prototypical
interactive exhibition for the archaeological museum of
thessaloniki. IJHDE 2 (2013), 75–100.

8. Eva Hornecker. 2008. ”I don’t understand it either, but it
is cool”-visitor interactions with a multi-touch table in a
museum. In TABLETOP’08. IEEE, 113–120.
9. Sherry Hsi and Holly Fait. 2005. RFID enhances
visitors’ museum experience at the Exploratorium.
Commun. ACM 48, 9 (2005), 60–65.
10. Jin Huang, Chun Yu, Yuntao Wang, Yuhang Zhao, Siqi
Liu, Chou Mo, Jie Liu, Lie Zhang, and Yuanchun Shi.
2014. FOCUS: enhancing children’s engagement in
reading by using contextual BCI training sessions. In
Proc. CHI’14. ACM, 1905–1908.
11. Carrie A Joyce, Irina F Gorodnitsky, and Marta Kutas.
2004. Automatic removal of eye movement and blink
artifacts from EEG data using blind component
separation. Psychophysiology 41, 2 (2004), 313–325.
12. Rasoul Karimi, Alexandros Nanopoulos, and Lars
Schmidt-Thieme. 2012. RFID-enhanced museum for
interactive experience. In MM4CH. Springer, 192–205.
13. B Joseph Pine and James H Gilmore. 1998. Welcome
to the experience economy. HBR 76 (1998), 97–105.
14. Alan T Pope, Edward H Bogart, and Debbie S
Bartolome. 1995. Biocybernetic system evaluates
indices of operator engagement in automated task.
Biological psychology 40, 1 (1995), 187–195.
15. Ivo Roes, Natalia Stash, Yiwen Wang, and Lora Aroyo.
2009. A personalized walk through the museum: The
CHIP interactive tour guide. In CHI’09 EA. ACM,
3317–3322.
16. James A Russell. 1980. A circumplex model of affect.
Journal of personality and social psychology 39, 6
(1980), 1161.

17. Stefan Schneegass, Johannes Knittel, Benjamin Rau,
and Yomna Abdelrahman. 2014. Exploring Exhibits:
Interactive Methods for Enriching Cultural Heritage
Items. In TEI’14 EA. ACM.
18. Alireza Sahami Shirazi, Markus Funk, Florian
Pfleiderer, Hendrik Glück, and Albrecht Schmidt. 2012.
MediaBrain: Annotating Videos based on
Brain-Computer Interaction.. In Mensch & Computer.
263–272.
19. Daniel Szafir and Bilge Mutlu. 2012. Pay attention!:
designing adaptive agents that monitor and improve
user engagement. In Proc. CHI’12. ACM, 11–20.
20. Jonathan R Wolpaw, Niels Birbaumer, Dennis J
McFarland, Gert Pfurtscheller, and Theresa M
Vaughan. 2002. Brain–computer interfaces for
communication and control. Clin. neurophysio. 113, 6
(2002), 767–791.
21. Kridsakon Yaomanee, Setha Pan-ngum, and Pasin
Israsena Na Ayuthaya. 2012. Brain signal detection
methodology for attention training using minimal EEG
channels. In ICT’12. IEEE, 84–89.
22. Myeung-Sook Yoh, Joonho Kwon, and Sunghoon Kim.
2010. NeuroWander: a BCI game in the form of
interactive fairy tale. In Adjunct Proc. Ubicomp’10.
ACM, 389–390.
23. Qing Zhang and Minho Lee. 2009. Analysis of positive
and negative emotions in natural scene using brain
activity and GIST. Neurocomputing 72, 4 (2009),
1302–1306.

