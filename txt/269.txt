1

Gender and Emotion Recognition from Implicit
User Behavior Signals

arXiv:2006.13386v1 [cs.HC] 23 Jun 2020

Maneesh Bilalpur, Seyed Mostafa Kia, Member, IEEE, Mohan Kankanhalli, Fellow, IEEE, Ramanathan
Subramanian, Senior Member, IEEE
Abstract—This work explores the utility of implicit behavioral cues, namely, Electroencephalogram (EEG) signals and eye
movements for gender recognition (GR) and emotion recognition (ER) from psychophysical behavior. Specifically, the examined cues
are acquired via low-cost, off-the-shelf sensors. 28 users (14 male) recognized emotions from unoccluded (no mask ) and partially
occluded (eye or mouth masked) emotive faces; their EEG responses contained gender-specific differences, while their eye
movements were characteristic of the perceived facial emotions. Experimental results reveal that (a) reliable GR and ER is achievable
with EEG and eye features, (b) differential cognitive processing of negative emotions is observed for females and (c) eye gaze-based
gender differences manifest under partial face occlusion, as typified by the eye and mouth mask conditions.
Index Terms—Gender and Emotion Recognition, Emotional Face Perception, Implicit User Behavior, Electroencephalography, Eye
Gaze Tracking, Unoccluded vs occluded faces.

F

1

I NTRODUCTION

G

E nder human-computer interaction (HCI) [1] and Affective HCI [2] have evolved as critical HCI sub-fields,
as it is critical for computers to appreciate and adapt to
the user’s gender and emotional state. Inferring users’ soft
biometrics such as gender and emotion would benefit interactive and gaming systems in terms of a) visual and interface
design [3], [4], (b) game and product recommendation (via
ads) [5], [6], and (c) provision of appropriate motivation
and feedback for optimizing user experience [7]. Gender
recognition (GR) and emotion recognition (ER) systems
primarily work with facial [8], [9] or speech [10], [11] cues
which are biometrics encoding a person’s identity. Also, they
can be recorded without the user’s knowledge, posing grave
privacy concerns [12].
This work examines GR and ER from implicit user
behavioral signals, in the form of EEG brain signals and
eye movements. Implicit behavioral signals are inconspicuous
to the outside world, and cannot be recorded without express user cooperation making them privacy compliant [13].
Also, behavioral signals such as EEG and eye movements
are primarily anonymous as little is known regarding their
uniqueness to a person’s identity [14].
Specifically, we attempt GR and ER using signals captured by commercial, off-the-shelf devices which are minimally intrusive, affordable, and popularly used in gaming
as input or feedback modalities [15], [16]. The Emotiv EEG
wireless headset consists of 14 dry (plus two reference)
electrodes having a configuration as shown in Fig. 1. While
being lightweight, wearable and easy-to-use, neuro-analysis

•
•
•
•

Maneesh Bilalpur is with the School of Computing and Information, Univ.
Pittsburgh, USA. (E-mail: mab623@pitt.edu)
Seyed Mostafa Kia is with the Donders Institute at Radboud University,
Nijmegen, The Netherlands. (E-mail: s.kia@donders.ru.nl)
Mohan Kankanhalli is with School of Computing at National University
of Singapore, Singapore. (E-mail: mohan@comp.nus.edu.sg)
Ramanathan Subramanian is with the Indian Institute of Technology,
Ropar. (E-mail: s.ramanathan@iitrpr.ac.in)

Fig. 1: Emotiv Epoc+ electrode configuration: The headset
comprises 14 sensing plus two reference electrodes.
with Emotiv can be challenging due to relatively poor signal
quality. Likewise, EyeTribe is a low-cost eye-tracker whose
suitability for research has been endorsed [17]. We show
how relevant gender and emotion-specific information is
captured by these low-cost devices via examination of
event-related potential (ERP) and eye fixation patterns, and
also through recognition experiments.
We set out to discover gender differences in human
visual perception by designing a facial emotion recognition
(FER) experiment. Males and females respond differently
to affective information [7], [18], [19], [20], and user eye
movements are also characteristic of the perceived facial
emotion [21], [22], [23], [24], [25], enabling stimulus ER.
Our study performed with 28 viewers (14 males) confirms
that women achieve superior FER, mirroring prior findings.
Hypothesizing that enhanced female emotional sensitivity should reflect via their implicit behavior, we examined
EEG and eye-gaze patterns to find that (1) Stronger ERPs
are observed for females while processing negative facial
emotions, (2) Female eye-gaze is more focused on the eyes
for the purpose of FER, and (3) Emotion and informationspecific gender differences manifest starkly, enabling better
GR under particular stimulus conditions.
Building on preliminary results [20], [26], our work
makes the following research contributions: (a) While prior

2

works have identified gender differences in emotional behavior, this is one of the first works to expressly perform
GR and ER from implicit behavioral signals; (b) Apart from
recognition experiments, we show that the employed devices capture meaningful perceptual information, as typified
by gender and emotion-specific event related potentials
(ERPs), and the extent of fixation over the eyes; (c) We
demonstrate a significant performance improvement in GR
with deep learning and boosting methods over traditional
approaches presented in [20], [26]; (d) The use of minimally intrusive, off-the-shelf and low-cost devices affirms
the ecological validity of our study, and the utility of our
experimental design for large-scale user profiling.
Hereon, Section 2 reviews related work. Section 3 describes our study. Section 4 examines explicit user responses,
which are correlated with implicit behaviors in Section 5,
followed by GR and ER experiments. Section 6 summarizes
and concludes the paper.

2

R ELATED W ORK

Many works perform ER with implicit behavioral signals
such as eye movements, EEG, Electromyogram (EMG), Galvanic Skin Response (GSR) [27], [28], [29], [30], [31] etc. However, very few works estimate soft biometrics such as gender,
cognitive load and personality traits with such signals [32],
[33], [34]. Also, while some works isolate emotion and
gender differences in eye movement and EEG responses
to emotional faces [24], [25], [35], [36], these differential
features are never utilized for gender prediction. To position
our work with respect to the literature, this section reviews
related work on (a) user-centered ER, and (b) gender differences in emotional face processing.
2.1

User-centered ER

Emotions evoked by multimedia stimuli are predicted
via content-centered or user-centered methods. Contentcentered methods attempt to find emotional multimodal
features [37], [38], [39], [40], while user-centered methods
monitor user behavioral cues (eye movements, EEG signals,
etc.) to deduce the evoked emotion. As emotions are subjective, many user-centered approaches predict emotions by
examining both explicit and implicit user behavioral cues.
Conspicuous facial cues are studied to detect multimedia
highlights in [9], while physiological measurements are
utilized to model emotions induced by music and movie
scenes in [29], [30]. EEG and eye movements are two popular modalities employed for ER, and many works have
used a combination of both [20], [27], [28] or either signal
exclusively [21], [24], [29], [31], [36], [41], [42].
Valence (positive vs. negative emotion) recognition from
eye-gaze features is achieved in [21]. ER from EEG and
pupillary responses is discussed in [27]. Deep unsupervised ER from raw EEG data is proposed in [41], and
its effectiveness is shown to be comparable to designed
features. Differential entropy EEG features are extracted to
train an integrated deep belief network plus hidden Markov
model for ER in [42]. A differential auto-encoder that learns
shared representations from EEG and eye-based features
is proposed for valence recognition in [28]. Almost all of

these works employ lab-grade eye-trackers and EEG sensors which are bulky and intrusive, and therefore preclude
naturalistic user behavior.
2.2

Gender Differences in Emotion Recognition

As facial emotions denote critical non-verbal communication cues in social interactions, many psychology studies
have studied human FER. Certain facial features encode
emotions better than others; the eyes, nose and mouth are
the most attractive facial regions [43], [44]. Visual attention is
localized around the eyes for mildly emotive faces, but the
nose and mouth attract substantial eye fixations in highly
emotive faces [45]. An eye tracking study [25] notes that
distinct eye fixation patterns emerge for different facial
emotions. The mouth is the most informative for the joy and
disgust emotions, whereas eyes mainly encode information
relating to sadness, fear, anger and shame. A similar study [46]
notes more fixations on the upper face half for anger as
compared to disgust, while no differences are observed on
lower face half for the two emotions. However, humans
may find it difficult to distinguish between similar facial
emotions– examples are the high overlap rate between the
fear–surprise and anger–disgust emotion pairs [44], [47].
Multiple works have discovered gender differences during facial emotion processing. Females are generally better
at FER irrespective of age [48]. Other FER studies [18], [19],
[49] also note that females recognize facial emotions more
accurately than males, even under partial information. Some
evidence also points to females achieving faster FER than
males [50], [51]. Gender differences in gaze patterns and
neural activations have been found while viewing emotional faces; female tendency to fixate on the eyes positively
correlates with their ER capabilities, while men tend to look
at the mouth for emotional cues [48], [50]. Likewise, EEG
Event-related potentials (ERPs) reveal that negative facial
emotions are processed differently and rapidly by women,
and do not necessarily entail selective attention towards
emotional cues [35], [52].
An exhaustive review of GR methodologies is presented
in [32], and the authors evaluate GR methods using metrics
like universality, distinctiveness, permanence and collectability.
While crediting bio-signals like EEG and Electrocardiography (ECG) for their accuracy and trustworthiness, authors
also highlight the invasiveness of bio-sensors. The sensors
used in this work are minimally intrusive, enabling naturalistic user experience, while also recording meaningful emotion and gender-related information. Among user-centric
GR works, EEG and speech features are proposed for age
and gender recognition in [53].
2.3

Analysis of Related Work

Close examination of the literature reveals (1) Many works
achieve ER from user-centered cues, both conspicuous and
latent, and a handful have discovered gender differences
in gaze patterns and neural activations; nevertheless, very
few works expressly predict gender from implicit user cues. Differently, we employ implicit signals for GR and ER, and
achieve reliable gender and valence detection (AUC > 0.9);
(2) Our GR/ER features are acquired from low cost, off-theshelf sensors, which record inferior user signals even while

3

enabling natural user behavior. We nevertheless show how
these sensors capture meaningful information via the analysis of ERPs and fixation distribution patterns; (3) Different
to prior works which either analyze explicit or implicit user
responses to discover gender differences, or do not expressly
isolate bio-signal patterns; in contrast, we show multiple
similarities among explicit and implicit user behaviors to
validate our findings.

3

M ATERIALS AND M ETHODS

Our study objective was to examine user behavior while
viewing unoccluded/partly occluded emotional faces, and
predict user gender therefrom. We hypothesized that gender
differences would be captured via EEG and eye-gaze patterns. Also, eye movements are known to be characteristic
of the perceived facial emotion [25], which enables inference
of the stimulus emotion; we limit ourselves to predicting
the stimulus valence, i.e., whether the face presented to the
viewer exhibits a positive or negative emotion?
We designed a study to examine gender differences in
visual emotional face processing with a) fully visible faces,
and (b) faces with the eye and mouth regions occluded via a
rectangular mask (Fig. 2). Salience of the occluded features
towards conveying facial emotions is well known [25], [45].
Specifically, we considered emotional faces corresponding
to four conditions: exhibiting high intensity (HI) and low
intensity (LI) emotions, and additionally, high intensity
emotions upon occluding the eye (eye-mask) or mouth
(mouth-mask) regions (Fig. 2). Since we hypothesized
that emotion perception under facial occlusion would be
considerably difficult (cf. Table 2), we did not study masked
and mildly emotive faces.
Participants: 28 students from different nationalities (14
male, age 26.1 ± 7.3 and 14 female, age 25.5 ± 6), with
normal or corrected vision, took part in our study. All users
provided informed consent, and were presented a token fee
for participation as directed by the ethics committee.
Stimuli: We used emotional faces of 24 models (12 male,
12 female) from the Radboud Faces Database (RaFD) [54].
RaFD includes facial emotions of 49 models rated for clarity,
genuineness and intensity, and the 24 models were chosen
such that their Ekman facial emotions (Anger, Disgust,
Fear, Happy, Sad and Surprise) were roughly matched
based on these ratings. We then morphed the emotive faces
from neutral (0% intensity) to maximum (100% intensity)
to generate intermediate morphs in steps of 5%. Derived
morphs with 55–100% intensity were used as HI emotions,
and 25–50% were used as LI emotions. Eye and mouthmasked faces were automatically generated upon locating
facial landmarks via Openface [55] over the HI morphs. The
eye mask covered the eyes and nasion, while the mouth mask
covered the mouth and the nose ridge. All stimuli were
resized to 361×451 pixels, encompassing a visual angle of
9.1◦ and 11.4◦ about x and y at 60cm screen distance.
Protocol: The experimental protocol is outlined in Fig.2,
and involved the presentation of unmasked and masked faces

to viewers over two separate sessions, with a break inbetween to avoid fatigue. We chose one face per model and
emotion, resulting in 144 face images (1 morph/emotion
× 6 emotions × 24 models). In the first session (no-mask
condition), these faces were shown in random order and
were again re-presented randomly with an eye or mouth mask
in the second session. We ensured a 50% split of the HI and
LI morphs in the first session, and eye/mouth-masked faces
in the second.
During each trial, an emotional face was displayed for
4s preceded by a fixation cross for 500 ms. The viewer then
had a maximum of 30s to make one out of seven choices
concerning the facial emotion (six Ekman emotions plus
neutral) via a radio button. Neutral faces were only utilized
for morphing purposes and not used in the experiment.
Viewers’ EEG signals were acquired via the 14-channel
Emotiv Epoc+ device, and eye movements were recorded
with the Eyetribe tracker during the trials. The face-viewing
experiment was split into 4 segments to facilitate sensor recalibration and minimize data recording errors, and took
about 90 minutes to complete.

4

U SER R ESPONSES

We first compare male and female sensitivity to emotions
based on explicitly observed user response times (RTs) and
recognition rates (RRs), and will then proceed to examine
their implicit eye movement and EEG responses. Our experimental design involved four stimulus types (HI, LI, eye
and mouth mask), and two user types (male and female),
resulting in 4 × 2 factor conditions.
4.1

Response time (RT)

Overall user RTs for the HI, LI, eye mask and mouth
mask conditions were respectively found to be 1.44 ±
0.24, 1.52 ± 0.05, 1.17 ± 0.12 and 1.25 ± 0.09 seconds,
implying that FER was fairly instantaneous, and viewer
responses were surprisingly faster with masked faces.
Fine-grained comparison of male (m) and female (f )
RTs across stimulus types (Fig. 3) revealed that females
(µRT = 1.40 ± 0.10s) were generally faster than males
(µRT = 1.60 ± 0.10s) at recognizing HI emotions. There
was no significant difference in RTs for LI emotions.
Female alacrity nevertheless decreased for masked faces,
with males responding marginally faster for eye masked
(µRT (m) = 1.13 ± 0.11s vs µRT (f ) = 1.21 ± 0.13s), and both
genders responding with similar speed for mouth masked
faces (µRT (m) = 1.24 ± 0.10s vs µRT (f ) = 1.25 ± 0.09s).

4.2

Recognition Rates

While females recognized facial emotions marginally faster,
we examined if they also achieved superior FER. Overall,
RRs for unoccluded HI emotions (µRR = 77.6) were expectedly higher than for eye-masked (µRR = 59.7), mouthmasked (µRR = 63.5) and LI emotions (µRR = 49.1). Happy
faces were recognized most accurately in all four conditions.
Specifically focusing on gender differences (Fig. 4), females
recognized facial emotions more accurately than males and
this was particularly true for negative (A, D, F, S) emotions;

4

Fig. 2: Experimental Protocol: Viewers were required to recognize the facial emotion from either an unmasked face (Session
1), or from an eye/mouth masked face (Session 2). Trial timelines for the two conditions are shown at the bottom.
outperformed females in the LI condition (µRR (m) = 49.8
vs µRR (f) = 47.7). Overall, (a) HI emotion morphs were
recognized more accurately than LI morphs, (b) females
recognized negative emotions better, (c) Happy was the
easiest emotion to recognize, and (d) Higher RRs (across
gender) were noted in the eye-mask condition for four of the
six Ekman emotions, implying that deformations around the
mouth were more informative for FER under occlusion.
Fig. 3: Emotion-wise RTs of females and males in the
various conditions. y -axis denotes response time in seconds.
Error bars denote unit standard error (best-viewed in color).

5

As females achieved quicker and superior FER for negative
emotions, we hypothesized that these behavioral differences
should also reflect via implicit eye gaze and EEG patterns.
We first describe the EEG and eye-movement descriptors
employed for analyses, before discussing (stimulus) emotion and (user) gender recognition results.
5.1

Fig. 4: Emotion-wise RRs in various conditions.
male vs female RRs for these emotions differed significantly
in the HI (µRR (m) = 54.3 vs µRR (f ) = 61.2, p < 0.05)
and eye mask conditions (µRR (m) = 51.8 vs µRR (f ) =
58.1, p < 0.05), and marginally for the mouth mask condition
(µRR (m) = 52 vs µRR (f) = 55.8, p = 0.08). Males marginally

A NALYZING I MPLICIT R ESPONSES

EEG preprocessing

We extracted EEG epochs for each trial (4.5s of stimulusplus-fixation viewing time at 128 Hz sampling rate), and
the 64 leading pre-stimulus samples were used to remove
DC offset. This was followed by (a) EEG band-limiting to
within 0.1–45 Hz, (b) Removal of noisy epochs via visual
inspection, and (c) Independent component analysis (ICA)
to remove artifacts relating to eye-blinks, and eye/muscle
movements. Muscle movement artifacts in EEG are mainly
concentrated in the 40–100 Hz band, and are removed

5

upon band-limiting and via inspection of ICA components.
Finally, a 7168 dimensional (14 channel ×4s×128 Hz) EEG
feature vector was generated and fed to different classifiers
for GR and ER (Section 5.3).
5.1.1 Event Related Potentials
Event Related Potentials are time-locked neural responses
related to sensory and cognitive events, and denote the EEG
response averaged over multiple users and trials. As examples, P300 and N100, N400 are exemplar ERPs which are
typically noted around 300, 100 and 400 ms post stimulus
onset. ERPs occurring within 100 ms post stimulus onset are
stimulus-related (exogenous), while later ERPs are cognitionrelated (endogenous). We examined the leading 128 EEG
epoch samples (one second of data) for ERP patterns relating
to emotion and gender.
Prior works have observed ERP-based gender differences from lab-grade sensor recordings [35], [52], [56].
Specifically, [52] notes enhanced negative ERPs for females
in response to negative valence stimuli. However, capturing
ERPs with commercial devices is challenging due to their
low signal-to-noise ratio [57]. Figs. 5 and 6 present the P300,
visual N100 and N400 ERP components in the occipital O1
and/or O2 electrodes (see Fig. 1 for sensor positions) corresponding to various face morphs. Note that the occipital
lobe is the visual processing center in the brain, as it contains
the primary visual cortex.
Comparing O1/O2 male and female ERPs for positive
(H, Su) and negative (A, D, F, Sa) emotions, no significant
differences can be observed between male positive and
negative ERP peaks for HI or LI faces (columns 3,4 in Fig.5).
However, we observe stronger N100 and P300 peaks in the
negative female ERPs for both HI and LI faces (columns
1,2). Also, a stronger female N400 peak can be noted for
HI faces consistent with prior findings [52]. Contrastingly,
lower male N100 and P300 latencies are observed for positive HI emotions, with the pattern being more obvious at
O2. Likewise, lower male N400 latencies can be generally
noted at O2 for positive emotions. The positive vs negative
ERP difference for females is narrower for LI faces, revealing
the difficulty in identifying mild LI emotions. That LI faces
produce weaker ERPs at O1 and O2 than HI faces further
supports this observation.
Fig. 6 shows female ERPs observed in the occipital O2
electrode for the HI and eye mask conditions. Clearly, one can
note enhanced N100 and P300 ERP components for negative
HI emotions (Fig. 6(left)). This effect is attenuated in the
eye mask (Fig. 6(right)) and mouth mask cases, although one
can note stronger N400 amplitudes for D and F with eye
mask. This ERP pattern is invisible for males, confirming thir
gender-specificity. Overall, ERP patterns affirm that gender
differences in emotional face processing can be reliably
isolated with the low-cost Emotiv device.
5.2

Eye-tracking analysis

Gender differences in gaze patterns during emotional face
processing have been noted by prior works [25], [48]. We
used the low-cost Eyetribe device with 30 Hz sampling to
record eye movements. Raw gaze data output by the tracker
were processed to compute fixations (prolonged gazing at

scene regions to assimilate visual information) and saccades
(transition from one fixation to another) via the EyeMMV
toolbox [58]. Upon extracting fixations and saccades, we extracted features employed for valence recognition in [21] to
compute an 825-dimensional feature vector for our analyses.
5.2.1 Fixation analysis
To study gender differences in fixating patterns, we computed the distribution of fixation duration (FD) over six
facial regions, namely, eyes, nose, mouth, cheeks, forehead and
chin. Fig. 7 presents the males and female FD distribution
over these facial regions for various conditions. For both
genders, the time spent examining the eyes, nose and mouth
accounted for over 80% of the total FD, with eyes (≈45%)
and nose (≈30%) attracting maximum attention as observed
in [25], [36], [45]. Relatively similar FD distributions were
noted for both genders with HI and LI morphs (Fig. 7 a,b).
Fig.7(c,d) present FD distributions in the eye and mouth
mask conditions. An independent t-test revealed a significant difference (p<0.05) between male and female FDs for
the eye region in the mouth mask condition; prior works [23]
have observed that females primarily look at the eyes for
emotional cues, which is mirrored by longer FDs around the
eyes in the HI and mouth mask conditions. Fig.7(c) shows
that when eye information is unavailable, females tend to
focus on the mouth and nasal regions for FER.
5.3

Experiments and Results

Having noticed gender-specific patterns in user responses,
EEG ERPs and eye movements, we attempted binary emotion recognition (ER) and gender recognition (GR) employing EEG features, eye-based features and their combination
for the various conditions. Recognition was attempted only
on trials where viewers correctly recognized the presented
facial emotion. We considered (i) EEG features, (ii) eyebased features, (iii) concatenation of the two (early fusion
or EF), and (iv) probabilistic fusion of the EEG and eyebased classifier outputs (late fusion or LF) for our analyses.
The West technique [59] was used to fuse the EEG and eyebased outputs, and denotes maximum possible recognition
performance as optimal weights maximizing the test AUC
metric were determined via a 2D grid search.
We considered the area under ROC curve (AUC) plotting true vs false positive rates, as the performance metric
for benchmarking. AUC is suitable for evaluating classifier
performance on unbalanced data, and a random classifier
will achieve an AUC of 0.5. As we attempted recognition
with few training data, we report ER/GR results over five
repetitions of 10-fold cross validation (CV) (i.e., total of 50
runs). CV is typically used to overcome the overfitting problem, and train generalizable classifiers on small datasets.
5.3.1 Baseline Classification Approaches
As baselines, we considered the Naive-Bayes (NB), linear
SVM (LSVM) and radial-basis SVM (RSVM) classifiers. NB
is a generative classifier that estimates the test label based on
the maximum a-posteriori criterion, p(C | X), assuming classconditional feature independence. C and X respectively
denote test class-label and feature vector. LSVM and RSVM
denote the linear and radial basis kernel versions of the

6

Fig. 5: ERPs for HI morphs (top) and LI morphs (bottom): (left to right) O1 and O2 ERPs for females and males. y -axis
shows ERP amplitude in µV , refer (h) for legend. As per convention, ERPs are plotted upside down (view in color).
TABLE 1: Summary of the deep CNN model.
Parameter
Learning rate
Kernel shape
Stride size
Pool size
Batch size
# Kernels(layer wise)
Momentum
Weight decay
Dropout

Fig. 6: Female ERPs in the (left) HI and (right) eye mask
conditions from the O2 channel. Best viewed in color.
Support Vector Machine (SVM) classifier. SVM hyperparameters C (LSVM) and γ (RSVM) were tuned from within
[10−4 , 104 ] via an inner 10-fold CV on the training set.
5.3.2 CNN for EEG-based GR & ER
Deep learning frameworks have recently become popular
due to their ability to automatically learn optimal task
features from raw data, thereby obviating the need for data
cleaning and feature extraction. However, unlike in image
or video processing, temporal dynamics of the human brain
are largely unclear; designing relevant features is therefore
hard. We hypothesized that CNNs would encode EEG patterns efficiently, and also be robust to artifacts. These factors
inspired us to feed raw EEG data to a CNN, without any
data processing (as in Sec. 5.1); preprocessed EEG data was
nevertheless fed to the above baseline classifiers.
We adopted a 3-layer Convolutional Neural Network
(CNN) [60] to learn a robust EEG representation for gender
and valence recognition. Three convolution layers together
with rectified linear unit (ReLU) activation, and average
pooling layers are stacked to learn EEG descriptors (Fig. 8).
The convolutions employed are 1-dimensional along time.
Batch normalization [61] is used after the third CNN layer to
minimize covariate shift and accelerate training. To prevent
overfitting, we used dropout after the fully connected layer
with 128 neurons. A softmax over two output neurons was
used for classification.
The number of kernels increase with network depth (cf.
Fig 8) analogous to the VGG architecture [62]. We optimized
the network for categorical cross-entropy loss using stochas-

Value
0.01
3
2
2
32
16,32,32
0.9
0.0001
0.1

tic gradient descent with Nesterov momentum and weight
decay. To decrease data dimensionality and avoid overfitting without sacrificing temporal EEG dependencies, we
downsampled EEG data to 32Hz (reducing per-channel EEG
dimensionality to 32×4=128). A tenth of training data was
used for validation and early-stopping enforced to prevent
overfitting. CNN hyperparameters specified in Table 1 were
either adopted from [60] or upon cross-validation.
The CNN was traditionally trained for GR with He normal initializers (normally distributed weights). Conversely,
we adopted two-stage training for ER. In the first stage, the
model was pre-trained with EEG data acquired over all four
experimental conditions (Fig 2); the second stage involved
fine-tuning with data for a specific condition. The objective
here was to extract valence-related features irrespective of
stimulus type in the first stage, and fine-tune them to learn
condition-specific descriptors. Experiments were designed
using Keras [63] with Tensorflow back-end on a 16 GB CPU
machine and an Intel i5 processor.
5.3.3 AdaBoost for ER from eye-gaze
We employed the Adaboost classifier for ER from eyegaze features. AdaBoost is an ensemble classifier popularly
used for face detection [64] and FER [65]. Adaboost combines a number of weak classifiers (decision stumps), each
marginally better than random, to cumulatively achieve optimal classification. The class label is based on the weighted
output of each weak classifier. Our features (Sec. 5.1) and
classifier design were inspired by [65], who capture local dependencies via histograms of gaze measures like fixations,
saccades and saliency. We employed SAMME (Stagewise
Additive Modeling using a Multi-class Exponential loss

7

Fig. 7: (a–d) Fixation duration distributions for males and females in the HI, LI, eye and mouth-masked conditions.

Fig. 8: CNN architecture showing various layers in the model and parameters.
function) [66] for training. Similar to [65], local Gaussian and
Gabor features were extracted at multiple scales to generate
composite features for AdaBoost.
We only obtained sparse gaze features in our study as
viewer gaze was (a) localized to face regions of emotional
importance and (b) recorded via a low-cost and low sampling rate device. We firstly performed feature selection
to prune features relevant for ER. Feature selection was
based on sequential addition (or removal) for optimal performance [21] using sequential forward (SFS) or backward
(SBS) selection. User data compiled for this study along
with the CNN and Adaboost models employed for GR
and ER respectively are available at https://github.com/
bmaneesh/emotion-xavier.
5.4

Results

5.4.1 Emotion Recognition
We modeled stimulus ER as a binary classification problem,
where the objective is to categorize positive (H, Su) and negative (A, D, F, Sa) valence stimuli employing gaze and EEGbased cues. ER results with (CNN-based) EEG, (Adaboostbased) eye gaze features, and late fusion of two modalities, with data acquired for different conditions characterized by user-gender (M/F) and stimulus type (HI/LI/eyemask/mouth-mask) are shown in Fig. 9. Evidently, gaze
features perform significantly better than EEG. Gaze features achieve near-ceiling valence recognition barring the

Fig. 9: Valence recognition results for different conditions.
case where female users view LI emotions. Optimal performance is noted with male data for HI emotions (AUC =
0.99) similar to [67]; ER from female eye-gaze data on HI
emotions (AUC = 0.98) is also high.
On the other hand, ER results with EEG largely produced near-chance performance, with only male and female
data for the mouth-masked condition being exceptions.
These results reveal that the raw EEG features are not
optimal for ER. Given the vast difference in ER performance
with the gaze and EEG modalities, late fusion results are
only occasionally superior to unimodal ones. Fusion slightly
outperforms unimodal methods for the eye mask condition
for both males (0.98 vs. 0.99) and females (0.97 vs. 0.98), and
noticeably with males for mouth mask faces (0.91 vs. 0.94).
Overall, while recognizing the stimulus facial emotion
was not our primary objective, the extracted gaze features
are nevertheless highly effective for valence recognition

8

(VR). Adaboost employing gaze features significantly outperforms the CNN trained with EEG data. The ability of
eye movements to characterize emotional differences is unsurprising. Distinctive eye movement patterns have been
observed while scanning scenes and faces with different
emotions [21], [24], [25]. Eye-based features are found to
achieve 52.5% VR accuracy in [21], where emotions are
induced by presenting diverse emotional scenes, while our
study is specific to emotive faces. Prior neural studies [29],
[56] which perform emotion recognition with lab-grade
sensors achieve around 60% VR. Superior performance of
eye-based features can be attributed to the fact that the
saccade and saliency-based statistics can capture discrepancies in gazing patterns for positive and negative facial
emotions [21], [24]; in contrast, neural features have only
been moderately effective at decoding emotions conveyed
by audio-visual music and movie stimuli [29], [30], [56],
which can presumably elicit emotions more effectively than
plain imagery.
5.4.2

Gender Recognition

GR involves labeling the test EEG or eye-gaze sample
as arising from a male/female user. Table 2 presents GR
achieved with baseline classifiers, while Table 3 shows
AUC scores obtained with the EEG-based CNN, and the
Adaboost ensemble fed with eye-gaze features. Both tables present GR results when user data corresponding to
All emotional faces, and data for each of the six Ekman
emotions (Emotion-wise) are employed for model training.
Also, the best EF and LF results along with the relevant
classifier are specified for the All condition. Cumulatively,
Tables 2 and 3 clearly convey that the CNN and Adaboost
frameworks considerably outperform baseline classifiers.
Focusing on Table 2, one can clearly note that the AUC
scores with HI emotions are typically higher (cf. columns
1,2); also, AUC metrics achieved in the mouth-mask condition are typically higher than with eye-mask (columns 3,4).
These findings from implicit user behavior mirror explicit
recognition results in Fig. 4. Specifically, significant differences can be noted with EEG-based results for the above
conditions, while differences with eye-gaze are inconspicuous. These results cumulatively convey that (a) analyses
of explicit and implicit user behavior affirm similar trends
relating to visual FER; (b) gender differences are better
encoded while perceiving high-intensity emotions, and how
eye cues are interpreted for emotion inference, and (c) EEG
signals better encode gender differences in visual emotion
processing than gaze patterns.
Examining fusion results, LF is generally superior to
EF when all emotions are considered, while EF achieves
superior performance when emotion-specific user data are
utilized for GR. We attempted GR from emotion-specific
data to follow up on findings in Sec. 4, conveying females
to be more sensitive to negative emotions. In Table 2, eyegaze based GR performance improves significantly when
emotion-specific data are considered (peak AUC of 0.68 for
fear under eye-mask, vs peak AUC of 0.53 under mouthmask for all emotions), while EEG results remain stable
(peak AUC of 0.71 for HI anger vs peak AUC of 0.71 for
all HI emotions). Optimal GR results (in bold) are mostly

achieved for the A, D, F and Sa emotions implying that
gender differences best manifest for negative valence.
Table 3 largely replicates the trends noted from Table 2, with higher AUC scores achieved with the CNN
and Adaboost classifiers. The one notable difference is that
while the optimal NB baseline performs poorly with gaze
features but much better with EEG in Table 2, the Adaboost
framework outperforms the EEG-based CNN in the eye and
mouth-mask conditions; EEG nevertheless encodes gender
differences better while processing HI and LI emotions.
These results imply that (a) eye movements in pursuit of
FER, especially under partial face occlusion, are distinctive
of gender, and (b) the discriminative Adaboost framework
is able to better learn distinctive eye movement patterns as
compared to the generative NB classifier. Late fusion results
denoting the combination of decisions made by the CNN
and Adaboost reveal that when one modality is considerably more potent than the other (EEG >> Eye for HI and
LI, while Eye > EEG for mask), LF does not necessarily
outperform constituent modalities.
5.4.3 Spatio-temporal EEG analysis
We also examined spatio-temporal characteristics of the
EEG signal to examine if (i) gender differences in visual
emotion processing are effectively captured by certain
electrodes, and are attributable to specific (functional) brain
lobes and (ii) any time window(s) were critical for GR over
4s of visual processing.
Spatial: We evaluated the ability of each EEG channel
(cf. Fig 1) to capture gender-discriminative information
by feeding the deep network (Fig. 8) with single-channel
EEG input. Consistent with prior findings [20], [67], [68],
we noted optimal GR with the frontal and occipital brain
lobes. Single channel GR followed a trend similar to Table 3,
with optimal GR achieved for HI emotions, and worst GR
performance noted with mouth-mask data. A symmetricity
was noted among the optimal EEG channels, namely, AF3
and AF4, and F3 and F4. These results mirror observations
relating to the existence of brain hemispheres [69] for ER.
Temporal: As isolation of gender-specific ERPs is possible
from 1s time-windows (Sec. 5.1.1), we considered four nonoverlapping 1s windows W1–W4 spanning 4s of stimulus
viewing. Fig. 10 presents GR AUCs achieved over W1–W4
from emotion-specific EEG data. Plots confirm that reliable,
above-chance GR is achieved over each of W1–W4 across
the different viewing conditions. Highest GR performance
is noted for HI morphs and lowest GR for eye mask faces,
consistent with Table 3. While temporal analyses revealed
no significant GR differences across windows, optimal GR
was generally achieved for W1 and W2 over all morph
conditions suggesting a primacy effect. Masked conditions
result in higher GR performance variance, perhaps due to
the initial difficulty in FER owing to facial occlusions.

6

D ISCUSSION & C ONCLUSION

A critical requirement of today’s ubiquitous computing
devices is to sense user intention, emotion and cognition
from multimodal cues, and devise effective interactions to

9

TABLE 2: GR results with baseline classifiers over different modalities and their fusion.
AUC
EEG (NB)
EYE (NB)
Early fusion (RSVM)
Late fusion (LSVM)

All

EEG (NB)

A
D
F
H
Sa
Su
A
D
F
H
Sa
Su
A
D
F
H
Sa
Su
A
D
F
H
Sa
Su

LF (RSVM)

EF (RSVM)

EYE (NB)

Emotion wise

HI

LI

Eyemask

Mouthmask

± 0.002
± 0.013
± 0.035
± 0.022
0.708 ± 0.064
0.673 ± 0.055
0.643 ± 0.059
0.696 ± 0.047
0.674 ± 0.048
0.692 ± 0.048
0.601 ± 0.021
0.577 ± 0.011
0.595 ± 0.015
0.560 ± 0.021
0.539 ± 0.015
0.555 ± 0.008
0.555 ± 0.021
0.535 ± 0.024
0.618 ± 0.011
0.575 ± 0.017
0.540 ± 0.021
0.579 ± 0.013
0.543 ± 0.062
0.542 ± 0.044
0.519 ± 0.029
0.526 ± 0.031
0.573 ± 0.076
0.562 ± 0.073

± 0.005
± 0.017
± 0.022
± 0.035
0.580 ±0.074
0.696 ± 0.062
0.596 ± 0.089
0.668 ± 0.046
0.634 ± 0.064
0.636 ± 0.071
0.565 ± 0.031
0.632 ± 0.009
0.535 ± 0.029
0.538 ± 0.017
0.605 ± 0.030
0.555 ± 0.018
0.581 ± 0.037
0.622 ± 0.025
0.619 ± 0.041
0.597 ± 0.009
0.598 ± 0.024
0.574 ± 0.014
0.571 ± 0.081
0.597 ± 0.093
0.597 ± 0.170
0.508 ±0.017
0.584 ± 0.102
0.568 ± 0.125

± 0.03
± 0.06
± 0.07
± 0.05
0.610 ± 0.16
0.592 ± 0.06
0.605 ± 0.14
0.586 ± 0.07
0.652 ± 0.08
0.633 ± 0.08
0.470 ± 0.20
0.480 ± 0.14
0.680 ± 0.25
0.555 ± 0.13
0.445 ± 0.15
0.624 ± 0.13
0.320 ± 0.25
0.521 ± 0.18
0.700 ± 0.22
0.575 ± 0.13
0.532 ± 0.16
0.433 ± 0.13
0.570 ± 0.15
0.590 ± 0.10
0.645 ± 0.16
0.564 ± 0.09
0.513 ± 0.04
0.581 ± 0.08

± 0.04
± 0.05
± 0.06
± 0.07
0.672 ± 0.07
0.650 ± 0.14
0.564 ± 0.12
0.624 ± 0.08
0.590 ± 0.08
0.650 ± 0.09
0.590 ± 0.16
0.521 ± 0.27
0.580 ± 0.17
0.540 ± 0.12
0.455 ± 0.13
0.494 ± 0.16
0.390 ± 0.18
0.500 ± 0.24
0.522 ± 0.18
0.524 ± 0.14
0.502 ± 0.14
0.515 ± 0.16
0.590 ± 0.09
0.570 ± 0.14
0.691 ± 0.12
0.552 ± 0.10
0.580 ± 0.11
0.583 ± 0.08

0.714
0.493
0.522
0.549

0.600
0.481
0.524
0.523

0.690
0.471
0.520
0.540

0.654
0.525
0.520
0.610

TABLE 3: GR results with CNN and Adaboost classifiers over different modalities and their fusion.
AUC

LF

EYE (Ada)

EEG (CNN)

All

Emotion wise

HI
EEG (CNN)
EYE (Ada)
Late fusion
A
D
F
H
Sa
Su
A
D
F
H
Sa
Su
A
D
F
H
Sa
Su

± 0.011
± 0.016
± 0.015
0.757 ± 0.113
0.758 ± 0.047
0.738 ± 0.083
0.790 ± 0.049
0.739 ± 0.089
0.720 ± 0.076
0.651 ± 0.064
0.591 ± 0.038
0.569 ± 0.078
0.574 ± 0.090
0.587 ± 0.087
0.530 ± 0.093
0.720 ± 0.082
0.676 ± 0.083
0.675 ± 0.140
0.770 ± 0.050
0.729 ± 0.062
0.723 ± 0.090
0.931
0.521
0.920

TABLE 4: Spatial performance evaluation. Parentheses denote channel for which best AUC was achieved.
1
2
3

HI
0.8266 (AF4)
0.8200 (F3)
0.8131 (O2)

LI
0.8008 (F3)
0.7772 (O2)
0.7745 (AF4)

Eye-mask
0.7160 (F4)
0.7143 (F3)
0.7055 (AF4)

Mouth-mask
0.7135 (F3)
0.7124 (F4)
0.7107 (AF3)

LI

± 0.020
± 0.023
± 0.045
0.719 ± 0.086
0.644 ± 0.080
0.618 ± 0.107
0.701 ± 0.054
0.737 ± 0.078
0.636 ± 0.042
0.651 ± 0.079
0.600 ± 0.068
0.528 ± 0.157
0.529 ± 0.050
0.576 ± 0.068
0.546 ± 0.052
0.651 ± 0.113
0.557 ± 0.153
0.470 ± 0.164
0.683 ± 0.088
0.555 ± 0.107
0.604 ± 0.105
0.882
0.538
0.850

eye mask

mouth mask

± 0.038
± 0.015
± 0.017
0.626 ± 0.165
0.594 ± 0.161
0.542 ± 0.128
0.714 ± 0.082
0.689 ± 0.097
0.649 ± 0.098
0.840 ± 0.160
0.950 ± 0.045
0.950 ± 0.067
0.931 ± 0.049
0.959 ± 0.051
0.938 ± 0.067
0.835 ± 0.167
0.939 ± 0.070
0.887 ± 0.139
0.897 ± 0.042
0.939 ± 0.062
0.826 ± 0.142

± 0.023
± 0.014
± 0.037
0.676 ± 0.157
0.673 ± 0.108
0.585 ± 0.199
0.751 ± 0.051
0.731 ± 0.079
0.652 ± 0.067
0.957 ± 0.038
0.910 ± 0.108
0.881 ± 0.075
0.929 ± 0.051
0.958 ± 0.041
0.921 ± 0.107
0.860 ± 0.132
0.967 ± 0.105
0.869 ± 0.097
0.926 ± 0.060
0.843 ± 0.133
0.943 ± 0.058

0.892
0.969
0.974

0.886
0.966
0.946

optimize users’ individual and social behaviors. Affective
computing (AC), whose objective is to empower devices
to interact naturally and empathetically with users, plays
a crucial role here. Apart from inferring user emotions,

10

Fig. 10: (a–d) Temporal EEG analyses: GR for HI, LI, Eye and Mouth mask conditions over temporal windows (W1–W4).
it would also be beneficial for AC systems to predict soft
biometrics such as user age and gender for effective interaction [70]. There is also a strong need to develop sensing
mechanisms that respect user privacy concerns [71], and
eye-movements and EEG signals represent privacy preserving
implicit behaviors which enable inference of user traits even
as the user identity remains hidden.
The primary objective of this study is to explore the utility of eye movements and EEG for user gender prediction. We
designed a study where these implicit user behaviors were
recorded as 28 users (14 male) recognized facial emotions;
since eye movements are known to be characteristic of the
perceived facial emotion [25], [45], our study design additionally enabled recognition of the facial (stimulus) valence.
Crucially, our study employs lightweight and inexpensive
sensors for inference, as against bulky and intrusive lab
sensors which are typically used in user-centered analyses
but significantly constrain user behavior. Also, to examine
user efficacy for FER under occlusions, we presented both
unoccluded (high and low-intensity), and occluded (with
the eye or mouth region masked) emotive faces to users.
The fact that gender differences exist in visual emotional processing is demonstrated at multiple levels via
our experiments. In terms of explicit user responses, female
users are found to quickly and accurately perform FER on
unoccluded faces, especially for negative valence emotions
(Section 4). Likewise, females also achieve higher recognition of negative emotions with mouth-masked faces. Subsequent examination of implicit cues revealed interesting correlations; examination of eye fixation distributions showed
greater female fixation around the eyes than males irrespective of stimulus type, and fixations on eyes were significantly longer in the mouth-mask condition. This observation
is consistent with prior studies [23], and the proposition that
females primarily look at the eyes for emotional cues.
Analysis of EEG ERPs also conveyed interesting patterns. While processing unoccluded faces, stronger N100
and P300 peaks are noted in female ERPs for negative emotions, as well as a stronger N400 peak for strongly negative
faces. This suggests a differential cognitive processing of
negative vs positive emotions by females, which results
in their enhanced sensitivity towards negative emotions.
Likewise, lower male N400 latencies are generally noted at
the O2 electrode for positive emotions. Stronger N100 and
P300 female ERPs are also noted for negative mouth-masked

emotions, suggesting that negative emotions are processed
rapidly by women, and do not necessarily entail selective
attention to emotional cues.
That implicit and differential behaviors can be isolated
using data acquired via low-cost sensors affirms that our
experimental design can effectively capture emotion and
gender-specific information. Greater female sensitivity to
negative emotions can be attributed to several factors like
social structure, environment and evolution, way of living
and social stereotypes [72]. While we did not seek to expressly elicit emotions through facial imagery, facial expressions are known to induce emotions in the viewer [73], and
examination of users’ emotional behavior enables prediction
of both the user gender and stimulus valence.
Valence recognition experiments employing eye-gaze
features (processed by an Adaboost ensemble) and EEG features (input to a 3-layer CNN performing 1D convolutions)
revealed the following. Eye-gaze patterns on HI emotional
faces were highly characteristic of facial valence for both
male and female users, resulting in AUC scores ≥ 0.98. Contrastingly, EEG produced largely near-chance performance,
implying that the EEG features are sub-optimal for ER. Gulf
in the efficacy of eye-gaze and EEG features meant that late
fusion of the classifier outputs was hardly beneficial. That
human eye movements are highly characteristic of stimulus
valence is unsurprising, with prior studies [21] achieving
better-than-chance accuracy with gaze features compiled
for diverse scenes; on the contrary, our study is specific to
emotional faces.
Gender recognition results are summarized as follows.
Table 2 presenting GR results achieved with the baseline
NB, LSVM and RSVM classifiers affirms that consistent with
the recognition rate statistics and ERP analyses, EEG-based
gender differences best manifest for HI emotional faces,
and are least observable for LI emotional faces. Emotionspecific analyses affirm that peak GR AUC scores are mostly
achieved with negative valence data. Also, the NB classifier
achieves much superior results with EEG as compared to
eye-gaze features. On the other hand, Table 3 shows that
much superior GR results are achievable with the CNN and
Adaboost classifiers respectively fed with the EEG and eyegaze data. Interestingly, very high AUC scores are achieved
with eye-gaze features compiled for the mask conditions
(this trend is unobservable for the HI and LI conditions),
implying that eye movements in pursuit of FER under par-

11

tial face occlusion are highly gender-specific. The substantial
disparity in EEG and eye-based results across the different
conditions results in the fusion of the two modalities is
hardly beneficial.
While the presented study involves only a small pool
of (N=28) users, the observed GR and ER results are nevertheless highly promising and demonstrate the utility of
implicit behaviors for privacy preserving user profiling. The
ever-increasing and commonplace availability of sensors
employed in this work also opens up the possibility of
conducting large-scale (crowdsourced) user-centric studies.
Our larger endeavor is to predict soft biometrics (age, gender,
emotional and cognitive state) of users via implicit and multimodal behavioral cues to empower gaming, advertising,
augmented and virtual reality applications for behavioral
change, and mental health monitoring systems for disorders
like Alexithymia. Future work will focus on the modeling
of shared behaviors (joint embedding of EEG and eye-gaze
features) for efficient user-trait prediction employing techniques such as multi-task learning, and prototyping real-life
profiling applications. Investigating the optimality of predesigned features (e.g., power-spectral density for EEG) or
learned feature descriptors represents another interesting
line of work.

R EFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]

[8]
[9]

[10]
[11]
[12]
[13]
[14]
[15]

S. Wiedenbeck, V. Grigoreanu, L. Beckwith, and M. Burnett, “Gender HCI: What about the software?” Computer, vol. 39, pp. 97–101,
2006.
R. W. Picard, Affective Computing. Cambridge, MA, USA: MIT
Press, 1997.
D. Passig and H. Levin, “The interaction between gender, age, and
multimedia interface design,” Education and Info Tech., vol. 6, no. 4,
pp. 241–250, 2001.
M. Czerwinski, D. S. Tan, and G. G. Robertson, “Women take a
wider view,” in CHI, 2002, pp. 195–202.
W. Zhang, M. L. Smith, L. N. Smith, and A. R. Farooq, “Gender
and gaze gesture recognition for human-computer interaction,”
Computer Vision and Image Understanding, vol. 149, pp. 32–50, 2016.
B. D. Homer, E. O. Hayward, J. Frye, and J. L. Plass, “Gender
and player characteristics in video game play of preadolescents,”
Computers in Human Behavior, vol. 28, no. 5, pp. 1782–1789, 2012.
J. D. Schwark, I. Dolgov, D. Hor, and W. Graves, “Gender and personality trait measures impact degree of affect change in a hedonic
computing paradigm,” Int’l Journal Human-Computer Interaction,
vol. 29, no. 5, pp. 327–337, 2013.
C. B. Ng, Y. H. Tay, and B.-M. Goi, “Vision-based human gender
recognition: A survey,” CoRR, vol. abs/1204.1611, 2012.
H. Joho, J. Staiano, N. Sebe, and J. M. Jose, “Looking at the
viewer: analysing facial activity to detect personal highlights of
multimedia contents,” Multimedia Tools and Applications, vol. 51,
no. 2, pp. 505–523, 2011.
M. Li, K. J. Han, and S. Narayanan, “Automatic speaker age and
gender recognition using acoustic and prosodic level information
fusion,” Comp. Speech & Language, vol. 27, no. 1, pp. 151–167, 2013.
C. M. Lee and S. S. Narayanan, “Toward detecting emotions in
spoken dialogs,” IEEE Trans. Speech & Audio Process., vol. 13, no. 2,
pp. 293–303, 2005.
“Biometric security poses huge privacy risks,” 2013.
[Online]. Available: https://www.scientificamerican.com/article/
biometric-security-poses-huge-privacy-risks/
P. Campisi and D. La Rocca, “Brain waves for automatic biometricbased user recognition,” IEEE Trans. Info Forensics & Security,
vol. 9, no. 5, pp. 782–800, 2014.
S. Yang and F. Deravi, “On the effectiveness of EEG signals as a
source of biometric information,” in EST. IEEE, 2012, pp. 49–52.
M. van Vliet, A. Robben, N. Chumerin, N. V. Manyakov, A. Combaz, and M. M. V. Hulle, “Designing a brain-computer interface
controlled video-game using consumer grade EEG hardware,” in
BRC, 2012, pp. 1–6.

[16] W. L. Lim, O. Sourina, and L. Wang, “Mind-an EEG neurofeedback
multitasking game,” in Cyberworlds. IEEE, 2015, pp. 169–172.
[17] E. Dalmaijer, “Is the low-cost eyetribe eye tracker any good for
research?” PeerJ PrePrints, Tech. Rep., 2014.
[18] B. Montagne, R. P. C. Kessels, E. Frigerio, E. H. F. de Haan, and
D. I. Perrett, “Sex differences in the perception of affective facial
expressions: Do men really lack emotional sensitivity?” Cognitive
Processing, vol. 6, no. 2, pp. 136–141, 2005.
[19] J. A. Hall and D. Matsumoto, “Gender differences in judgments of
multiple emotions from facial expressions,” Emotion, vol. 4, no. 2,
pp. 201–206, 2004.
[20] M. Bilalpur, S. M. Kia, T.-S. Chua, and R. Subramanian, “Discovering gender differences in facial emotion recognition via implicit
behavioral cues,” arXiv preprint arXiv:1708.08729, 2017.
[21] H. R.-Tavakoli, A. Atyabi, A. Rantanen, S. J. Laukka, S. NeftiMeziani, and J. Heikkil, “Predicting the valence of a scene from
observers eye movements,” PLoS ONE, vol. 10, no. 9, pp. 1–19,
2015.
[22] S. Vassallo, S. L. Cooper, and J. M. Douglas, “Visual scanning in
the recognition of facial affect: Is there an observer sex difference?”
Journal of Vision, vol. 9, no. 3, pp. 11–11, 2009.
[23] L. J. Wells, S. M. Gillespie, and P. Rotshtein, “Identification of
emotional facial expressions: Effects of expression, intensity, and
sex on eye gaze,” PloS one, vol. 11, no. 12, 2016.
[24] R. Subramanian, D. Shankar, N. Sebe, and D. Melcher, “Emotion
modulates eye movement patterns and subsequent memory for
the gist and details of movie scenes.” Journal of vision, vol. 14,
no. 3, pp. 1–18, 2014.
[25] M. W. Schurgin, J. Nelson, S. Iida, H. Ohira, J. Y. Chiao, and
S. L. Franconeri, “Eye movements during emotion recognition in
faces,” Journal of Vision, vol. 14, no. 13, pp. 1–16, 2014.
[26] M. Bilalpur, S. M. Kia, M. Chawla, T.-S. Chua, and R. Subramanian,
“Gender and emotion recognition with implicit user signals,” in
Int’l Conference on Multimodal Interaction, 2017, p. 379387.
[27] W.-L. Zheng, B.-N. Dong, and B.-L. Lu, “Multimodal emotion
recognition using EEG and eye tracking data,” in EMBC, 2014,
pp. 5040–5043.
[28] W. Liu, W.-L. Zheng, and B.-L. Lu, “Multimodal emotion
recognition using multimodal deep learning,” arXiv preprint
arXiv:1602.08225, 2016.
[29] M. Abadi, R. Subramanian, S. Kia, P. Avesani, I. Patras, and
N. Sebe, “DECAF: MEG-based multimodal database for decoding
affective physiological responses,” IEEE Trans. Affective Computing,
vol. 6, no. 3, pp. 209–222, 2015.
[30] S. Koelstra, C. Mühl, M. Soleymani, J.-S. Lee, A. Yazdan,
T. Ebrahimi, T. Pun, A. Nijholt, and I. Patras, “DEAP: A Database
for Emotion Analysis Using Physiological Signals,” IEEE Trans.
Affective Computing, vol. 3, no. 1, pp. 18–31, 2012.
[31] R. Subramanian, J. Wache, M. Abadi, R. Vieriu, S. Winkler, and
N. Sebe, “ASCERTAIN: Emotion and personality recognition using commercial sensors,” IEEE Trans. Affective Computing, 2016.
[32] Y. Wu, Y. Zhuang, X. Long, F. Lin, and W. Xu, “Human gender
classification: A review,” arXiv preprint arXiv:1507.05122, 2015.
[33] M. Bilalpur, M. Kankanhalli, S. Winkler, and R. Subramanian,
“Eeg-based evaluation of cognitive workload induced by acoustic
parameters for data sonification,” in International Conference on
Multimodal Interaction, 2018, p. 315323.
[34] S. Hoppe, T. Loetscher, S. A. Morey, and A. Bulling, “Eye movements during everyday behavior predict personality traits,” Frontiers in human neuroscience, vol. 12, no. 105, 2018.
[35] M. D. Zotto and A. J. Pegna, “Processing of masked and unmasked
emotional faces under different attentional conditions: an Electrophysiological investigation,” Frontiers in psychology, vol. 6, p. 1691,
2015.
[36] H. Katti, R. Subramanian, M. Kankanhalli, N. Sebe, T.-S. Chua, and
K. R. Ramakrishnan, “Making computers look the way we look:
exploiting visual attention for image understanding,” in ACM
Multimedia, 2010, pp. 667–670.
[37] A. Hanjalic and L.-Q. Xu, “Affective video content representation
and modeling,” IEEE Trans. Multimedia, vol. 7, no. 1, pp. 143–154,
2005.
[38] H. L. Wang and L.-F. Cheong, “Affective understanding in film,”
IEEE Trans. Circ. Syst. Video Tech., vol. 16, no. 6, pp. 689–704, 2006.
[39] V. Vonikakis, R. Subramanian, J. Arnfred, and S. Winkler, “A
probabilistic approach to people-centric photo selection and sequencing,” IEEE Trans. Multimedia, 2017.

12

[40] A. Shukla, S. S. Gullapuram, H. Katti, K. Yadati, M. Kankanhalli,
and R. Subramanian, “Affect recognition in ads with application to
computational advertising,” arXiv preprint arXiv:1709.01683, 2017.
[41] X. Li, P. Zhang, D. Song, G. Yu, Y. Hou, and B. Hu, “Eeg based
emotion identification using unsupervised deep feature learning,”
in Workshop Neuro-Phys. Methods in IR, 2015.
[42] W.-L. Zheng, J.-Y. Zhu, Y. Peng, and B.-L. Lu, “EEG-based emotion
classification using deep belief networks,” in ICME, 2014, pp. 1–6.
[43] G. J. Walker-Smith, A. G. Gale, and J. M. Findlay, “Eye movement
strategies involved in face perception,” Perception, vol. 6, pp. 313–
326, 1977.
[44] M. L. Smith, G. W. Cottrell, F. Gosselin, and P. G. Schyns, “Transmitting and decoding facial expressions,” Psychological Science,
vol. 16, no. 3, pp. 184–189, 2005.
[45] R. Subramanian, V. Yanulevskaya, and N. Sebe, “Can computers
learn from humans to see better?: Inferring scene semantics from
viewers’ eye movements,” in ACM Multimedia, 2011, pp. 33–42.
[46] H. Aviezer, R. R. Hassin, J. Ryan, C. Grady, J. Susskind, A. Anderson, M. Moscovitch, and S. Bentin, “Angry, disgusted, or afraid?”
Psych. Science, vol. 19, no. 7, pp. 724–732, 2008.
[47] N. L. Etcoff and J. J. Magee, “Categorical perception of facial
expressions,” Cognition, vol. 44, no. 3, pp. 227–240, 1992.
[48] S. Sullivan, A. Campbell, S. B. Hutton, and T. Ruffman, “What’s
good for the goose is not good for the gander: age and gender
differences in scanning emotion faces,” Journals of Gerontology,
Series B: Psychological Sciences and Social Sciences, pp. 1–6, 2015.
[49] J. N. Bassili, “Emotion recognition: the role of facial movement
and the relative importance of upper and lower areas of the face,”
Journal Pers. Social Psych., vol. 37, no. 11, pp. 2049–2058, 1979.
[50] J. K. Hall, S. B. Hutton, and M. J. Morgan, “Sex differences in scanning faces: Does attention to the eyes explain female superiority
in facial expression recognition?” Cognition and Emotion, vol. 24,
no. 4, pp. 629–637, 2010.
[51] Q. Rahman, G. D. Wilson, and S. Abrahams, “Sex, sexual orientation, and identification of positive and negative facial affect,” Brain
and Cognition, vol. 54, no. 3, pp. 179–185, 2004.
[52] C. Lithari, C. A. Frantzidis, C. Papadelis, A. B. Vivas, M. A. Klados,
C. Kourtidou-Papadeli, C. Pappas, A. A. Ioannides, and P. D.
Bamidis, “Are females more responsive to emotional stimuli? a
neurophysiological study across arousal and valence dimensions,”
Brain Topography, vol. 23, no. 1, pp. 27–40, 2010.
[53] P. Nguyen, D. Tran, X. Huang, and W. Ma, “Age and gender
classification using eeg paralinguistic features,” in NER. IEEE,
2013, pp. 1295–1298.
[54] O. Langner, R. Dotsch, G. Bijlstra, D. H. J. Wigboldus, S. T. Hawk,
and A. van Knippenberg, “Presentation and validation of the
radboud faces database,” Cognition and Emotion, vol. 24, no. 8, pp.
1377–1388, 2010.
[55] T. Baltrušaitis, P. Robinson, and L.-P. Morency, “Openface: an open
source facial behavior analysis toolkit,” in WACV, 2016.
[56] C. Muhl, B. Allison, A. Nijholt, and G. Chanel, “A survey of
affective brain computer interfaces: principles, state-of-the-art, and
challenges,” Brain-Computer Interfaces, vol. 1, no. 2, pp. 66–84, 2014.
[57] C. T. Vi, I. Jamil, D. Coyle, and S. Subramanian, “Error related
negativity in observing interactive tasks,” in SIGCHI Conference on
Human Factors in Computing Systems, 2014, p. 37873796.
[58] V. Krassanakis, V. Filippakopoulou, and B. Nakos, “Eyemmv
toolbox: An eye movement post-analysis tool based on a two-step
spatial dispersion threshold for fixation identification,” Journal of
Eye Movement Research, vol. 7, no. 1, 2014.
[59] S. Koelstra and I. Patras, “Fusion of facial expressions and EEG
for implicit affective tagging,” Image and Vision Computing, vol. 31,
no. 2, pp. 164–174, 2013.
[60] N. M. Rad, S. M. Kia, C. Zarbo, T. van Laarhoven, G. Jurman,
P. Venuti, E. Marchiori, and C. Furlanello, “Deep learning
for automatic stereotypical motor movement detection using
wearable sensors in autism spectrum disorders,” Signal Processing,
vol. 144, pp. 180 – 191, 2018. [Online]. Available: http://www.
sciencedirect.com/science/article/pii/S0165168417303705
[61] S. Ioffe and C. Szegedy, “Batch Normalization: Accelerating deep
network training by reducing internal covariate shift,” in ICML,
F. Bach and D. Blei, Eds., vol. 37, 2015, pp. 448–456. [Online].
Available: http://proceedings.mlr.press/v37/ioffe15.html
[62] K. Simonyan and A. Zisserman, “Very deep convolutional
networks for large-scale image recognition,” CoRR, vol.
abs/1409.1556, 2014.
[63] F. Chollet et al., “Keras,” https://github.com/fchollet/keras, 2015.

[64] P. Viola and M. J. Jones, “Robust real-time face detection,” Int. J.
Comput. Vision, vol. 57, no. 2, pp. 137–154, May 2004. [Online].
Available: https://doi.org/10.1023/B:VISI.0000013087.49260.fb
[65] P. Silapachote, D. R. Karuppiah, and A. R. Hanson, “Feature
selection using AdaBoost for face expression recognition,” UMASS
DEPT OF COMP. SCI., Tech. Rep., 2005.
[66] T. Hastie, S. Rosset, J. Zhu, and H. Zou, “Multi-class adaboost,”
Statistics and its Interface, vol. 2, no. 3, pp. 349–360, 2009.
[67] M. Bilalpur, S. M. Kia, M. Chawla, T.-S. Chua, and R. Subramanian,
“Gender and emotion recognition with implicit user signals,” in
ICMI, 2017.
[68] E. B. McClure, C. S. Monk, E. E. Nelson, E. Zarahn, E. Leibenluft,
R. M. Bilder, D. S. Charney, M. Ernst, and D. S. Pine, “A developmental examination of gender differences in brain engagement
during evaluation of threat.” Biological psychiatry, vol. 55, pp. 1047–
55, 2004 Jun 1 2004.
[69] G. Vingerhoets, C. Berckmoes, and N. Stroobant, “Cerebral hemodynamics during discrimination of prosodic and semantic emotion in speech studied by transcranial doppler ultrasonography.”
Neuropsychology, vol. 17, no. 1, p. 93, 2003.
[70] S. Rukavina, S. Gruss, H. Hoffmann, J.-W. Tan, S. Walter, and H. C.
Traue, “Affective computing and the impact of gender and age,”
PLOS ONE, vol. 11, no. 3, pp. 1–20, 03 2016.
[71] C. Reynolds and R. Picard, “Affective sensors, privacy, and ethical
contracts,” in CHI 04 Extended Abstracts on Human Factors in
Computing Systems, 2004, p. 11031106.
[72] Y. Deng, L. Chang, M. Yang, M. Huo, and R. Zhou, “Gender differences in emotional response: Inconsistency between experience
and expressivity,” PloS one, vol. 11, no. 6, p. e0158666, 2016.
[73] E. Siedlecka and T. F. Denson, “Experimental methods for inducing basic emotions: A qualitative review,” Emotion Review, vol. 11,
no. 1, pp. 87–97, 2019.
Maneesh Bilalpur is a Ph.D. student at the
University of Pittsburgh. He holds a Bachelor’s
degree in Electronics and Communication Engineering from the Vellore Institute of Technology, and a Master of Science by Research from
the Center for Visual Information Technology at
International Institute of Information Technology
Hyderabad, India. His research interests include
Machine Learning, Affective Computing, Multimedia Analysis and Computer Vision.
Seyed Mostafa Kia received his PhD degree in
Computer Science from the University of Trento,
Italy. His research focused on the interpretability
of MEG decoding models. He is now a post doctoral researcher at the Radboud university medical center in Nijmegen, Netherlands. He seeks
scalable machine learning solutions to solve intractable problems in clinical neuroimaging data
analysis.
Mohan Kankanhalli is a Professor and Dean
of the School of Computing at the National University of Singapore (NUS). Mohan obtained his
BTech from IIT Kharagpur and MS and PhD
from Rensselaer Polytechnic. His current research interests are in Multimedia Systems (content processing, retrieval) and Multimedia Security (surveillance and privacy). Mohan is actively
involved in organizing major conferences, and
serves on the editorial board of several journals.
Ramanathan Subramanian received his Ph.D.
in Electrical and Computer Engg. from NUS in
2008. He is an Associate Professor in Computer
Science and Engg. at IIT Ropar. His past affiliations include IHPC (Singapore), U Glasgow (Singapore), IIIT Hyderabad (India) and UIUC-ADSC
(Singapore). His research focuses on Humancentered computing, and especially on extracting and modeling non-verbal behavioral cues for
interactive analytics. He is an IEEE Senior Member and a member of the ACM and AAAC.

