IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 15, NO. 3, JUNE 2014

959

Using a Head-up Display-Based Steady-State
Visually Evoked Potential Brain–Computer
Interface to Control a Simulated Vehicle
Luzheng Bi, Member, IEEE, Xin-an Fan, Ke Jie, Teng Teng, Hongsheng Ding, and Yili Liu, Member, IEEE

Abstract—In this paper, we propose a new steady-state visually
evoked potential (SSVEP) brain–computer interface (BCI) with
visual stimuli presented on a windshield via a head-up display, and
we apply this BCI in conjunction with an alpha rhythm to control
a simulated vehicle with a 14-DOF vehicle dynamics model. A
linear discriminant analysis classifier is applied to detect the alpha
rhythm, which is used to control the starting and stopping of the
vehicle. The classification models of the SSVEP BCI with three
commands (i.e., turning left, turning right, and going forward) are
built by using a support vector machine with frequency domain
features. A real-time brain-controlled simulated vehicle is developed and tested by using four participants to perform a driving
task online, including vehicle starting and stopping, lane keeping,
avoiding obstacles, and curve negotiation. Experimental results
show the feasibility of using the human “mind” alone to control
a vehicle, at least for some users.
Index Terms—Brain-controlled vehicle, head-up display (HUD),
human–vehicle interaction, steady-state visually evoked potential
(SSVEP) brain–computer interface (BCI).

I. I NTRODUCTION

U

SING the human “mind” rather than the limbs to drive
a vehicle (such vehicles can be called brain-controlled
vehicles) has significant practical values. Such systems may
provide a complementary or alternative way for individuals
with severe neuromuscular disorders to drive a vehicle to extend
their scope of mobility and, thus, improve their quality of life
and independence. For the broader driving community, these
systems have the potential to facilitate the development of
human-centric driver assistance systems to better assist drivers
in driving a vehicle [1].
Brain–computer interfaces (BCIs) can establish a direct pathway between the human “mind” and external devices. Owing to
its low cost and convenience of use, the EEG has been the most
popular signal that is used in developing BCI systems, such
as controlling a cursor on the screen [2], [3], selecting letters
from a virtual keyboard [4], [5], browsing the Internet [6]–[8],
Manuscript received July 30, 2013; revised October 23, 2013; accepted
November 13, 2013. Date of publication December 11, 2013; date of current
version May 30, 2014. This work was supported by the National Natural
Science Foundation of China under Grants 61374192 and 61004114 and in part
by the Fundamental Research Funds for the Central Universities under Grant
2012CX01018. The Associate Editor for this paper was R. I. Hammound.
L. Bi, X. Fan, K. Jie, T. Teng, and H. Ding are with the School of Mechanical
Engineering, Beijing Institute of Technology, Beijing 100081, China.
Y. Liu is with the Department of Industrial and Operations Engineering,
College of Engineering, University of Michigan, Ann Arbor, MI 48109 USA.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TITS.2013.2291402

and playing games [9], [10]. Recently, the BCIs that are based
on EEG signals have been used to control wheelchairs to help
bring mobility back to some severely disabled people [11]–[13].
A more detailed review regarding brain-controlled wheelchairs
can be seen in our work in [1]. The EEG signals that are
commonly used in developing BCIs include 1) P300 potentials,
which are positive potential deflections on the ongoing brain
activity signal at a latency of roughly 300 ms after the random
occurrence of a desired target stimulus from nontarget stimuli [14]; 2) steady-state visually evoked potentials (SSVEPs),
which are visually evoked by a stimulus and are modulated
at a fixed frequency and occur as an increase in the EEG
activity at the stimulus frequency [15]; and 3) the event-related
desynchronizations (ERDs) and the event-related synchronization (ERS), which are induced by performing mental tasks, such
as motor imagery, mental arithmetic, or mental rotation [16]. A
variety of classifiers have been used to translate EEG signals
into an output command, from simple classifiers, such as the
linear discriminant analysis (LDA), to nonlinear classifiers,
such as support vector machines (SVMs). A more detailed
review regarding classifiers can be seen in our work in [1].
Compared with brain-controlled wheelchairs, braincontrolled vehicles have more complicated dynamic
characteristics, and they run faster in a more complicated
environment. The highest velocities of all existing braincontrolled wheelchairs in a laboratory and in a virtual world
are 1 and 1.5 m/s, respectively.
Few studies have explored how to drive a vehicle using
EEG signals. Hood et al. [17] used an SSVEP-based BCI with
visual stimuli that are developed with light-emitting diodes to
control a simulated car to turn left, turn right, and go straight
in a driving simulator. They announced that the preliminary
experimental results from one subject provided an indication of
the feasibility of using EEG signals to drive a vehicle. However,
they did not provide the specific experimental results of controlling the simulated vehicle and the specific testing conditions
(including the speed of the simulated car and testing scenarios).
Furthermore, when they tested the direction control by using the
BCI (i.e., turning left and right), the speed control was by the
driver via his feet. Rojas et al. [18] applied a commercial BCI
product (i.e., the EPOC cap from Emotiv) to control a vehicle
as a way to test the feasibility of using brain signals to control a
vehicle. The BCI that is embedded in the commercial cap is an
ERD/ERS-based BCI, which can issue at most four commands
by translating EEG signals. Rojas et al. tested their system by
using only one subject to perform lane keeping on a closed

1524-9050 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

960

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 15, NO. 3, JUNE 2014

airfield. The experimental result showed poor steering control
(turning left and right) performance, although it only involved a
constant speed of 2 m/s. The maximum and standard deviation
of the lateral error reached about 10 and 2 m, respectively.
Moreover, in their whole testing process, when the vehicle
imminently collides or if it is about to leave the free drive zone,
the computer instantly stops the vehicle. That is, the vehicle
was controlled by the BCI with the intelligence of the vehicle
by perceiving the surroundings.
Considering that driverless techniques have been gradually
reaching maturity [19], Bi et al. [20] have proposed a HUDbased P300 BCI and applied it for developing a vehicle destination selection system. Users can use the destination selection
system to select a desired destination from predefined destinations, and then use an autonomous navigation system to drive a
vehicle to reach the desired destination.
Our long-term goal is to develop a brain-controlled vehicle
by using a BCI system to select a destination and another BCI
system to issue a motion control command when users want
or need it. In this paper, we move a step toward this goal by
developing a HUD-based SSVEP and by applying it to control
a simulated vehicle to demonstrate the feasibility of using the
BCI to control a simulated vehicle without any assistance from
a driver and the intelligence of the vehicle.
For a brain-controlled vehicle, compared with the control of
speed, direction control is more common and important since it
is currently only possible for brain-controlled vehicles to travel
at a low speed. Thus, in this paper, we investigate how to use
the HUD-based SSVEP BCI to control the vehicle direction in
conjunction with using an alpha rhythm to control the starting
and stopping of the vehicle. The reasons for selecting the
SSVEP BCI rather than the P300 and ERD/ERS BCIs to control
a vehicle are as follows. First, the P300 BCI needs to take
about 4–10 s to issue a command. In contrast, the SSVEP and
ERD/ERS BCIs can issue commands at 1 s or even shorter
intervals. Thus, the P300 BCI systems are not as suitable to use
for issuing motion control commands. Second, the ERD/ERS
BCIs require extensive training that may take several weeks (or
even longer). Compared with the ERD/ERS BCIs, SSVEP BCIs
require minimal training and have stable performance and high
accuracy [1]. The potential weakness of the SSVEP BCIs is that
they need external stimulation compared with ERD/ERS BCIs.
The remainder of this paper is organized as follows. In
Section II, we introduce the methods for developing a HUDbased SSVEP BCI and using it to control simulated vehicles.
Section III describes the experiments that were conducted to
test the system, whose results are described in Section IV. In
Section V, we conclude this paper with a discussion of the
current challenges and future research directions.
II. M ETHODOLOGY
A. Visual Stimuli and Execution Protocol
The SSVEP visual stimuli consisted of two flashing rectangle
(5.65 cm × 16.9 cm) checkerboards, which are displayed on
a real windshield (whose top, bottom, left, and right edges
are 102, 138, 59, and 59 cm, respectively) of vehicles via a
HUD system that we constructed [20], as shown in Fig. 1.

Fig. 1.

Stimuli interface of the HUD-based SSVEP BCI.

Each checkerboard included a grid of 30 × 10 checkers. The
checkerboard pattern needs to reverse colors several times per
second to elicit an SSVEP. The left checkerboard reversed its
color 12 times each second, generating a stimulation frequency
of 12 Hz, whereas the right checkerboard reversed its color
13 times, generating a stimulation frequency of 13 Hz.
When the user wants to control the vehicle to turn left
or turn right, he/she needs to focus his/her attention on the
corresponding checkerboards (the left and right checkerboards
are associated with turning left and right, respectively), and the
BCI interprets the EEG signals to infer the stimulus to which
the user is attending. When the user wants to control the vehicle
to go forward (keep the current heading direction), he/she is
required to not attend to any SSVEP stimulus, and the BCI
classifies the EEG signals into the going forward command.
If the user wants to stop the moving vehicle, the user needs
to close his/her eyes until the system outputs a kind of sound
when the BCI detects the eye closing from the EEG signals and
stops the vehicle. If the user wants to start the vehicle when
the vehicle is stopped, he/she also needs to close his/her eyes
until the system issues another kind of sound when the BCI
detects the eye-closing state from the EEG signals and starts
the vehicle.
B. Data Collection
We used a 16-channel amplifier to acquire the EEG signals
in ten standard locations (i.e., Fz, Cz, Pz, Oz, P3, P4, P7, P8,
O1, and O2), as shown in Fig. 2. The reference potential was
the average of the potentials of the left and right ear lobes. The
EEG signals were amplified and digitalized with a sampling
rate of 1000 Hz and a power-line notch filter to remove the line
noise.
C. BCI System of Brain-Controlled Vehicles
The whole BCI system includes two classification models
(i.e., Model I for detecting the alpha rhythm and Model II for
the SSVEP BCI) and two control models (i.e., the starting/
stopping control model and the direction control model). Classification Model I first interprets 4 s of the EEG data (i.e.,
its time window length is 4 s) with the step size of 0.5 s to
detect the alpha rhythm, and if the alpha rhythm is detected,
Classification Model II then classifies the EEG data into turning

BI et al.: USING HUD-BASED SSVEPs BCI TO CONTROL SIMULATED VEHICLE

961

2) Classification Model of the SSVEP BCI: We extracted
frequency-domain features using the sums of the powers
of the half frequencies ±0.5 Hz, fundamental frequencies
±0.5 Hz, double frequencies ±0.5 Hz, and thrice frequencies
±0.5 Hz of the visual stimuli in the power spectrum of the
EEG data from the ten channels. In other words, we obtained
eight features at each channel. Thus, we obtained a total of
80 features as follows:
x = [x(1), x(2), . . . , x(i), . . . , x(80)]T

Fig. 2.
black.

Placements of the ten channels used to collect the EEG data marked in

(1)

where x(i) is ith feature.
Since the SSVEP BCI includes three classes (i.e., turning
left, turning right, and going forward), it is a multiclass classification, and we adopted the one-versus-rest classification
strategy. In other words, we built a single binary classifier per
class to distinguish this class from the other two classes. In this
paper, we used an SVM with a radial basis function as the kernel
function to develop all the binary classifiers as follows:
 
Nj
2 



wij exp −g xji − x + bj
(2)
yj =
i=1

where Nj is the number of the support vector of the jth
classifier, wij is the weight of the ith support vector of the jth
classifier, wij is the bias of the jth classifier, and xji is the ith
support vector of the jth classifier. We applied the LIBSVM
software library developed by Chang and Lin to determine the
parameters of each binary classifier [21].
To classify the EEG data, the class that is associated with
the maximal score of all binary classifiers was determined as
classification result R, i.e.,
R = arg max(yj ).

(3)

j=1,2,3

3) Control Models: In the brain-controlled vehicle system,
there are two control models. One model is the starting/stopping
control model that is based on the alpha rhythm as follows:
⎧
⎨ f (n) =∼ f (n − 1), if g(n) = 1
V (n) = V0 × f (n) f (n) = f (n − 1),
otherwise (4)
⎩
f (0) = 0; g(0) = 0; n ≥ 1
where Vn is the speed at the nth update, g(n) is the output
of Model I, g(n) = 1 means eye closing, g(n) = 0 means eye
opening, and V0 is a positive speed constant and is initially set
to 3 m/s.
The other model is the direction control model that is based
on the SSVEP, which is defined as
Fig. 3.

Signal flowchart of the BCI system of the brain-controlled vehicles.

α(n) = min {|α(n − 1) + Δα × l(n)| , αmax } ,
l(n) ∈ [−1, 0, 1]

left, turning right, or going forward commands. The signal
flowchart of the whole BCI system is shown in Fig. 3.
1) Alpha Rhythm Detection Model: We applied the ratio of
the sum of the powers of the alpha wave (i.e., 8–13 Hz) to
the sum of the powers of 0–40 Hz in the power spectrum as
features from the EEG data at the ten channels. In other words,
we obtained one feature at each channel, with ten features in
total. Furthermore, the LDA was used to develop the classifier.

α(0) = 0◦

αmax = 100◦

Δα = 10◦
n≥1

(5)

where α(n) is the steering angle at the nth update; l(n) is the
output of Model II, with l(n) = 1 for turning left, l(n) = −1
for turning right, and l(n) = 0 for going forward; and Δα is
a positive angle constant and is initially set to be 10◦ , which
can be adjusted. Furthermore, when the brain-controlled vehicle
turns, the speed decreases to 2 m/s.

962

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 15, NO. 3, JUNE 2014

Fig. 4. Block diagram of the brain-controlled virtual vehicle.

III. E XPERIMENT
A. Subjects
Four participants, with three healthy males and one healthy
female (a mean age of 25), voluntarily participated in the
experiments and received no monetary compensation. All participants had no history of brain disease, and their visual acuity
was normal or normal after adjustments. All the participants
had never participated in any experiment of brain-controlled
simulated vehicle before the experiment.
B. Experimental Platform
A brain-controlled simulated vehicle has been designed and
constructed. It includes three main parts, as shown in Fig. 4. The
first main part is the HUD-based SSVEP BCI with the alpha
rhythm, which consists of the SSVEP visual stimuli presented
on a real windshield via a HUD, the EEG measurement system,
and the proposed signal processing and classification module,
as described in Section II. Second is the control models that
are described in Section II, which include the simulated vehicle
with 14-DOF dynamics and the 3-D driving scene. Finally, we
have the communication system between the computer supporting control models, the 3-D driving scene, and the virtual
vehicle and the other running the EEG acquisition and signal
processing module.
C. Experimental Procedures
Before the start of the experiment, we explained the experimental procedures to the participants so that they can become
familiar with the experimental protocol. We displayed the visual stimuli on a real windshield via the constructed HUD. The
subjects were asked to sit in a seat in front of the windshield,
with similar distance of a vehicle driver to the windshield and
similar vision angles when driving real vehicles. We adjusted
the position, size, and intensity of the visual stimuli, and we
set the parameters of the EEG collection system. We properly
attached electrodes on the scalp of the subjects. Before the start

of data collection, the contact impedance between the scalp and
the EEG electrodes was calibrated to be below 10 kΩ.
The experiment includes two parts. The first part is for
training the alpha rhythm detection mode and the HUD-based
SSVEP BCI model for driving the vehicle, and the second part
is for evaluating and testing the brain-controlled vehicle. The
participants were given 15 min of break between the two parts
of the experiments, and they all began with some minutes of
practice.
The first part of the experiment includes three phases. The
first phase is for collecting the EEG data that are associated
with the turning left and right commands when participants
attend to the left and right SSVEP stimuli. The second phase
is for collecting the EEG data that are associated with the
going forward command when participants do not attend to any
stimuli, given the SSVEP stimuli flick all the time. The third
phase is for collecting the EEG data that are associated with the
starting/stopping command when participants close their eyes.
In the first phase, the participants were required to complete
six sessions of attending to the left and right SSVEP stimuli.
In each session, the subjects were required to complete four
trials with an interval of 10 s between two consecutive trials. In
each trial, they respectively attended to the left and right SSVEP
visual stimuli for 12 s. In the second phase, the participants
were required to complete three sessions of not attending to any
stimuli, with each session including eight trials at an interval
of 10 s between two consecutive trials. In each trial, they
were required not to attend to any stimuli for 12 s, given the
SSVEP stimuli flick all the time. In the third phase, the subjects
were required to complete three sessions of closing their eyes,
with each session including eight trials at an interval of 10 s
between two consecutive trials. In each trial, the participants
were required to close their eyes for 12 s.
We set the time window length to 4 s and the step size to 0.5 s
to extract the EEG epoch as samples. This way, we obtained
408 ([(12 − 4)/0.5 + 1] × 6 × 4) samples that are associated
with the turning left and right commands, 408 ([(12 − 4)/0.5 +
1] × 3 × 8) samples that are associated with the going forward
command, and 408 ([(12 − 4)/0.5 + 1] × 3 × 8) samples that
are associated with the starting/stopping command.
In the second part of the experiment for evaluating and
testing the brain-controlled vehicle, we required the participants
to control a simulated vehicle using the proposed BCI to travel
along the centerline of the lane with two stopped vehicles as
obstacles until the vehicle reaches the destination. We required
the participants to follow two rules. First, they should try to
keep the vehicle inside the boundaries, and once the vehicle
crossed the boundary, they should try to go back to the path.
Second, when the participants see an obstacle, they need to
control the vehicle to avoid it. Ten runs were conducted for each
participant. The experiment task scene is shown in Fig. 5.

IV. R ESULTS
A. BCI Performance
We randomly selected 204 samples from the collected samples (408) that are associated with eyes closed and 204 samples

BI et al.: USING HUD-BASED SSVEPs BCI TO CONTROL SIMULATED VEHICLE

Fig. 5.

Experiment task scene for controlling the vehicle.
TABLE I
ACCURACY OF THE A LPHA R HYTHM D ETECTION M ODEL

TABLE II
ACCURACY OF THE SSVEP BCI

from the samples (1224) that are associated with eyes open
(i.e., the samples that are associated with the left and right
SSVEP stimuli and nonstimulus) to train the alpha rhythm
detection model, and we used the remaining samples to test the
performance of the detection model. In addition, we randomly
selected 272 samples from the samples that are associated
with turning left, turning right, and going forward, respectively,
to train the SSVEP BCI model, and we used the remaining
samples that are associated with the three classes to test the
performance of the SSVEP BCI.
Table I shows the accuracy of the alpha rhythm detection
model. This shows that, except for Subject D, the detection
accuracy of the alpha rhythm for the other three subjects is
greater than 98%, indicating that the subjects have good ability
in controlling the starting and stopping of the vehicle.
Table II shows the accuracy of the SSVEP BCI for the turning
left, turning right, and going forward commands. Subject B
shows the best performance, with her accuracy ranging from
96.32% to 100%. Furthermore, Subject A shows good performance, with his accuracy ranging from 86.47% to 90.20%,

963

Fig. 6. Accuracy of the SSVEP BCI over time.

whereas the accuracy of Subject C is around 70%. However,
Subject D has poor performance, with his accuracy ranging
from 33.09% to 72.79%; the accuracy of turning left is even
less than the chance (33.33%), and the accuracy of turning right
is slightly higher than the chance.
It should be noted that the accuracy percentages in Table II
were computed without considering the transitions among the
turning left, turning right, and going forward commands, which
frequently happen during the driving. Moreover, the accuracy
during the phase of transitions among the three commands is
rather important to driving since the transitions occur when the
drivers need to correct the vehicle movement. Thus, we need
to analyze the performance of the BCI during the transitions.
Fig. 6 shows the average accuracy of the SSVEP BCI during
the transitions among the three commands for each subject.
In this figure, the x axis represents the time length of the new
data in 4 s of window data with a step size of 0.5 s, whereas the
y axis represents the accuracy that is gained if the time length
of the new data is the same as that of the x axis. We can see that
the accuracy across the subjects is almost at the level of chance
(33.33%) within 2 s after the transitions start. However, after
2 s, the accuracy of Subjects A and B rises to about 60% by
2.5 s, to more than 75% by 3 s, and to more than 87% by 3.5 s,
whereas the accuracy of Subjects B and C hardly increases
within 3 s and only rises to less than 50% by 3.5 s.
B. Brain-Controlled Simulated Vehicle Performance
We applied the task success rate, the task completion time,
the ratio of task completion time to the nominal time, the
mean lateral error, the obstacle avoidance success rate, the false
stopping rate per kilometer, and the out-of-boundary rate per
kilometer as metrics to evaluate the performance of the braincontrolled simulated vehicles. It should be noted that, except for
the task success rate, all metrics can be computed only when the
vehicle can successfully reach the destination.
The task success rate was calculated as the number of vehicles that successfully reaches the destination over the number
of trials that is required to be finished. Note that, when the time
taken is greater than the timeout limit, which is set to be twice

964

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 15, NO. 3, JUNE 2014

TABLE III
P ERFORMANCE OF THE B RAIN -C ONTROLLED S IMULATED V EHICLE

the nominal time, the trial is stopped and is considered failing to
finish the driving task. The nominal time was calculated as the
length of the centerline of the lane over the maximum speed.
The task completion time was defined as the time taken from
the vehicle being started to reaching the destination. The mean
lateral error was defined as the mean of the lateral error between
the track of the simulated vehicle controlled by the BCI and
the average track of the vehicle controlled by a driver via
limbs in three runs. The false stopping rate per kilometer was
defined as the number of stopping commands that is recognized
per kilometer after the vehicle starts and before it reaches the
destination.
Table III shows the performance of the brain-controlled
simulated vehicle across all participants. In Table III, we can
see that Subject D failed to reach the destinations in all the runs.
Subject A shows good performance in using the EEG signals to
control the simulated vehicle. He not only successfully reached
the destinations in all ten runs but he also had a relatively small
mean lateral error, a high obstacle avoidance success rate, a low
out-of-boundary rate, and no false stopping. Subject B showed
90% task success rate with 94.44% obstacle avoidance rate,
and Subject C had 60% task success rate with 100% obstacle
avoidance rate. However, both Subjects B and C did not have as
good real-time control performance as Subject A. The vehicle
that they controlled showed great lateral error, high out-ofboundary rate, and some false stopping times. Fig. 7 shows
the comparison between the average track of the simulated
vehicle controlled by a driver via limbs and the actual trajectories when the brain-controlled vehicles successfully reach
the destination. The black solid line is the average track of
the vehicle controlled by a driver via limbs in three runs. The
red dashed line is the centerline of the lane. The other lines
are the actual trajectories when the brain-controlled vehicles
successfully reach the destination.
V. D ISCUSSION AND C ONCLUSION
In this paper, we have developed a new SSVEP BCI with
visual stimuli presented on a real vehicle windshield via a

Fig. 7. Comparison of the tracks of the simulated vehicle controlled by the
BCI and a driver’s limbs. (a) Subject A. (b) Subject B. (c) Subject C.

HUD, and we have applied this HUD-based BCI in conjunction
with the alpha rhythm to control a simulated vehicle with a
14-DOF vehicle dynamics model. A real-time brain-controlled
simulated vehicle was developed and evaluated by using four
participants to perform a driving task online, including starting/
stopping the vehicle, lane keeping, avoiding obstacles, and

BI et al.: USING HUD-BASED SSVEPs BCI TO CONTROL SIMULATED VEHICLE

curve negotiation. The results demonstrate the feasibility of
using the human “mind” to control a vehicle at a speed ranging
from 7.2 to 10.8 km/h, at least for some users who have good
performance in the BCI.
However, it is impossible for some users (e.g., Subject D) to
drive the brain-controlled simulated vehicle because the SSVEP
BCIs for these users have rather low accuracy. Some studies
have shown that existing BCI systems cannot work for all users,
which is called by some researchers as the “BCI illiteracy”,
when the accuracy is lower than 70% [22]–[24]. This is because
some users cannot produce the necessary brain activity patterns
for a particular kind of BCI systems. In addition, although some
users, such as Subject B, show high accuracy on the SSVEP
BCI, they have poor performance in using the BCI to control the
simulated vehicle. The main reason for this is that they likely
did not learn how to use the BCI to control the vehicle well. It
should be noted that all the subjects had never participated in
any experiment on brain-controlled simulated vehicles before
the formal experiment. As they drive a vehicle using their limbs,
more practice should be helpful to improve the performance of
the brain-controlled vehicles.
The work that has been reported in this paper moves a
step toward developing a brain-controlled vehicle by using a
BCI system to select a destination and by using another BCI
system to issue a motion control command. However, several
challenges need to be addressed, and they open future research
opportunities along this direction.
First, the maximum speed of the brain-controlled vehicle is
only about 11 km/h, which is too low for a vehicle. In practice,
it is desired that the brain-controlled vehicle can travel at a
30–40 km/h speed. Higher speeds require better performance
of the BCI (i.e., higher accuracy and shorter time of issuing a
command), particularly during transitions among commands,
which can be likely dealt with by some new classification
methods or by selecting new features that can represent the
change of EEG signals during the phase of transitions.
Second, the proposed BCI system cannot currently perform
the speed control of brain-controlled simulated vehicles. This
problem can be likely handled by adding two additional SSVEP
stimuli that represent accelerating and decelerating, respectively. Another possible solution is to develop a hybrid BCI,
which combines the SSVEP BCI and another BCI, and to apply
two independent EEG signals to control the speed and direction
of the vehicle, respectively.
Third, under the constraints of the limited and unstable performance of all existing BCI systems, finding ways to enhance
and ensure the overall driving performance is very important.
Future potential directions include developing methods to combine the information from the BCI and vehicle intelligence
(such as surrounding sensing and path planning), and fusing
additional useful information from other sources, such as predicted driver intentions.
Fourth, we have conducted the experiment in a laboratory,
where the environmental factors were constant. However, in
practice, vehicles are used outdoors, and increased noise can
be expected. Real-world driving environments may have significant effects on users’ psychological states, which may affect
the EEG signals and thus affect the BCI system performance.

965

Our future work aims to address the aforementioned issues,
including testing the proposed system in a real vehicle in the
testing field, improving the performance of the BCI, particularly during the stage of transitions among commands, and
finding ways to improve the overall driving performance of
brain-controlled vehicles by combining the BCI with other
intelligent techniques and other information resources.
ACKNOWLEDGMENT
The authors would like to thank the volunteers for their
participation in the experiments.
R EFERENCES
[1] L. Bi, X. Fan, and Y. Liu, “EEG-based brain-controlled mobile robots:
A survey,” IEEE Trans. Human Mach. Syst., vol. 43, no. 2, pp. 161–176,
Mar. 2013.
[2] J. R. Wolpaw, D. J. McFarland, G. W. Neat, and C. A. Forneris, “An EEGbased brain-computer Interface for cursor control,” Electroencephalogr.
Clin. Neurophysiol., vol. 78, no. 3, pp. 252–259, Mar. 1991.
[3] B. Z. Allison, C. Brunner, C. Altstatter, I. C. Wagner, S. Grissmann,
and C. Neuper, “A hybrid ERD/SSVEP BCI for continuous simultaneous
two dimensional cursor control,” J. Neurosci. Methods, vol. 209, no. 2,
pp. 299–307, Aug. 2012.
[4] N. Birbaumer, N. Ghanayim, T. Hinterberger, I. Iversen, B. Kotchoubey,
A. Kubler, J. Perelmouter, E. Taub, and H. Flor, “A spelling device for the
paralyzed,” Nature, vol. 398, no. 6725, pp. 297–298, Mar. 1999.
[5] B. Hong, F. Guo, T. Liu, X. Gao, and S. Gao, “N200-speller using
motion-onset visual response,” Clin. Neurophysiol., vol. 120, no. 9,
pp. 1658–1666, Sep. 2009.
[6] M. Bensch, A. A. Karim, J. Mellinger, T. Hinterberger,
M. Tangermann, M. Bogdan, W. Rosenstiel, and B. N. Nessi, “An
EEG controlled web browser for severely paralyzed patients,” Comput.
Intell. Neurosci., vol. 2007, pp. 71863-1–71863-5, Apr. 2007.
[7] A. A. Karim, T. Hinterberger, and J. Richter, “Neural Internet: Web surfing
with brain potentials for the completely paralyzed,” Neurorehabil. Neural
Repair, vol. 20, no. 4, pp. 508–515, Dec. 2006.
[8] E. Mugler, M. Bensch, S. Halder, M. W. Rosenstiel, N. Bogdan, and
A. Birbaumer, “Control of an Internet browser using the P300 eventrelated potential,” Int. J. Bioelectromagn., vol. 10, no. 1, pp. 56–63, 2008.
[9] R. Krepki, B. Blankertz, G. Curio, and K. R. Müller, “The Berlin
Brain–Computer Interface (BBCI): Towards a new communication channel for online control in gaming applications,” J. Multimedia Tools Appl.,
vol. 33, no. 1, pp. 73–90, Apr. 2007.
[10] M. Tangermann, M. Krauledat, K. Grzeska, M. Sagebaum, C. Vidaurre,
B. Blankertz, and K. R. Müller, “Playing pinball with non-invasive BCI,”
in Proc. 22nd Annu. Conf. Neural Inf. Process. Syst., Vancouver, Canada,
2008, pp. 1641–1648.
[11] B. Rebsamen, C. Guan, H. Zhang, C. Wang, C. Teo, M. H. Ang, and
E. Burdet, “A brain controlled wheelchair to navigate in familiar environments,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 18, no. 6, pp. 590–
598, Dec. 2010.
[12] I. Iturrate, J. M. Antelis, A. Kubler, and J. Minguez, “A noninvasive brainactuated wheelchair based on a P300 neurophysiological protocol and
automated navigation,” IEEE Trans. Robot., vol. 25, no. 3, pp. 614–627,
Jun. 2009.
[13] C. Escolano, J. M. Antelis, and J. Minguez, “A telepresence mobile robot
controlled with a noninvasive brain–computer interface,” IEEE Trans.
Syst., Man, Cybern. B, Cybern., vol. 42, no. 3, pp. 793–804, Jun. 2012.
[14] L. A. Farwell and E. Donchin, “Talking off the top of your head: Toward
a mental prosthesis utilizing event related brain potentials,” Clin. Neurophysiol., vol. 70, no. 6, pp. 510–523, Dec. 1988.
[15] E. E. Sutter, “The brain response interface: Communication through
visually-induced electrical brain response,” J. Microcomput. Appl.,
vol. 15, no. 1, pp. 31–45, Jan. 1992.
[16] G. Pfurtscheller and F. H. Lopes da Silva, “Event-related EEG/MEG
synchronization and desynchronization: basic principles,” Clin. Neurophysiol., vol. 110, no. 11, pp. 1842–1857, Nov. 1999.
[17] D. Hood, D. Joseph, A. Rakotonirainy, S. Sridharan, and C. Fookes, “Use
of brain computer interface to drive: Preliminary results,” in Proc. 4th
Int. Conf. Autom. User Interfaces Interactive Veh. Appl., Portsmouth, NH,
USA, Oct. 2012, pp. 103–106.

966

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 15, NO. 3, JUNE 2014

[18] D. Gohring, D. Latotzky, M. Wang, and R. Rojas, “Semi-autonomous
car control using brain computer interfaces,” in Advances in Intelligent
Systems and Computing. Berlin, Germany: Springer-Verlag, 2013,
pp. 393–408.
[19] [Online]. Available: http://singularityhub.com/2012/09/27/californiabecomes-third-state-to-legalize-driverless-cars/
[20] L. Bi, X. Fan, N. Luo, K. Jie, Y. Li, and Y. Liu, “A head-up display-based
P300 brain-computer interface for destination selection,” IEEE Trans.
Intell. Transp. Syst., vol. 14, no. 4, pp. 1996–2001, 2013.
[21] C. C. Chang and C. J. Lin, LIBSVM—A Library for Support Vector
Machines. [Online]. Available: http://www.csie.ntu.edu.tw/~cjlin/libsvm/
[22] Z. Allison, E. W. Wolpaw, and R. Wolpaw, “Brain-computer interface
systems: Progress and prospects,” Exp. Rev. Med. Dev., vol. 4, no. 4,
pp. 463–474, Jul. 2007.
[23] B. Blankertz, F. Losch, M. Krauledat, G. Dornhege, G. Curio, and
K.-R. Muller, “The Berlin brain–computer interface: Accurate performance from first-session in BCI-naive subjects,” IEEE Trans. Biomed.
Eng., vol. 55, no. 10, pp. 2452–2462, Oct. 2008.
[24] C. Brunner, B. Z. Allison, D. J. Krusienski, V. Kaiser, G. R. Muller-Putz,
G. Pfurtscheller, and C. Neuper, “Improved signal processing approaches
in an offline simulation of a hybrid brain-computer interface,” J. Neurosci.
Methods, vol. 188, no. 1, pp. 165–173, Apr. 2010.

Luzheng Bi (M’08) received the Ph.D. degree in
mechanical engineering from Beijing Institute of
Technology, Beijing, China, in 2004.
He was a Visiting Scholar with the Department
of Industrial and Operations Engineering, College
of Engineering, University of Michigan, Ann Arbor,
MI, USA. He is currently an Associate Professor with the School of Mechanical Engineering,
Beijing Institute of Technology. He is the author
of refereed journal articles in the IEEE T RANSAC TIONS ON I NTELLIGENT T RANSPORTATION S YS TEMS , the IEEE T RANSACTIONS ON S YSTEM , M AN , AND C YBERNETICS ,
the International Journal of Human–Computer Interaction, and other journals. His research interests include intelligent human–vehicle systems, braincontrolled robots and vehicles, and driver behavior modeling and driving safety.
Dr. Bi has been a Reviewer for the IEEE T RANSACTIONS ON I NTELLI GENT T RANSPORTATION S YSTEMS , the IEEE T RANSACTIONS ON S YSTEM ,
M AN , AND C YBERNETICS, and the IEEE T RANSACTIONS ON V EHICULAR
T ECHNOLOGY.

Xin-an Fan received the M.S. degree in mechanical
engineering from Beijing Institute of Technology,
Beijing, China, in 2010. He is currently working
toward the Ph.D. degree in the School of Mechanical
Engineering, Beijing Institute of Technology.
He is the author of several refereed articles in the
IEEE T RANSACTIONS ON I NTELLIGENT T RANS PORTATION S YSTEMS and the International Journal
of Human–Computer Interaction. His research interests include intelligent human–vehicle systems and
brain-controlled robots.
Mr. Fan received the Outstanding Master’s Thesis Award from Beijing
Institute of Technology in 2010.

Ke Jie received the B.E. degree in mechanical
engineering from Beijing Institute of Technology,
Beijing, China, in 2011. He is currently working toward the M.Eng. degree in the School of Mechanical
Engineering, Beijing Institute of Technology.
His research interests include brain–computer
interface and brain-controlled robots.

Teng Teng received the M.Eng. degree in mechanical engineering from Tianjin Polytechnic University,
Tianjin, China, in 2013. He is currently working
toward the Ph.D. degree in the School of Mechanical Engineering, Beijing Institute of Technology,
Beijing, China.
His research interests include brain–computer
interface and brain-controlled robots.

Hongsheng Ding received the university diploma in
mechanical engineering from the Beijing Institute of
Technology, in 1977.
He is currently a Professor with the School of
Mechanical Engineering and the Dean of the National Engineering Training Center, Beijing Institute
of Technology, Beijing, China. He is the author or
coauthor of more than 70 papers. His research interests include mechanical design, mobile robots, and
engineering practice education.
Prof. Ding is a member of the standing committee
of the mechanical design branch of the Chinese Mechanical Engineering
Society.

Yili Liu (S’90–M’91) received the M.S. degree in
computer science and the Ph.D. degree in engineering psychology from the University of Illinois at
Urbana–Champaign, Champaign, IL, USA.
He is currently an Arthur F. Thurnau Professor and
a Professor of industrial and operations engineering
with the Department of Industrial and Operations
Engineering, College of Engineering, University of
Michigan, Ann Arbor, MI, USA. He is the author
of numerous refereed journal articles in the IEEE
T RANSACTIONS ON I NTELLIGENT T RANSPORTA TION S YSTEMS , the IEEE T RANSACTIONS ON S YSTEM , M AN , AND C YBERNETICS , the Association for Computing Machinery (ACM) Transactions
on Computer–Human Interaction, Human Factors, Psychological Review,
Ergonomics, and several other journals. He is a coauthor of a human factors
textbook entitled An Introduction to Human Factors Engineering (Upper Saddle River, NJ: Prentice Hall, 1997 and 2003). His research interests include
cognitive ergonomics, human factors, computational cognitive modeling, and
engineering aesthetics.
Dr. Liu is a member of the ACM, Human Factors and Ergonomics Society,
American Psychological Association, and Sigma Xi. He is a recipient of the
University of Michigan Arthur F. Thurnau Professorship Award (selected by
the Provost and approved by the Regents of the University of Michigan), the
College of Engineering Education Excellence Award, the College of Engineering Society of Women Engineers and Society of Minority Engineers Teaching
Excellence Award for two times, and the Alpha Pi Mu Professor of the Year
Award for five times.

