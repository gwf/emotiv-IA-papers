20th Annual IEEE International Conference and Workshops on the Engineering of Computer Based Systems (ECBS)

Design of Virtual Reality based HMIs (Human
Machine Interfaces) of Complex Systems
(ATM-Air Traffic Management, UAVs-Unmanned Aerial Vehicles)!

Francesca De Crescenzio

Giuseppe Frau

Department of Industrial Engineering, DIN
University of Bologna
Bologna, Italy
francesca.decrescenzio@unibo.it

Department of Mechanical and Industrial Engineering
University of Roma 3
Rome, Italy
giuseppe.frau@uniroma3.it
applied to the design of simulation based prototypes for
interfaces in aeronautics. This method supports the engineering
task of exploring a wide range of technical and technological
alternatives that are available for each functional or physical
block in which the system can be decomposed. The
implementation of such method in two case studies is then
described. Aspects on the major advantages and lesson learned
are then discussed in each section. This approach aims at
providing a structured procedure to define the interfaces at a
system level and different concepts can be derived changing
the blocks contents and the interfaces between the blocks,
when applicable.
Even if the projects described below are recent,
considerable advances have been made since their publications.
Such advances invite to completely reconsider the way to
interact. Therefore, a selected set of new technologies that are
currently being discussed in the research community or are
facing the market is then presented.

Abstract— Aeronautical transport system is rapidly growing
and more demanding. It has become a total system of complex
systems in which the human is recognized as the decisional point
that is asked to act quickly and safely. In this context, innovative
technologies provide the challenge to design revolutionary
Human Machine Interfaces for the people involved.
In this paper a report of recent works in Human Machine
Interfaces in aeronautics developed at the University of Bologna
is presented. Synthetic visualization and task automation are the
main commonalities in these projects. What we can expect from
the future is then explored through an insight into technological
aspects.
Keywords—Human
Synthetic Displays;

Machine

I.

Interface;

Virtual

Reality,

INTRODUCTION

With the introduction of higher levels of automation in the
air transport system we expect radical changes that, as it often
happens, introduce new challenges for the research community.
If such automation will lead us to better global performances of
the system, on the other hand there is a growing concern
regarding the human role in a highly automated system. A
significant change in the kind of tasks performed by the
operator is estimated. The interaction concerns related with this
change are not new in the scientific community and have been
faced since the very first studies on the automation impact
[1][2]. Thus, issues like avoiding the operator to be assigned to
tasks like vigilance/monitoring of the system or to be the
automation backup in case the system fails, or again solving
the issue of the automation transparency (giving the right
information to the operator in order to answer the question
“Why is the system suggesting me/deciding this?”) or the
interaction intuitiveness needed, must be faced before the
whole design process of the system starts.
Therefore, while single interactive technologies, such as
Virtual Reality, Augmented Reality, Brain Computer Interface,
Gestural Interfaces and many others, are being developed, their
efficient integration in interfaces that could support in solving
the above mentioned concerns is still an issue.
In the following paragraphs a method based on the
identification of solutions for each critical sub problem is
978-0-7695-4991-0/13 $26.00 © 2013 IEEE
DOI 10.1109/ECBS.2013.33

II.

V-LAB HMI PROJECTS

In the recent years a number of projects aimed at the design
and development of simulation based prototypes for HMIs in
aeronautics has been realized at the Virtual Reality laboratory
of the University of Bologna. We observed that interfaces
designed for different operators, such an Air Traffic Controller
at work or a ground operator in a UAV Ground Control Station
share fundamental requirements, such as the need of
maintaining the situation awareness while being stressed by the
appropriate level of mental workload [3]. Such needs are the
most important requirements in the design of efficient Human
Machine Interface and make the conceptual phase a very
demanding and multidisciplinary task. Engineers, final users,
human factors experts, cognitive experts are some of the
competencies that should participate in such design
cooperation.
Usually, projects in this field start for different reasons. One
could be to increase competitiveness or to follow an evolving
context or to estimate what will be the best way to act in the
environment that will take place due to new regulations or
trends. Moreover, the availability of new technologies that
could replace the old ones can be experimented for the benefit

181

of the system. Generally, single projects are inspired by a mix
of these motivations.
The systems, that will be described in detail below, have
been conceived following a structured design procedure. In
particular such process is focused onto two main blocks that
build the interface: the display block and the input block
(Fig.1). The method is based on a classification tree that helps
in figuring any combination in the display/input configuration
phase. This phase follows the requirements setting and the task
analysis of a single application.

With Screen Design we intend the visualization technique
(front projection vs. rear projection, stereoscopic display vs.
conventional synthetic, virtual display vs. augmented reality
see-through,…). Virtual Reality applications can be proposed
in several configurations with 3D stereoscopic displays of
different sizes and with different features (single user vs
multiple users). Moreover, Augmented reality deserves
attention in this moment, since portability and intrusiveness of
devices are significantly improving [5][6].
The Input Block contains one the most challenging
activities in interface design, which are the functional task
design and the input suite design. Even if these are presented in
a classification each block need to be accomplished iteratively
with the others.
In the Task Design automation and human-automation
concepts are generally recalled. Activation commands can be
very direct or can be indirect and need to be interpreted by the
machine that is demanded to derive a simple gesture of the
human in a series of low level commands. Interfaces that
implement high level commands are characterised by an high
level of automation and are based on the concept of the
Supervisory Control [7].
The Device Suite section is deserved to the hardware means
to interact with the system. This phase aims at identifying
which means we can use to perform the task once we have the
information provided by the Display and taking into account
the available technologies. Up to now most of the Virtual
Reality applications have suffered of an high level of
intrusiveness. Currently, multimodal interfaces based on the
combination of non intrusive systems seem to be the future [8].
Thus, a detailed knowledge of these and the capability of
combining them in a not cumbersome suite are the key success
factor of an interface designer.

Fig. 1. Main blocks in Human Machine Interface

Going into detail, the display block is composed of the
Data Design and the Screen Design. Each of these sections is
then deployed in order to identify, through a set of selection
criteria, a focus group or, eventually, the involvement of the
final users, the final set of specifications (Fig.2). The Data
Design is a very significant part of the work. Its complexity
depends on the level of abstraction of the simulated application
with regard to a real environment. Automation can enter at this
point with algorithms that are capable of selecting the most
relevant data to be displayed at the right moment and with an
intuitive shape. These characteristics impact on the awareness
of the final user and on his ability to perceive, comprehend and
project any situation in any operative condition [4]. In the new
generation interfaces possible solutions, such as suggested
paths or trajectories, can be presented to the user in the form of
decision support tools, especially when the computational
effort would be unaffordable for the humans.

A. UAV Ground Control Station
The envisaged autonomy and the complex and networked
scenarios are the main reasons for building a project that
considers the role of the human operators in a ground control
station. The main issue in this application is the lack of a
significant amount of data coming from the experience on the
field. This is due to the fact that such vehicles are primary
implemented in military missions and up to know they can’t fly
in the European skies. Therefore, this project proposes the use
of a Virtual Reality based system in order to collect design
features for future UAV Ground Control Stations through a
simulation of missions [9].
The simulation is concentrated to a specific task, which is
the planning of a mission and the replanning of the mission
during its development in case unexpected events occur. The
flow presented in Fig.2 has been applied to this case. A test
session has followed. The situation awareness of the operator
and the appropriate level of automation to be implemented are
the main aspects that are to be observed.
Two displays compose the interface: the command panel
and the 3D large virtual display. Thus there are two Display
Blocks to deploy.

Fig. 2. Deployment of the Display Block and the Input Block

182

Being the vehicle an UAV it is assumed that the planning
task, specifically the identification of an appropriate route in
terms of a list of waypoints, is performed autonomously by the
system and the operator can interact asking for a replanning
(manual mode), choosing among a set of replanning strategies
proposed by the system (semiautomatic mode), or observing
the automatic replanning of the vehicle (automatic mode).
The prototype has been tested by simulating several
missions and to collect users’ impressions and judgments about
the interface and about both the level of situation awareness
and of the workload required of the operator during the
supervision of UAV missions.
The tests were performed with student pilots as testers. The
most important test result was that the semiautomatic activation
of the replanning command allowed the operator to have the
highest level of situation awareness and an acceptable level of
workload during the supervision of the mission. This is because
it guarantees an active role of the operator during the mission
and also cooperation between the operator and the vehicle.
The planning algorithm has been investigated as a major
component of the automation [10]. The algorithm aims at
increasing the autonomy of the vehicle by handling high level
commands received from the operator, as well as deciding how
to replan the mission when unforeseen situations occur. It is
based on a graph, but differs from the others because it allows
the exploration of more portions of space, replicating the
approach of human pilots, and enables the vehicle to avoid
obstacles of any shape while requiring a short computation
time.

Fig. 3. Display Design for the UAV Ground Control Station

The command panel allows the operator to interact with the
system providing a map view of the scenario and a general user
interface to set up and activate the commands through a touch
screen. The large display is a stereoscopic screen that
visualizes an exocentric or optionally an egocentric view of the
scenario in which the vehicle operates, augmented with 3D
relevant flight parameters. The command panel is a single user
interface, while the 3D large virtual display is based on a
theatre-like configuration also for multiple users.
The operator gives inputs through the command panel
(Fig.4). It depicts a map and a command key panel. The
command panel has been conceived to enable the operator to
input high level commands.

B. Systems for ATM (Air Traffic Management)
Collaborative virtual environments have been proposed for
applications such as training or support especially in
operational critical environments such as medical, military,
emergency, and Air Traffic Control. The design of innovative
and appropriate HCIs in these application fields represents,
nowadays, a very important issue. Common displays are
designed and optimized to allow visualization and interaction
of a single user. So these are not suitable for co-located
multiple users which need to share and interact with the same
data at the same time.

Fig. 4. Input design for the UAV Ground Control Station

Each command is composed of three functions: “what task
to accomplish,” “where to execute it,” and “how to perform it,”
and can be activated defining the following parameters:
1. Mission Task: Fly to a destination point, survey a
target, and monitor a set of locations.
2. Target: A destination point, an extended area, or
the center of a target area.
3. Priority: The priority indicates the way the task
has to be accomplished. The vehicle can follow the
shortest or the fastest or the most fuel efficient
paths, among a set of candidate.

Fig. 5. TABO (TAbletop at uniBO)

183

could provide a valid alternative, will be discussed in the next
session.
III.

INNOVATIVE TECHNOLOGIES

We have considered the interface as a combination of a
display that gives to the human information from the machine
and a platform by which the humans gives inputs to the
machine (Fig.7). We can observe that the full set of means that
we have to give inputs comprises the body, both for direct and
emotional expressions, the eyes, the voice and, as the ultimate
source of information coming from our intentions, the brain.
On the display side we have the visual, the audio and the
tactical senses. A natural interaction philosophy should take
into account all these sources. Nevertheless, up to now with the
machines we do not use the emotional and gestural expressions
that we use with the other humans.

Fig. 6. Display Block and the Input Block for Air Traffic Control application
of TABO

A system called TABO (TAbletop at uniBO) has been
designed and developed in the Virtual Reality Lab in order to
study horizontal displays in Virtual Reality (Fig.5). One of the
applications developed by means of TABO is based on the
visualization of ATC (Air Traffic Control) scenarios (Fig.6).
On the Display side, in the Data Block a photorealistic
virtual environment is populated with virtual aircrafts animated
by different data sources. A traffic is replicated by means of a
set of recorded flights while an other aircraft, the core vehicle,
is animated by parameters data representing sent from a flight
simulator being flown by a pilot at the same time a group of
ATC trainers and trainees are at the TABO.
For what concerns the Screen Design parameters, TABO is a
low cost, passive stereo, rear projected visualization device
conceived to support the analysis of complex simulated
environments [11]. In the case of the ATC visualization, the
use of stereoscopy enhances the perception of depth, giving to
the user more awareness about the vertical separation of
airplanes The horizontal lay-out of the projection screen is
suitable for the visualization of synthetic environments in that
fields in which the top is considered as a natural point of view
(God’s eye view). On the other side, the projection area size
provides a multi-user co-located visualization, assuming the
slight difference between the single users’ view due to the fact
that we have not implemented an head tracking system.
On the input side it has been observed that there are three
main input functions required by the TABO operators. The first
one is the communication with the pilot in the Flight Simulator.
This is performed via conventional devices as a microphone
and a telecommunication link. The second function is needed
when the ATC application of TABO is run off line animating
the core aircraft with data recorded during the simulated flight.
In the off line mode the users can perform a debriefing of the
mission on the TABO in order to better comprehend what
happened moment by moment. Thus a timeline that allows
functions such as play, forward, rewind, fast forward and fast
rewind is required. The last function is the navigation of the
virtual scenario in terms of rotation of the point of view and
zoom. These latter are currently activated on a desktop
interface placed aside the TABO. This means that the system is
not really interactive. This is due to the fact that a single device
or a set of single devices that do not result in an intrusive or
cumbersome system had not been identified at the time of the
project development. Wireless or hand free technologies, that

Fig. 7. Human senses and means to interact

Since new technologies are facing the market future
HMIs will probably overcome such limitations. The single
technologies recently succeed one another very quickly but the
last 6-7 years has been one of the most prolific periods in the
field of enabler technologies for new interaction styles. Surely
some of them already exist several years before in some
experimental lab but we can affirm that their TRL (Technology
Readiness Level) is now sufficient enough to allow a deeper
study about their usage. We still design bad user interfaces for
the mouse-and-keyboard devices and that technology has been
around for more than 30 years.
The Leap (www.leapmotion.com) is a little controller device
able to track hands and fingers of the user with a very high
precision in the 3D space (1/100th of a millimetre). All the

184

For what concerns the design of innovative interfaces in
aeronautics we can observe that major initiatives in air
transport, such as Sesar and NextGen, are providing the vision
for the short and long term evolution of the system [14][15].
According to these programmes, the design of future interfaces
must be accomplished aiming at the achievement of excellent
performances such as the Safety, the Efficiency, the
Sustainability and the Economy.
One of the hard duties of the research in this field will
consist in finding a fine tuned multi modal interaction that can
really help the user in the accomplishment of the task in the
best way contributing at the achievement of such tasks. Thus, it
is necessary to proof that the usage of those kind of interactions
can have real benefits on the tasks performances and on safety:
literature studies suggest that the introduction of automation
may have side effects on the operator’s situation awareness or
it can relegate him/her to unsuitable roles like the monitoring
one. Those side effects can be mitigated keeping the user
engaged in the decision process, which implies a sufficient
level of interaction with the system. Our hypothesis is that
natural interactions enabled by new interaction technologies
should lead the operator to be more involved in the process at
an higher level of abstraction. On the other hand the more
intuitive are the interfaces, the more economic and efficient
should be the training of people and their flexibility to
innovation. This fact should impact on the economy and
efficiency of the overall system. On the safety side, it is worth
to note that the machine can receive from the human not only
direct inputs but also information about his level of attention.
For example, the attention monitoring by means of BCIs or eye
tracking could be a powerful tool for the reduction human
factors issues of pilots in the cockpit.

sensing technology and gesture recognition software run
inside the device, which is equipped with a dedicated CPU and
a set of LEDS and sensors for the motion capture. The Leap is
now under testing by a selected but big group of developers
and will be delivered in the first part of 2013 together with a
rich set of APIs.
Gestures are also enabled, with a different approach, by
XTR3D (www.xtr3d.com.). Extreme Reality Ltd has
developed a cross platform software that turns any 2D
webcam in a full body motion capture device. In this case all
the calculations are performed by the user’s computer and the
bigger captured volume can generate high demanding
operations.
Switching to a different kind of interaction, The Eye Tribe
(www.theeyetribe.com) and Tobii (www.tobii.com) have both
worked to build devices for the gaze interaction. The Eye Tribe
have focused on mobile devices and it allows users to activate
the screen, perform scrolling actions or even play with certain
games. The user’s sight is captured with a front camera and the
images are analysed by computer-vision algorithms. Tobii,
instead has a more general approach and it appears to have a
higher Technology Readiness Level.
On the visualization side, while Computer Graphics is
always more performing, new devices seem to open the way to
Augmented Reality. After a decade of experimental
applications there are now less intrusive wearable technologies
that can favour its spreading in several applications.
An example of wearable projection technology are the
Vuzix Smart Glasses (www.vuzix.com). They can be seen as
the major competitor of the more famous Google Glass
project (https://plus.google.com/+projectglass/). Both these
devices in fact offer the possibility to receive information
through a display mounted on a pair of glasses. In the Vuzix
case, however, the display doesn’t interact with the context and
it lives in its own ecosystem. The Google glass concept instead
is supposed to communicate with the smartphone of the users,
recognize what the user is looking at in that particular moment
and help him/her to find more information about it. Google is
expected to ship the first units of its device in the first part of
2013 to a restricted set of developers.
Except for the wearable technologies, most of the times
those new devices are designed as if they are fated to be the
only mean of interaction with the machines. However, users
would probably get their arms easily tired if they were forced
to interact only with free hand gestures. Also gaze interaction
can create fatigue in a medium time usage.
On the BCI (Brain Computer Interface) side we have
invasive and non invasive systems. One of the most promising
and lean system is the EMOTIV EPOC, a portable EEG
system based on a recording of the electrical activity of the
brain directly from the head produced by neurons activation
within the brain (wwwemotiv.com). One suite uses the signals
measured by the neuroheadset to interpret player facial
expressions in real-time. The Affectiv suite monitors player
emotional states in real-time. The Cognitiv suite reads and
interprets a user's conscious thoughts and intent.
Tactile devices generally appeal to the cutaneous senses by
skin indentation, vibration, skin stretch or electrical and
thermal stimulation [13].

IV.

DISCUSSION

It is evident that good design practices for future Human
Machine Interfaces are needed, especially in complex
environments, such as the air transport world. Even if the
technological advances presented in the last section are only a
part of the state of the art in this domain, there are several
considerations that can be done. We are moving toward a more
automated world and the interface with machine and computers
are leading to a more natural way to interact. Interfaces will
have to be extremely flexible to face changes in missions,
technologies and humans. Simulation will help to measure the
capability of interfaces of meeting the specific application
requirements.
Moreover, it is important to remind the importance of the
evaluation of the interfaces from the cognitive point of view.
Especially in this System of Systems context any prototype
must be assessed using empirical methods in an experimental
setting. That should be accomplished in an environment of
significant ecological validity so that the probability of being
able to generalize the findings is maximized [16].
REFERENCES
[1]

185

N.B. Sarter, D.D. Woods, and C.E. Billings, “Automation
Surprises”, Handbook of Human Factors & Ergonomics, second
edition, G. Salvendy (Ed.), Wiley, 1997.

[2]
[3]
[4]
[5]

[6]
[7]
[8]
[9]

[10]

L. Bainbridge, “Ironies of automation”, Automatica, 19(6),
1993, pp. 775–779, doi:10.1016/0005-1098(83)90046-8.
C.D. Wickens, J.D. Lee, Y. Liu, S.G. Becker, An Introduction to
Human Factors Engineering, second edition, Pearson, Prentice
Hall, Upper Saddle River, New Jersey 07458, 2004.
M.R. Endsley, “Automation and Situation Awareness”,
Automation and human performance: Theory and applications,
Mahwah, NJ: Lawrence Erlbaum, 1996, pp.163-181.
H. Benko, R. Jota, “MirageTable: freehand interaction on a
projected augmented reality tabletop”, Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems,
2012, pp.199-208.
M. Gervautz, D Schmalstieg, “Anywhere Interfaces Using
Handheld Augmented Reality”, Computer, 45 (7), 2012, pp.2631.
T.B. Sheridan, Humans and Automation, a John Wiley & Son,
Inc., Santa Monica, Publications 2002.
A. Jaimes, N. Sebe, “Multimodal human–computer interaction:
A survey”, Computer Vision and Image Understanding 108,
2007, pp.116–134.
F. De Crescenzio, G. Miranda, F. Persiani, T. Bombardi, “A first
implementation of an advanced 3D Interface to control and
supervise UAV (Uninhabited Aerial Vehicles) missions”
Presence, teleoperators and Virtual Environments. vol. 18, n°3,
2009, pp. 171 – 184, doi:10.1162/pres.18.3.171.
F. Persiani , F. De Crescenzio ,G. Miranda , T. Bombardi M.
Fabbri , F. Boscolo, “3D Obstacle Avoidance Strategies for

[11]

[12]

[13]
[14]
[15]
[16]

186

UASs Mission Planning and Re-planning”, Journal of Aircraft.
vol. 46 issue 3, 2009, pp. 832 – 846, doi: 10.2514/1.37244.
F. Persiani, F. De Crescenzio, M. Fantini, S. Bagassi, “A
Tabletop-Based Interface to Simulate Air Traffic Control in a
Distributed Virtual Environment”, Atti del XVI Congresso
ADM e XIX Congreso Ingegraf. PERUGIA. 6-8th June 2007.
(pp. RV8). ISBN: 978-884671842-6. PI: ETS (ITALY).
F. De Crescenzio, S. Bagassi, M. Fantini, F. Lucchi, Virtual
reality based HUD (Head Up Display) to simulate 3D conformal
symbols in the design of future cockpits. Proceedings of the 3rd
CEAS (Council of the European Aerospace Societies),
Air&Space Conference, 21st AIDAA Congress, Venezia -Italy.
24/28th october, 2011. (pp. 1716 - 1721). ISBN: 978-88-9642718-7. : (ITALY).
V.G. Chouvardas, A.N. Miliou, M.K. Hatalis, “Tactile displays:
Overview and recent advances”, Displays, 29, 2008, pp.185–
194, doi:10.1016/j.displa.2007.07.003.
European ATM Master Plan 2009, www.sesarju.eu.
NextGen Implementation Plan 2012, www.faa.gov.
S.E. Martin, E.M. Biddle, “Human Engineering for Crew
Performance in High Stress Environments: Balancing
Automation, Cognitive Workload and Operator Authority”, The
role of Human-Machine Interface in airspace applications and
the enabling technologies, Forlì, Italy, September 2012.

