PREMOC: Plataforma de reconocimiento multimodal
de emociones
Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval
Instituto Tecnol√≥gico de Culiac√°n, Culiac√°n, Sinaloa,
M√©xico
{rzaratain, lbarron, m14170101}@itculiacan.edu.mx

Resumen. En a√±os recientes la computaci√≥n afectiva ha venido a mejorar la
interacci√≥n humano-computadora, pues ayuda a la computadora a conocer el
estado afectivo del usuario para mejorar la toma de decisiones. Este art√≠culo
presenta los avances en el proyecto PREMOC, una plataforma que brinda un
servicio web para el reconocimiento de emociones en texto, im√°genes de rostros,
sonidos de voz y se√±ales EEG de manera mono-modal y multimodal. PREMOC
ayuda a los desarrolladores a integrar el reconocimiento de afecto a sus
aplicaciones o sistemas de software. Cada uno de los reconocedores se
implement√≥ aplicando diferentes t√©cnicas tanto para extraer caracter√≠sticas como
para clasificar emociones; adem√°s para el reconocimiento multimodal se
integraron las emociones mediante un sistema difuso. Esta plataforma ya est√°
siendo utilizada por diferentes proyectos en el laboratorio de la Maestr√≠a en
Ciencias de la Computaci√≥n del Instituto Tecnol√≥gico de Culiac√°n.
Palabras claves: Computaci√≥n afectiva, inteligencia artificial, reconocimiento
de emociones.

PREMOC: Multimodal Emotion Recognition Platform

Abstract. In recent years, affective computing has been improving humancomputer interaction, because it helps computers to know the emotional state of
the user to improve the decision-making process. This paper presents the advance
in the PREMOC project, a platform that provides a web service for emotion
recognition in text, facial images, speech and EEG signals in mono and
multimodal mode. PREMOC assists developers to incorporate emotion
recognition on their applications. Each recognizer was implemented with
different techniques for feature extraction and emotion classification; for
multimodal recognition, emotions were integrated through a fuzzy system. This
platform is being used on different projects in the Computer Science Laboratory
of Instituto Tecnol√≥gico de Culiac√°n.
Keywords: Affective computing, artificial intelligence, emotion recognition.

pp. 97‚Äì110; rec. 2016-03-15; acc. 2016-05-21

97

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

1.

Introducci√≥n

Las computadoras est√°n adquiriendo desde hace varios a√±os la capacidad de detectar
la emoci√≥n de sus usuarios con el uso de diferentes sensores y dispositivos. Esto facilita
la interacci√≥n entre el usuario y la computadora para crear una l√≠nea de comunicaci√≥n
de dos canales, con el cual la computadora obtiene informaci√≥n adicional del usuario.
A esta nueva √°rea del conocimiento se le nombra com√∫nmente como Computaci√≥n
Afectiva [1]. Hoy en d√≠a se han desarrollado una gran cantidad de proyectos y
aplicaciones en diferentes √°mbitos como la mercadotecnia, la medicina, la educaci√≥n,
entre otras.
Uno de los dispositivos m√°s usados para detectar emociones para la computaci√≥n
afectiva es la c√°mara, pues seg√∫n Mehrabian [2] los gestos faciales transmiten el 58%
de la comunicaci√≥n humana. Otra raz√≥n por la cual es la m√°s usada es la existencia de
caracter√≠sticas universales en los rostros que reflejan ciertas emociones sin importar la
cultura facial de la persona; estas emociones son conocidas como las emociones de
Ekman [3].
Otro dispositivo utilizado en la computaci√≥n afectiva es el micr√≥fono, pues la
evaluaci√≥n de la voz humana tambi√©n ha sido utilizada para definir emociones como
por ejemplo en Kun Han [4], donde al igual que muchos trabajos relacionados con el
sonido, dividen el audio en peque√±os segmentos y con ellos entrenan una red neuronal.
En este m√©todo existen dos variables muy importantes, la intensidad y la valencia.
Tambi√©n se puede detectar emociones a partir del texto o documentos, ya que con
el uso del internet, redes sociales y blogs, la cantidad enorme de informaci√≥n en estos
medios abre una ventana muy grande para encontrar nuevos elementos dentro de estos
datos, como es en este caso las emociones o afecto. Existen distintos m√©todos en
diferentes idiomas de como reconocer emociones en el texto. Uno de los m√©todos m√°s
utilizados es el de la t√©cnica keywords [5], que consiste en tener una base de datos
previamente clasificada que contiene palabras etiquetadas con una emoci√≥n y un grado
de pertenencia.
Un dispositivo m√°s que podemos encontrar en la literatura es la diadema que emite
se√±ales electroencefalogr√°ficas (EEG). Este tipo de dispositivos puede ser muy efectivo
para encontrar se√±ales en tiempo real como se hace en Liu [6] donde se identific√≥ las
regiones del cerebro m√°s importantes para interpretar emociones en un usuario.
Cada uno de estos dispositivos alcanza una tasa de asertividad de la emoci√≥n
dependiendo de su implementaci√≥n. Hay trabajos que combinan dos o m√°s dispositivos
para mejorar esa tasa de asertividad o para considerar diferentes aspectos del usuario
antes de determinar su emoci√≥n. A la t√©cnica donde se usan dos o m√°s dispositivos o
sensores para reconocer emociones, se le nombra reconocedor multimodal de
emociones o afecto.
PREMOC ofrece cuatro reconocedores de emociones en forma de servicio web, es
decir, cualquier aplicaci√≥n cliente puede consultar al servicio web ofrecido por
PREMOC y obtener una emoci√≥n de una imagen de un rostro, de un texto, de un sonido
de voz o de un archivo de se√±ales electroencefalogr√°ficas. Una de las ventajas de
PREMOC es su modo multimodal, donde el usuario puede enviar en la misma consulta,
dos o m√°s diferentes tipos de archivos para obtener una emoci√≥n o cada una por
separado. Otra ventaja es su gran flexibilidad y facilidad para ser usado, lo cual se da
por medio de un servicio Web. Finalmente, otra ventaja para usar esta herramienta es
Research in Computing Science 111 (2016)

98

PREMOC: Plataforma de reconocimiento multimodal de emociones

la identificaci√≥n de emociones en texto y voz para el lenguaje Espa√±ol, lo cual no existe
en otros reconocedores multimodales de emociones, en el estudio que llevamos a cabo.
Este articulo est√° organizado de la siguiente manera: en el cap√≠tulo 2 se describen
trabajos relacionados a PREMOC, tanto reconocedores individuales, como
multimodales y algunos ejemplos que ofrecen el servicio en l√≠nea. En el cap√≠tulo 3 se
describe cada uno de los reconocedores de manera individual, as√≠ como tambi√©n se
habla de los m√©todos que tiene PREMOC para la integraci√≥n multimodal de emociones
y la estructura de la plataforma. En el cap√≠tulo 4 se mencionan los experimentos y en el
cap√≠tulo 5 las conclusiones y trabajo futuro.

2.

Trabajos relacionados

Existen actualmente muchos trabajos relacionados a la obtenci√≥n de emociones
tanto monomodales como multimodales, y muchos de ellos trabajan con diferentes
m√©todos de extracci√≥n y clasificaci√≥n de caracter√≠sticas.
Entre los que detectan emociones por medio del texto, Seol [5] utiliza el algoritmo
keywords para extraer caracter√≠sticas de las palabras y una red neuronal artificial basada
en conocimiento (KBANN por sus siglas en ingl√©s) para clasificarlas y obtener una
emoci√≥n. As√≠ como Shaheen [7] extrae las caracter√≠sticas del texto mediante reglas de
reconocimiento de emociones y utiliza k-vecinos cercanos (KNN por sus siglas en
ingl√©s) para la clasificaci√≥n de la emoci√≥n. Tambi√©n el m√©todo Random Forest fue
usado por Zheng [8] para clasificaci√≥n de emociones en el texto.
Por medio del rostro, Khandait [9] y Gangwar [10] utilizan una red neuronal para
la clasificaci√≥n de las caracter√≠sticas y obtenci√≥n de emociones. Para la extracci√≥n de
caracter√≠sticas utilizan SUSAN edge detection y Hue-Saturation-Value
respectivamente. Mientras que De [11] usa distancia euclidiana para obtener la emoci√≥n
y 2D Discrete Cosine Tranform para la extracci√≥n de caracter√≠sticas.
En los trabajos relacionados a las emociones por medio de voz, Kostoulas [12]
utiliza el algoritmo C4.5 de la librer√≠a Weka para la clasificaci√≥n de la emoci√≥n. Vogt
[13] con EmoVoice utiliza una m√°quina de soporte vectorial (SVM por sus siglas en
ingl√©s) por la rapidez para hacerlo en tiempo real, ya que EmoVoice es una aplicaci√≥n
en tiempo real. Caballero-Morales [14] utiliza Modelos Ocultos de Markov para la
clasificaci√≥n de la emoci√≥n (HMM por sus siglas en ingl√©s) en su trabajo para el idioma
espa√±ol. Los 3 trabajos presentados utilizan Mel-Frequency Cepstral Coefficent
(MFCC) como m√©todo de extracci√≥n de caracter√≠sticas.
En los trabajos donde se obtiene la emoci√≥n de una persona por medio de se√±ales
EEG los m√©todos son m√°s variados. Por ejemplo, Liu [6] utiliza directamente los datos
ofrecidos por Emotive Software Development Kit para la extracci√≥n de caracter√≠sticas
y hace la clasificaci√≥n de las emociones con un Modelo de Dimensi√≥n Fractal. Nie [15]
obtiene las caracter√≠sticas EEG por medio de la Transformada R√°pida de Fourier (FFT
por sus siglas en ingl√©s) y clasifica las emociones con SVM. Lee [16] utiliza NeuroScan
para la extracci√≥n de caracter√≠sticas en las se√±ales y las clasifica en emociones con
An√°lisis Discriminante Cuadr√°tico.
Hay trabajos bimodales como Alonso-Martin [17] que cre√≥ un sistema para la
detecci√≥n de emociones durante la interacci√≥n Humano-Robot, para que los robots sean
m√°s sociales, y utiliza como par√°metros el rostro y la voz humana. Tambi√©n Busso [18]
99

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

cre√≥ un analizador bimodal de voz y rostros para demostrar que al usar ambos, la
asertividad de la emoci√≥n dada es mejor que en modo individual. El autor utiliz√≥ SVM
para clasificar emociones en el rostro y Praat para la voz. Chuang [19] trabaj√≥ con voz
y texto, y us√≥ SVM para la clasificaci√≥n de emociones en la voz y keywords para el
texto.
Existen tambi√©n sistemas multimodales como el creado por D¬¥Mello[20] para
a√±adirle afecto al famoso sistema tutor inteligente AutoTutor donde adem√°s de utilizar
voz y rostro, tambi√©n tomaron en cuenta el lenguaje corporal. Caridakis [21] tambi√©n
utiliza estos 3 par√°metros para la obtenci√≥n multimodal de emociones y alcanza un
88,2% de asertividad, y clasifica 8 diferentes emociones con redes bayesianas. Tambi√©n
hay investigaci√≥n sobre aprovechar todo lo que se utiliza mientras se trabaja. Por
ejemplo Paredes [22] trabaja en su investigaci√≥n con mouse y teclado adem√°s de la
c√°mara para la emoci√≥n a trav√©s del rostro.
En el trabajo realizado por Wagner [23] se introduce el nombre de Online Emotion
Recognition System (OER) para referirse a los sistemas que ofrecen reconocimiento de
emociones en l√≠nea. Wagner presenta Smart Sensor Integration que es un framework
que ofrece reconocimiento de emociones de manera multimodal para voz, rostros y
movimiento en l√≠nea. Otro framework multimodal es ABE [24] que consiste en una
federaci√≥n de agentes en l√≠nea donde cada agente se encarga de un dispositivo de
entrada. ABE trabaja usando se√±ales del cerebro (EEG), rastreo de ojos, expresiones
faciales, conductividad en la piel, presi√≥n arterial y postura corporal.

3.

PREMOC

PREMOC es una plataforma en l√≠nea que ofrece reconocimiento de emociones en
los dispositivos anteriormente mencionados, lo cual hace a trav√©s de un servicio web
para que los desarrolladores agreguen reconocimiento de emociones a sus aplicaciones.
La comunicaci√≥n entre PREMOC y los usuarios es a trav√©s de formato JSON, en donde
el usuario env√≠a sus datos de entrada por medio de un POST en formato JSON, y recibe
la emoci√≥n como respuesta.
1.1. Reconocedores de emociones
PREMOC tiene definidos cada uno de los reconocedores de forma aut√≥noma, los
cu√°les reciben el archivo con datos de la imagen, voz, texto y/o informaci√≥n cerebral,
lo procesan, y obtienen la emoci√≥n que regresan. Los reconocedores fueron creados por
separado en el laboratorio de la Maestr√≠a en Ciencias de la Computaci√≥n del Instituto
Tecnol√≥gico de Culiac√°n, y algunos ya fueron utilizados en una aplicaci√≥n [26, 27].
1.1.1 Reconocedor de emociones en texto
Para el reconocimiento de emociones en texto, PREMOC utiliza un algoritmo
sem√°ntico denominado ASEM. ASEM es un algoritmo basado en keywords, pues
contiene una colecci√≥n de palabras previamente etiquetadas a una emoci√≥n.
Research in Computing Science 111 (2016)

100

PREMOC: Plataforma de reconocimiento multimodal de emociones

El algoritmo ASEM cuenta con 5 diferentes corpus, con diferentes funciones cada
uno de ellos: stop words, palabras impropias, corpus emocional, new words y corpus
sem√°ntico.
Stop words. Es una colecci√≥n de palabras que no influyen en la emoci√≥n de la
oraci√≥n, como art√≠culos, preposiciones, entre otros. Las palabras que est√°n aqu√≠ son
eliminadas de la oraci√≥n a evaluar.
Palabras impropias. Es una colecci√≥n de, como su nombre lo dice, palabras
impropias, que est√°n calificadas a una emoci√≥n y su factor de probabilidad afectiva a
dicha emoci√≥n (FPA) donde 0 < FPA < 1.
Corpus emocional. Es una colecci√≥n de palabras, que est√°n calificadas a una
emoci√≥n y su FPA a dicha emoci√≥n, donde 0 < PFA < 1.
New words. Aqu√≠ se coleccionan las palabras nuevas que no se tienen en el corpus
emocional. Esto es para el mantenimiento del algoritmo ya que las nuevas palabras
encontradas aqu√≠ ser√°n calificadas cada cierto tiempo por un experto. El experto
determinar√° a que emoci√≥n pertenece esa palabra y su FPA.
Corpus sem√°ntico. Es una colecci√≥n de palabras que potencializa o inhibe el valor
FPA sem√°nticamente. Por ejemplo, la palabra muy potencializa el valor FPA de la
siguiente palabra.

Fig. 1. Algoritmo ASEM.

Como se muestra en la Figura 1, el algoritmo ASEM funciona de la siguiente
manera:
a) Se recibe un texto de entrada.
b) Se normaliza el texto quitando palabras que no agregan valor emocional usando
el corpus Stop Words.
c) Se busca cada palabra en el corpus emocional para obtener su emoci√≥n y su
FPA. Si no est√°, se busca en el corpus palabras impropias. Si no est√°, se agrega
al corpus new words para que sea m√°s adelante clasificada por un experto. En la
evaluaci√≥n presente, dicha palabra es ignorada para la clasificaci√≥n de la
emoci√≥n en el texto.
d) Se buscan palabras en el corpus sem√°ntico que puedan potencializar o inhibir
los valores FPA ya obtenidos.
101

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

e) Se calcula el valor de cada una de las 5 emociones (feliz, sorprendido, neutral,
triste, enojado) en el texto con la f√≥rmula:
emocion(n) = ‚àë5ùëõ=1(ùêπùëÉùê¥ ‚àó ùëâùëÉùêºùëõ ).
f) Se selecciona la emoci√≥n con el mayor valor obtenido y esa emoci√≥n se regresa
al usuario.
1.1.2.

Reconocedor de emociones en rostro

El reconocedor de expresiones faciales en PREMOC consiste de dos fases:
entrenamiento de una red neuronal y uso del reconocedor.
La fase de entrenamiento consisti√≥ en entrenar la red neuronal encargada de la
clasificaci√≥n de rostros. Para esto se tomaron como datos de entrenamiento las
im√°genes del corpus RaFD [27]. Del corpus se tomaron 955 im√°genes, 191 de cada una
de las 5 emociones con las que se va a trabajar que son feliz, sorpresa, neutral, enojado
y triste. De cada una de las im√°genes se extrajo un vector de 10 caracter√≠sticas, donde
cada una es una distancia euclidiana en diferentes puntos del rostro. Con esto se entren√≥
una red neuronal usando la librer√≠a Weka [28] para Java.
La fase de ejecuci√≥n en l√≠nea funciona de la siguiente manera y como muestra la
Figura 2.

Fig. 2. Algoritmo del reconocedor de emociones en rostros.

a) Recibe una imagen.
b) Detecta el rostro en la imagen con OpenCV.
c) Repite los siguientes pasos para boca, ojo izquierdo, ojo derecho, ceja izquierda,
ceja derecha, lo llamaremos objetivo:
ÔÄ≠ Ubicar dentro del rostro posible zona de ubicaci√≥n del objetivo, se
llamar√° regi√≥n de inter√©s.
ÔÄ≠ Dentro de la regi√≥n de inter√©s aplicar un m√©todo de b√∫squeda en cascada
de OpenCV para buscar en concreto el objetivo que estamos buscando.
ÔÄ≠ Ya con el objetivo encontrado, se le aplican filtros de OpenCV [29] como
se muestra en la Figura 3 para determinar los puntos clave en el rostro.
d) Se crea un vector de caracter√≠sticas de 10 elementos, donde cada uno es una
distancia euclidiana entre diferentes puntos clave ubicados en el rostro.
e) Se ingresa el vector de caracter√≠sticas a la red neuronal ya entrenada para que lo
clasifique a una de las emociones.
Research in Computing Science 111 (2016)

102

PREMOC: Plataforma de reconocimiento multimodal de emociones

f) Regresa la emoci√≥n obtenida.
Este reconocedor regresa los valores de emociones para feliz, triste, enojado, sorpresa y neutral.

Fig. 3. Filtros de OpenCV en la boca.

1.1.3.

Reconocedor de emociones en voz

El reconocedor de emociones en voz que utiliza PREMOC identifica la valencia del
audio recibido (positiva o negativa) y tambi√©n consiste en dos fases: entrenamiento y
ejecuci√≥n.
En la fase de entrenamiento se utilizaron un total de 45 audios de 9 sujetos
diferentes, donde cada audio est√° clasificado en positivo o negativo. De los audios se
extraen las caracter√≠sticas utilizando su espectrograma con la librer√≠a musicg [30] para
obtener los par√°metros para entrenar una SVM de la librer√≠a LibSVM [31]. La instancia
de la SVM ya entrenada es guardada para utilizarla online en la clasificaci√≥n de sonidos
de PREMOC.
La fase de ejecuci√≥n en l√≠nea funciona de la siguiente manera y como se muestra en
la Figura 4.

Fig. 4. Algoritmo del reconocedor en voz.

a) Se recibe un audio en formato wav serializado en Base64.
b) Se obtienen el espectrograma del audio para obtener los valores de tono e
intensidad a lo largo del audio.
c) Se calculan los siguientes valores:
ÔÄ≠ El valor de la intensidad m√°s alta del audio.
ÔÄ≠ El valor de la intensidad m√°s baja del audio.
ÔÄ≠ El valor del tono m√°s alto del audio.
ÔÄ≠ El valor del tono m√°s bajo del audio.
d) Estos 4 valores se introducen a la SVM para obtener la valencia de la voz.
e) Se regresa la valencia obtenida.
Este reconocedor regresa la valencia de la voz (positiva o negativa).
103

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

1.1.4. Reconocedor de emociones en se√±ales EEG
El reconocedor de emociones por medio de se√±ales EEG tambi√©n consta de dos
fases: entrenamiento e implementaci√≥n en l√≠nea. Este reconocedor identifica la valencia
de la emoci√≥n (positivo, negativo o neutral) del archivo recibido de se√±ales EEG. El
reconocedor fue creado y probado para la Interface Cerebro-Computadora (BCI)
Emotiv Epoc [32]. Para seleccionar los canales apropiados que necesitamos del BCI,
se consider√≥ el trabajo de Mahajan [33] que usa los canales frontales AF3, AF4, F3,
F4, F5 Y FC6. Tambi√©n el trabajo de Liu [6] que usa los canales AF3, F4 Y FC6. Al
final se consideraron los canales AF3, F3, F4, FC5 y FC6 porque se concluy√≥ que son
los que m√°s se aproximan a las emociones humanas.
La caracter√≠stica principal extra√≠da de las se√±ales EEG es el Exponente de Hurst, el
cual es usado en an√°lisis de series de tiempo para identificar un comportamiento
inestable de las se√±ales EEG para identificar tendencias en los datos. Se eligi√≥ el
Exponente de Hurst por los buenos resultados mostrados en Wang [34](71.38 %).
La fase de entrenamiento esta descrita en [35]. Y la fase en l√≠nea funciona como
muestra la Figura 5 y como se describe a continuaci√≥n.

Fig. 5. Reconocedor de se√±ales EEG.

a) PREMOC recibir√° un archivo edf con las se√±ales EEG.
b) Se extraer√°n la informaci√≥n solamente de los 5 canales mencionados
anteriormente.
c) Se calcular√° el Exponente de Hurst de cada canal para obtener un vector de
tama√±o 5.
d) Alimentamos la SVM de la librer√≠a LibSVM [31] con el vector obtenido y se
obtendr√° la valencia de la emoci√≥n.
Este reconocedor regresa la valencia de la emoci√≥n: positiva, negativa o neutral.
1.2. Integraci√≥n de las salidas de los reconocedores
Para mayor control sobre lo que el usuario requiere, la plataforma ofrece 3 tipos de
modo de respuesta, que son: simple, multi y student. Cada uno de ellos hace uso de los
reconocedores individuales descritos en la secci√≥n anterior para determinar sus
respectivas salidas.
1.2.1.

Modo simple

En el modo simple, la respuesta que ofrece PREMOC es la que da cada reconocedor
por separado sin integrarlos; es decir, si un usuario requiere la emoci√≥n en un rostro y
en un sonido de voz, PREMOC le da en respuesta la emoci√≥n del rostro y la emoci√≥n
de la voz. Las emociones que regresa el modo simple dependen de cada reconocedor
individual.
Research in Computing Science 111 (2016)

104

PREMOC: Plataforma de reconocimiento multimodal de emociones

Como se muestra en la Figura 6, el integrador re√∫ne las respuestas de los
reconocedores individuales para crear el json de respuesta para el usuario.
1.2.2. Modo multi
En el modo multi, la respuesta que da el usuario es multimodal; es decir, regresa
una sola emoci√≥n al usuario de los dispositivos que √©l requiera. En el mismo caso que
el ejemplo anterior, si el usuario requiere la emoci√≥n en un rostro y en un sonido de voz
y lo quiere en forma multimodal, PREMOC le da en respuesta una emoci√≥n obtenida
despu√©s de analizar mediante reglas difusas la emoci√≥n que obtuvo desde el
reconocedor de rostro y desde el reconocedor de voz. Las emociones que se generan
con las reglas difusas son las mismas: feliz, sorpresa, neutral, enojado y triste.

Fig. 6. Funcionamiento de Modo Simple y Modo Multi.

Como muestra la Fig. 6, en modo multi, una vez aplicados los reconocedores
individuales, a diferencia del modo simple, utiliza un motor difuso que consta de una
serie de reglas difusas para determinar una sola emoci√≥n considerando en cuenta dos o
m√°s.
Se cre√≥ un generador de reglas difusas para la creaci√≥n de reglas en todas las
combinaciones de dispositivos que pudieran darse. El generador de reglas est√° basado
en la tabla de Whissell (Figura 7). Se ubica a cada emoci√≥n de los diferentes
dispositivos en la tabla y se calcula un promedio para determinar la emoci√≥n final.
La tabla de Whissell est√° basada en activaci√≥n y evaluaci√≥n, y est√°n distribuidas en
ella las emociones.
Table 1. Ubicaci√≥n de las emociones en la tabla de Whissell.
Emoci√≥n

Punto

Feliz

(2.2 , 1)

Sorpresa

(1.5 , 2)

Neutral

(0 , 0)

Enojado

(-0.5 , 1.1)

Triste

(-1.5 , -0.5)
105

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

Fig. 7. Tabla de Whissell.

Los reconocedores de voz y de se√±ales EEG solo regresan la valencia (emoci√≥n
positiva y negativa), y tambi√©n emoci√≥n neutral en el caso de se√±ales EEG. Tomando
en cuenta esto y para la integraci√≥n de estas se√±ales con el resto (expresiones faciales
y texto), se realizaron diferentes pruebas para determinar una ubicaci√≥n promedio de
una valencia positiva o negativa en la tabla de Whissell y se determin√≥ ubicarlas en los
puntos que muestra la tabla 2.
Table 2. Ubicaci√≥n de positivo y negativo de wav y EEG en la tabla de Whissell.

Emoci√≥n

Punto

Positivo

(1,0.8)

Negativo
(-0.6,0.1)
Con estos valores definidos, ya se pueden ubicar todos los resultados de los cuatro
reconocedores dentro de la tabla de Whissell. El procedimiento para obtener una
emoci√≥n multimodal en PREMOC es:
a) Ubicar las emociones de los reconocedores individuales en la tabla de Whissell.
b) Obtener un punto promedio entre las ubicaciones de las emociones.
c) Calcular la distancia desde el punto promedio hasta la ubicaci√≥n de las
emociones de la tabla 1.
Research in Computing Science 111 (2016)

106

PREMOC: Plataforma de reconocimiento multimodal de emociones

d) Seleccionar la emoci√≥n que tenga la distancia menor al punto promedio.
De esta manera se selecciona la emoci√≥n final para el modo multimodal en el
generador de reglas; las reglas son creadas con el formato de la regla que se muestra en
la Figura 8, y son procesadas por la librer√≠a jFuzzyLogic [36]. De esta manera se crean
todas las reglas difusas necesarias para la integraci√≥n de emociones en el modo
multimodal. En total fueron creadas 470 reglas para las combinaciones de estos 4
reconocedores.
RULE 1: IF Txt IS Feliz AND Img IS Feliz AND Wav IS
Positiva THEN Emocion IS Feliz;
Fig. 8. Regla difusa.

1.2.3. Modo student
En modo student, el usuario obtendr√° como respuesta el valor de una emoci√≥n
tomando en cuenta las que normalmente son comunes en un √°mbito de aprendizaje.
Estas emociones son enganchado, aburrido, frustrado y neutral.
La creaci√≥n de reglas difusas del modo student fue similar al modo multi, usando la
tabla de Whissell. El modo student regresa al usuario las emociones educativas
enganchado, frustrado, aburrido y neutral.

4.

Experimentos

Actualmente PREMOC est√° siendo utilizado en varios proyectos dentro del
laboratorio y entre ellos tenemos a EasyLogic y STAAM; ambos son Sistemas Tutores
Inteligentes y Afectivos, donde el reconocimiento de emociones es realizado con
PREMOC.
EasyLogic es un sistema web, que act√∫a como Sistema Tutor Inteligente Afectivo en el
proceso de aprendizaje de programaci√≥n de un lenguaje. EasyLogic utiliza el
reconocimiento de emociones en rostro de PREMOC en modo student cada 15 segundos,
para mostrar al estudiante dos tipos de ayudas mientras aprende l√≥gica algor√≠tmica:
informativas y motivacionales (figura 10).
STAAM es un Sistema Tutor Inteligente Afectivo para matem√°ticas para ni√±os de
primer a√±o de secundaria. STAAM utiliza el reconocedor de emociones en rostros y el
reconocedor de emociones en texto de PREMOC para calificar los ejercicios resueltos
por los alumnos. STAAM considera el estado afectivo del estudiante adem√°s del tiempo
y la resoluci√≥n del ejercicio y entrega un trofeo diferente al estudiante dependiendo del
estado de √°nimo con el que resolvi√≥ el ejercicio.

5.

Conclusiones y trabajo futuro

Como conclusi√≥n, el reconocedor de emociones en im√°genes seg√∫n pruebas
realizadas tiene una tasa de √©xito del 80%. El reconocedor de emociones en texto tiene
poco m√°s de 80% seg√∫n pruebas realizadas con estudiantes. El reconocedor de
107

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

emociones en voz obtuvo 64%. El reconocedor de emociones en se√±ales EEG muestra
su tasa de √©xito en 70%. Las pruebas fueron realizadas con los mismos datos de
entrenamiento.
Se puede concluir que tener un sistema de reconocimiento multimodal de
emociones en l√≠nea, facilita la integraci√≥n de afecto a los desarrolladores, ya que pueden
crear su software sin realizar el entrenamiento de reconocedores de emociones.
Actualmente se est√° trabajando en mejorar el reconocedor de emociones en rostros
con diferentes t√©cnicas de extracci√≥n de caracter√≠sticas como: Action Units [37] y
Binary Pattern [38]. Como trabajo futuro se considera la mejora del reconocedor de
emociones en voz con m√°s datos de entrenamiento y que no sea de solo valencia, sino,
de las emociones feliz, enojado, triste y neutral. Tambi√©n se considera mejorar el
reconocedor de texto y de se√±ales EEG con otros tipos de extractores de caracter√≠sticas.
Tambi√©n integrar el reconocimiento de otros dispositivos el rastreo de ojos.

Referencias
1.
2.
3.
4.
5.

6.

7.
8.
9.

10.
11.
12.

13.

14.

15.

Picard, R.W.: Affective Computing. No. 321, pp. 1‚Äì16 (1995)
Mehrabian, A.: Comunication without words. Psychol. Today, Vol. 2, pp. 53‚Äì56 (1968)
Ekman, P.: Strong Evidence for Universals in Facial Expressions: A Reply to Russell‚Äôs
Mistaken Critique. Psychol, Bull., Vol. 2, No. 1, pp. 268‚Äì287 (1994)
Han, K., Yu, D., Tashev, I.: Speech Emotion Recognition Using Deep Neural Network and
Extreme Learning Machine. pp. 223‚Äì227 (2014)
Seol, Y.S., Kim, D.J., Kim, H.W.: Emotion Recognition from Text Using Knowledge-based
ANN. Proceedings of 23rd International Technical Conference on Circuits/Systems,
Computers and Communications, pp. 1569‚Äì1572 (2008)
Liu, Y., Sourina, O., Nguyen, M.K.: Real-time EEG-based Human Emotion Recognition
and Visualization. Trans. Comput. Sci. XII, Vol. 6670, Special Issue on Cyberworlds, pp.
256‚Äì277 (2011)
Shaheen, S., El-Hajj, W., Hajj, H., Elbassuoni, S.: Emotion Recognition from Text Based
on Automatically Generated Rules. IEEE Int. Conf. Data Min. Work., pp. 383‚Äì392 (2014)
Zheng, D., Tian, F., Liu, J., Zheng, Q., Qin, J.: Emotion Chat: A Web Chatroom with
Emotion Regulation for E-Learners. Phys. Procedia, Vol. 25, pp. 763‚Äì770 (2012)
Khandait, S.P., Thool, R.C., Khandait, P.D.: Automatic Facial Feature Extraction and
Expression Recognition based on Neural Network. Int. J. Adv. Comput. Sci. Appl., Vol. 2,
No. 1, pp. 113‚Äì118 (2011)
Gangwar, S., Shukla, S., Arora, D.: Human Emotion Recognition by Using Pattern
Recognition Network. Int. J. Eng. Res. Appl., Vol. 3, No. 5, pp. 535‚Äì539 (2013)
De, A., Saha, A., Pal, M.C.: A Human Facial Expression Recognition Model Based on Eigen
Face Approach. Procedia Comput. Sci., Vol. 45, pp. 282‚Äì289 (2015)
Kostoulas, T., Fakotakis, N.: A speaker dependent emotion recognition framework. Proc.
5th Int. Symp. Commun. Syst. Networks Digit. Signal Process., University of Patras, pp.
305‚Äì309 (2006)
Vogt, T., Andr√©, E., Bee, N.: EmoVoice ‚Äî A Framework for Online Recognition of
Emotions from Voice. Perception in Multimodal Dialogue Systems, Berlin, Heidelberg:
Springer Berlin Heidelberg, pp. 188‚Äì199 (2008)
Caballero-Morales, S.O.: Recognition of emotions in Mexican Spanish speech: An approach
based on acoustic modelling of emotion-specific vowels. Sci. World J., Vol. 2013, p. 13
(2013)
Nie, D., Wang, X.W., Shi, L.C., Lu, B.L.: EEG-based emotion recognition during watching
movies. 5th Int. IEEE/EMBS Conf. Neural Eng. NER, pp. 667‚Äì670 (2011)

Research in Computing Science 111 (2016)

108

PREMOC: Plataforma de reconocimiento multimodal de emociones

16. Lee, Y.Y., Hsieh, S.: Classifying different emotional states by means of eegbased functional
connectivity patterns. PLoS One, Vol. 9, No. 4, pp. 1‚Äì13 (2014)
17. Alonso-Mart√≠n, F., Malfaz, M., Sequeira, J., Gorostiza, J.F., Salichs, M.A. A multimodal
emotion detection system during human-robot interaction. Sensors (Basel), Vol. 13, No. 11,
pp. 15549‚Äì15581 (2013)
18. Busso, C., Deng, Z., Yildirim, S., Bulut, M., Lee, C.M., Kazemzadeh, A., Lee, S. Neumann,
U., Narayanan, S.: Analysis of emotion recognition using facial expressions, speech and
multimodal information. 6th Int. Conf. Multimodal interfaces, pp. 205‚Äì211, (2004)
19. Chuang, Z.J., Wu, C.: Multi-modal emotion recognition from speech and text. J. Comput.
Linguist. Chinese, Vol. 9, No. 2, pp. 45‚Äì62 (2004)
20. D‚ÄôMello, S.K., Graesser, A.: Multimodal semi-automated affect detection from
conversational cues, gross body language, and facial features. User Model. User-Adapted
Interact., Vol. 20, No. 2, pp. 147‚Äì187 (2010)
21. Caridakis, G., Castellano, G., Kessous, L., Raouzaiou, A., Malatesta, L., Asteriadis, S.,
Karpouzis, K.: Multimodal emotion recognition from expressive faces, body gestures and
speech. IFIP Int. Fed. Inf. Process., Vol. 247, pp. 375‚Äì388 (2007)
22. Paredes, P., Berkeley, U.C., Sun, D.: Sensor-less Sensing for Affective Computing and
Stress Management Technology. Pervasive Comput. Technol. Healthc. Work., pp. 459‚Äì463
(2013)
23. Wagner, J., Andre, E., Jung, F.: Smart sensor integration: A framework for multimodal
emotion recognition in real-time. 3rd International Conference on Affective Computing and
Intelligent Interaction and Workshops, pp. 1‚Äì8 (2009)
24. Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E., Atkinson, R., Burleson, W.: ABE: An
agent-based software architecture for a multimodal emotion recognition framework. Proc. 9th Work. IEEE/IFIP Conf. Softw. Archit. WICSA, April 2016, pp. 187‚Äì193, (2011)
25. Zatarain Cabada, R., Barron Estrada, M.L., Hernandez, F.G., Bustillos, R.O.: An Affective
Learning Environment for Java. IEEE 15th International Conference on Advanced Learning
Technologies, pp. 350‚Äì354 (2015)
26. Zatarain-Cabada, R., Barr√≥n-Estrada, M.L., Garc√≠a-Liz√°rraga, J., Mu√±oz-sandoval, G.,
Rios-Feliz, J.M.: Java Tutoring System with Facial and Text Emotion Recognition. Res.
Comput. Sci., Vol. 106, No. April 2016, pp. 49‚Äì58 (2015)
27. Langner, O., Dotsch, R., Bijlstra, G., Wigboldus, D.H.J., Hawk, S.T., van Knippenberg, A.:
Presentation and validation of the Radboud Faces Database. Cogn. Emot., Vol. 24, No. 8,
pp. 1377‚Äì1388, (2010)
28. Witten, I. H., Frank, E., Trigg, L., Hall, M., Holmes, G., Cunningham, S.J.: Weka : Practical
Machine Learning Tools and Techniques with Java Implementations. Seminar, Vol. 99, pp.
192‚Äì196 (1999)
29. OpenCV
Filtros.
[Online].
Available:
http://docs.opencv.org/2.4/modules/
imgproc/doc/imgproc.html.
30. Musicg, Specetrogram. [Online]. Available: http://deeplearning4j.org/canovadoc/
musicg/extension/Spectrogram.html.
31. Chang, C., Lin, C.: LIBSVM : A Library for Support Vector Machines. ACM Trans. Intell.
Syst. Technol., Vol. 2, pp. 1‚Äì39 (2011)
32. Inc, E., Epoc - Emotiv. [Online]. Available: https://emotiv.com/epoc.php.
33. Mahajan, R., Bansal, D., Singh, S.: A Real Time Set Up for Retrieval of Emotional States
from Human Neural Responses. World Acad. Sci. Eng. Technol., Vol. 8, No. 3, pp. 144‚Äì
149 (2014)
34. Wang, X.-W., Nie, D., Lu, B.-L.: Emotional state classification from EEG data using
machine learning approach. Neurocomputing, Vol. 129, pp. 94‚Äì106 (2014)
35. Barr√≥n-Estrada, M.L., Zatarain-Cabada, R., Aispuro-Gallegos, C.L., Sosa-Ochoa, C.G.
Lindor-Valdez, M.: Affective Environment for Java Programming Using Facial and EEG
Recognition. Res. Comput. Sci., Vol. 106, pp. 39‚Äì47 (2015)
109

Research in Computing Science 111 (2016)

Ram√≥n Zatarain-Cabada, Mar√≠a Lucia Barr√≥n-Estrada, Gilberto Mu√±oz-Sandoval

36. Cingolani, P., Alcala-Fdez, J.: jFuzzyLogic: a Java Library to Design Fuzzy Logic
Controllers According to the Standard for Fuzzy Control Programming. Int. J. Comput.
Intell. Syst., Vol. 6, No. sup1, pp. 61‚Äì75 (2013)
37. Tian, Y.-L., Kanade, T., Cohn, J.F.: Recognizing upper face action units for facial
expression analysis. Proc. IEEE Conf. Comput. Vis. Pattern Recognition. CVPR 2000 (Cat.
No.PR00662), Vol. 1, No. 2, pp. 1‚Äì19 (2000)
38. Feng, X., Pietik√§inen, M., Hadid, A.: Facial Expression Recognition with Local Binary
Patterns and Linear Programming. Pattern Recognit. Image Anal., Vol. 15, No. 2, pp. 546‚Äì
548 (2005)

Research in Computing Science 111 (2016)

110

