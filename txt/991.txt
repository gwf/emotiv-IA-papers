Available online at www.sciencedirect.com

ScienceDirect
Procedia Computer Science 72 (2015) 398 – 405

The Third Information Systems International Conference

Evaluating OpenBCI Spiderclaw V1 Headwear’s Electrodes
Placements for Brain-Computer Interface (BCI) Motor
Imagery Application
Hatma Suryotrisongko, Febriliyan Samopa
Institut Teknologi Sepuluh Nopember, Department of Information Systems, Kampus ITS Sukolilo Surabaya 60111, Indonesia

Abstract
Motor imagery can be defined in terms of imagined movement from the first person perspective. It has been getting
many researchers’ attention since it could be implemented in many important applications such as neurological
rehabilitation, sports training, prosthesis movement control, and so on. This research evaluates OpenBCI for Motor
Imagery application, especially whether the OpenBCI Spiderclaw V1 headwear electrodes placements are sufficient
for motor imagery application. OpenBCI 32 bit board with daisy chain (16 channels) was used in this research.
OpenVibe’s motor imagery CSP scenarios were adopted. After subjects had finished working with the OpenVibe
motor imagery scenarios, they were asked to fill Movement Imagery Questionnaire-3 (MIQ-3). MIQ-3 results were
used to validate whether subject suffer from “BCI illiteracy”. It could be concluded that the OpenBCI Spiderclaw V1
electrodes placements are not optimum for motor imagery application. The average of accuracy measurements which
was around 60% for all subjects shows poor motor imagery performance. Furthermore, 16 channel electrodes
configuration with a wide temporal filter range [8-30 Hz] showed better performance compared to other settings in
this research. However, further study is needed to improve the statistical significance of these findings. On the MIQ-3
results, kinesthetic imagery score reflects the most correlated with the accuracy measurement, supporting the
suggestion that questionnaire could be used to predict user’s motor imagery performance.
© 2015
Authors.by
Published
Elsevier
B.V. This
is anpeer-review
open access article
the CC BY-NC-ND
license committee of
©
2015The
Published
ElsevierbyLtd.
Selection
and/or
underunder
responsibility
of the scientific
(http://creativecommons.org/licenses/by-nc-nd/4.0/).
The
Third Information Systems International Conference (ISICO 2015)
Peer-review under responsibility of organizing committee of Information Systems International Conference (ISICO2015)

Keywords: Brain-Computer Interface, BCI, Motor Imagery, OpenBCI, OpenVibe, MIQ-3

1. Introduction
1.1. Motor Imagery and OpenBCI
Motor imagery means simulating an action/movements in individual’s mind. It can be defined in terms
of imagined movement from the first person perspective [18][9]. Motor imagery has been getting many

1877-0509 © 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license
(http://creativecommons.org/licenses/by-nc-nd/4.0/).
Peer-review under responsibility of organizing committee of Information Systems International Conference (ISICO2015)
doi:10.1016/j.procs.2015.12.155

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

researchers’ attention since it could be implemented in many important applications such as neurological
rehabilitation, sports training, prosthesis movement control, and so on.
Motor imagery research and applications have been developed using various electroencephalogram
(EEG) devices: expensive medical EEG device, Commercial off-the-shelf EEG devices such as Emotiv or
NeuroSky, and so on. However, recently many researchers have been giving attention to the OpenBCI, the
open-source brain-computer interface (BCI) device (not to be confused with OpenBCI software http://openbci.pl). OpenBCI has its roots as a crowdfunding project. The OpenBCI Board is a versatile
and affordable bio-sensing microcontroller that can be used to sample electrical brain activity (EEG),
muscle activity (EMG), heart rate (EKG), and more. It is compatible with almost any type of electrode
and is supported by an ever-growing, open-source framework for signal processing applications
(http://openbci.com) [23]. This device open opportunities for researchers to develop innovative BCI
research and applications because of its open source nature, which means its software and hardware might
be modified and developed as needed.
So far, only limited OpenBCI research reports have been published. For instances, Azokar [3] used
OpenBCI to control a Quadrotor. Bondre and Kapgate [4] develop a framework for Steady State Visually
Evoked Potentials (SSVEP) in Brain Computer Interface (BCI). Firtina et al. [6] develop Emotion Engine
using OpenBCI, which acts as a hub between the computer and the user. It takes the user's physiological
data through body sensors and continuously estimate the user's emotional state based on previously
collected data from the user. The contribution of this research is that it might be one among the first which
evaluating OpenBCI for Motor Imagery application.
1.2. OpenVibe and OpenBCI Spiderclaw V1 Headwear
OpenVibe (http://openvibe.inria.fr) is a novel open-source software platform to design, test and use
brain-computer interfaces in real and virtual environments [17][15]. OpenVibe is meant to be a set of
software modules for the acquisition, pre-processing, processing and visualization of cerebral data, as well
as for the interaction with virtual reality displays [16]. OpenVibe has been implemented in many BrainComputer Interface research, such as P-300 [5][11][12][10], as well as motor imagery [1]. This research
adopted the OpenVibe’s motor imagery scenarios.
Many researchers suggested the optimum electrodes placements for motor imagery application are
around the C3 and C4 locations [14]. However, this research questioned the accuracy of motor imagery
when the electrodes placements utilize the OpenBCI Spiderclaw V1 Headwear’s scheme. According to
the 10-20 systems, they were located at Fp1, Fp2, C3, C4, T5, T6, O1, O2, F7, F8, F3, F4, T3, T4, P3, and
P4. Therefore, another contribution of this research is that a conclusion whether the Spiderclaw V1
electrodes placements sufficient for motor imagery application would be drawn.
2. Methods
2.1. OpenBCI
The first step of this research was to construct OpenBCI EEG device and its headwear. The Spiderclaw
v1 headwear design (available on OpenBCI website) was 3d printed (Figure 1.a). However, the size of its
design seemed to be too large for most of the users in this research (Figure 1.b and Figure 1.c). Therefore,
it was decided that in this experiment, the EEG electrodes were placed manually on users’ head, based on
10-20 systems (see Figure 2.a and Figure 2.b). OpenBCI 32 bit board with daisy chain (16 channels) was
used in this research. OpenBCI GUI software was used to check whether the electrode placement had
been working correctly (Figure 2.c).

399

400

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

Figure 1. (a) OpenBCI electrodes placed on user's scalp. (b) Direct placement without headwear. (c) OpenBCI GUI software

Figure 2. (a) 3d printed OpenBCI headwear. (b) Prototype used by respondent (c) The size is too large for some users

2.2. OpenVibe Motor Imagery
This research adopted OpenVibe’s motor imagery CSP scenarios. It consists of several steps: signal
monitoring, acquisition, CSP training, classifier training, online testing, and replay (see Figure 3).
According to [24], the signal monitoring scenario (Figure 3.a) was used to check the quality of the
signals before starting an experiment. One should check the quality of the signals and ensure that: eye
blinks are visible; jaw clenching is visible; alpha waves are visible when closing eyes. Temporal filter
(Butterworth band pass) was used.
The acquisition scenario (Figure 3.b) was used as a first step to collect some training data. Those data
will later be used to train a classifier for online testing. After 40 seconds running this scenario, it starts the
instruction sequence. Left/right arrows will be presented to let users imagine left/right-hand movements.
There will be 20 arrows of each side (see Figure 4.a). The stimulator configuration was written in Lua
script (www.lua.org).
The CSP training scenario (Figure 3.c) trains the Common Spatial Pattern spatial filter that will be
used in the further steps. Then the Stimulation based epoching boxes provide examples for the CSP
Spatial Filter Trainer: class 1 for LEFT trials; class 2 for RIGHT trials. Spatial filter coefficients
computed according to the Common Spatial Pattern algorithm. The CSP algorithm increases the signal
variance for one condition while minimizing the variance for the other condition. The goal of the
algorithm is to improve the discrimination of two types of signals. The spatial filters are constructed in a
way they maximize the variance for signals of the first condition while at the same time they minimize it

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

Figure 3. Motor imagery scenario in OpenVibe

Figure 4. (a) Motor Imagery Instruction. (b) Classification result shown to user during testing

for the second condition. This can be used for discriminating the signals of two commonly used motor
imagery tasks (e.g. left versus right-hand movement).
In the classifier trainer scenario (Figure 3.d), the CSP spatial filter configuration produced in the
previous scenario is used prior to the feature extraction, followed by the feature extraction part. Then
stimulation based epoching is used to select four seconds of signal half a second after the instruction was
shown to the user. The signal is then splitted in blocks of 1 second every 16th second and the logarithmic

401

402

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

band power is computed. The matrices can then be converted into feature vectors. This scenario produces
a classifier configuration file at the end of the experiment that will be used during online sessions.
Finally, the online testing scenario can be seen at Figure 3.e. This scenario can be used online once the
CSP spatial filter and the classifier are trained. The CSP spatial filter produced in the earlier scenario is
used prior to the feature extraction, followed by the feature extraction part similar with at the previous
scenario. The spatial filter maps M inputs to N outputs by multiplying each input vector with a matrix.
Finally, the feature vectors are classified with an LDA classifier. Note that the state vector of the classifier
(which in the case of the LDA is the distance to the separation plane) is sent to the Graz Visualization box
for feedback (see Figure 4). In order to display correct feedback, the Graz Visualization box expects a
negative value for one class and a positive value for the other class.
2.3. Data collection
This research involved 10 participants, all healthy male between 22 to 30 years old. OpenVibe’s motor
imagery scenario with CSP algorithm was modified. EEG recording was done in two phases. The first
stage was conducted using eight-channel configurations, which electrodes were placed at Fp1, Fp2, C3,
C4, T5, T6, O1 and O2 according to the 10-20 systems. Next, additional eight more channels (16 channel
in total) placed at F7, F8, F3, F4, T3, T4, P3, and P4 were added. The recorded EEG data would be
studied under two temporal filters setting. First, configuration with low cut frequency at 1 Hz and high cut
frequency at 30 Hz. Second, filter in a smaller frequency band [8-12 Hz]. Therefore, for each participant,
eight motor imagery accuracy measurements would be collected. The accuracy is computed given the
results from classifiers, compared to the targets received. As a result, 80 accuracy measurements would be
gathered from all participants in total.
After the participant had finished working with the OpenVibe motor imagery scenarios, they were
asked to fill a questionnaire. The questionnaire used in this research was the Movement Imagery
Questionnaire-3 (MIQ-3), which is the most recent version of the Movement Imagery Questionnaire [8].
2.4. Precautions towards BCI Illiteracy using Movement Imagery Questionnaire-3 (MIQ-3)
Since the cognitive function of each might slightly differ during practicing motor imagery,
measurement of motor imagery ability is an important issue. According to the recent literature review
conducted by Laura et al. [13], explicit motor imagery ability can be measured by questionnaire and
mental chronometry. Moreover, implicit motor imagery ability can be measured through prospective
action judgment and motorically driven perceptual decision paradigms.
The Movement Imagery Questionnaire-3 (MIQ-3) is the most recent version of the Movement Imagery
Questionnaire [8] and the Movement Imagery Questionnaire-Revised [7]. It is a 12-item questionnaire to
assess individual's ability to image four movements using internal visual imagery, external visual
imagery, and kinesthetic imagery [22]. MIQ-3 requires the respondent to image four movements; a knee
lift, jump, arm movement, and waist bend. Participants are asked to perform physically, and afterwards
image the movement. Each movement is imaged three times, once from an external visual perspective,
once from an internal visual perspective, and once kinesthetically, resulting in a total of 12 movements
physically performed and then imaged. Following each image, participants rate the ease they can produce
the image on a 7-point Likert-type scale. It is ranging from 1 (very hard to see/feel) to 7 (very easy to
see/feel). A higher score, therefore, represents a higher ability to perform visual or kinesthetic imagery
[21]. Williams et al. [22] identified the MIQ-3 to be a valid and reliable questionnaire.
The phenomenon of “BCI illiteracy” means that not everybody could use BCI application effectively,
about 20% of subjects are not proficient with a typical BCI system [2]. In this research, MIQ-3 results

403

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

were used to validate whether a subject suffers from “BCI illiteracy”. Any subject with this phenomenon
would be excluded from further data analysis.
3. Results and Discussion
The MIQ-3 results from all participants are presented in Table 1. As suggested by previous research
[19][20], motor imagery questionnaire could be used as a method to detect BCI illiteracy. From the Table
1, it could be seen that all participants were able to do the imagery task quite well. In average, most
subjects report that “somewhat it was easy to see or to feel” when completing the task given in the
questionnaires. Because of “BCI illiteracy” phenomenon did not emerge in this research. Therefore, the
motor imagery accuracy data from all respondents would be used for further data analysis.
Table 2 shows the accuracy measurements from motor imagery application when the user run the
experiment. The number shown in bold and highlighted means the peak of user’s performance, compared
to the user’s performance in other configurations. After repeating the experiment, almost all subjects’
accuracy were increasing and reaching top performance under 16 channel electrodes configuration with a
wide temporal filter range [8-30 Hz]. However, based on the ANOVA analysis that resulted in F= 0.21
lower than F crit= 2.72 means that the null hypothesis was accepted. It might need larger data size, to be
able to conclude the statistical significance of this finding.
Furthermore, to compare the accuracy of 8 channels with 16 channel configuration and 8-30Hz with 812Hz temporal filter setting, F-Test and t-Test were conducted. Both F-Test concluded that the variances
of the two populations are equal (accept the null hypothesis). Similarly, the t-Test result suggested not
rejecting the null hypothesis. The observed differences between the sample means were not convincing
enough to say that the average number of study hours between 8-30Hz and 8-12Hz, 8 channels and 16
channel as well, differ significantly.
The average of accuracy measurements was around 60% for all subjects. It shows that the motor
imagery performances using OpenBCI Spiderclaw v1 configurations were still inferior to experiments
with electrodes positioned around C3 and C4 for optimum setting [14]. This finding suggests that those
who want to develop motor imagery application should place electrodes around C3 and C4, instead of
insist on the Spiderclaw v1 design.
Table 1. MIQ-3 results
Subject

Internal Visual Imagery

External Visual Imagery

Kinesthetic Imagery

average

subj1

6

6

3.75

5.25

subj2

6.25

5.75

5.75

5.92

subj3

6.25

4.5

5.75

5.50

subj4

6.5

5.75

6.5

6.25

subj5

6.75

6.5

5

6.08

subj6

6.25

7

4.75

6.00

subj7

4.5

6

3.75

4.75

subj8

5.75

7

6

6.25

subj9

6.75

6

3.75

5.50

subj10

5.75

5.25

4.5

5.17

404

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

Table 2. Users' accuracy when running the Motor Imagery scenario
Motor Imagery CSP Scenario's Accuracy
8-30 Hz filter

8-12 Hz filter

Subject
8 channel

16 channel

8 channel

16 channel

1st trial

2nd trial

1st trial

2nd trial

1st trial

2nd trial

1st trial

2nd trial

subj1

50.78%

64.56%

47.15%

69.75%

60.23%

63.53%

46.02%

63.53%

subj2

52.78%

66.27%

54.46%

70.29%

49.90%

60.93%

49.59%

60.93%

subj3

52.38%

70.65%

55.43%

71.26%

49.65%

70.35%

51.01%

70.35%

subj4

65.34%

67.71%

52.81%

68.56%

58.32%

66.62%

57.54%

66.62%

subj5

49.65%

65.45%

47.78%

62.36%

44.26%

62.14%

54.11%

62.14%

subj6

53.08%

63.67%

45.33%

73.86%

44.76%

63.47%

52.23%

63.47%

subj7

71.64%

66.52%

49.23%

73.19%

65.46%

64.78%

63.90%

64.78%

subj8

50.30%

82.85%

59.81%

82.83%

56.05%

74.32%

69.83%

74.32%

subj9

46.97%

62.10%

43.91%

66.12%

49.90%

60.38%

44.62%

60.38%

subj10

48.91%

65.74%

51.28%

68.50%

45.28%

64.55%

54.60%

64.55%

Table 3. Correlation between MI-CSP accuracy and MIQ-3
MI-CSP accuracy
MI-CSP accuracy

Internal Visual

Kinesthetic

External Visual

MIQ-3 average

1

Internal Visual

-0.595360519

1

Kinesthetic

0.434405169

0.317475218

1

External Visual

0.114363829

0.004181174

-0.08261231

1

MIQ-3 average

0.090546213

0.641609096

0.761381681

0.441080048

1

As can be seen in Table 3, considering the correlation between the performance of motor imagery
experiment and the MIQ-3 result, kinesthetic imagery score reflects the most correlated with the accuracy
measurement. However, the external visual imagery was removed from regression analysis because of its
P-values below 0.05. The R Square values from regression analysis show good value, as much as 0.79. It
means that 79% of the variation in MI-CSP accuracy was explained by the independent variables Internal
Visual Imagery and Kinesthetic Imagery. This finding supports the Vuckovic’s [20] suggestion that
questionnaire could be used to predict user’s performance while running BCI applications, especially
motor imagery applications.
4. Conclusion
It could be concluded that the OpenBCI Spiderclaw V1 electrodes placements is not optimum for
motor imagery application. It was located at Fp1, Fp2, C3, C4, T5, T6, O1, O2, F7, F8, F3, F4, T3, T4,

Hatma Suryotrisongko and Febriliyan Samopa / Procedia Computer Science 72 (2015) 398 – 405

P3, and P4. The average of accuracy measurements which was around 60% for all subjects shows poor
motor imagery performance. Electrodes might be better concentrated at around C3 and C4 for motor
imagery application, imagining right hand and left-hand movement. Additionally, 16 channel electrodes
configuration with a wide temporal filter range [8-30 Hz] showed better performance compared to other
settings in this research. However, further study is needed to improve the statistical significance of these
findings. Utilizing MIQ-3 self-report questionnaire, “BCI illiteracy” phenomenon were not observed from
experiment subjects. Interestingly, kinesthetic imagery score reflects the most correlated with the
accuracy measurement, supporting the suggestion that questionnaire could be used to predict user’s motor
imagery performance.
Acknowledgements
This research was supported and funded by the “Pemula” research grant 2015.
References
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]
[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]

M.K. Ahirwal, N.D. Londhe, Offline Study of Brain Computer Interfacing for Hand Movement Using OpenVIBE, in: 2011
Int. Conf. Process Autom. Control Comput. PACC, 2011: pp. 1–5.
B.Z. Allison, C. Neuper, Could Anyone Use a BCI?, in: D.S. Tan, A. Nijholt (Eds.), Brain-Comput. Interfaces, Springer
London, 2010: pp. 35–54.
A.F. Azocar, Evaluation of the OpenBCI Neural Interface for Controlling a Quadrotor Simulation, Thesis, 2014.
C. Bondre, D. Kapgate, Development of Framework for Steady State Visually Evoked Potentials in Brain Computer Interface,
(2015).
M. Congedo, M. Goyat, N. Tarrin, G. Ionescu, L. Varnet, B. Rivet, et al., “ Brain Invaders”: a prototype of an open-source
P300-based video game working with the OpenViBE platform, in: 5th Int. Brain-Comput. Interface Conf. 2011 BCI 2011,
2011: pp. 280–283.
C. Fırtına, M.K. Iman, A. Tekat, M.A. Yesilyaprak, Emotion Engine, (2015).
C.R. Hall, K.A. Martin, Measuring movement imagery abilities: a revision of the movement imagery questionnaire., J. Ment.
Imag. (1997).
C.R. Hall, J. Pongrac, Movement imagery: questionnaire, University of Western Ontario Faculty of Physical Education, 1983.
M. Jeannerod, The cognitive neuroscience of action., Blackwell Publishing, 1997.
H.S. Kisakye, Brain Computer Interfaces: OpenViBE as a Platform for a P300 Speller, (n.d.).
E. Maby, G. Gibert, P.E. Aguera, M. Perrin, O. Bertrand, J. Mattout, The OpenViBE P300-Speller scenario: a thorough online
evaluation, in: Hum. Brain Mapp. Conf., 2010.
E. Maby, M. Perrin, D. Morlet, P. Ruby, O. Bertrand, S. Ciancia, et al., Evaluation in a Locked-in Patient of the OpenViBE
P300-speller, Proc. 5th Int. Brain-Comput. Interface Graz Austria. 2224 (2011) 272275.
L.P. McAvinue, I.H. Robertson, Measuring motor imagery ability: A review, Eur. J. Cogn. Psychol. 20 (2008) 232–251.
G. Pfurtscheller, C. Neuper, Motor imagery and direct brain-computer communication, Proc. IEEE. 89 (2001) 1123–1134.
Y. Renard, L. Bonnet, B. Payan, L. Bougrain, A. Lécuyer, OpenViBE Tutorial: A Novel Open-ǦSource Software to Design,
Test and Use Brain-ǦComputer Interfaces, in: BCI Meet. 2010, 2010.
Y. Renard, G. Gibert, M. Congedo, F. Lotte, E. Maby, B. Hennion, et al., OpenViBE: An open-source software platform to
easily design, test and use brain-computer interfaces, in: BCI Meets Robot. Challenging Issues Brain-Comput. Interact. Shar.
Control MAIA Workshop, 2007: p. 49.
Y. Renard, F. Lotte, G. Gibert, M. Congedo, E. Maby, V. Delannoy, et al., OpenViBE: an open-source software platform to
design, test, and use brain-computer interfaces in real and virtual environments, Presence Teleoperators Virtual Environ. 19
(2010) 35–53.
G. Rizzolatti, M.A. Arbib, The representing brain. Neural correlates of motor intention and imagery, Trends Neurosci. 21
(1998) 188–194.
A. Vuckovic, Motor imagery questionnaire as a method to detect BCI illiteracy, in: 2010 3rd Int. Symp. Appl. Sci. Biomed.
Commun. Technol. ISABEL, 2010: pp. 1–5.
A. Vuckovic, B.A. Osuagwu, Using a motor imagery questionnaire to estimate the performance of a Brain–Computer
Interface based on object oriented motor imagery, Clin. Neurophysiol. 124 (2013) 1586–1595.
S.E. Williams, J. Cumming, Measuring athlete imagery ability: the sport imagery ability questionnaire, J. Sport Exerc.
Psychol. 33 (2011) 416–440.
S.E. Williams, J. Cumming, N. Ntoumanis, S.M. Nordin-Bates, R. Ramsey, C. Hall, Further Validation and Development of
the Movement Imagery Questionnaire, J. Sport Exerc. Psychol. 34 (2012) 621–646.
OpenBCI, (n.d.)., http://openbci.com (Accessed: 14 August 2015).
OpenViBE | Software for Brain Computer Interfaces and Real Time Neurosciences, (n.d.)., http://openvibe.inria.fr (Accessed:
14 August 2015).

405

