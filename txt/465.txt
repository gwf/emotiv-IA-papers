Prospect. Vol. 14, No. 2, Julio - Diciembre de 2016, págs. 99-110

Prótesis de mano virtual movida por señales encefalograficas – EEG
Virtual hand prosthesis moved by encephalographic signals
Karin Correa Arana1, Andrés Vivas Albán2
M.Sc. Ingeniería Automática. Universidad del Cauca.
Grupo de Investigación en Automática Industrial AI, Popayán, Colombia.
2
Ph.D. Robótica. Profesor Investigador de la Facultad de Ingeniería Electrónica y Telecomunicaciones. Universidad del Cauca.
Grupo de Investigación en Automática Industrial AI. Popayán, Colombia.
Email: kcorrea@unicauca.edu.co
1

Recibido 3/08/2016,
Aceptado 28/05/2016

Cite this article as: K. Correa, A. Vivas, “Virtual hand prosthesis moved by encephalographic
signals”, Prospect, Vol 14, N° 2, 99-110, 2016.

RESUMEN
Este articulo presenta el proyecto de investigación que tiene como reto la manipulación de una prótesis de mano
en un ambiente virtual de simulación utilizando una interfaz natural basada en una BCI (Brain Computer Interface),
lo cual debería plantear un nuevo paradigma para la manipulación de prótesis de mano. Para la manipulación
de la prótesis de mano virtual se utiliza la información suministrada por las señales encefalográficas (EEG)
captadas a partir de un casco Emotiv®. Este sistema cuenta con catorce electrodos repartidos sobre el cráneo del
usuario, quien luego de una fase de entrenamiento puede producir órdenes sencillas a través del software del
fabricante. Finalmente, la presente investigación muestra los resultados obtenidos hasta el momento, donde las
señales encefalográficas del usuario logran mover una mano virtual construida en el computador (utilizando las
herramientas de software libre Qt y VTK). Se espera que un usuario pueda entrenarse y reproducir en la mano
virtual diversos agarres tales como el cilíndrico, el esférico y el tipo pinza.
Palabras clave: Prótesis robóticas de mano; Señales electroencefalográficas; Interfaces naturales; Interfaz cerebro
computador.

ABSTRACT
This paper presents a research project that has the challenge of manipulating a prosthetic hand in a
virtual simulation environment using a natural interface based on a BCI (Brain Computer Interface), which
should propose a new paradigm for the manipulation of a prosthetic hand. The information provided by
encephalographic (EEG) signals captured from an Emotiv® headset is used for the manipulation of virtual
hand prosthesis. This system has fourteen electrodes distributed over the skull of the user, who after a
training phase can produce simple commands through the manufacturer's software. Finally, this research
presents the results obtained so far, where the user’s encephalographic signals move a virtual hand built in
the computer (using free software tools as Qt and VTK). It is expected that the user can train and reproduce
various grasps such as cylindrical, spherical and pincer grasp in the virtual hand.
Key words: Robotic prosthetic hand; Encephalographic signals; Natural interface; Brain-computer interface.

Doi: http://dx.doi.org/10.15665/rp.v14i2.664

99

Prótesis de mano virtual movida por señales encefalograficas – EEG

1. INTRODUCCIÓN
En Colombia las amputaciones totales o parciales
de un miembro o una extremidad no solo se dan por
enfermedades, sino por accidentes y por el conflicto
armado. La Asociación Colombiana de Medicina
Física y Rehabilitación estima que la incidencia de
amputación en el país se da de 200 a 300 personas
por cada cien mil habitantes [1]. Esta cifra tiende a
incrementarse en población con factores de riesgo
como la diabetes, problemas en los sistemas vasculares
y por las enfermedades crónicas. De acuerdo con
la Asociación de Medicina Física, las amputaciones
quirúrgicas se realizan con dos objetivos: el primero
es eliminar o contrarrestar la causa que la originó,
para disminuir riesgos y preservar la vida. El segundo
es permitir una adecuada rehabilitación posterior,
para lograr la mejor adaptación de una prótesis y
restablecer las funciones motoras asociadas a la mano
lo mejor posible [2]. Si bien, el uso de las prótesis es
consecuencia de los inconvenientes mencionados
anteriormente [3], el desarrollo de estas ha estado
estrechamente vinculado con el desarrollo de la
robótica asistencial constituyendo hoy día una de las
aplicaciones a resaltar en Bioingeniería. Aunque la
historia de estos dispositivos comienza hace más de 30
años, su evolución no ha sido la que inicialmente pudo
preverse. Hay diversas razones que justifican este
estancamiento, por una parte, la poca funcionalidad
que se obtiene respecto al uso de brazos y manos
sanos, debido no tan sólo a las limitaciones mecánicas
o de estrategias de manipulación, sino también a las
posibilidades del usuario a transmitir las órdenes
adecuadas para producir los movimientos deseados [4].
Una de las formas más utilizadas para manipular las
prótesis implantadas ya en los pacientes, se basa en
utilizar las señales mioeléctricas del usuario generadas
en los músculos para activar las prótesis, sin embargo si
el paciente tiene limitaciones presentes, en sus músculos
remanentes y no los puede contraer fácilmente, las
señales electromiográficas (EMG) del usuario no
pueden ser captadas y no es posible la manipulación
de la prótesis sin métodos no invasivos [5]. También se
presentan otros inconvenientes como la confusión en la
correcta identificación de la intención de movimiento
del usuario, y en la degradación de la señal muscular
en pacientes cuya lesión ocurrió hace mucho tiempo, ya
que el paciente pierde vías de comunicación al sistema
nervioso por lo que desconoce el estado de su músculo
y debe aprender a contraerlo a voluntad, lo cual no es
fácil [6].
Igualmente, en los últimos años, se han invertido
muchos esfuerzos en el desarrollo de sistemas biónicos
híbridos capaces de unir, vía interfaces naturales,
el sistema nervioso humano con prótesis, o incluso

máquinas robóticas externas, con el principal objetivo
de recuperar las funciones motoras y sensoriales de
pacientes con lesiones: en la médula espinal, en el
cerebro, a causa de enfermedades degenerativas o
por accidentes que han llevado a los pacientes a sufrir
amputaciones [7].
Este proyecto de investigación busca una técnica
novedosa para la manipulación de una prótesis de
mano virtual robotizada utilizando una interfaz natural
basada en una BCI (Brain Computer Interface). Manipular
este tipo de prótesis partiendo de señales cerebrales se
hace importante cuando se entiende que los desarrollos
en esta área son mínimos en comparación con otros
estudios en la parte médica y que puede ser una
solución a los inconvenientes descritos anteriormente,
referentes al movimiento natural de las manos [8].
2. TEORIA
2.1 Prótesis de Mano
Actualmente se presentan diversos tipos de prótesis
para extremidades superiores, y sus tipos varían
ajustándose a las necesidades básicas de los pacientes
que las usan. Algunas prótesis parecen muy reales,
otras tienen una tecnología tan avanzada que pueden
considerarse como robots. Algunas prótesis no se
mueven nada en absoluto (prótesis estéticas), otras
pueden ajustarse solo en posiciones específicas y otras
son mecánicas y están controladas por músculos, cables
y guayas [9]. Sin embargo en todos los tipos de prótesis
mencionados, el mecanismo de control ha sido una
limitante en la funcionalidad de dichas prótesis.
En el campo de las prótesis robotizadas, las principales
investigaciones, desarrollos y aplicaciones basados en
la robótica asistencial y en el desarrollo de estas prótesis
de miembro superior, se han realizado en países como
Estados Unidos, Japón, Francia, Alemania y Europa
en general [10]. El alto grado de precisión de esta
tecnología, ha despertado un enorme interés dentro
del ámbito médico e ingenieril. Entre estas prótesis de
mano robóticas, las más conocidas son: Michelangelo®,
I – Limb Ultra, The Dextrus Hand, Be Bionic, Handie, entre
otras [11-13].
En cuanto a su operación, algunas de las formas más
utilizadas para manipular la prótesis implantada
ya en los pacientes es: a través de una aplicación en
un dispositivo móvil y la otra se basa en las señales
electromiográficas (EMG) del usuario. La primera
técnica presenta la desventaja de la manipulación del
dispositivo móvil por el paciente con su mano sana,
situación que restringe notoriamente la manipulación
de los objetos que se encuentran en su alrededor.
La segunda técnica se basa en utilizar las señales

100

Prospect. Vol. 14, No. 2, Julio - Diciembre de 2016, págs. 99-110

mioeléctricas del usuario generadas en los músculos
para activar las prótesis [5], sin embargo si el usuario
presenta limitaciones en sus músculos remanentes
y no los puede mover sin gran esfuerzo, las señales
bioeléctricas asociadas del usuario no pueden ser
registradas y no es posible operar la prótesis con éxito.
Adicionalmente, para el uso de esta técnica, se debe
tener en cuenta que el paciente puede perder varias de
las vías de comunicación al sistema nervioso debido a
la amputación por lo que no se conoce con exactitud el
estado de su músculo y debe pasar por un largo proceso
para aprender a contraerlo voluntariamente.
De igual manera, para países en desarrollo es más
difícil adquirir y aplicar esta tecnología principalmente
por su costo. Por ejemplo la prótesis de mano BeBionic
manipulada a través de señales electro miograficas y/o
por una aplicación móvil [13] cuesta aproximadamente
10.000 dólares, y esto sin tener en cuenta el costo
del tratamiento de rehabilitación del paciente que ha
sufrido la lesión o amputación. En la figura 1 se muestra
la prótesis de mano BeBionic.
Figura 1. Prótesis de Mano Robótica BeBionic.
Figure 1. Robotic Prosthesis Hand BeBionic.

los nervios del paciente a la mano, con esta prótesis
es posible controlar la fuerza realizada en el agarre
de objetos y es manipulada con la interpretación de
señales cerebrales.
Actualmente, también se están utilizando interfaces
BCI, que hacen uso de sensores fisiológicos no invasivos,
usan una serie de electrodos para leer señales eléctricas
en el cuero cabelludo causadas por la actividad cerebral
y este tipo de BCI no invasiva, se puede conseguir
como una opción viable para el control de dispositivos
y aplicaciones mediante la interpretación de las señales
cerebrales; que son relativamente más accesible que sus
contrapartes médicas [7, 16].
2.2 Señales Encefalograficas (EEG)
Las señales eléctricas producidas por el cerebro
son generadas por la diferencia de potencial en la
membrana celular de las neuronas y este proceso es la
base del funcionamiento de nuestro sistema nervioso.
El registro de estas bioseñales es lo que se conoce como
Electroencefalograma (EEG) y los ritmos de la actividad
neuronal constituyen un lenguaje de comunicación
propio de las neuronas. A medida que han avanzado
los algoritmos de procesamiento de señales que ayudan
a comprender el significado de las señales EEG, se
han generado aplicaciones clínicas y tecnológicas
que hasta hace algunos años pertenecían al área de la
ciencia ficción [17]. En la figura 2 se muestran algunas
alteraciones físicas o mentales que pueden presentarse
en pacientes y que son diagnosticadas gracias a la
interpretación del electroencefalograma.
Figura 2. Diagnóstico de enfermedades o afecciones que
se pueden detectar a partir de la toma e interpretación
de un electroencefalograma.
Figure 2. Diagnosis of diseases or conditions that can
be detected from the making and interpretation of the
EEG.

Fuente: RSL Steeper, BeBionic. 2014 [13].
Las situaciones anteriores han llevado a recientes
investigaciones en la manipulación de las prótesis
utilizando interfaces cerebro – computador. Las
últimas investigaciones (altamente costosas) que se
han realizado en este campo, utilizan interfaces BCI
invasivas, es decir, que implican procedimientos
quirúrgicos dentro de la cabeza del paciente para la
conexión de los sensores que registran las señales
bioeléctricas producidas por el cerebro humano [14].
Uno de los proyectos más destacado en esta área es
el llamado Life Hand II, llevado a cabo por científicos
europeos (Italia, Suiza y Holanda) para procesos de
investigación [15]. Esta prótesis de mano robotizada es
capaz de reproducir el sentido del tacto, ya que conecta

Fuente: Correa, K. 2014.

101

Prótesis de mano virtual movida por señales encefalograficas – EEG

El registro de las señales EEG también es la base de
las denominadas BCI pertenecientes al grupo de las
interfaces naturales.
2.3 Interfaz Cerebro - Computador
Una BCI o interfaz cerebro computador pertenece al
grupo de interfaces naturales, estas se utilizan como
medio de interacción (HMI), debido a que permiten
la manipulación de aplicaciones o dispositivos
mediante el registro e interpretación de las señales
electroencefalográficas (EEG) sin la dependencia de
dispositivos mecánicos. El uso de interfaces naturales
por personas con discapacidad, es una novedosa
aplicación, ya que con estas y con el uso de prótesis
robóticas, el usuario puede realizar los movimientos
naturales que ejecutaba antes de perder su extremidad.
A continuación se presenta el casco Emotiv®.

Este proyecto utiliza como plataforma el prototipo
de prótesis virtual desarrollado por el grupo de
Automática Industrial de la Universidad del Cauca.
La aplicación utiliza Qt como framework de interface
de usuario (ventana), también hace uso de las librerías
de visualización gráfica VTK ampliamente utilizada
en aplicaciones médicas. El desarrollo se realizó en
lenguaje C++ en la plataforma Visual Studio 2010.
La aplicación permite manipular cada una de las
articulaciones de una mano virtual antropomórfica,
que se puede observar en la figura 4.
Figura 4. Prototipo de prótesis de mano virtual
antropomórfica.
Figure 4. Prototype anthropomorphic virtual prosthetic
hand.

2.4 Casco basado en la tecnología Emotiv®
Este casco cuenta con una BCI y con catorce electrodos
cerebrales no invasivos más dos referencias (ubicados
según el estándar 10-20), que permiten el registro
de las señales encefalográficas, específicamente los
ritmos: Delta (0.5 - Hz), Theta (4 - 8 Hz), Alpha (8 - 14
Hz), y Beta (14 - 26 Hz). Posteriormente estas señales
son enviadas al computador a través de Bluetooth. La
figura 3 muestra el casco Emotiv® [16].
Figura 3. Interfaz Cerebro Máquina- Casco basado en la
tecnología Emotiv®.
Figure 3. Brain Machine Interface – Headset based
Emotiv® technology.

3.2 Interfaz de Programación de la Aplicación (API)
Emotiv ofrece un conjunto de suites que permiten
la interpretación de las señales EEG y operan con
EmoEngine a través de Emotiv API. Estas suites
son: ExpressivTM, AffectivTM y CognitivTM. La
interpretación de las señales según la suite utilizada es
mostrada en el Control Panel Emotiv.

Fuente: Emotiv EEG Systems. 2014 [16].

El casco Emotiv® Neuroheadset cuenta con una
autonomía de 12 horas y la aplicación del fabricante
Test BenchTM que permite visualizar las señales EEG.
Ahora, el objetivo principal de este proyecto de
investigación, es la manipulación de una prótesis
virtual de mano utilizando señales EEG capturadas con
una BCI de bajo costo, en este caso con el casco Emotiv®
Neuroheadset, lo cual debería plantear un nuevo
paradigma para la manipulación de prótesis de mano,
y probablemente se solucionen los inconvenientes
presentados anteriormente.

Figura 5. Panel de Control de Emotiv. Izquierda.
Selección de Usuario y calidad de la señal. Derecha.
Cognitive Suite Panel.
Figure 5. Emotiv Control Panel. Left. Select User and
signal quality. Right. Cognitive Suite Panel.

3. METODOLOGÍA
3.1 Descripción del Sistema
102

Prospect. Vol. 14, No. 2, Julio - Diciembre de 2016, págs. 99-110

Figura 6. Disposición de la prótesis de mano virtual para
diferentes agarres. A. Pinza Fina. B. Agarre Cilíndrico.
C. Agarre Esférico. D. Mano Relajada.
Figure 6. Arrangement of virtual hand prosthesis for
differents grips. A. Fine Tip. B. Cylindrical Grasp. C.
Spherical Grasp. D. Relaxed Hand.

Fuente: Emotiv EEG Systems. 2014 [16].

Dentro del panel de control se presentan las diferentes
suites. Particularmente la suite CognitiveTM muestra
un cubo que puede ser manipulado (mover atrás,
adelante, a la izquierda, a la derecha, entre otras) a
través de las señales EEG después de una fase de
entrenamiento.
3.3 Integración del Sistema

Fuente: Correa, K. 2014.

La integración de la prótesis de mano virtual y el casco
Emotiv® se realizó con la API (específicamente con
la librería de enlace dinámico edk.dll) y el conjunto
de herramientas del SDK (Software Development
Kit), ya que este último provee las clases y funciones
en C++ para el manejo de las suites mencionadas
precedentemente.

que se muestra en la figura 7, después de superar la
etapa de entrenamiento, se generará una acción de
control que es interpretada por el software, esta indica
que la prótesis de mano virtual debe adaptarse para
el agarre de pinza fina. Similarmente se generan otras
acciones de control para la ejecución de los otros agarres
mencionados anteriormente.

3.4 Manipulación de la Prótesis de Mano Virtual

Figura 7. Prótesis de mano virtual en el agarre de pinza
fina y realimentación para el usuario.

Para la manipulación de la prótesis de mano virtual, se
escogió, que esta pueda realizar los agarres: de pinza
(tip), cilíndrico (cylindrical grasp) y esférico (spherical
grasp), igualmente debe poderse cerrar completamente
y abrirse. La manipulación de la mano se hace a
través de los pensamientos del usuario y se infiere
una intención de movimiento, para esta aplicación
se hace uso de la suite CognitiveTM donde el usuario
debe pasar y aprobar una etapa de entrenamiento,
en la cual imagina diferentes imágenes (relacionadas
a la apertura y diferentes grasp con su mano) y sus
patrones cerebrales son guardados y asociados a la
generación de las diferentes acciones de control para la
manipulación de la prótesis en los agarres mencionados
anteriormente. En la figura 6 se muestra la disposición
de la mano virtual en los diferentes tipos de agarres.
Si el usuario esta relajado (corresponde al entrenamiento
en el modo cognitivo a la acción neutral y se recomienda
que sea la primera acción entrenada) la prótesis de
mano virtual debe estar totalmente abierta, como
se observa en la figura 6D. Por ejemplo, si el usuario
mueve hacia atrás el cubo (a lo largo del eje z en la
interfaz de usuario)

Figure 7. Virtual hand prosthesis of fine tip grip and
feedback to the user.

Fuente: Correa, K. 2014.

En la figura 8, se presenta uno de los usuarios
manipulando la prótesis de mano virtual.

103

Prótesis de mano virtual movida por señales encefalograficas – EEG

Figura 8. Usuario manipulando la prótesis de mano
virtual.
Figure 8. User by manipulating the virtual hand
prosthesis.

desempeño que permiten establecer la viabilidad de la
manipulación de la mano virtual, y así extender estos
resultados para predecir y concluir si es posible y en qué
condiciones se puede manipular una prótesis robótica
de mano real con señales EEG captadas con el casco
Emotiv® Neuroheadset. En el estudio se realizaron
cinco pruebas con cuatro usuarios (pacientes) y se usan
como indicadores de desempeño: el porcentaje de éxito
en las acciones a realizar (para las primeras cuatro
pruebas) y el tiempo promedio por secuencia completada
correctamente; para la última prueba se usa como
indicador la matriz de contingencia o error [18-20].
4.1 Primera prueba
Esta consiste en que cada usuario realice una secuencia
que involucra dos acciones de la mano virtual, esta
es: cerrar mano y seguidamente abrir mano. Para ello
el usuario cuenta con 40 intentos durante cada sesión
(para realizar la secuencia 1). En las tablas 1 y 2 se
muestran los resultados obtenidos durante esta prueba.

Fuente: Correa, K. 2014.

3.5 Condiciones de Funcionamiento y Restricciones
Para que el sistema funcione correctamente se deben
cumplir las siguientes condiciones:

Tabla 1. Porcentajes de acierto de los usuarios, durante
la ejecución de la secuencia 1.
Table 1. Percentages of success of users during the
execution of sequence 1.

Todos los electrodos deben estar registrando
correctamente las señales EEG (indicadores en verde en
el Panel de Control de Emotiv).

PORCENTAJE DE ACIERTO

USUARIO

Cada usuario debe pasar por la etapa de entrenamiento
para realizar los agarres mencionados anteriormente y
debe obtener un skill rating mayor al 70%, esta tarea
varía según el usuario y puede ser realizada en un
tiempo promedio de seis horas.
El usuario debe tener el cabello corto y los felt sensors
deben estar humedecidos con el gel conductor.
El casco Emotiv Neuro Headset puede estar alejado del
receptor USB como máximo una distancia de un metro
para evitar la pérdida de la señal.

P1
P2
P3
P4

4. RESULTADOS Y DISCUSIÓN
A continuación se presentan los indicadores de

2:00 p. m.

6:00 p. m.

90,0

92,5

85,0

% de
Acierto
87,5
90,0
92,5

% de
Acierto
85,0
87,5
90,0

% de
Acierto
85,0
87,5
82,5

Tabla 2. Duración de la sesión con cada usuario durante
la primera prueba.
Table 2. Duration of session with each user during the
first test.

La aplicación está diseñada solo para trabajar con un
solo casco Emotiv Neuro Headset al tiempo, ya que
solo admite cargar el perfil de entrenamiento de un
usuario a la vez.
El número de agarres que se puede realizar con la
prótesis de mano virtual está restringido a tres y a estos
se les suma: mano abierta y completamente cerrada;
esta limitación se presenta para evitar falsos positivos.

9:00 a. m.

DURACIÓN
USUARIO

9:00 a. m.

2:00 p. m.

6:00 p. m.

Duración de la Sesión (minutos)
P1

35

40

37

P2

38

45

47

P3

40

34

42

P4

43

46

51

Se aclara que en la tabla 2, no se ha incluido el tiempo
de entrenamiento de cada usuario, este es en promedio
alrededor de 10h. Estas 10h de entrenamiento, son el
resultado de acumular sesiones de entrenamiento con

104

Prospect. Vol. 14, No. 2, Julio - Diciembre de 2016, págs. 99-110

una duración de máxima 15 minutos (se recomienda
esta duración en la sesión de entrenamiento para evitar
que el usuario pierda fácilmente la concentración). En
la figura 9 se resumen los porcentajes de éxito obtenidos
por los cuatro usuarios durante esta prueba.

Tabla 3. Porcentajes de acierto de los usuarios, durante
la ejecución de la secuencia 2.
Table 3. Percentages of success of users during the
execution of sequence 2.
PORCENTAJE DE ACIERTO

Figura 9. Comparación de los porcentajes de acierto de
los usuarios durante la primera prueba (secuencia 1).

9:00 a. m.

2:00 p. m.

6:00 p. m.

% de Acierto

% de Acierto

% de
Acierto

P1

82,5

80,0

72,5

P2

80,0

82,5

67,5

P3

87,5

75,0

77,5

P4

75,0

70,0

75,0

USUARIO

Figure 9. Comparison of the percentages of success
from users during the first test (sequence 1).

Tabla 4. Duración de la sesión con cada usuario durante
la segunda prueba.
Table 4. Duration of session with each user during the
second test.
DURACIÓN
Fuente: Correa, K. 2014.

USUARIO

Los resultados de esta prueba son satisfactorios,
indicando que cada uno de los usuarios puede
manipular la mano virtual con el casco Emotiv, para
que esta realice una secuencia sencilla. El mayor
porcentaje de acierto durante la prueba fue 92,5% y el
menor 82,5%, este último obtenido durante una sesión
nocturna.
4.2 Segunda prueba
Esta consiste en que cada usuario realice una secuencia
que involucra tres acciones de la mano virtual, esta es:
cerrar mano, abrir mano y finaliza con el agarre de pinza
fina. Para ello el usuario cuenta con 40 intentos durante
cada sesión (para realizar la secuencia 2). Se aclara que
para realizar cada una de las acciones de la mano virtual,
el usuario asocia una imagen (una para cada acción)
que es conocida durante la etapa de entrenamiento para
grabar el patrón de sus señales EEG, y concentrándose
en esta, se presenta el movimiento del cubo 3D y se
genera la acción determinada en la mano virtual. En
las tablas 3 y 4 se muestran los resultados obtenidos
durante la segunda prueba.
En la figura 10 se resumen los porcentajes de éxito
obtenidos por los cuatro usuarios durante esta prueba.
Los resultados de esta prueba son buenos, los
porcentajes de acierto (11 de 12) de los usuarios son
mayores o iguales al 70%, como era de esperarse el
tiempo promedio por secuencia aumentó con respecto a
la prueba anterior, esto debido a que el usuario necesita

9:00 a. m.

2:00 p. m.

6:00 p. m.

Duración de la Sesión (minutos)
P1

47

44

51

P2

42

45

46

P3

51

40

43

P4

40

53

61

Figura 10. Comparación de los porcentajes de acierto de
los usuarios durante la segunda prueba (secuencia 2).
Figure 10. Comparison of the percentages of success
from users during the second test (sequence 2).

Fuente: Correa, K. 2014.

mayor tiempo de concentración, más la adición de la
última acción realizada con la mano. Observando la
figura 10, los porcentajes de acierto de los usuarios P1
y P2 disminuyeron en la sesión nocturna, el usuario P2
mencionó que le era difícil concentrarse, por el contrario
el usuario P3 manifestó que en la prueba nocturna, le
fue más sencillo manipular la mano virtual.

105

Prótesis de mano virtual movida por señales encefalograficas – EEG

4.3 Tercera prueba
Esta consiste en que cada uno de los usuarios realice
una secuencia que involucra cuatro acciones de la
mano virtual, más la repetición de la acción abrir mano.
La secuencia es: cerrar mano, abrir mano, pinza fina,
abrir mano y finaliza con el agarre cilíndrico. Para ello
el usuario nuevamente cuenta con 40 intentos durante
cada sesión (para realizar la secuencia 3). En las tablas
5 y 6 se muestran los resultados obtenidos durante la
tercera prueba.
Tabla 5. Porcentajes de acierto de los usuarios, durante
la ejecución de la secuencia 3.
Table 5. Percentages of success of users during the
execution of sequence 3.
PORCENTAJE DE ACIERTO
9:00 a. m.

2:00 p. m.

6:00 p. m.

% de Acierto

% de Acierto

% de
Acierto

P1

42,5

40,0

40,0

P2

37,5

35,0

37,5

P3

37,5

40,0

35,0

P4

45,0

32,5

30,0

USUARIO

4.4 Cuarta prueba

Tabla 6. Duración de la sesión con cada usuario durante
la tercera prueba.
Table 6. Duration of session with each user during the
third test.
DURACIÓN
USUARIO

9:00 a. m.

2:00 p. m.

6:00 p. m.

Duración de la Sesión (minutos)
P1

61

70

51

P2

59

63

44

P3

64

62

57

P4

55

53

72

Los resultados de esta prueba muestran que los
porcentajes de acierto de los usuarios decrecieron
sustancialmente, estos están en el intervalo de 30% a
45%. Indicando que manipular la mano fiablemente
con cuatro acciones es muy difícil. Se debe recordar
que se está trabajando con señales EEG y una de las
posibles causas de esta caída en el porcentaje de acierto,
se debe a que el algoritmo que clasifica los patrones
cerebrales ya no puede distinguirlos fácilmente, al
igual que para el usuario es difícil repetir los mismos
patrones cerebrales durante la etapa de entrenamiento
y en la ejecución de las acciones que sigue la mano
virtual en una secuencia, que ahora se realiza en un
tiempo en el intervalo de 16,2s – 23,2s y que implica
mayor concentración y fatiga mental. Sin embargo, se
presenta una característica a destacar en esta prueba:
los mejores resultados obtenidos por los usuarios
fueron en la sesión de las 9:00 am; esta situación indica
que a los usuarios les fue más fácil concentrarte en esas
horas, debido a que la actividad cerebral es más estable
gracias al tiempo de descanso durante la noche [21].

En la figura 11 se resumen los porcentajes de éxito
obtenidos por los cuatro usuarios durante la tercera
prueba.

Esta consiste en que cada usuario realice una secuencia
más compleja, que involucra cinco acciones de la mano
virtual, más la triple repetición de la acción abrir mano.
La secuencia es: cerrar mano, abrir mano, pinza fina, abrir
mano, agarre cilíndrico, abrir mano y finaliza con el agarre
esférico. Para ello el usuario cuenta con 40 intentos
durante cada sesión (para realizar la secuencia 4). En
las tablas 7 y 8 se muestran los resultados obtenidos
durante la cuarta prueba.
Tabla 7. Porcentajes de acierto de los usuarios, durante
la ejecución de la secuencia 4.
Table 7. Percentages of success of users during the
execution of sequence 4.

Figura 11. Comparación de los porcentajes de acierto
de los usuarios durante la tercera prueba (secuencia 3).
Figure 11. Comparison of the percentages of success
from users during the third test (sequence 3).

Fuente: Correa, K. 2014.

106

PORCENTAJE DE ACIERTO
9:00 a. m.

2:00 p. m.

6:00 p. m.

% de Acierto

% de Acierto

% de
Acierto

P1

27,5

25,0

17,5

P2

35,0

37,5

35,0

P3

25,0

20,0

20,0

P4

20,0

22,5

17,5

USUARIO

Prospect. Vol. 14, No. 2, Julio - Diciembre de 2016, págs. 99-110

Tabla 8. Duración de la sesión con cada usuario durante
la cuarta prueba.
Table 8. Duration of session with each user during the
fourth test.

Tabla 9. Tiempos consolidados de los usuarios durante
las sesiones.
Table 9. Total time for the users in the sessions.

DURACIÓN
2:00 p. m.

SESIÓN

USUARIO

9:00 a. m.

6:00 p. m.

P1

79

70

83

P2

68

75

66

P3

64

62

71

P4

69

81

86

Duración de la Sesión (minutos)

En la figura 12 se resumen los porcentajes de éxito
obtenidos por los cuatro usuarios durante la cuarta
prueba.
Figura 12. Comparación de los porcentajes de acierto
de los usuarios durante la cuarta prueba (secuencia 4).
Figure 12. Comparison of the percentages of success
from users during the fourth test (sequence 4).

Duración de la Sesión (minutos)
P1

P2

P3

P4

Prueba 1

112

130

116

140

Prueba 2

142

133

134

154

Prueba 3

182

166

183

180

Prueba 4

232

209

197

236

Total

668

638

630

710

de la mano virtual utilizando el casco Emotiv, se
presenta la última prueba donde se quiere analizar
(utilizando la matriz de error) cómo el algoritmo de
caracterización de patrones cerebrales con EmoEngine
clasifica las acciones entrenadas por cada usuario sin
que este siga una secuencia preestablecida (o conocida
por este).
4.5 Quinta prueba

Los resultados obtenidos durante esta prueba
muestran que el porcentaje de acierto de los usuarios
en la ejecución de la secuencia son muy bajos, y no se
puede manipular la mano virtual confiablemente en
una secuencia que involucre las cinco acciones de la
mano mencionadas anteriormente. Entre los usuarios,
se destacó el P2, que aunque no obtuvo porcentajes de
éxito representativos, mejoraba su overall skill rating
en cada sesión más de entrenamiento, lo que fue
muy difícil para los otros usuarios. A medida que las
pruebas presentaban una secuencia que involucraba
más acciones, las sesiones fueron cada vez más largas
con los usuarios y estos manifestaban cansancio físico
y mental; incluso el usuario P4 manifestó que sufría
de fuertes dolores de cabeza después de cada sesión
de entrenamiento y pruebas (a partir de la segunda).
En la tabla 9 se resumen los tiempos totales de los
usuarios en las pruebas, sin incluir las largas jornadas
de entrenamiento.

Esta consiste en que cada usuario realice 56 acciones
(mano abierta, mano cerrada y los tres agarres
mencionados anteriormente) con la mano virtual. La
prueba incluye: 20 acciones abrir mano, 15 acciones
cerrar mano, 9 agarres de pinza fina, 6 agarres cilíndricos
y 6 agarres esféricos. En esta prueba el usuario no sabe
en qué secuencia se le van a solicitar los agarres. Sin
embargo, la persona que dirige la prueba, le presenta
al usuario cada una de las imágenes asociadas a las
diferentes acciones que puede realizar la mano virtual
con la finalidad que el usuario pueda responder
al estímulo presentado o a la acción requerida en
ese instante. Este último test trata de aproximar la
manipulación de la mano en un ambiente en el que el
usuario pueda utilizarla, de acuerdo a sus necesidades.
En la tabla 10 se muestran las matrices de contingencia
(de confusión o de error) para cada usuario durante la
quinta prueba; esta fue realizada por todos los usuarios
en horas de la mañana.
Los resultados obtenidos en esta prueba serán
favorables si las frecuencias relativas en las diagonales
de cada matriz tienden a 1 (implica un % de éxito
del 100). Ahora, los datos obtenidos en esta prueba,
muestran que varias de las acciones que intentan
realizar los usuarios son confundidas por el algoritmo
privado de clasificación que utiliza EmoEngine.

Aunque las pruebas 1, 2, 3 y 4 presentan información
de las que se pueden obtener conclusiones valiosas y
pertinentes acerca del desempeño de la manipulación
107

Prótesis de mano virtual movida por señales encefalograficas – EEG

Tabla 10. Matrices de contingencia para los usuarios P1, P2, P3 y P4.
Table 10. Contingency matrices for P1, P2, P3 and P4 users.
Usuario P1

AM

PF

AC

CM

AE

AM

6/20

1/20

0/20

6/20

7/20

AC

0/6

3/6

1/6

0/6

2/6

PF

0/9

1/9

6/9

1/9

1/9

CM

2/15

3/15

1/15

6/15

3/15

Usuario P2

AM

PF

AC

CM

AE

AE

1/6

1/6

2/6

1/6

1/6

AM

8/20

3/20

5/20

4/20

0/20

AC

0/6

3/6

2/6

1/6

0/6

PF

3/9

2/9

1/9

1/9

2/9

CM

2/15

5/15

2/15

5/15

1/15

Usuario P3

AM

PF

AC

CM

AE

AE

1/6

2/6

1/6

1/6

1/6

AM

8/20

2/20

0/20

5/20

5/20

AC

0/6

2/6

2/6

2/6

0/6

PF

1/9

4/9

2/9

1/9

1/9

CM

3/15

0/15

4/15

7/15

1/15

Usuario P4

AM

PF

AC

CM

AE

AE

0/6

1/6

1/6

0/6

3/6

AM

3/20

10/20

4/20

2/20

1/20

AC

0/6

4/6

1/6

0/6

1/6

PF

CM
AE

1/9

2/15
1/6

5/9

1/9

5/15

4/15

3/6

Donde: AM: abrir mano. PF: pinza fina. AC: agarre
cilíndrico. CM: cerrar mano. AE: agarre esférico.
Otra de las posibilidades es que para el usuario sea
muy difícil tratar de concentrarse en una acción en
especial, sin que el patrón que está asociado a dicha
acción sea confundido con otro, esta situación es
muy probable, debido a las mismas características de
las señales encefalográficas y a su fácil alteración con
cualquier estímulo que reciba el usuario.

2/6

0/9

3/15
0/6

2/9

1/15
0/6

robóticas colocadas en los pacientes, ya que estos
necesitan que su prótesis sea manipulada con un alto
porcentaje de confiabilidad y se pueda realizar diversos
tipos de agarres que los usuario requieran al manipular
su entorno.
5. CONCLUSIONES

Después de realizar las cinco pruebas con el sistema
y específicamente en las pruebas que involucran la
generación de las cinco acciones que se pueden realizar
con la mano virtual, y basándose en los índices de
desempeño presentados (por ejemplo: menores del
37,5% en la cuarta prueba), que esta BCI (Casco Emotiv
+ Modulo Cognitivo), no sería el más adecuado a
utilizar como interfaz de control de prótesis de mano
108

• En este artículo se presenta el proyecto de
manipulación de una prótesis de mano virtual,
utilizando señales encefalográficas, lo cual plantea
varios retos científicos y se resalta que este tipo
de estudios brindan otra forma de manipular
las prótesis de mano por parte de personas con
discapacidades.
• Las cinco pruebas realizadas para determinar el
desempeño del sistema arrojan como conclusión
que este sistema control (Casco Emotiv + Modulo

Prospect. Vol. 14, No. 2, Julio - Diciembre de 2016, págs. 99-110

Cognitivo), no sería el más confiable (porcentajes
de éxito en las pruebas con todos los agarres
mencionados por debajo del 37,5%) para la
manipulación de una prótesis robótica en un
paciente, ya que la interfaz de control de este tipo
de prótesis, debe presentar al usuario la posibilidad
de adaptarse a varios tipos de pre- shaping y
de agarres con un alto grado de repetibilidad
y reproducibilidad, y lógicamente debe poder
realizar los agarres que el usuario requiera para
desenvolverse en la vida cotidiana.
• Otra razón, por lo cual esta BCI no está lista para
la manipulación de una prótesis robótica en un
ambiente real (usando solo los pensamientos
del usuario), es porque las detecciones del
módulo cognitive son alteradas fácilmente por el
movimiento de este, implicando que el usuario
esté restringido en su movilidad.
• Aunque este sistema no es fiable actualmente para
la manipulación de prótesis de miembro superior
robotizadas, los resultados obtenidos en las pruebas
uno y dos muestran avances prometedores de esta
nueva técnica, ya que se obtuvieron porcentajes de
acierto en el intervalo 67,5 % a 92,5%. De acuerdo
a estos resultados, se espera que más adelante,
esta técnica pueda constituirse en una alternativa
al tradicional manejo de prótesis de mano a partir
de señales electromiográficas, y que sean varias las
intenciones de movimiento que se puedan detectar
con este tipo de interfaces naturales y lógicamente,
que se investigue en nuevos algoritmos que
puedan ser capaces de clasificar las diferentes
intenciones de movimiento o la detección de tareas
cognitivas que puedan ser asociadas a comandos
de manipulación de la prótesis de mano virtual.
REFERENCIAS
[1] D. Smith, “Prótesis de Extremidad Superior.
Segunda parte”, Motion, 17 (4), 1-7, 2007.
[2] S. Micera, E. Cavallaro, R. Belli, F. Zaccone, E.
Guglielmelli et al. (2003, September) “Functional
Assessment of Hand Orthopedic Disorders Using
a Sensorised Glove: Preliminary Results”, IEEE
International Conference on Robotics & Automation,
ICRA 2003, pp. 2212-2217.
[3] P. Dario, S. Micera, A. Menciassi, M.C. Carrozza,
M. Zecca, T. Steiglitz, T. Oses, X. Navarro, D. Ceballos,
(2002, October) "Cyberhand - A consortium project for
enhanced control of powered artificial hands based
on direct neural interfaces", 33rd Neural Prosthesis
Workshop.

[4] A. Kaysa, W. Suprijanto, A. Widyotriatmo. (2013,
August) “Design of Brain-Computer Interface
Platform for Semi Real-Time Commanding Electrical
Wheelchair Simulator Movement”. 3rd International
Conference on Instrumentation Control and
Automation.
[5]
W.
Ouyang,
K.
Cashion,
V.
Asari,
“Electroencephalograph Based Brain Machine
Interface for Controlling a Robotic Arm”. Department
of Electrical and Computer Engineering, University of
Dayton, Dayton, OH 45410, USA, 2013.
[6] S. Sun, C. Rosales, and R. Suarez. (2010, June) “Study
of Coordinated Motions of the Human Hand for
Robotics Applications,” The 2010 IEEE International
Conference on Information and Automation, pp. 776–
781, Harbin, China.
[7] J. Muñoz, C. Muñoz, O. Henao, “Diseño de una
Estación de Trabajo para Personas con Discapacidad
en Miembros Superiores Usando una Interfaz Cerebro
Computador”. Tecno Lógicas, Edición Especial,
Octubre 2013.
[8] A. Kawala-Janik, M. Podpora, M. Pelc, P. Piatek, J.
Baranowski. (2013, September) “Implementation of an
Inexpensive EEG Headset for the Pattern Recognition
Purpose”. The 7th IEEE International Conference
on Intelligent Data Acquisition and Advanced
Computing Systems: Technology and Applications,
Berlin, Germany.
[9] G. Matrone, C. Cipriani, E. Secco, G. Magenes, and
M. Carrozza, “Principal components analysis based
control of a multi-dof under actuated prosthetic
hand,” Journal of Neuro Engineering and Rehabilitation,
vol. 7, no. 1, 1-16, 2010.
[10] T. Wimbock, C. Ott, A. Albu - SChaffer, and
G. Hirzinger, “Comparison of object-level grasp
controllers for dynamic dexterous manipulation,” The
International Journal of Robotics Research, 31 (1), 3–23,
2012.
[11] Otto Bock Healthcare Products, “Michelangelo®”
[Acceso 4 abril, 2014]. Disponible http://www.
ottobock.com.
[12] Touch Bionics, “I-Limb Hand” [Acceso 5 de abril,
2014]. Disponible http://www.touchbionics.com
[13] RSL Steeper, “BeBionic” [Acceso 5 abril, 2014].
Disponible http://rslsteeper.com/
[14] S. Kim, J. Simeral, L. Hochberg et al, “Point and

109

Prótesis de mano virtual movida por señales encefalograficas – EEG

Click Cursor Control With an Intracortical Neural
Interface System by Humans With Tetraplegia”,
IEEE Transactions on Neural Systems and Rehabilitation
Engineering, 19 (2), 2011.

[18] C.A. Quinayas, C.A Gaviria, “Sistema de
Identificación de intención de movimiento para el
control mioeléctrica de una prótesis de mano robótica”,
Ingeniería y Universidad, 19 (1), 2015.

[15] G. Di Pino, A. Benvenuto, M. Tombini, G. Cavallo,
L. Denaro, V. Denaro et al. (2012, June) “Overview of
the implant of intraneural multielectrodes in human
for controlling a 5-fingeres hand prosthesis, delivering
sensorial feedback and producing rehabilitative
neuroplasticity”, 4th IEE RAS & EMBS International
Conference on Biomedical Robotics and Biomechatronics,
pp. 1831-1836, Roma, Italy.

[19] J. Muñoz, L. H. Ríos, O. A. Henao. (2014,
August) “Low Cost Implementation of a Motor
Imagery Experiment with BCI system and its use
in neurorehabilitation”,
Annual International
Conference of the IEEE Engineering in Medicine and
Biology Society, USA.

[16] Emotiv EEG Systems [Acceso 2 abril, 2014].
Disponible http://www.emotiv.com/
[17] N. Mohd, R. Jailani, H. Norhazman and N.
Mohamad. (2013, March) “Alpha and Beta Brainwave
Characteristics to Binaural Beat Treatment”. IEEE 9th
International Colloquium on Signal Processing and its
Applications, Kuala Lumpur, Malaysia.

[20] V. Gandhi, G. Prasad, D. Coyle, L. Behera, T. M.
McGinnity, “Evaluating Quantum Neural Network
filtered motor imagery brain computer interface using
multiple classification techniques”, Neurocomputing,
170 (C), 161-167, 2015.
[21] S.Grude, M. Freeland, C. Yang and H. Ma. (2013,
July) “Controlling mobile Spykee robot using Emotiv
Neuro Headset”. Proceedings of the 32nd Chinese
Control Conference, Xi’an, China.

110

