1996

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 14, NO. 4, DECEMBER 2013

[9]
[10]
[11]
[12]
Fig. 7. Behavior of rmse for different number of visible satellites.

VI. C ONCLUSION

[13]

A CP method has been presented for improving the performance of
the previous CP methods, fusing data from more sources including INS
and GPS Doppler shifts. The method is based on fusing GPS pseudoranges and Doppler shifts and the INS data of the participating vehicles.
The system is functional with at least four common visible satellites for
the vehicles. It was shown analytically and experimentally that adding
INS data can improve the precision of relative positioning compared
with the tight CP method without INS data. Precision improvements
of 10% and 24% were achieved over tight CP without INS and DGPSbased relative positioning, respectively. For temporary GPS outages,
experimental results show that up to 35% and 45% improvement
can be achieved over tight CP without INS and DGPS, respectively.
There is still a gap between the results of this work and submeter accuracy of relative positioning for crucial applications such as
collision avoidance. For this, more positioning information resources
such as in-vehicle odometer, radars, digital maps, and cameras can be
added to this process. These are considered as potential future works.

[14]

ACKNOWLEDGMENT
The authors would like to thank Dr. C. Hill and S. Ince from the Nottingham Geospatial Institute, University of Nottingham, Nottingham,
U.K., and A. Hasnur Rabiain from the Department of Infrastructure
Engineering, University of Melbourne, Parkville, Australia, for their
support and collaboration in the experiments conducted for this work.
R EFERENCES
[1] S. E. Shladover and S. K. Tan, “Analysis of vehicle positioning accuracy
requirements for communication-based cooperative collision warning,”
J. Intell. Transp. Syst.: Technol., Plann. Oper., vol. 10, no. 3, pp. 131–140,
Jul. 2006.
[2] E. D. Kaplan and C. J. Hegarty, Understanding GPS Principles and
Applications, 2nd ed. Norwood, MA, USA: Artech House, 2006.
[3] B. Hofmann-Wellenhof, H. Lichtenegger, and J. Collins, Global Positioning System Theory and Practice, 5th ed. New York, NY, USA: SpringerVerlag, 2001.
[4] R. Parker and S. Valaee, “Vehicular node localization using receivedsignal-strength indicator,” IEEE Trans. Veh. Technol., vol. 56, no. 6,
pp. 3371–3380, Nov. 2007.
[5] N. Patwari, A. O. Hero, M. Perkins, N. S. Correal, and R. J. O’Dea,
“Relative location estimation in wireless sensor networks,” IEEE Trans.
Signal Process., vol. 51, no. 8, pp. 2137–2148, Aug. 2003.
[6] N. Alam, A. T. Balaei, and A. G. Dempster, “Range and range-rate measurements using DSRC: Facts and challenges,” presented at the Int. Global
Navigation Satellite Systems (IGNSS) Symp., Surfers Paradise, Australia,
2009.
[7] N. Alam, “Three dimensional positioning with two GNSS satellites
and DSRC for vehicles in urban canyons,” in Proc. 24th Int. Tech.
Meet. Satellite Div. Inst. Navig. (ION GNSS), Portland, OR, USA, 2011,
pp. 3975–3983.
[8] N. Alam, A. T. Balaei, and A. G. Dempster, “A cooperative positioning
method for VANETs using DSRC carrier frequency offset,” presented

[15]

[16]
[17]

at the Int. Global Navigation Satellite Systems (IGNSS) Symp., Sydney,
Australia, 2011.
N. Alam, A. T. Balaei, and A. G. Dempster, “An instantaneous lane-level
positioning using DSRC carrier frequency offset,” IEEE Trans. Intell.
Transp. Syst., vol. 13, no. 4, pp. 1566–1575, Dec. 2012.
N. Alam, A. T. Balaei, and A. G. Dempster, “Relative positioning
enhancement in VANETs, a tight integration approach,” IEEE Trans.
Intell. Transp. Syst., vol. 14, no. 1, pp. 47–55, Mar. 2013.
T. S. Dao, K. Y. K. Leung, C. M. Clark, and J. P. Huissoon, “Markovbased lane positioning using intervehicle communication,” IEEE Trans.
Intell. Transp. Syst., vol. 8, no. 4, pp. 641–650, Dec. 2007.
E. Richter, M. Obst, R. Schubert, and G. Wanielik, “Cooperative relative
localization using vehicle-to-vehicle communications,” in 12th Int. Conf.
Inf. Fusion, Seattle, WA, USA, 2009, pp. 126–131.
N. El-Sheimy and X. Niu, “The promise of MEMS to the navigation
community,” Inside GNSS, pp. 46–56, Mar./Apr. 2007.
D. Titterton and J. Weston, Strapdown Inertial Navigation Technology,
2nd ed. Reston, VA, USA: Amer. Inst. Aeronaut. Astronaut., 2004.
G. Dissanayake, S. Sukkarieh, E. Nebot, and H. Durrant-Whyte, “The
aiding of a low-cost strapdown inertial measurement unit using vehicle
model constraints for land vehicle applications,” IEEE Trans. Robot.
Autom., vol. 17, no. 5, pp. 731–747, Oct. 2001.
M. S. Grewal and A. P. Andrews, Kalman Filtering Theory and Practice.
Englewood Cliffs, NJ, USA: Prentice-Hall, 1993.
FCC 02-302, Amendment of the Commission’s Rules Regarding Dedicated Short-Range Communication Services in the 5.850–5.925 GHz
Band (5.9 GHz Band), 2002.

A Head-Up Display-Based P300 Brain–Computer
Interface for Destination Selection
Luzheng Bi, Xin-An Fan, Nini Luo, Ke Jie, Yun Li, and Yili Liu

Abstract—In this paper, we propose a P300 brain–computer interface
(BCI) with visual stimuli presented on a head-up display and we apply this
BCI for selecting destinations of a simulated vehicle in a virtual scene. To
improve the usability of the selection system, we analyze the effects of the
number of electroencephalogram (EEG) rounds on system performance.
Experimental results from eight participants show that the BCI-based
model of destination selection can be built with EEG data from eight
channels, and participants can use this BCI to select a desired destination
with an accuracy value of 93.6% ± 1.6% (mean value with standard
error) in about 12 s of selection time. This paper lays a foundation for
developing vehicles that use a BCI to select a desired destination from a list
of predefined destinations and then use an autonomous navigation system
to reach the desired destination.
Index Terms—Destination selection, head-up display (HUD),
human–vehicle interaction, intelligent driver assistant systems, P300
brain–computer interface (BCI).

Manuscript received December 11, 2012; revised February 17, 2013, April 5,
2013, and May 16, 2013; accepted May 30, 2013. Date of publication July 9,
2013; date of current version November 26, 2013. This work was supported
in part by the National Natural Science Foundation of China under Grant
61004114 and Grant 90920304 and in part by the Fundamental Research Funds
for the Central Universities under Grant 2012CX01018. The Associate Editor
for this paper was L. Li.
L. Bi, X.-A. Fan, N. Luo, K. Jie, and Y. Li are with the School of Mechanical
Engineering, Beijing Institute of Technology, Beijing 100081, China (e-mail:
bhxblz@bit.edu.cn; 3120100145@bit.edu.cn; niniluo@bit.edu.cn; qjyjyuj@
sina.com; liyun20@126.com).
Y. Liu is with the Department of Industrial and Operations Engineering, University of Michigan, Ann Arbor, MI 48109 USA (e-mail: yililiu@umich.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TITS.2013.2266135

1524-9050 © 2013 IEEE

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 14, NO. 4, DECEMBER 2013

I. I NTRODUCTION
Using the human “mind” rather than limbs to select destinations
for vehicles has significant practical values. Such systems can help
people with severe neuromuscular disorders convey their desired
destinations to intelligent vehicles in order to expand their scope of
mobility and improve their quality of life. For the broader driving
community, the destination selection systems have the potential to
facilitate the development of intelligent driving systems to better assist
drivers [1].
Brain–computer interfaces (BCIs) can translate user brain activity
patterns into corresponding commands to communicate with or control
the external world without using conventional communication channels (i.e., muscles and speech). In contrast, speech and eye-tracking
interface systems often have special requirements for user speech and
eye movement capabilities.
Signal recording of brain activities used by BCIs can be either invasive or noninvasive. Compared with invasive BCIs, noninvasive BCIs
do not require surgery to implant electrodes directly on or inside the
cortex. Noninvasive BCIs can use various brain signals as inputs, such
as electroencephalograms (EEGs), magnetoencephalograms, bloodoxygen-level-dependent signals, and (de) oxyhemoglobin concentrations [2]. Due to the low cost and convenience of use, EEG has been the
most popular signal used in developing BCI systems. The brain signals
widely used to develop EEG-based BCIs include: 1) P300 potentials,
which are a positive potential deflection on the ongoing brain activity
signal at latency of roughly 300 ms after the random occurrence of
a desired target stimulus from nontarget stimuli [3]; 2) steady-state
visual evoked potentials (SSVEPs), which are visually evoked by a
stimulus modulated at a fixed frequency and occur as an increase in
EEG activities at the stimulus frequency [4]; and 3) the event-related
desynchronizations (ERDs) and event-related synchronization (ERS),
which are induced by performing mental tasks such as motor imagery,
mental arithmetic, or mental rotation [5].
Researchers have developed various applications of EEG BCIs,
such as controlling a cursor on the screen [6], [7], selecting letters
from a virtual keyboard [8], [9], browsing the Internet [10]–[12],
and playing games [13], [14]. Recently, BCIs based on EEG signals
have been used to control wheelchairs to help bring mobility back to
some severely disabled people. Generally, there are two main groups
of methods to develop brain-controlled wheelchairs. One is to use
a BCI to translate user intentions into motion commands (such as
turning left and right and accelerating) for controlling a wheelchair
[15]–[18]. The other is to use a BCI to select a desired destination
from a list of destinations and then use an autonomous system to
drive the wheelchair to reach the desired destination. Since P300-based
BCI systems are more suitable to output more commands compared
with SSVEP-based and ERD/ERS-based BCI systems and have a
relatively high level of accuracy, P300-based BCIs are currently used
for destination selections [19]–[21]. A more detailed review regarding
brain-controlled wheelchairs can be seen in our paper [1].
Few researchers have explored how to drive a vehicle using EEG
signals. In the same spirit as that of the first group of methods to
develop brain-controlled wheelchairs described above, Rojas is likely
to be the first to propose using EEG to drive a vehicle [22]. Hood et al.
[23] used an SSVEP-based BCI to control a simulated car to turn left,
turn right, and go straight in a driving simulator. The preliminary experimental results provide an indication of the feasibility of using EEG
to drive a vehicle. However, they did not describe the specific speed of
the controlled simulated car and the specific accuracy. Furthermore,
Gohring et al. [24] applied a commercial BCI product (i.e., the EPOC
cap from Emotiv) to control a vehicle as a way to test the feasibility
of using brain signals to control a vehicle. The BCI embedded in the

1997

commercial EPOC cap is an ERD/ERS-based BCI, which can issue
at most four commands by analyzing EEG signals from 16 channels.
Gohring et al. tested their system with only one subject on a closed
airfield. The experimental result showed poor steering control (turning
left and right) performance even if it only involved a constant speed
of 2 m/s. The maximum and the standard deviation of the lateral error
reached about 10 and 2 m, respectively, and the maximum and the
standard deviation of the angular error were over 150◦ and more than
10◦ , respectively. This shows that currently, it is rather difficult, if not
impossible, to directly control a vehicle in the real world by using a
BCI to issue motion commands.
Considering that driverless techniques have been gradually reaching
maturity [25], one possible method to deal with this problem is to first
use a P300 BCI to select a desired destination from predefined ones
and then use an autonomous navigation system to drive the vehicle to
reach the desired destination, just like the second group of methods to
develop brain-controlled wheelchairs. However, all P300 BCI systems
require external stimulation, and all the existing P300 systems for
wheelchair applications use an LCD monitor to show P300 visual
stimuli. When these P300 BCI systems are applied to vehicles, several
limitations exist. First, showing visual stimuli on an LCD/CRT is not
suitable and not natural for vehicles. Second, it greatly obstructs the
field of view of disabled users. It is quite undesirable in vehicles for
drivers to use the P300 BCI to select a desired destination and use
another BCI system to issue motion control commands (e.g., stopping)
according to the surroundings. In contrast, since wheelchairs run
slowly (i.e., the maximum speed of all brain-controlled wheelchairs
is less than 1 m/s) and in familiar environments, users need to pay
less attention to the surroundings. Of course, the proposed system can
be also applied to brain-controlled wheelchairs. Third, the P300 BCI
destination selection systems for brain-controlled wheelchairs use over
15 channels of EEG signals. In practice, it is desirable to use fewer
channels for better usability of the selection system since these EEG
electrodes need to be placed on the scalp of drivers and thus may
disturb the drivers.
Our long-term goal is to develop a brain-controlled vehicle by using
BCI systems to select a destination and issue a control command. In
this paper, we propose a new P300 BCI with visual stimuli presented
on a windshield via a head-up display (HUD) and develop a destination
selection system for a simulated vehicle using the proposed BCI.
Furthermore, to improve the usability of this destination selection
system, we analyze the effects of the number of rounds of EEG on
the performance of the proposed system. This paper lays a foundation for developing a brain-controlled vehicle that uses a BCI to
select a desired destination from a list of predefined destinations
and then uses an autonomous navigation system to reach the desired
destination.
The remainder of this paper is organized as follows. In Section II, we
introduce the method of developing a HUD-based P300 BCI system
for destination selection. Section III describes the experiments, whose
results are described in Section IV. In Section V, we conclude this
paper with a discussion of the current challenges and future research
directions.

II. HUD-BASED P300 BCI FOR D ESTINATION S ELECTION
A. Visual Stimuli
The P300 visual stimuli used in this paper are a 3 × 3 matrix
of characters, which are displayed on a real windshield (whose top,
bottom, left, and right edges are 102, 138, 59, and 59 cm, respectively)
of vehicles via a HUD system constructed by ourselves. Each character
(i.e., stimulus) represents a predefined destination. For example, “B”

1998

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 14, NO. 4, DECEMBER 2013

Fig. 2.
black.

Fig. 1. HUD system.

represents “Bank” and “H” represents “Hospital.” In practice, each
stimulus can be also an image or the full spelling of a destination.
The HUD system mainly consists of a projecting device (including
a portable projector and several pieces of mirrors) and some light
reflection films (17.5 cm × 13 cm in size) pasted on the bottom-left
area of the windshield (i.e., the top and left edges of the films are
32 cm to the top edge and 25 cm to the left edge of the windshield,
respectively), as shown in Fig. 1. All of the nine characters flash on the
windshield in sequence and in random order in each round. Each flash
lasts 125 ms with an interstimulus interval of 15 ms, and thus, each
round takes 1260 ms ((125 + 15) × 9). When the user wants to reach
a destination, he focuses attention on the character associated with the
destination, and the BCI interprets the EEG to infer the character to
which the user is attending.

Placements of eight channels used to collect EEG data are marked in

that a P300 potential is a positive potential deflection on the ongoing
brain activity at latency of roughly 300 ms after the random occurrence
of a desired target stimulus among nontarget stimuli, the 0–512-ms
EEG potentials from the onset of each character being intensified are
selected as preliminary features. In other words, there are 256 data
values for each channel and 2048 (256 × 8) for all of the 8 channels
(i.e., the number of the preliminary features is 2048). To reduce the
feature dimensionality in order to lower the redundancy of the features,
the principal component analysis (PCA) is used, which transforms
the feature space into an orthogonal space consisting of uncorrelated
variables called principal components, and selects components with
the highest eigenvalues as new features. Finally, the components with
the highest N eigenvalues are selected as new features, which account
for above 95% of the original features.
Thus, the input to the classifier of P300 is a vector of N dimensions,
as follows:
x = [x(1), x(2), . . . , x(N )]

where x(N ) is the N th new feature of each sample selected from the
original features by using PCA.
Since the Fisher linear discriminant is simple and widely used in
P300 recognition [26], it was first used to develop the classifier, which
can be represented in the following form:
y = wT x

B. Data Collection
We used a 16-channel amplifier to acquire the EEG signals at eight
standard locations (i.e., Fz, Cz, Pz, Oz, P3, P4, P7, and P8), as shown in
Fig. 2. The reference potential was the average of the potentials of the
left and right earlobes. The EEG signals were amplified and digitalized
with a sampling rate of 1000 Hz and a power-line notch filter to remove
the line noise.

(2)

where w is the projection direction determined by maximizing the
following cost function:
JF (w) =

w T Sb w
w T Sw w

(3)

where Sb is the between-class scatter matrix, and Sw is the withinclass scatter matrix.

C. Signal Processing and Classification
The P300 is classified according to the following method. The
collected EEG data are first decimated by a factor of 2 and filtered with
a bandpass filter between 0.53 and 15 Hz. In addition, to improve the
signal-to-noise ratio, M rounds of EEG data are summed. Considering

(1)

III. E XPERIMENT
A. Subjects
Eight healthy male subjects (aged 21–27) participated in the
experiments voluntarily and received no monetary compensation.

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 14, NO. 4, DECEMBER 2013

1999

The work procedure of the communication system is as follows.
When the virtual vehicle gets started, the client first automatically
connects the server and then waits to receive the desired destination
from the server. The server sends the desired destination upon the
BCI system attaining it and transmits the destination to the navigation system of the virtual vehicle, which controls the vehicle to the
destination.
C. Experimental Procedures

Fig. 3.

Block diagram of the brain-controlled virtual vehicle.

All subjects have driving licenses. However, they drive less than
100 km each year except for two subjects. All subjects had no history
of brain disease, and their visual acuity was normal or normal after
adjustment.
B. Experimental Platform
A simulated vehicle has been designed and constructed. It includes
three main parts, as shown in Fig. 3: 1) the HUD-based P300 BCI
system for destination selection, which consists of the P300 visual
stimuli presented on the windshield via a HUD, the EEG measurement
system, and the proposed signal processing and classification module,
as described in Section II; 2) the 3-D driving scene and simulated
vehicle based on the virtual reality technology; and 3) the communication system between the computer supporting the 3-D driving scene
and virtual vehicle and the other running the EEG acquisition, signal
processing, and classification module.
1) Three-Dimensional Driving Scene and Virtual Vehicle: We simulated the virtual vehicle and the 3-D driving scene with MATLAB/
Simulink. The detailed development procedure of the scene based
on virtual reality technology is as follows. First, we created models
of various objects (such as cars, houses, roads, and trees) for the
scene with 3-D Max. Then, we exported them into the 3-D animation
module of MATLAB/Simulink and set up the corresponding positions,
attitudes, and other parameters. After that, we developed the program
of controlling the virtual vehicle to reach a destination using the
user-defined functions module of MATLAB/Simulink. Developing
an autonomous navigation system is beyond the scope of this paper, whose purpose is to show how to develop a HUD-based P300
BCI and apply it to select a predefined destination for driving a
vehicle.
2) Communication System: The communication system integrated
the HUD-based P300 BCI system with the virtual vehicle, which
operated as the link between them. The communication system was
based on a Transmission Control Protocol/Internet Protocol (TCP/IP)
connection between the computer running the BCI and the computer
supporting the 3-D driving scene and virtual vehicle. The software
architecture consisted of a server embedded in the BCI system, which
was implemented with SOCKET API in VC++, and a client integrated
within the simulation system of the virtual vehicle, which was implemented with the TCP/IP receive submodule of the Instrument Control
Toolbox module of MATLAB/Simulink.

Before the start of the experiment, we explained the experimental
procedure to the participants so that they become familiar with the
experimental protocol. We displayed the visual stimuli on a real
windshield via the constructed HUD. The subjects were asked to sit in
a seat in front of the windshield with the similar distance of a vehicle
driver to the windshield and similar vision angles when driving real
vehicles. We adjusted the position, size, and intensity of the visual
stimuli and set the parameters of the EEG collection system. We pasted
electrodes on the scalp of the subjects properly. Before the start of
data collection, the contact impedance between the scalp and EEG
electrodes was calibrated to be below 10 kΩ.
The experimental procedure includes two phases: The first is for
training the HUD-based P300 BCI model for destination selection, and
the second one is for evaluating and testing the destination selection
system. Participants were given a 15-min break between the two
phases.
The experimental stage began with several minutes of practice. In
the phase of training the destination selection model, each participant
completed four sessions of the P300 experiment in order to collect
the data and train the model offline. In each session, the subjects
were required to concentrate their attention on the corresponding
target character of the interface matrix according to the predefined
and randomized sequence of nine characters, representing nine destinations. The simple method to concentrate attention is to count the
flashing times of the target character in the head, and the subjects
were instructed to do so. Each character flashed ten rounds, and there
was a pause of 3 s between the two successive target characters. Each
character was used as the target once per session. Thus, we collected 36
(9 targets/session × 4 sessions) epochs of target data (P300 data) and
288 (8 nontargets × 9 × 4) epochs of nontarget data (non-P300 data)
for each subject. However, to balance the target and nontarget samples,
we used all 36 target samples and randomly selected 72 samples from
all the collected nontarget samples to train the corresponding model of
each subject.
In the second phase, we investigated the effects of the number
of EEG rounds on the performance of the proposed system and
demonstrated the simulated vehicles based on the destination selection
system using a static and a dynamic experiment, respectively. In the
static experiment with 45 target samples (destination selections), the
selection system only outputs the recognition result in real time; it
does not actually control the simulated vehicle. Fifteen minutes after
the participants finished the static experiment, we demonstrated the
proposed system using a dynamic experiment with ten destination
selections, in which the simulated vehicle needs to autonomously reach
the destination generated by the selection system.
IV. R ESULTS
We used the 108 training samples collected in the experimental
procedure to determine the parameters of the model of each subject,
and 50 features selected by PCA are used for each model. In practice,
first, it is desirable for the destination selection system to have high
accuracy and short selection time. Since the number of rounds of EEG

2000

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 14, NO. 4, DECEMBER 2013

Fig. 4. Accuracy of the models as a function of the number of rounds of EEG
data in testing.

Fig. 5. Model accuracy as a function of the number of rounds of EEG data
used in training the models.

used online by the selection system mainly determines the selection
time, we investigated the destination selection accuracy as a function
of the number of rounds when this system is used online. Second, given
the required accuracy and selection time, it is desirable to reduce the
time needed to build the corresponding model. Since the training time
mainly depends on the number of rounds of EEG data used to build
the models, we analyzed the selection accuracy as a function of the
number of rounds used in building the model of each subject given the
required selection time.

of rounds and ten rounds are not significant. Furthermore, the model
accuracy (92.5% ± 1.7%) built by using five rounds almost reaches
the desired accuracy and level of stability, which suggests that the
duration of the training session can be decreased by at least about
50%. In other words, the BCI recognition model can be built with EEG
data of at most about 4 min, given the required accuracy and selection time.

A. Destination Selection Accuracy
Fig. 4 shows the accuracy of the HUD-based BCI system used to
select a destination as a function of the number of rounds of EEG
used online by the model of the selection system. In this figure, the
x-axis represents the number of rounds whereas the y-axis represents
the accuracy gained if the BCI system selects a destination using
only as many rounds as the x-axis. In other words, the models were
built by using ten rounds of EEG data from eight channels, and they
were tested by using from 1 to 10 rounds of EEG data from eight
channels.
As shown in Fig. 4, the average accuracy with standard error is
71.4% ± 2.9% by the third round (about 4 s), 90.3% ± 1.5% by the
eighth round (about 10 s), and smoothly rises to 93.6% ± 1.6% by the
tenth round (about 12 s). Furthermore, t-test shows that the accuracy
increases marginally significantly between the eight rounds and ten
rounds (p = 0.033).
B. Reducing the Time Needed to Build a Model
Given the desired accuracy and selection time (such as 93.6% ±
1.6% with about 12 s) to explore whether we can reduce the time
needed to build the models, we analyzed the accuracy as a function
of the number of rounds used in building the model of each subject.
Fig. 5 shows the accuracy of the models as a function of the number
of rounds used to build models by using ten rounds of EEG data
to test each model. In this figure, the x-axis represents the number
of rounds of EEG data used to train a model whereas the y-axis
represents the accuracy of the model by using ten rounds of EEG
data to select a destination. Pairwise t-tests show that the accuracy
difference between using three rounds and ten rounds is significant
(p = 0.01), whereas the differences between using another number

C. Demonstration Result of a Simulated Vehicle Based
on the Destination Selection
To demonstrate and further test the proposed system, we conducted
a dynamic experiment, in which the simulated vehicle needs to autonomously reach the destination generated by the selection system.
The experimental result from eight subjects shows that the proposed
system built with ten-round EEG data from eight channels can achieve
an accuracy value of 93.8% ± 3.8% (mean value with standard error)
with the selection time of about 12 s.
V. D ISCUSSION AND C ONCLUSION
As reported in this paper, we have developed a new P300 BCI
with visual stimuli presented on a vehicle windshield via a HUD and
applied this HUD-based BCI to destination selection for a simulated
vehicle in a virtual scene. Experimental results from eight participants
show that using EEG data from eight channels to train a destination selection model, the destination selection system can effectively function
with an accuracy value of 93.6% ± 1.6% (mean value with standard
error) in about 12 s of selection time.
The proposed method has almost the same average accuracy as those
BCI systems applied in wheelchair control. Compared with the P300
BCI destination selection systems for brain-controlled wheelchairs, the
proposed system has at least two strengths. First, this system is more
suitable and natural for a vehicle and less likely to obstruct the user’s
field of view. Second, it uses fewer channels to collect EEG data and
build the system without reducing accuracy and thus help improve its
usability.
In practice, the proposed method is most suitable for people with
severe neuromuscular disorders. However, it can be also used by
healthy individuals, e.g., for selecting desired destinations and radio
channels.

IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS, VOL. 14, NO. 4, DECEMBER 2013

The work reported in this paper represents a promising beginning
of developing EEG-based brain-controlled vehicles. It raises many
questions and opens many future research opportunities along this
direction. First, although the accuracy of using this BCI to select
a desired destination is 93.6% ± 1.6% (mean value with standard
error) with the selection time of about 12 s, this accuracy may not
be high enough for some applications. This issue can be addressed
by using another BCI (such as an SSVEP BCI) or one P300 stimulus
as a validation stimulus to perform some validations before sending
the selected destination to an autonomous vehicle. Although selection
time will increase, its usefulness will not suffer since destination
selection occurs rather infrequently during the whole process of
driving.
Second, the number of predefined destinations of the current system
is small. This problem can be handled by using multiple pages of
destinations with two of the stimuli on each page as page-turning
commands.
Third, the users lose their control of the vehicle after the BCI-based
destination selection system sends a destination to the autonomous
vehicle. This issue can be addressed by combining the proposed
BCI with other BCIs for issuing related motion commands (e.g.,
stopping).
Fourth, we conducted the experiment in a laboratory, where the
environmental factors were constant. However, in practice, vehicles
are used outdoors. Since the P300 stimuli of the HUD-based BCI
are displayed on a transparent windshield, some outdoor factors such
as weather cannot be controlled and may influence the elicitation
of the P300 component. Real-world driving environments may have
significant effects on users’ psychological states, which may affect
EEG signals and thus affect the BCI system performance. These issues
need to be investigated in future research, and the current work lays a
foundation for future work along this direction.
Fifth, we obtained the results (shown in Fig. 4) of the accuracy of the
HUD-based BCI as a function of the number of rounds of EEG used
online by using ten rounds of EEG data to build the models. However,
it is not clear whether the results would be affected when different
numbers of rounds are applied to develop the models.
Our future work focuses on addressing the issues listed above,
including testing the proposed destination selection in real-world driving, investigating the effects of some outdoor environmental factors
on system performance, and developing brain-controlled vehicles by
combining the HUD-based BCI with other BCIs for issuing motion
commands. The current and future research in this area will help
further improve the mobility, independence, and quality of life for
people with disabilities, as well as the general public.
ACKNOWLEDGMENT
The authors would like to thank the volunteers for their participation
in the experiments.
R EFERENCES
[1] L. Bi, X. Fan, and Y. Liu, “EEG-based brain-controlled mobile robots:
A survey,” IEEE Trans. Hum. Mach. Syst., vol. 43, no. 2, pp. 161–176,
Mar. 2013.
[2] A. Nijholt, D. Tan, G. Pfurtscheller, C. Brunner, J. D. R. Millán,
B. Allison, B. Graimann, F. Popescu, B. Blankertz, and K. R. Müller,
“Brain–computer interfacing for intelligent systems,” IEEE Intell. Syst.,
vol. 23, no. 3, pp. 72–79, May/Jun. 2008.
[3] L. A. Farwell and E. Donchin, “Talking off the top of your head: Toward
a mental prosthesis utilizing event related brain potentials,” Clin. Neurophysiol., vol. 70, no. 6, pp. 510–523, Dec. 1988.
[4] E. E. Sutter, “The brain response interface: Communication through
visually-induced electrical brain response,” J. Microcomput. Appl.,
vol. 15, no. 1, pp. 31–45, Jan. 1992.

2001

[5] G. Pfurtscheller and F. H. Lopes da Silva, “Event-related EEG/MEG
synchronization and desynchronization: Basic principles,” Clin. Neurophysiol., vol. 110, no. 11, pp. 1842–1857, Nov. 1999.
[6] J. R. Wolpaw, D. J. McFarland, G. W. Neat, and C. A. Forneris, “An EEGbased brain–computer interface for cursor control,” Electroencephalogr.
Clin. Neurophysiol., vol. 78, no. 3, pp. 252–259, Mar. 1991.
[7] B. Z. Allison, C. Brunner, C. Altstatter, I. C. Wagner, S. Grissmann,
and C. Neuper, “A hybrid ERD/SSVEP BCI for continuous simultaneous
two dimensional cursor control,” J. Neurosci. Methods, vol. 209, no. 2,
pp. 299–307, Aug. 2012.
[8] N. Birbaumer, N. Ghanayim, T. Hinterberger, I. Iversen, B. Kotchoubey,
A. Kubler, J. Perelmouter, E. Taub, and H. Flor, “A spelling device for the
paralyzed,” Nature, vol. 398, no. 6725, pp. 297–298, Mar. 1999.
[9] B. Hong, F. Guo, T. Liu, X. Gao, and S. Gao, “N200-speller using motiononset visual response,” Clin. Neurophysiol., vol. 120, no. 9, pp. 1658–
1666, Sep. 2009.
[10] M. Bensch, A. A. Karim, J. Mellinger, T. Hinterberger,
M. Tangermann, M. Bogdan, W. Rosenstiel, and B. N. Nessi, “An
EEG controlled web browser for severely paralyzed patients,” Comput.
Intell. Neurosci., vol. 2007, pp. 71863-1–71863-5, 2007.
[11] A. A. Karim, T. Hinterberger, and J. Richter, “Neural Internet: Web surfing
with brain potentials for the completely paralyzed,” Neurorehab. Neural
Repair, vol. 20, no. 4, pp. 508–515, Dec. 2006.
[12] E. Mugler, M. Bensch, S. Halder, W. Rosenstiel, M. Bogdan,
N. Birbaumer, and A. Kubler, “Control of an Internet browser using
the P300 event-related potential,” Int. J. Bioelectromagn., vol. 10, no. 1,
pp. 56–63, 2008.
[13] R. Krepki, B. Blankertz, G. Curio, and K. R. Müller, “The Berlin
Brain–Computer Interface (BBCI): Towards a new communication channel for online control in gaming applications,” J. Multimedia Tools Appl.,
vol. 33, no. 1, pp. 73–90, Apr. 2007.
[14] M. W. Tangermann, M. Krauledat, K. Grzeska, M. Sagebaum,
C. Vidaurre, B. Blankertz, and K. R. Müller, “Playing pinball with noninvasive BCI,” in Proc. 22nd Annu. Conf. Neural Inf. Process. Syst.,
Vancouver, Canada, 2008, pp. 1641–1648.
[15] K. Tanaka, K. Matsunaga, and H. O. Wang, “Electroencephalogram-based
control of an electric wheelchair,” IEEE Trans. Robot., vol. 21, no. 4,
pp. 762–766, Aug. 2005.
[16] G. Vanacker, J. del R. Millan, E. Lew, P. W. Ferrez, F. G. Moles, J. Philips,
H. V. Brussel, and M. Nuttin, “Context-based filtering for brain-actuated
wheelchair driving,” Comput. Intell. Neurosci., vol. 2007, pp. 25130-1–
25130-12, May 2007.
[17] K. Choi, “Control of a vehicle with EEG signals in real-time and system evaluation,” Eur. J. Appl. Physiol., vol. 112, no. 2, pp. 755–766,
Jun. 2011.
[18] C. Mandel, T. Luth, T. Laue, T. Rofer, A. Graser, and B. Krieg-Bruckner,
“Navigating a smart wheelchair with a brain–computer interface interpreting steady-state visual evoked potentials,” in Proc. IEEE/RSJ Int. Conf.
Intell. Robots Syst., St. Louis, MO, USA, pp. 1118–1125.
[19] B. Rebsamen, C. Guan, H. Zhang, C. Wang, C. Teo, M. H. Ang, and
E. Burdet, “A brain controlled wheelchair to navigate in familiar environments,” IEEE Trans. Neural Syst. Rehab. Eng., vol. 18, no. 6, pp. 590–598,
Dec. 2010.
[20] I. Iturrate, J. M. Antelis, A. Kubler, and J. Minguez, “A noninvasive brainactuated wheelchair based on a P300 neurophysiological protocol and
automated navigation,” IEEE Trans. Robot., vol. 25, no. 3, pp. 614–627,
Jun. 2009.
[21] C. Escolano, J. M. Antelis, and J. Minguez, “A telepresence mobile
robot controlled with a noninvasive brain–computer interface,” IEEE
Trans. Syst., Man, Cybern. B, Cybern., vol. 42, no. 3, pp. 793–804,
Jun. 2012.
[22] [Online].
Available:
http://spectrum.ieee.org/automaton/robotics/
robotics-software/braindriver-a-mind-controlled-car
[23] D. Hood, D. Joseph, A. Rakotonirainy, S. Sridharan, and C. Fookes, “Use
of brain computer interface to drive: Preliminary results,” in Proc. 4th
Int. Conf. Autom. User Interfaces Interactive Veh. Appl., Portsmouth, NH,
USA, Oct. 2012, pp. 103–106.
[24] D. Gohring, D. Latotzky, M. Wang, and R. Rojas, “Semi-autonomous car
control using brain computer interfaces,” in Adv. Intell. Syst. Comput.,
2013, vol. 194, pp. 393–408.
[25] [Online].
Available:
http://singularityhub.com/2012/09/27/
california-becomes-third-state-to-legalize-driverless-cars/
[26] M. Salvaris and F. Sepulveda, “Visual modifications on the P300
speller BCI paradigm,” J. Neural Eng., vol. 6, no. 4, p. 046011,
Aug. 2009.

