EmoActivity - An EEG-Based Gamified Emotion HCI for
Augmented Artistic Expression: The i-Treasures
Paradigm
Vasileios Charisis1, Stelios Hadjidimitriou1, Leontios Hadjileontiadis1,
Deniz Uğurca2, and Erdal Yilmaz2
1Department

of Electrical & Computer Engineering, Aristotle University of Thessaloniki, Thessaloniki, Greece

vcharisis@ee.auh.gr, stelios@psyche.ee.auth.gr,
leontios@auth.gr
2Argedor

Information Technologies, Ankara, Turkey

{dugurca,erdlylmz}@gmail.com

Abstract. There are important cultural differences in emotions that can be predicted and connected to each other in the light of cultural and artistic expressions. The main differences reflected at the affective space are expressed
through initial response tendencies of appraisal and action readiness. Capturing
and handling the emotions during artistic activities could be used as a dominant
source of information to acquire and augment the cultural expression and maximize the emotional impact to the audience. This paper presents a novel EEGbased game-like application, to learn and handle affective states and transitions
towards augmented artistic expression. According to the game scenario, the user has to reach and sustain one or more target affective states based on the level
of the game, the difficulty setting and his/her current affective state. The game,
although at its first version, has been demonstrated to a small group of potential
users and has received positive feedback. Its use by a wider audience is anticipated within the realization of the i-Treasure FP7 EU Programme (2013-2017).
Keywords: Human-computer interaction. Emotion game. Affective state detection. Game-based learning. Contemporary music composition. Valence-arousal
space. EEG. Emotiv. EmoActivity. i-Treasures.

1

Introduction

1.1

Emotion Recognition (ER) and Human Computer Interaction (HCI)

One of the essential aspects that affect human’s daily enterprises that require productive social interaction and communication skills is emotion. Considering the proliferation of machines in our commonness, it is essential that they interact with people
the same way people interact with each other. In order to accomplish such a pragmatic
HCI, it becomes crucial to imbue machines with the ability to detect, recognize and
respond to human emotional states and reactions, that is, to outfit machines with emoadfa, p. 1, 2011.
© Springer-Verlag Berlin Heidelberg 2011

tional intelligence. The latter is the target of affective computing that deals with the
design of systems and devices that can detect, recognize and process human emotions
with the ultimate goal of implementing the aforementioned more reliable HCI.
Emotion recognition is one of the most important issues affective computing brings
forward and plays a dominant role in the effort to incorporate computers and generally machines, with the ability to interact with humans by expressing cues that postulate
and demonstrate emotional intelligence-related attitude. Successful ER will enable
machines to recognize the affective state of the user and collect emotional data for
processing in order to proceed toward the terminus of emotion-based HumanMachine-Interaction, the emotional-like response.
1.2

Emotions in Musical Artistic Expression

It is clearly conceivable and apparent that music expresses emotions as personal affective experiences during music listening. However, a lot of research efforts and
approaches, from philosophical to biological, have been presented in order to shed
light on further insights in this phenomenon [1]. It has been suggested that such music-induced emotions are governed by universality in terms of musical culture, meaning, listeners with different cultural backgrounds can infer emotions in culturespecific music to a certain extent. Such evidence led to the assumption that neurobiological functions underlying such emotional experiences do not differ across members
of different cultures, as the neural networks responsible may be fixed. In general, the
processing of musical stimuli involves the gradual analysis of music structural elements from basic acoustic features to musical syntax that leads to the perception of
emotions and semantic meanings underlying the stimuli [2]. It is becoming evident
that the structure of music defines what it expresses. To be more accurate, music does
not literally express emotion as it is not a sentient creature, but it is its structural elements and production performance shaping the acoustic outcome that foster the induction of emotional states to the listener, who is indeed a sentient being.
Written music can be performed in different ways, just like a piece of text can be
read with various tones. In essence, music exists only when it is performed and performances of the same work can differ significantly. The latter form the concept of
performance expression that refers to both; (a) the correlation between the performer's
interpretation of a musical excerpt and the small-scale variations in timing, dynamics,
vibrato, and articulation that shape the microstructure of the performance and (b) the
relationship between such variations and the listener's perception of the performance.
It has been proposed that performance expression emerges from five different
sources, i.e., Generative rules, Emotion expression, Random fluctuations, Motion
principles, and Stylistic unexpectedness, referred to as the GERMS model [3]. Here,
the focus is placed on emotional expression that allows the performer to convey emotions to listeners by manipulating features, such as tempo and loudness, in order to
render the performance with the emotional characteristics that seem suitable for the
particular musical piece. By capturing and handling these emotions, and, even better,
their dynamic character during artistic activities, could be used as a dominant source

of information to acquire and augment the cultural expression and maximize the emotional impact to the audience.
1.3

The i-Treasures Paradigm

“Intangible Cultural Heritage” (ICH) is defined as a part of the cultural heritage of
societies, groups or sometimes individuals and includes practices, presentations, expressions, knowledge, skills and related tools to all of these, such as equipment and
cultural sites. This intangible heritage passes from generation to generation and gives
people a sense of identity and continuity; it is the result of the continuous interaction
of communities and groups with their nature and history and it promotes respect for
cultural diversity and human creativity.
The main objective of i-Treasures project [4] is to build a public and expandable
platform to enable learning and transmission of rare know-how of intangible cultural
heritage. One of the use cases that i-Treasures deals with is Contemporary Music
Composition (CMC). According to UNESCO, music is the most universal form of the
performing arts since it can be found in every society, usually as an integral part of
other performing art forms and other domains of ICH. Music can be found in a large
variety of contexts, such as classical, contemporary or popular, sacred etc. Instruments, artefacts and objects in general are closely linked with musical expressions and
they are all included in the Convention’s definition of the ICH. Music that fits with
the western form of notation is better protected. Nevertheless, those that do not fit
with the western notation are usually threatened. In order to prevent such ICH expressions from extinction, the aim of CMC use case of i-Treasures project is to provide a
tool that will allow people to practice the ICH expressions. Towards this goal, gamelike educational applications are developed.
Digital games employed in education can be broadly subdivided in two categories
[5]: 1) mainstream games, i.e., games that are created solely for fun, and 2) learning
games, i.e., games that are expressly designed with explicit educational purposes.
Games in the latter category are also referred to as Serious Games (SGs) [6]. The iTreasures game-like applications can be reasonably considered SGs, since they have
been built having in mind a “serious” educational purpose.
The adoption of SGs in i-Treasures follows a quite well consolidated trend in the
Technology Enhanced Learning field. As a matter of fact, at present, digital games are
increasingly adopted to sustain learning and training in a variety of educational fields
(for example, school education, military training, medical training); this is done for a
wide range of target populations, ranging from children to adults [7]. The educational
potential of games has been widely explored and highlighted by researchers within
the wider research area of Game-Based Learning [8].
1.4

Proposed Application

This paper presents a novel SG emotion Human-Computer-Interaction application,
namely EmoActivity, to learn and handle affective states and transitions towards
augmented artistic expression of a contemporary music composer/performer/learner.

EmoActivity is part of the CMC use case of i-Treasures project. CMC aims to develop a novel intangible musical instrument, which maps natural gestures, performed in a
real-world environment, to music by taking into account the performer’s emotional
status that influences the outputted acoustic result. One of the innovative aspects of
EmoActivity is its ER module, based on EEG, as described in the succeeding sections.

2

Electroencephalogram (EEG)-Based ER

Towards effective ER, a large variety of methods and devices have been implemented, mostly concerning ER from face, speech and signals from autonomous nervous system (ANS), i.e. heart rate and galvanic skin response. A relatively new field in
the ER area is EEG-based ER, which overcomes some fundamental reliability issues
that arise with ER from face, voice or ANS-related signals. For instance, a facial expression recognition approach would be useless for people with inability to express
emotions via face or for situations of human social masking. Moreover, voice and
ANS signals are vulnerable to “noise” related to activity that does not derive from
emotional experience, i.e., GSR signals are highly influenced by inspiration which
may not be caused from emotional activity, especially in case of musical performance. The game architecture of the EmoActivity builds upon the EEG-based ER
concept via specific time series analysis of the acquired EEG signal to infer for the
current emotional state of the user; a detail description of the complete EmoActivity
architecture follows.

3

Game Architecture

3.1

Input Data Capture

EEG signals are acquired using the EPOC wireless recording headset (Emotiv Systems, Inc., San Francisco, CA). EPOC (Fig. 1(a)) bears 14 channels (AF3, F7, F3,
FC5, T7, P7, O1, O2, P8, T8, FC6, F4, F8), referenced to the common mode sense
(CMS –left mastoid) - driven right leg (DRL - right mastoid) ground (Fig. 1(b)), under a 128 Hz sampling frequency. The recorded EEG signals are streamed in real-time
to a paired PC. The communication between the headset and the PC is based on a
proprietary wireless protocol that requires an EPOC USB receiver plugged in to the
computer. The headset is powered by a lithium battery that provides 12 hours of autonomy on a full charge.
3.2

Affective States

The most popular model representing the emotional states is the 2D model of valence and arousal [9] (Fig. 2(a)). Valence denotes if an affective state is positive or
negative, while arousal constitutes a measure of excitation. Thus, each affective state
can be modeled as a point on the plane defined by the orthogonal axes of valence and

(a)

(b)

Fig. 2. (a) EPOC wireless EEG headset by Emotiv, (b) Electrode positions of the EPOC headset according to the 10/20 International System

(a)

(b)

Fig. 1. (a) 2D valence-arousal space, (b) Affective state recognition software

arousal. In this work, four affective states are discriminated: a) positive valence – high
arousal, b) positive valence – low arousal, c) negative valence – high arousal, and d)
negative valence – low arousal.
3.3

Affective State Recognition Software

The EPOC headset is combined with an in house-developed affective state recognition software. The recognition software gathers the EEG data, in the background, and
computes the current affective state of the user. In particular, the software captures the
stream of the 14 channel raw EEG data and feeds them to a fractal dimension (FD)
threshold-based recognition algorithm, based on the works of [10], [11] that yields the
valence (positive or negative) of the users' affective state in real-time. Its recognition
cycle requires 4 seconds; thus, the user's affective valence is outputted every 4 seconds. In particular, for this time interval, the difference between the sum of the EEG

FD values corresponding to channels of the left brain hemisphere and the respective
sum corresponding to channels of the right brain hemisphere is computed. If the difference is greater than zero, valence is considered positive; else, it is considered negative, based on the fact that the FD of an EEG signal constitutes a measure of brain
activation [12], combined with the EEG asymmetry theory [13]. In the latest version
of the algorithm, the excitement levels, as produced by the EPOC headset, are used as
an indicator of the user's arousal (high or low) and in combination with the detected
valence, a basic, yet sufficient, characterization of his/her affective state based on the
valence-arousal model is achieved. Specifically, four general affective states can be
detected, as described above (Fig. 2(a)), represented by the top-right, bottom-right,
top-left, and bottom-left quartile of the valence-arousal plane, respectively.
Moreover, the recognition software is accompanied by a user interface that provides the user with basic information (Fig. 2(b)). Specifically, the interface provides
feedback on whether the EPOC headset is paired with the PC or not. Moreover, the
detected affective state is shown by highlighting the corresponding quartile of the
valence-arousal plane, with representative colors. Additionally, the progress of each
recognition cycle is shown via a progress bar.
The software was developed in C#, while the FD estimation method was realized
in Matlab (Math-works, Inc., Natick, MA) and ported as a .NET dynamic link library
(DLL). The software interface can be minimised on the task tray (Windows OS) with
the recognition algorithm continuing to work on the background. Communication
with third-party applications has also been embedded in the software. The recognition
module communicates with the visualization module (and Max/MSP if required) via
messages over User Datagram Protocol (UDP), i.e., the detected affective state is sent
to the visualization module in order to be visualized or used accordingly. In particular,
two separate values are sent, i.e., the value of valence (0 for positive or 1 for negative)
and the value of arousal (0 for low or 1 for high).
3.4

Game Scenario

EmoActivity is basically designed to prompt the user to reach and sustain certain
affective states via a gamified process involving affective images. The game consists
of three levels with each level having three difficulty settings (easy, medium, and
hard). In Level 1 the user is asked to reach a certain affective state and sustain it,
while in Levels 2 and 3 the user is asked to reach two and three consecutive states,
respectively. In order to unlock levels 2 and 3, the user should complete successfully
Level 1 and Level 2, respectively. In order to facilitate the emotion elicitation, affective images are used. These images are drawn from the International Affective Picture
System (IAPS) [14] database based on the valence-arousal rating provided with the
database.
The difficulty in each level lies in the difficulty of the affective transition the user
should undergo; in other words, the target affective state that the user is asked to
reach based on his/her current emotional status. Related studies have shown that conditional dependencies exist between emotional states [15] and the probability of transition from one emotional state to another varies significantly [16]. For example, if

Fig. 3. Main menu of Contemporary Music Composition application

difficulty level is set to easy and the user is calm (positive valence – low arousal), s/he
will be asked to reach a target affective state of excitement (positive valence – high
arousal) as this transition is more easy that a state of sadness or anger/fear.
At the completion of each level a performance score is provided according to the
performance of the user.
3.5

User Interface

The game interface (visualization module) is implemented by Unity 3D game engine. The reasons to choose this development tool are a) Unity 3D is an industry
proven game development environment, b) it is easy to deploy the games to various
platforms ranging from Desktop PCs to mobile devices, and c) the development team
is highly experienced in Unity.
Main Menu Screen. The main menu screen of the CMC application concerning
CMC use case is shown on Fig. 3. The “Getting Started” panel provides a general
tutorial about CMC use case whereas the first three activities (Activity 1, Activity 2
and Final Challenge) are focused on the intangible musical instrument. These are not
going to be presented because they are beyond the scope of this work. The last activity is EmoActivity. The “Getting Started” section provides detailed information about
the usage, setup and preparation of the Emotiv EPOC EEG headset. A snapshot of the
introduction to the headset is shown in Fig. 4. The introduction video provides instructions to the users in order to wear and configure the device on their own. The

Fig. 4. Snapshot of the introduction to the EPOC headset video as a “Getting Started” guide

“Practice” section is the actual game where the user can practice in reaching and sustaining certain affective states.
EmoActivity Screens. By clicking on the “Practice” button the game begins. Each
level starts with an introductory screen (Fig. 5(a)) that informs the user about the current level and the target, for example, “Level 1. A series of pictures will be shown.
You must reach the target affective state and sustain it”. In this screen, each faded
color represents a quartile of the valence-arousal plane. After five seconds, the caption disappears and a progress bar is shown for 20 seconds (Fig. 5(b)). During this
period, the current affective state of the user is detected. Then, based on the detected
current affective state and the difficulty setting the target emotional state is displayed
for five seconds by highlighting the corresponding faded color on the valence arousal
space and by displaying a caption “Feel”, as illustrated in Fig. 5(c). Thereafter, a series of random affective images are displayed sequentially for five seconds each, as
shown in Error! Reference source not found.d in order to facilitate emotion elicitation. The background of the images has the same color as the corresponding quartile
of valence-arousal space that the target affective state belongs. Afterwards, the detected emotional status is displayed by highlighting the corresponding faded color (Fig.
5(e)) and accompanied by a caption “You felt” and a green tick (if the detected affective state is correct) or a red “x” (if the detected affective state is wrong). In case the
correct emotional target is achieved, the learner is told that he/she reached the target
affective state. However, there are two cases: 1) the learner gets 10 points if the affective target is sustained, otherwise, 2) the learner gets 5 points. On the other hand, if
the target affective is not reached the learner gets 0 points and he/she is asked to try
again (Fig. 5 (f)).
An analogous procedure is applied for levels 2 and 3 but is not described here for
space reasons.

Fig. 5. EmoActivity screens: (a) initial screen, (b) progress bar for current affective state calculation, (c) target affective state, (d) affective pictures display, (e) results screens ((e1) correct
affective state reached, (e2) wrong affective state reached), (f) score screens ((f1) correct affective state reached and sustained, (f2) correct affective state reached but not sustained, (f3)
wrong affective state reached)

As far as the current affective state detection and the affective state transition/sustenance detection are concerned (see Section 3.3 "Affective State Recognition
Software"), the user's emotional status is determined by the affective state outputted
the most by the recognition algorithm after a number of recognition cycles. For example, during the 20 s of the user's current state detection, five recognition cycles take
place; hence, the outcome outputted the most after the five recognition cycles is considered to represent the user's affective state.
Colors. Colors representing the quartiles of the valence-arousal plane are representative of the affective state corresponding to each quartile. In Fig. 6 the colors along
with their hexadecimal codes are depicted. In particular, dark red (top-left quartile)
usually represents anger or fear, i.e., emotions of negative valence and of high arousal, while dark grey (bottom-left quartile) conveys a feeling of negative valence and of
low arousal, such as sadness. On the other hand, the blue color (bottom-right quartile)
is linked to the feeling of calmness and serenity (positive valence - low arousal),
while orange is representative of excitement and joy (positive valence - high arousal).
The latter are based on statistical research [17], mostly in the Western World; however, there is no universal agreement on the association of colors to emotions and such

Fig. 6. The hexadecimal codes of the colors representing the four quartiles of the valencearousal space

relationships highly depend on cultural background and even personal preferences.
Nevertheless, as the EmoActivity requires mental effort, the interface was kept as
minimal as possible, with only basic information displayed on the screen.

4

Conclusion

This work presents the first version of a novel interactive game-like emotion HCI
application that is expected to assist the user to learn and handle affective states and
transitions towards augmented artistic expression. EmoActivity is part of the Contemporary Music Composition use case of i-Treasures project that targets the development of an intangible musical instrument that will produce sound through a natural
user interface and will be affected by the user’s affective state. Acoustic cues manipulated by user’s affective states, such as happiness, tenderness, sadness, fear and anger
are: pitch, intensity, tempo and timbre [1].
The game entails three levels with three difficulty settings per level. Depending on
the current affective state of the user and the difficulty setting, the user is asked to
reach one, two or three consecutive affective states at levels 1, 2 and 3, respectively.
From a technical point of view, EmoActivity comprises of an affective state recognition software and a visualization module that communicate through UDP. Moreover,
the Emotiv EPOC headset is required for capturing and wirelessly transmitting the
EEG data.
The first version of EmoActivity has been demonstrated to the Vocational Training
Institute of Thermi, Thessaloniki, Greece and Municipal Concervatory of Kalamaria,

Thessaloniki, Greece, and has received positive feedback from both teachers and students. They displayed inspiration and enthusiasm for the concept and scenario of the
game while they proactively sought more complex levels.

5

Future Work

Future work includes the use of not only affective images but also sounds and videos in order to achieve improved emotional triggering/elicitation and, consequently,
more reliable affective state recognition results. Additionally, more detailed digitization of the valence-arousal space is planned, i.e. four emotional states for each quartile of valence-arousal space leading to 16 states in total; however, such an advance
should coincide with an appropriate improvement of the emotional state recognition
software to detect 16 emotional states. Last but not least, a new level will be developed where the user will be asked to reach multiple consecutive affective states while
listening a contemporary musical piece that will be distorted accordingly in case of
failure to reach the targets.
Acknowledgements. This work is funded by the European Commission via the iTreasures project (Intangible Treasures - Capturing the Intangible Cultural Heritage
and Learning the Rare Know-How of Living Human Treasures FP7-ICT-2011-9600676-i-Treasures). It is an Integrated Project (IP) of the European Union's 7th
Framework Programme 'ICT for Access to Cultural Resources.

References
1. Juslin, P.N., Sloboda, J.A.: Handbook of music and emotions theory, research, applications. Oxford University Press, New York (2000)
2. Koelsch, S., Siebel, W.A.: Towards a neural basis of music perception. Trends in Cognitive Sciences 9, 578–584 (2005)
3. Juslin, P.N.: Five facets of musical expression: A psychologist's perspective on music performance. Psychology of Music 31, 273-302 (2003)
4. The i-Treasures Project, http://www.i-treasures.eu.
5. Kirriemuir, J., McFarlane, A.: Literature Review in Games and Learning. A NESTA Futurelab Research report – report 8 (2004)
6. Michael, D., Chen, S.: Serious games: Games that educate, train, and inform. Thomson
Course Technology, Boston (2006)
7. Charlier, N., Ott, M., Remmele, B., Whitton, N.: Not Just for Children: Game-Based
Learning for Older Adults. In: 6th European Conference on Games Based Learning, pp.
102-108 (2012)
8. Van Eck, R.: Digital Game-Based Learning: It's Not Just the Digital Natives Who Are
Restless. EDUCAUSE Review 41, 17-30 (2006)
9. Russell, J.A.: A circumflex model of affect. J. Pers. Soc. Psychol. 39, 1161--1178 (1980)
10. Higuchi, T.: Approach to an irregular time series on the basis of the fractal theory. Physica
D 31, 277–283 (1988)

11. Liu, Y., Sourina, O., Nguyen, M.K.: Real-time EEG-based emotion recognition and its applications. In: Gavrilova, M.L., Kenneth Tan C.J., Sourin, A., Sourina, O. (eds.) Lecture
Notes in Computer Science. Transactions on Computational Science XII, vol. 6670, pp.
256-277, Springer, Heidelberg (2011)
12. Accardo, A., Affinito, M., Carrozzi, M., Bouquet, F.: Use of the fractal dimension for the
analysis of electroencephalographic time series. Biological Cybernetics 77, 339–350
(1997)
13. Davidson, R.J.: What does the prefrontal cortex “do” in affect: perspectives on frontal
EEG asymmetry research. Biological Psychology 67, 219-233 (2004)
14. Lang, P.J., Bradley, M.M., Cuthbert, B.N.: International affective picture system (IAPS):
Affective ratings of pictures and instruction manual. Technical Report A-8, University of
Florida (2008)
15. Sudhof, M., Emilsson, A.G., Maas, A.L., Potts, C.: Sentiment expression conditioned by
affective transitions and social forces. In: 20th ACM SIGKDD International Conference of
Knowledge Discovery and Data Mining, pp. 1136-1145 (2014)
16. D Baker, R.S.J., Mercedes, M., Rodrigo, T., Xolocotzin, U.E.: The dynamics of affective
transitions in simulation problem-solving environments. In: 2nd International Conference
on Affective Computing and Intelligence Interactions, pp. 666-677 (2007)
17. Cherry, K.: Color psychology: how colors impact moods, feelings, and behaviors.

http://psychology.about.com/od/sensationandperception/a/colo
rpsych.htm

