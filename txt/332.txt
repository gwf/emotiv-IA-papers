	  
ICMA	  Array,	  vol	  2016,	  Special	  Issue:	  
Proceedings	  of	  Si15,	  Singapore,	  August	  2015,	  pp	  86-­‐88.	  	  

A	  Brain-­‐Computer	  Music	  Interface	  for	  Music	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  	  
Expression	  
Rafael	  Ramirez,	  Sergio	  Giraldo,	  Zacharias	  Vamvakousis	  
Music	  Technology	  Group	  ,	  Universitat	  Pompeu	  Fabra,	  Barcelona,	  Spain	  
rafael.ramirez@upf.edu	  

Abstract	  
	  
Active	  music	  listening	  is	  a	  way	  of	  listening	  to	  music	  through	  active	  interactions.	  In	  this	  paper	  we	  present	  an	  
expressive	   brain-­‐computer	   interactive	   music	   system	   for	   active	   music	   listening,	   which	   allows	   listeners	   to	   ma-­‐
nipulate	   expressive	   parameters	   in	   music	   performances	   using	   their	   emotional	   state,	   as	   detected	   by	   a	   brain-­‐
computer	  interface.	  The	  proposed	  system	  is	  divided	  in	  two	  parts:	  a	  real-­‐time	  system	  able	  to	  detect	  listeners’	  
emotional	   state	   from	   their	   EEG	   data,	   and	   a	   real-­‐time	   expressive	   music	   performance	   system	   capable	   of	   adapt-­‐
ing	  the	  expressive	  parameters	  of	  music	  based	  on	  the	  detected	  listeners’	  emotion.	  We	  comment	  on	  an	  applica-­‐
tion	  of	  our	  system	  as	  a	  music	  neurofeedback	  system	  to	  alleviate	  depression	  in	  elderly	  people.	  	  
Keywords:	  Brain-­‐computer	  interfaces,	  emotion,	  expressive	  music	  performance	  	  

1.	  Introduction	  	  
every	   day	   interaction	   among	   humans,	   a	   great	  
part	   of	   the	   research	   in	   this	   area	   has	   explored	  
detecting	   emotions	   from	   facial	   and	   voice	   in-­‐
formation.	   Under	   controlled	   conditions,	   cur-­‐
rent	   emotion-­‐detection	   computer	   systems	  
based	   on	   such	   information	   are	  able	   to	   classify	  
emotions	   with	   considerable	   accuracy	  
(Takahashi,	  2004).	  However,	  emotions	  are	  not	  
always	   manifested	   by	   means	   of	   facial	   expres-­‐
sions	   and	   voice	   information.	   Facial	   and	   voice	  
information	   is	   related	   only	   to	   behavioral	   ex-­‐
pression,	   which	   can	   be	   consciously	   controlled	  
and	   modified,	   and	   which	   interpretation	   is	   of-­‐
ten	  subjective.	  A	  still	  relatively	  new	  field	  of	  re-­‐
search	   in	   affective	   brain-­‐computer	   interaction	  
attempts	  to	  detect	  a	  person’s	  emotional	  state	  
using	   electroencephalograms	   (EEGs)	   (Chanel,	  
2006;	  Choppin,	  2000).	  	  

In	   recent	   years,	   active	   music	   listening	   has	  
emerged	   as	   a	   study	   field	   that	   aims	   to	   enable	  
listeners	   to	   interactively	   control	   different	   as-­‐
pects	  of	  music	  in	  real-­‐time.	  Most	  of	  the	  work	  in	  
this	   area	   has	   focused	   on	   controlling	   music	   as-­‐
pects	  such	  as	  playback,	  equalization,	  browsing	  
and	   retrieval.	   However,	   there	   have	   been	   very	  
few	   attempts	   to	   control	   the	   expressive	   aspects	  
of	   music	   performance,	   such	   as	   pitch,	   timing,	  
amplitude	   and	   timbre.	   The	   manipulation	   of	  
these	   sound	   properties	   is	   clearly	   distinguisha-­‐
ble	   by	   the	   listeners	   who	   often	   experience	  
strong	  emotions	  as	  a	  consequence	  of	  this.	  Ex-­‐
pressive	   music	   performance	   research	   (for	   an	  
overview	   see	   Gabrielsson,	   1999,	   2003)	   investi-­‐
gates	   the	   manipulation	   of	   these	   sound	   proper-­‐
ties	   in	   an	   attempt	   to	   understand	   how	   it	   con-­‐
veys	   emotion	   and	   to	   recreate	   expression	   in	  
performances.	  	  

In	   this	   paper	   we	   go	   one	   step	   beyond	   brain	  
activity	   data	   sonification	   by	   mapping	   brain	  
activity	  data	  into	  a	  high-­‐level	  emotional	  space	  
description,	  and	  use	  this	  description	  to	  control	  
expressive	  aspects	  of	  music	  performances.	  The	  
result	  is	  an	  expressive	  brain-­‐computer	  interac-­‐

The	  study	  of	  emotions	  in	  human-­‐computer	  
interaction	   has	   increased	   in	   recent	   years.	   This	  
is	   due	   to	   the	   growing	   need	   for	   computer	   appli-­‐
cations	   capable	   of	   detecting	   the	   emotional	  
state	   of	   users	   (Picard,	   2002).	   Motivated	   by	  

86	  

	  

tive	   music	   system	   for	   active	   music	   listening,	  
which	   allows	   listeners	   to	   manipulate	   expres-­‐
sive	   parameters	   in	   music	   performances	   using	  
their	   emotional	   state,	   as	   detected	   by	   a	   brain-­‐
computer	   interface.	   We	   describe	   the	   applica-­‐
tion	   of	   our	   system	   as	   a	   music	   neurofeedback	  
system	   to	   alleviate	   depression	   in	   elderly	   peo-­‐
ple.	  

2.	  A	  Brain-­‐Computer	  Music	  Interface	  for	  
Music	  Expression	  
Our	   approach	   to	   emotion-­‐based	   expressive	  
brain-­‐computer	   music	   interaction	   is	   depicted	  
in	   Figure	   1.	   	   The	   system	   consists	   of	   a	   real-­‐time	  
feedback	   loop	   in	   which	   the	   brain	   activity	   of	   a	  
person	  is	  captured	  as	  an	  EEG	  signal,	  the	  signal	  
is	   processed	   in	   order	   to	   filter	   some	   frequency	  
bands	   and	   to	   compute	   the	   estimated	   emo-­‐
tional	   state	   of	   a	   person	   as	   a	   coordinate	   in	   the	  
arousal-­‐valence	   2D	   space.	   This	   coordinate,	  
together	   with	   a	   music	   score	   or	   audio	   file,	   is	   the	  
input	   to	   a	   previously	   trained	   expressive	   music	  
performance	  model,	  which	  outputs	  an	  expres-­‐
sive	   rendition	   of	   the	   score/audio.	   The	   expres-­‐
sive	   performance	   output	   is	   presented	   to	   the	  
user	  as	  feedback	  of	  his/her	  emotional	  state.	  	  

!"#$%&&'(%)
#%$*+$,-./%)
,+0%1))

	  
Figure	  1.	  System	  overview	  

We	   trained	   four	   expressive	   performance	  
models	   (one	   for	   each	   emotion,	   happy,	   sad,	  
angry	   and	   tender)	   using	   machine	   learning	  
techniques.	   In	   order	   to	   train	   the	   model	   we	  
have	   collected	   a	   training	   set	   of	   music	   scores	   of	  
pieces	  and	  recorded	  performances	  of	  the	  piec-­‐
es	   in	   the	   four	   different	   emotions	   by	   a	   profes-­‐
sional	   musician.	   The	   modelled	   expressive	   per-­‐
formance	   actions	   are	   duration	   ratio	   (ratio	   be-­‐
tween	   the	   score	   duration	   and	   the	   performed	  
duration),	   the	   energy	   ratio	   (ratio	   between	  
loudness	  of	  a	  note	  and	  average	  loudness),	  and	  
articulation	  ratio	  (level	  of	  staccato	  -­‐	  legato).	  

For	   acquiring	   the	   brain	   activity	   of	   the	   user,	  
we	   use	   the	   Emotiv	   EPOC	   headset,	   recently	  
released	   by	   the	   Emotiv	   Company	   (Emotiv,	  
2014).	   This	   headset	   consists	   of	   14	   data-­‐
collecting	   electrodes	   and	   2	   reference	   elec-­‐
trodes,	   located	   and	   labeled	   according	   to	   the	  
international	   10-­‐20	   system	   (Niedermeyer,	  
2004).	  
From	  the	  EEG	  signal	  of	  a	  person,	  we	  deter-­‐
mine	   the	   level	   of	   arousal,	   i.e.	   how	   relaxed	   or	  
excited	   the	   person	   is,	   by	   computing	   the	   ratio	  
of	   the	   beta	   and	   alpha	   brainwaves	   as	   recorded	  
by	   the	   EEG.	   	   In	  order	  to	  determine	  the	  valence	  
level,	  i.e.	  negative	  or	  positive	  state	  of	  mind,	  we	  
compare	  the	  activation	  levels	  of	  the	  two	  corti-­‐
cal	   hemispheres.	   Details	   about	   arousal	   and	  
valence	  calculation	  can	  be	  found	  in	  Ramirez	  et	  
al.	  2012.	  

In	  order	  to	  apply	  the	  expressive	  model	  with	  
an	  arousal-­‐valence	  coordinate	  as	  input,	  we	  in-­‐
terpolate	  (using	  a	  linear	  regression	  model)	  the	  
four	   trained	   (happy,	   sad,	   angry	   and	   tender)	  
models.	   This	   allows	   us	   to	   apply	   intermediate	  
models,	   not	   just	   the	   four	   trained	   models.	   The	  
performance	  actions,	  i.e.	  duration,	  energy	  and	  
articulation,	   are	   calculated	   using	   the	   model	  
and	   the	   interpolated	   coefficients	   based	   on	   a	  
control	   position	   input	   on	   the	   arousal	   valence	  
plane.	  	  	  

	  

87	  

	  

input	  for	  synthesis.	  Finally,	  we	  have	  briefly	  de-­‐
scribed	  the	  application	  of	  the	  proposed	  system	  
as	   a	   music	   neurofeedback	   system	   to	   alleviate	  
depression	  in	  elderly	  people.	  

3.	  A	  music	  neurofeedback	  application	  
The	   proposed	   expressive	   brain-­‐computer	   mu-­‐
sic	   interface	   has	   been	   applied	   as	   a	   music	   neu-­‐
rofeedback	   system	   for	   treating	   depression	   in	  
elderly	   people.	   Ten	   adults	   (nine	   female	   and	  
one	   male,	   mean=84,	   SD=5.8)	   with	   normal	  
hearing	   participated	   in	   the	   neurofeedback	  
study	   consisting	   of	   ten	   sessions	   (two	   sessions	  
per	   week)	   of	   fifteen	   minutes	   each.	   Participants	  
listened	  to	  music	  pieces	  preselected	  according	  
to	   their	   music	   preferences,	   and	   were	   encour-­‐
aged	   to	   increase	   the	   loudness	   and	   tempo	   of	  
the	  pieces,	  based	  on	  their	  arousal	  and	  valence	  
levels,	  respectively:	  Pre	  and	  post	  evaluation	  of	  
six	   participants	   was	   performed	   using	   the	   BDI	  
depression	   test,	   showing	   an	   average	   im-­‐
provement	   of	   17.2%	   in	   their	   BDI	   scores	   at	   the	  
end	  of	  the	  study.	  

	  
Acknowledgements	  
This	   work	   has	   been	   partly	   sponsored	   by	   the	  
Spanish	   TIN	   project	   TIMUL	   (TIN2013-­‐48152-­‐
C2-­‐2-­‐R)	  

References	  
Chanel,	   G.,	   Kronegg,	   J.,	   Grandjean,	   D.,	   Pun,	   T.:	  
Emotion	   Assessment:	   Arousal	   Evaluation	   Using	  
EEG’s	  and	  Peripheral	  Physiological	  Signals.	  In:	  Gun-­‐
sel,	   B.,	   Jain,	   A.K.,	   Tekalp,	   A.M.,	   Sankur,	   B.	   (eds.)	  
MRCS	   2006.	   LNCS,	   vol.	   4105,	   pp.	   530–537.	   Spring-­‐
er,	  Heidelberg	  (2006)	  
Choppin,	   A.:	   Eeg-­‐based	   human	   interface	   for	  
disabled	   individuals:	   Emotion	   expression	   with	   neu-­‐
ral	   networks.	   Masters	   thesis,	   Tokyo	   Institute	   of	  
Technology,	  Yokohama,	  Japan	  (2000)	  

4.	  Conclusions	  
We	   have	   presented	   an	   expressive	   brain-­‐
computer	   interactive	   music	   system	   for	   active	  
music	   listening,	   which	   allows	   listeners	   to	   ma-­‐
nipulate	   expressive	   parameters	   in	   music	   per-­‐
formances	   using	   their	   emotional	   state,	   as	   de-­‐
tected	  by	  their	  brain	  activity	  as	  an	  EEG	  signal.	  
The	  proposed	  system	  is	  divided	  in	  two	  parts:	  a	  
real-­‐time	   system	   able	   to	   detect	   listeners’	   emo-­‐
tional	   state	   from	   their	   EEG	   data,	   and	   a	   real-­‐
time	   expressive	   music	   performance	   system	  
capable	   of	   adapting	   the	   expressive	   parameters	  
of	   music	   based	   on	   the	   detected	   listeners’	   emo-­‐
tion.	  For	  acquiring	  the	  brain	  activity	  of	  the	  lis-­‐
tener,	  we	  use	  the	  low-­‐cost	  Emotiv	  EPOC	  head-­‐
set,	   and	   we	   determine	   his/her	   emotional	   es-­‐
tate	   by	   computing	   arousal	   and	   valence	   values	  
from	  the	  alpha	  and	  beta	  waves	  in	  the	  prefron-­‐
tal	   cortex.	   The	   expressive	   performance	   model	  
is	  trained	  using	  recordings	  of	  musical	  pieces	  in	  
four	   emotions	   (happy,	   relaxed,	   sad,	   and	   an-­‐
gry).	  The	  input	  of	  the	  system	  is	  the	  coordinate	  
consisting	   of	   the	   computed	   instantaneous	  
arousal	  and	  valence	  values.	  The	  coefficients	  of	  
the	  four	  expressive	  models	  (one	  for	  each	  emo-­‐
tion)	   are	   interpolated	   to	   obtain	   the	   prediction	  
of	   the	   performance	   actions,	   which	   serve	   as	  

Emotiv	   Systems	   Inc.	   Researchers,	   (2014).	   	   URL	  
http://www.emotiv.com/researchers/.	  
Gabrielsson,	   A.	   The	   performance	   of	   music.	   In:	  
Deutsch,	   D.	   (Ed.),	   The	   Psychology	   of	   Music,	   second	  
ed.	  Academic	  Press	  (	  1999).	  
Gabrielsson,	   A.	   Music	   performance	   research	   at	  
the	   millennium.	   Psychol.	   Music	   31	   (3),	   221–272	  
(2003)	  
Niedermeyer,	   E.,	   da	   Silva,	   F.L.:	   Electroenceph-­‐
alography,	   Basic	   Principles,	   Clinical	   Applications,	  
and	   Related	   Fields,	   p.	   140.	   Lippincott	   Williams	   &	  
Wilkins	  (2004)	  
Picard,	   R.W.,	   Klein,	   J.:	   Toward	   computers	   that	  
recognize	  and	  respond	  to	  user	  emotion:	  Theoretical	  
and	   practical	   implications.	   Interacting	   with	   Com-­‐
puters	  14(2),	  141–169	  (2002)	  
Ramirez,	   R.,	   Vamvakousis,	   Z.	   (2012).	   Detecting	  
Emotion	  from	  EEG	  Signals	  Using	  the	  Emotive	  Epoc	  
Device,	  Brain	  Informatics,	  LNCS	  7670,	  pp.	  175–184.	  
Takahashi,	  K.:	  Remarks	  on	  emotion	  recognition	  
from	   bio-­‐potential	   signals.	   In:	   2nd	   International	  
Conference	   on	   Autonomous	   Robots	   and	   Agents,	  
pp.	  186–191	  (2004)	  

88	  

