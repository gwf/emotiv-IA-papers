June 20-23, 2017, Pennsylvania State University

The 23rd International Conference on Auditory Display (ICAD 2017)

BSONIQ: A 3-D EEG SOUND INSTALLATION
Marlene Mathew

Mert Cetinkaya

Agnieszka Roginska

New York University,
Music Technology
New York, NY USA

New York University,
Music Technology
New York, NY USA

New York University,
Music Technology
New York, NY USA

mm5351@nyu.edu

mc5993@nyu.edu

roginska@nyu.edu

ABSTRACT
Brain Computer Interface (BCI) methods have received a lot
of attention in the past several decades, owing to the exciting
possibility of computer-aided communication with the outside
world. Most BCIs allow users to control an external entity
such as games, prosthetics, musical output etc. or are used for
offline medical diagnosis processing. Most BCIs that provide
neurofeedback, usually categorize the brainwaves into mental
states for the user to interact with. Raw brainwave interaction
by the user is not usually a feature that is readily available for
a lot of popular BCIs. If there is, the user has to pay for or go
through an additional process for raw brain wave data access
and interaction.
BSoniq is a multi-channel interactive neurofeedback
installation which, allows for real-time sonification and
visualization of electroencephalogram (EEG) data. This EEG
data provides multivariate information about human brain
activity. Here, a multivariate event-based sonification is
proposed using 3D spatial location to provide cues about these
particular events. With BSoniq, users can listen to the various
sounds (raw brain waves) emitted from their brain or parts of
their brain and perceive their own brainwave activities in a 3D
spatialized surrounding giving them a sense that they are
inside their own heads.
1.

INTRODUCTION

Sonification is the method of rendering sound in response to
data and interactions and sets a clear focus on the use of sound
to convey information [1]. Electroencephalogram (EEG) is the
recording of electrical potential from the human scalp
containing multivariate data. EEG sonification has been very
useful in areas spanning data analysis and medical diagnosis to
general-purpose user interfaces in car navigation systems [2].
EEG sonification can give researchers or medical professionals
a better idea as to what is happening at a certain location in the
brain, when visual analysis can no longer be applied. For
example, with fMRI, visual images (scans) of the brain are
taken every millisecond, where the analysis of the brain
activity takes place after the scan. EEG sonification provides
information about brain activity in real-time by providing
auditory images that can more easily be interpreted due to their
spatial differences [7]. This is one of the main advantages of
auditory displays over visual displays. Listening is used as a
means to perceive data. Audio feedback for positional control

could be very useful in for example, the medical field [5].
Sound is a temporal indicator of the ongoing physical
processes in the world around us [16].
This paper presents “BSoniq”, a 3-D EEG sound installation,
in which, the user can perceive spatial characteristics of EEG
signals in a multi-channel environment. With this installation,
the users (listeners) wear a wireless EEG headset and listen to
sounds generated in real-time from their brain waves to
perceive brain activities which they may not be aware of in
their daily life. To accomplish a brain electrical activity
sonification, brainwave source localization features of multichannel EEG are converted into sound images. These allow for
simple interpretation, because of their spatial temporal
differences. Signals recorded from the scalp are “decoded”
from the multi-channel EEG, by applying filters and
modulation to the EEG signal with an audio file. The main
goal is to use sound to render the original data in a suitably
transformed way so that we can invoke our natural pattern
recognition capabilities to search for regularities and
structures. Brainwave sonification is also very practical in
brain-computer interface (BCI) user feedback design. Deciding
how the control of parameters, processing and filtering of
inaudible data are used is important in this process. Using
listening as a tool serves both as an aesthetic and/or scientific
purpose. The human hearing system is able to decode and
interpret complex auditory scenes. The more structured the
representation of the sonified data, the better the accessibility
and intelligibility of the chosen process [9].
We propose to employ auditory feedback, and thus provide
visualization of the brainwaves in the form of spatial sound
images, that is, to perform sonification of brain electrical
activity. The 14 channels used in this project represent the 14
sensors of the EEG device used.
2.

BACKGROUND

Efficient perceptualization of biofeedback or medical data
requires a multidisciplinary approach, including the fields of
computer
science,
engineering,
psychology
and
neurophysiology [5]. EEG provides a diagnostically important
stream of multivariate data of the activity of the human brain.
One of the first attempts of auditory EEG exploration was
reported in 1934 by E. Adrian and B. Matthews [15]. They
measured the brain activity from a human subject from

https://doi.org/10.21785/icad2017.001

213

The 23rd International Conference on Auditory Display (ICAD 2017)

electrodes that were applied to the head, and the channels
connected to these electrodes were viewed optically on
bromide paper while being directly transduced into sound.
T. Hermann et al. have presented different strategies of
sonification for human EEG [3]. Baier et al. used multivariate
sonification that displayed salient rhythms as well as used
pitch and spatial location to provide cues [15]. Hunt and
Hermann conducted experiments to explore interactive
sonifications, which they describe as the discipline of data
exploration by interactively manipulating the data's
transformation into sound [16]. They also realized that the
individuality of interacting with sound is important, meaning
that one must be able to detect a particular signal even if there
are other interfering signals and/or a noisy background
present.
There are many experiments converting multi-channel EEG to
sound. However, not many use 3D sound to provide spatial
cues. Hori and Rutkowski developed an EEG installation,
sonifying 14 EEG signals using 5 channels, where the
loudspeakers were geometrically located surrounding the
listener and termed "A" to "E" from the left to the right [2] on
the azimuth angle. By using only five channels, multiple EEG
data were combined into one, which was processed and sent to
a loud-speaker. This does not allow for details of a specific
sensor to be perceived. BSoniq sonifies all 14 channels to
speakers located at the azimuth and elevation angles related to
an EEG sensor's location for monitoring purposes.
The main areas of EEG sonification are: EEG monitoring,
EEG Diagnostics, Neurofeedback, Brain Computer
Interface(BCI) feedback and communication as well as EEG
mapping to music [11]. BSoniq's main focus is on monitoring
or listening. Monitoring generally requires the listener to
attend to a sonification over a course of time, to detect events,
and identify the meaning of the event in the context of the
system's operation [13].

3.

HARDWARE

The Emotiv EEG wireless device is used for signal acquisition
in this installation. This device has 14 sensors based on the
International 10-20 system located at AF3, F7, F3, FC5, T7,
P7, O1, O2, P8, T8, FC6, F4, F8 and AF4 (Fig. 1) [14]. The
International 10–20 system is an internationally recognized
method used to describe the location of scalp electrodes and
the underlying cerebral cortex [12]. It was created to ensure a
standard format so that the studies of a subject's EEG could be
compared over time and subjects could be compared to each
other. The "10" and "20" refer to the actual distances between
adjacent electrodes that are either 10% or 20% of the total
front–back or right–left distance of the skull.
The transmitted wireless EEG signals are received by the
Emotiv USB receiver which is connected to a USB port of a
PC and sent to Max/MSP for transformation. The data stream
coming from the Emotiv device is encrypted by proprietary
software, which is subsequently decrypted by Emotiv’s SDK.
The data is transmitted via the Emotiv’s API as raw EEG
values in microvolts. The EEG data is then stored as floating

June 20-23, 2017, Pennsylvania State University

point values which are converted from the unsigned 14-bit
output from the headset [10].

Figure 1: Area division of sensors
Once the EEG signals have been transformed, the sonified data
are converted to analog audio signals using audio interfaces
and sent to 14 speakers which are geometrically located around
the listener. The layout of the speakers represents the layout of
the sensors on the user’s head, giving the impression that the
user is inside his/her own head listening to the various
brainwaves in action. The ring topology of the speakers is to
provide cues of the azimuth and elevation in the horizontal
plane. This, to focus the listener’s attention to the correct angle
of the sonified signal. Locations of the 14 loudspeakers used in
this project are shown in Figure 4. A full sphere setup was used
with ten loudspeakers positioned horizontally around the
listener and the rest of the speakers were elevated
approximately 40 degrees above the listener’s head. Details of
the sonification process is discussed in the following section.

4.

SOFTWARE

Max/MSP, a visual programming language is used for EEG
sonification. The actual EEG data transmission is based on
Open Sound Control (OSC) protocol, provided by Mind Your
OSCs. This is an open source software that sends the raw EEG
values received from the Emotiv EEG SDK via User Datagram
Protocol (UDP) to Max/MSP for transformation. The
sonification is carried out sensor-wise with a sub-patch that
receives a single channel EEG signal, which is band-limited
and scaled to modulate a sample file. After the modulation
takes place, the audio signal is sent via a single channel out to
the loudspeaker. For example, the EEG signal of the AF3
channel after being transformed is sent to speaker 10. The full
assignments of the EEG channels to the speakers are shown in
Table 1. The sonified EEG signal of each electrode is sent to
the speaker representing the general location of that electrode
on the scalp.

214

June 20-23, 2017, Pennsylvania State University

The 23rd International Conference on Auditory Display (ICAD 2017)

Table 1: Speaker Assignment
Sensor
AF4
F8
T8
P8
O2
O1
P7
T7
F7
AF3
F3
F4
FC5
FC6

Speaker
1
2
3
4
5
6
7
8
9
10
11
12
13
14

4.2 Scaling
Figure 2: BSoniq flowchart

4.1 Data Representation
When dealing with sonification it is important to choose a
specific sound dimension to represent a given data
dimension[13]. EEG signals can range from 0.5 to 40 Hz,
which makes pitch a good sound attribute to represent any
changes in the EEG data. Here, frequency modulation is used
to transform the frequency of the EEG signal.
The EEG signal modulation used here is similar to that of a
regular Frequency Modulation (FM) synthesis, where you
have a carrier signal and a modulating signal. In this project
the modulator is the EEG signal and the carrier is the looping
sample file.

Scaling is important to determine how much the pitch of a
sound is used to convey a given change [13]. It shows the
relationship between the system and the EEG data. Because
each EEG signal may have different frequency and dynamic
characteristics, the ability for the user to manipulate the scaling
data is important for a better representation of the EEG data
and its characteristics.
When the EEG signal is received by the Max/MSP program, it
is first limited to 0-4500 uVolts and then scaled from 0-0.5.
The scaled EEG signal is then sent to another subpatch ’test’
(Fig. 3) to modulate it with a sample file. The sample file
(carrier) is a looping audio file excerpt selected by the user.
BSoniq provides the user with the flexibility as to what sounds
(sample files) to choose for the sonification process, therefor
enhancing the listening experience.
Since EEG signals typically range 0.5- 40 Hz, the data here has
been scaled by default between 440 – 880 Hz following an
octave music model. The user has the option to adjust the
range. The data is scaled to values between 0.5 and 1.5. These
values determine the amplitude of the modulating signal. The
default scaling translates higher EEG values into higher
amplitude vectors, and lower EEG values into lower amplitude
vectors, which has an effect on the sound output.
4.3 Modulation

Figure 3: 'test' modulation patch

At the end of dynamic scaling processes, the modulation
process is applied. In BSoniq, the sensors are divided into four
areas as shown in Figure 1. The user can choose a different
sample file for each area or the same sample file for all four
areas. The option for the user to select audio files, allows for a
better distinction between the various brain activity levels at
sensor level. After this process, the transformed EEG signal is
sent to the corresponding loudspeaker. For example, in Fig. 3
the O1 sensor in Area 3 is sonified and sent to loudspeaker 6.

215

June 20-23, 2017, Pennsylvania State University

The 23rd International Conference on Auditory Display (ICAD 2017)

4.5 Headphones
As described in the previous sections, BSoniq was first
designed as an installation for a 14-channel loudspeaker setup.
However, most people do not have a 14-channel loudspeaker
system accessible to them. Since this installation was intended
for general use and the ability to convolve each modulated
EEG signal with a corresponding loudspeaker impulse
response (IR), brought on the idea of taking this project to the
next level. Users would be able to experience BSoniq with a
pair of headphones allowing flexibility for its use in nonlaboratory environments.

Figure 4: Speaker Layout

In the binaural format of BSoniq, the approach is to start with
measuring impulse responses of each loudspeakers that are
geometrically setup to represent the 14 sensor locations of the
Emotiv EEG device using the Neumann KU-100 Dummy head.
The resulting recorded stereo impulse responses are then split
into left and right channels using Matlab, yielding a total of 28
impulse responses. Max/MSP is used here to provide the
binaural experience in real-time.

4.4 Visual Representation
Though the main feature of BSoniq is its auditory display, it
can also visually display the relationship between the activity
level of each EEG channel. This optional feature aids the user
in visualizing what sensors are active and how much. The
visualization is represented by a 3D model head as shown in
Fig. 5 and 14 balls representing the EEG sensors and location.
These balls, created in Max/MSP/Jitter uses the
'jit.gl.gridshape' object, which generates simple geometric
shapes as a connected grid, in this case a sphere. These
spherical shapes (balls) increase and decrease in size based on
the sensor activity levels. The values used for visualization are
the same scaled values used for sonification. The stronger the
EEG signal the larger balls become. BSoniq also provides the
user with several angles to turn the head model for a better
view of the sensors. For example, if the user wants to have a
better view of the back sensors, he/she can rotate the head for a
side or back view of the balls (sensors).

Figure 6: Partial HRTF patch

Figure 5: Visual representation of sensor activity

In Max/MSP, we used the “buffir~” object, which is described
as “a buffer-based FIR filter that convolves an input signal
with samples from an input buffer” [16]. In this case the EEG
signal is convolved with the corresponding IR. Since we are
dealing with many channels of audio, we used the
“polybuffer~” object to ease the process of loading and
delivering the IRs to corresponding “buffir~” objects. After
each EEG convolution, the output is sent to the corresponding
left or right channel, thus giving a virtual representation of the
various EEG sensor locations. To give an example, in Figure 6,
the signal from the AF4 sensor is sent to the buffir~ object,
which takes the left output signal and convolves it with IR
sample 17 and convolving right output signal with IR sample
18. After the convolution of the AF4 signal, both left and right
convolved signals are sent to the “ezdac~” object for output.

216

The 23rd International Conference on Auditory Display (ICAD 2017)

5.

PROTOTYPING

The prototyping phase included evaluation by five users; three
males and two females. Most users were able to distinguish
EEG frequency changes as a result of the modulation process.
Users did indicate that using a sample file of sound they were
very familiar with, helped them to quicker understand the
sonification process. For example, one user could not
distinguish much difference with a sample file that was a bell
sound, but was able to pick up on subtle frequency changes
with a sound sample that was a snippet of a song he knew very
well. While wearing headphones, users were also able to hear
the location from where a particular modulated EEG signal
was coming from, indicating the virtual placement of the
sensors.

6.

DISCUSSION

We presented BSoniq, which uses multi-channels to sonically
represent EEG data in real-time 3D space using frequency
modulation. BSoniq could be used for both online and offline
sonification. By applying filters and parameter controls, it is
possible for the user to focus on the area of interest within the
signal. This is useful for real-time applications like EEG
monitoring or EEG feedback. The inclusion of 360 degrees
spatial cues permits the parallel sonification of many or all
electrodes without losing clarity in the display. Clarity of the
sonification, however also depends on the strength of the EEG
signal capture from the device. The signal could also contain
artifacts, which could be reduced or removed in order to yield
a clearer signal for the display. What also needs to be noted is
that the perceptual capabilities of the listener is important. If
the listener is unable to distinguish sounds or incapable of
hearing certain frequencies, then this would affect the user's
perception of the installation's functionality.
Future work includes conducting additional evaluations for
necessary design improvements as well as upgrading BSoniq
to include other popular EEG devices. The current installation
only allows for the user to remain stationary. Allowing the
user's head movement and tracking, is a feature that will be
added to create a fully integrated system.
To conclude, we believe we accomplished our goal of EEG
sonification using 3D spatial cues. Even though BSoniq
started out as an installation, mainly for an aesthetic user
listening experience, we also believe that in addition to
sonification, the visualization component could also be
enhanced into an artistic EEG visualization application using
geometric data and transformations for artistic applications.
That is, exploring methods that uses OpenGL for example, to
create 3D spatial-spectral representations of an EEG signal.

7.

REFERENCES

[1] Thomas Hermann, Andy Hunt and John Neuhoff.
“Auditory Display and Sonification”. The Sonification
Handbook, 2011.
[2] Gen Hori and Tomasz M. Rutkowski. “Brain listening–a
sound installation with EEG sonification”. Journal of the
Japanese Society for Sonic Arts, 4(3):4–7, 2000.

June 20-23, 2017, Pennsylvania State University

[3] Thomas Hermann and Helge Ritter. “Listen to your data:
Model-based sonification for data analysis”. Advances in
intelligent computing and multimedia systems, 8:189–194,
1999.
[4] Stephen Barrass and Gregory Kramer.
sonification”. Multimedia systems, 7(1):23–31, 1999.

“Using

[5] Emil Jovanov, Dusan Starcevic, and Vlada Radivojevic.
“Perceptualization of biomedical data”. IN MEDICINE, page
189, 2001.
[6] Teruaki Kaniwa, Hiroko Terasawa, Masaki Matsubara,
Tomasz M Rutkowski, and Shoji Makino. “EEG auditory
steady-state synchrony patterns sonification”. In Signal &
Information Processing Association Annual Summit and
Conference (APSIPA ASC), 2012 Asia-Pacific, pages 1–6.
IEEE, 2012.
[7] Tomasz M Rutkowski. “Multichannel EEG sonification
with ambisonics spatial sound environment”. In Asia-Pacific
Signal and Information Processing Association, 2014 Annual
Summit and Conference (APSIPA), pages 1–4. IEEE, 2014.
[8] Tomasz M Rutkowski, Francois Vialatte, Andrzej
Cichocki, Danilo P Mandic, and Allan Kardec Barros.
“Auditory feedback for brain computer interface management–
an EEG data sonification approach”. In Knowledge-Based
Intelligent Information and Engineering Systems, pages 1232–
1239. Springer, 2006.
[9] Timothy Schmele and Imanol Gomez. “Exploring 3d audio
for brain sonification”.
In International Conference of
Auditory Display, 2012.
[10] Haracio Tome-Marques and Bruce Pennycook. “From the
unseen to the s[cr]een eshofuni, an approach towards real-time
representation of brain data”. 2014.
[11] A Väljamäe, T Steffert, S Holland, X Marimon, R
Benitez, S Mealla, A Oliveira, and S Jordà. “A review of realtime EEG sonification research”. In International Conference
of Auditory Display, 2013.
[12]
Neuroscience
For
Kids.
(n.d.).,
from
<http://faculty.washington.edu/chudler/1020.html> Retrieved
February 10, 2016.
[13] Walker, Bruce, and Nees, Michael. "Theory of
sonification."The Sonification Handbook: 9-39, 2011.
[14] Emotiv Epoc EEG. <https://www.emotiv.com>. Retrieved
July 13, 2015.
[15] Gerold Baier, Thomas Hermann, and Ulrich Stephani.
“Multi-channel sonification of human EEG”. In Proceedings
of the 13th International Conference on Auditory Display,
2007.
[16] Max/MSP/Jitter Graphic software development
environment. Cycling '74. <www.cycling74.com>. Retrieved
November 3, 2016.
This work is licensed under Creative Commons
Attribution – Non Commercial 4.0 International License.
The full terms of the License are available at
http://creativecommons.org/licenses/by-nc/4.0/
217

