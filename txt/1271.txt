Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

MoodMixer: EEG-based Collaborative Sonification
Grace Leslie

Tim Mullen

Department of Music and
Swartz Center for Computational Neuroscience
University of California, San Diego
gleslie@ucsd.edu

Department of Cognitive Science and
Swartz Center for Computational Neuroscience
University of California, San Diego
tmullen@ucsd.edu

ABSTRACT

approach. Lucier describes his response in a 1981 interview,

MoodMixer is an interactive installation in which participants collaboratively navigate a two-dimensional music space
by manipulating their cognitive state and conveying this
state via wearable Electroencephalography (EEG) technology. The participants can choose to actively manipulate
or passively convey their cognitive state depending on their
desired approach and experience level. A four-channel electronic music mixture continuously conveys the participants’
expressed cognitive states while a colored visualization of
their locations on a two-dimensional projection of cognitive state attributes aids their navigation through the space.
MoodMixer is a collaborative experience that incorporates
aspects of both passive and active EEG sonification and
performance art. We discuss the technical design of the installation and place its collaborative sonification aesthetic
design within the context of existing EEG-based music and
art.

“... most of my colleagues at Brandeis said,
“Oh, that’s a wonderful idea. You ought to tape
record it, speed the sounds of the brain waves
up, slow them down, reverberate them, filter
them”.... I had to eliminate those [techniques]
in order to get at the poetry of the piece, which
demanded that a solo performer sit in front of
an audience and try to get in that alpha state
and to make his or her brain waves come out.[6]”
Thirty-eight years later, Steve Mann, James Fung, Ariel
Garten and Chris Aimone produced the Regen/DECONcert
series in which 48 participants donned wearable EEG hardware to manipulate musical parameters of a jazz ensemble
performance [8]. The performers were not given any explicit instructions as to how they should manipulate their
cognitive state; rather, the collective alpha activity of the
population was mapped onto musical parameters directly.
These two pieces sit on the extremes of a passive-active
sonification axis; while Solo Performer represents an active approach in which the performer is consciously manipulating his or her brain state to reach a desired musical
effect, Regen embodies a passive approach where participants’ EEG is used for musical purposes irrespective of any
direct, conscious control on their part.
The passive-active continuum in EEG sonification systems – often referred to as brain-computer music interfaces
(BCMI) [9] – can be seen as a specific instance of a more general passive-active-reactive categorization scheme recently
proposed within the brain computer interface (BCI) community [13]. A passive BCI is one in which the cognitive
state of the individual is unobtrusively “monitored” without
conscious control on the part of the individual. Feedback is
not a necessary component. An active BCI is a closed-loop
system which derives its outputs from brain activity which
is directly and voluntarily controlled by the user, independently from external events, with the intention of controlling
an application. Real-time feedback indicating the current
output state of the system is generally an essential component. A reactive BCI is similar to an active BCI. Here the
system measures the neural response to external stimulation. The user exerts control over the system by voluntarily directing his attention or otherwise indirectly controling
how the brain processes the external stimuli. One example
of a reactive BCI system as a music interface is Mick Grierson’s adaptation of a standard “P300 Speller” BCI to allow
an individual to compose musical note sequences by selectively attending to different symbols randomly illuminated
on a computer display. When the attended symbol is briefly
illuminated, the brain generates a neural response, which is
detected in the EEG, and used to produce an associated
note [3].

Keywords
EEG, BCMI, collaboration, sonification, visualization

1.

INTRODUCTION

Alvin Lucier’s Music for Solo Performer was premiered in
1965 at Brandeis University. Widely considered the first
live brainwave music performance, it represented a break
from the electronic music performance tradition of the time,
and remains unique for a number of reasons. First, the
performer’s alpha (8-12.5 Hz) brainwaves drive percussion
instruments directly through coupled amplifiers, allowing
him to generate real acoustic events (not synthesized ones)
at roughly 10 Hz. The resulting “acoustic” sound material
contrasts with the synthesized or spliced concrète styles of
electronic music making of the time. Even today, Solo Performer remains somewhat distinct as an electronic performance in its physically manifest yet acousmatic materials.
A second, more subtle and important distinction that
Solo Performer holds, at least in 35 years of hindsight,
lies in the active and purposeful nature by which the performer modulates his attentional state to produce these
sound events. Approaches that we would now describe as
sonification were common in the tape music community of
the time, but Lucier avoided these in favor of a more active

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
NIME’11, 30 May–1 June 2011, Oslo, Norway.
Copyright remains with the author(s).

296

Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

Solo Performer and Regen also exemplify two extremes of
a solo-collaborative sonification axis. Solo Performer, like
many contemporary BCMI systems, involves a single individual interacting with the system in isolation. EEG-based
musical performances with multiple performers, while far
less common than solo performances, date back to Rosenboom’s 1971 Ecology of The Skin [12]. Here EEG alpha
power from ten observer-participants was used to control
ten parts of a synthesized sound texture. This is an early
example of a collaborative BCMI in which the state of the
system is determined by the neural state of two or more
individuals. More recently, Miranda and Brouse proposed
a “collaborative audification” approach in their Internetenabled InterHarmonium project which introduces additional possibilities for large-scale collaborative BCMIs [9].
Regen embodied an extreme end of the collaborative sonification spectrum wherein the state of the music interface
was determined by the collective neural activity of a large
number of people.
Our MoodMixer project was conceived to incorporate aspects of both the active and passive approaches to EEGbased music creation while using hardware, data treatments,
and electroacoustic and visualization techniques to facilitate
a multiuser, collaborative approach.

Novice participants may wish to adopt an observational
relationship with the system as their changing cognitive
state is passively monitored and represented in the audiovisual landscape. As participants use the feedback to gain
experience in manipulating their neural state and actively
controlling the interface they may shift to an “active” regime
where they may choose to “improvise” and independently
experiment with combinations of music drawn from two different regions of the music landscape. Alternately, participants may choose to attempt to cooperate closely with each
other in creating a composition by mirroring the other’s affective state (e.g., if one participant is in a relaxed, but
focused state, the other should attempt to do likewise).
Participants may define “games” which they play with each
other. For instance, during a “calm” interval one participant might execute a sequence of blinks triggering a sound
sample or visual pulse intended to induce dramatic changes
in the arousal of the other participant, and thereby a dramatic shift in the evolving composition. Real-time visual
feedback of each participant’s current position (represented
as a colored cursor) in the two-dimensional musical landscape affords an increased sense of control and interactive
engagement in the compositional process.

2.1
2.

Technical Design

The technical architecture of the system is depicted in Figure 2. In our first implementations we have used the Neurosky MindSetTM , a relatively low-cost (< $200) wearable
EEG system1 . As seen in Figure 2, the system features
a “headset” design with a single active electrode (left or
right prefrontal cortex), as well as reference and ground electrodes on the earlobes. The headsets utilize “dry” (gel-free)
sensing technology and feature integrated Bluetooth, alowing data to be streamed wirelessly to a laptop. Raw EEG
data from the single electrode is sampled at a rate of 512 Hz.
Spectral power in the delta (0.1-3 Hz), theta (4-7), alpha (812 Hz), low, midrange, and high beta (12-15, 16-20, and 2130 Hz, respectively) are calculated on the headset once per
second using fast fourier transforms. Two cognitive state
indices (“focus” and “meditation/relaxation”) are then calculated using specific combinations of these bandpower features intended to correlate with the degree of focused attention/cognitive load and meditation/relaxation [1, 11]. Although the specific combination of features is proprietary to
Neurosky, it is well-known that frontal alpha power is positively correlated with relaxation and/or meditative states of
mind, while frontal beta power is positively correlated with
increased concentration and focus. Experimenting with our
own frequency ratios confirmed this. However, Neurosky’s
indices are based on a large normative dataset and thus provide immediately useable (if only approximate) normalized
(0-100%) estimates of the indicated cognitive state without the need to collect any calibration data. The raw EEG
data stream and relaxation and focus indices are streamed
into a Max/MSP/Jitter patch via a Bluetooth connection
using an external by Kyle Machulis2 . The index values are
then smoothed using a four-second moving average. The
smoothed indices are used to control a four-way equal loudness panner which assigns the relative loudness “weights” of
four audio tracks, each containing music material representing a combination of two of the cognitive index extremes
(e.g., high focus, low relaxation). The music mix is projected via a four-channel surround sound system to create
a spatial representation of the participant’s mental state.
MoodMixer incorporates a two-dimensional colored visual

COLLABORATIVE SONIFICATION

The performance instructions for the installation are as follows: The two participants sit in a room, each wearing a
comfortable wireless EEG headset, as depicted in Figure 1.
From each EEG data stream, two indices, each measuring
a different aspect of the participant’s cognitive state, are
calculated. By mapping each measured state onto a onedimensional axis, each participant is able to independently
“navigate” within a shared two-dimensional musical interface; in this way, the musical aesthetic at a given point
in time is determined by the combined cognitive state of
both participants. In this instantiation, location along the
ordinate (x-axis) is determined by a calculated index that
roughly corresponds to the degree of relaxation or “meditation”, while location along the abscissa (y-axis) is determined by an index corresponding to the participant’s level
of sustained attention or “focus.” Another active control
option is provided, as sound samples and visual flashes may
be triggered by individual eye blinks or predefined blink
sequences.

1

Figure 1: Depiction of the installation in use.

2

297

http://www.neurosky.com
http://www.nonpolynomial.com

Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

2.2

:&3'1%#;-,+!<<&'3#1')#
=')!>#?1%+4%1@,'#

:&3'1%#;-,+!<<&'3#1')#
=')!>#?1%+4%1@,'#

?4-<,-#
?,'*-,%#
B#

!"!#$%&'(#
)!*!+*,-#
I-&33!-#
G5!'*#

&')!>#8#

&')!>#7#

I-&33!-#
G5!'*#

?4-<,-#
?,'*-,%#

./01"#21''&'3#+4-5!#

C#

67#

68#

69#

B#
C#

.#0!&3D*<#E,-#.#5&<41%#3-1)&!'*<#

.#0!&3D*<#E,-#.#14)&,#*-1+(<#
6.#

%,#

&')!>#8#

D&#

A

&')!>#7#

%,#

D&#

!"#$%&'()*+,--"$#'%$.'/$.,0'
1%&+2&%3*$'
==>'
4*2)",)'%$%&5-"-'%$.'
/$.,0'1%&+2&%3*$'
"$.,0;'

"$.,0<'

6*7"$#'%7,)%#,'
89'-,+*$.:'
"$.,0';'

"$.,0'<'

Figure 2: Diagram of the installation hardware
setup with the communication protocols between
components.
In our installation, index1 corresponds to “relaxation/meditation” and index2 to
“attention/focus”

Mixing Music to Match Mental States

The music mix was designed to fulfill a few simultaneous
expectations: first, each of the four tracks represents a combination of extreme values along the two axes, which, when
listened to in isolation, were intended to subjectively represent that particular extreme state. For instance, one might
include samples of beat-driven, yet ambient music to reflect
an alert/focused yet relaxed state of mind. Alternately,
more spastic, unpredictable samples of music could be chosen to reflect states of high focus and low relaxation (anxious, agitated). However, due to the aleatoric nature of the
design, there is a relatively low probability of the tracks being heard on their own for a sustained period of time. The
tracks mesh together effectively so that any combination of
all four tracks would sound good together while also conveying a state in between the focus-relaxation extremes. This
was primarily achieved in an intuitive fashion, by composing with music samples representing a particular extreme,
but having a sonic palette and rhythmic profile in common.

3.

F<!-#8#GGH#

&')!>#7#

!"!#$%&'(#
)!*!+*,-#

F<!-#7#GGH#

&')!>#8#

representation (implemented in Jitter) in which each vertex
of a square represents a combination of extreme values of
the two cognitive indices. The square is comprised of a
weighted mixture of four colored gradients, each associated
with one vertex of the square, the weights of which are determined by the four-way panning curve. Each participant
is given control of a uniquely-colored cursor, indicating his
or her position in the two-dimensional cognitive landscape.
Specifically, the participant’s two cognitive indices are respectively mapped to the x- and y- coordinates of the cursor.
Thus, as each cursor approaches a given vertex, the luminosity of the associated color gradient is smoothly increased
(and that of other vertices proportionately decreased) in accordance with the levels of the associated cognitive indices.
This approach is conceptually similar to Jacqueline Humbert’s 1974 Brainwave Etch-A-Sketch in which two individuals, by manipulating their alpha power, each control the x
or y position of a point of light on two-dimensional analog
interactive display [12]. However, Humbert’s approach is
restricted to exactly two participants. Giving each participant independent control over both axes allows the interface
to be used by one or many individuals, expanding the range
of possible applications.
Participants have the option to use eye blinks to trigger
sounds and visual effects. Eye blinks are detected by calculating, in real-time, the standard deviation of the raw EEG
signal within a short (200 ms) sliding window. If the standard deviation exceeds a predetermined threshold, a blink
is indicated and an event may be triggered. For example,
a blink can trigger a pulsatile flash which emphasizes the
quadrant(s) wherein the participants’ cursors are located,
optionally accompanied by a short audio sample (e.g., a
drum beat). Another option allows the use of predefined
sequences of eye blinks within a specified time interval to
trigger a larger repertoire of events, including additional audio samples or visual effects. Although some care is taken
to reduce false positives, for instance, suppressing an event
trigger if the interval between two consecutive suprathreshold events is less than 40 ms, as often occurs in movement
or muscle artifacts, more complex and accurate blink detection routines can be implemented and may be used in
future versions of the installation.

treatments of EEG data to extract indices which can represent a wider range of cognitive and affective states. Due to
its use of relatively inexpensive and easily obtainable hardware, MoodMixer can be readily extended to an arbitrary
number of participants who can collaboratively control the
system, linked over the Internet using the Open Sound Control (OSC) protocol. Conversely, MoodMixer can also currently be used by a single individual, with audio represented
in either the four-channel surround sound configuration or
collapsed to stereo for portable use.
Although the reported instantiation of MoodMixer uses
Neurosky’s measures of focus and relaxation, future instantiations may incorporate EEG-based measures of affective
state, such as emotional arousal (“active” vs. “calm”) and
valence (“positive/good” vs. “negative/bad”). Recent research performed on human subjects listening to short music pieces [5, 4], has shown that changes in self-reported
emotional arousal and valence induced by listening to music pieces, as well as tempo (fast/slow) and mode (major/minor) of listened music, is significantly correlated with

DISCUSSION AND FUTURE WORK

Future manifestations of this system will focus on expanding the collaborative aspect of the design and explore other

298

Proceedings of the International Conference on New Interfaces for Musical Expression, 30 May - 1 June 2011, Oslo, Norway

changes in EEG bandpower. Specifically, it was found that
listening to minor mode music was correlated with increased
frontal midline gamma power (25-60 Hz) while the converse
was true for major mode music. Additionally, listening
to slow-tempo music correlated with increased theta (4-8
Hz) activity. Furthermore, in a related study using the
same music dataset, positive emotional valence was positively correlated with theta power and negatively correlated
with delta power, while emotional arousal was positively
correlated with both delta and theta power [5]. A recent
study by our colleagues Onton and Makeig demonstrated
that EEG features associated with 15 prototypical emotional states (Joy, Love, Frustration, ...), when mapped onto
a two-dimensional plane using non-metric multidimensional
scaling (MDS), formed a circumplex pattern with features
corresponding to similar emotions located near each other
in the MDS space, and negative valence emotions arrayed
on the left and positive valence emotions on the right [10].
A natural extension of MoodMixer would allow participants
to emotionally navigate a similar two-dimensional audiovisual space, wherein music tracks associated with each of a
number of affective states are each assigned a corresponding coordinate in the transformed MDS space and continually mixed based on the participants’ positions in the transformed MDS space. Future generations of the MoodMixer
design may also incorporate algorithmic composition techniques to generate a musical “mood” mixture in real time
which corresponds with the participants’ affective or cognitive states.
Accurate detection of many of the aforementioned complex cognitive and affective states likely requires recording
of signals from multiple brain regions. While our current instantiation of MoodMixer uses a single-electrode wearable
EEG system, which is suitable for inferring basic states of
arousal and cognitive load, the use of a multichannel system would provide data from more scalp locations allowing for calculation of interhemispheric differences as well
as enabling robust spatiotemporal source separation techniques, such as Independent Component Analysis which can
dramatically improve the signal to noise ratio and interpretability of the acquired data [7, 2]. This in turn would
expand the range of cognitive and affective states that can
be monitored for control of the system. Fortunately, wearable multichannel EEG hardware is now becoming ubiquitous with a number of established and nascent companies offering affordable multichannel “dry” (gel-free) electrode systems (Emotiv, Quasar, BrainProducts GmbH, PICO imaging, g.tec GmbH, to name a few). We are currently experimenting with a 16-channel wearable EEG system and plan
to incorporate this technology into future instantiations of
the installation.
Our use of wearable, wireless EEG technology introduces
additional practical applications of MoodMixer. For instance, it may be used as a single-player or collaborative
game in which players attempt to actively manipulate their
cognitive or affective states to produce different musical/visual
mixtures. Or it might be used as a computer desktop gadget/widget or mobile phone app in which an aestheticallypleasing “background” electronic music mix (with optional
visual texture) is continually generated based on passive
monitoring of the user’s mental state as they go about their
day. Such a system may even have therapeutic applications,
for example allowing one to inobtrusively monitor their own
levels of stress or relaxation throughout the day. There are a
number of possibilities, and we hope that this and other extensions of the MoodMixer concept will expand the existing
repertoire of fun and aesthetically-pleasing systems for individual or collaborative, passive or active cognitive/affective

299

sonification.

4.

CONCLUSIONS

In this collaborative EEG sonification system, two participants control a music mix and corresponding visualization by actively or passively manipulating their cognitive
state. This approach is novel in its collaborative design,
and in that it affords both active and passive sonification
approaches. MoodMixer represents a first step towards new
media projects which explore new modes of social interaction and affective processing and control in brain-computer
music interfaces.

5.

ACKNOWLEDGMENTS

Thanks to Neurosky for donating two of their MindSets as
part of their academic partnership program.

6.

REFERENCES

[1] NeuroSky’s eSenseTM Meters and Detection of Mental
State. Technical report, 2009.
[2] A. Bell and T. Sejnowski. An informationmaximization approach to blind separation and blind
deconvolution. Neural computation, 7(6):1129–1159,
1995.
[3] M. Grierson. Composing With Brainwaves: Minimal
Trial P300 Recognition As An Indication of Subjective
Preference For the Control of a Musical Instrument.
International Computer Music Conference, 2008.
[4] Y. Lin, J. Duann, J. Chen, and T. Jung.
Electroencephalographic dynamics of musical emotion
perception revealed by independent spectral
components. NeuroReport, 21(6):410, 2010.
[5] Y. Lin, C. Wang, T. Jung, T. Wu, S. Jeng, J. Duann,
and J. Chen. EEG-Based Emotion Recognition in
Music Listening. IEEE Transactions on Biomedical
Engineering, 57(7):1798–1806, 2010.
[6] A. Lucier. Reflections: interviews, scores, writings=
Reflexionen: Interviews, Notationen, Texte. Köln:
MusikTexte, 1995.
[7] S. Makeig, A. Bell, T. Jung, T. Sejnowski, et al.
Independent component analysis of
electroencephalographic data. Advances in neural
information processing systems, pages 145–151, 1996.
[8] S. Mann, J. Fung, and A. Garten. DECONcert:
Bathing in the light, sound, and waters of the musical
brainbaths. 2007.
[9] E. Miranda and A. Brouse. Interfacing the Brain
Directly with Musical Systems: On developing
systems for making music with brain signals.
Leonardo, 38(4):331–336, 2005.
[10] J. Onton and S. Makeig. High-frequency broadband
modulations of electroencephalographic spectra.
Frontiers in human neuroscience, 3, 2009.
[11] G. Rebolledo-Mendez, I. Dunwell, E. Martı́nez-Mirón,
M. Vargas-Cerdán, S. de Freitas, F. Liarokapis, and
A. Garcı́a-Gaona. Assessing Neurosky’s usability to
detect attention levels in an assessment exercise. New
Trends in Human-Computer Interaction, pages
149–158, 2009.
[12] D. Rosenboom. Biofeedback and the arts: results of
early experiments. Aesthetic Research Centre of
Canada, 1976.
[13] T. Zander, C. Kothe, S. Jatzev, and M. Gaertner.
Enhancing human-computer interaction with input
from active and passive brain-computer interfaces.
Brain-Computer Interfaces, pages 181–199, 2010.

