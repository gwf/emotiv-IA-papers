2016 49th Hawaii International Conference on System Sciences

Enhancing the Professional Vision of Teachers:
A Physiological Study of Teaching Analytics
Dashboards of Students’ Repertory Grid
Exercises in Business Education
Ravi Vatrapu
* Computational Social Science Laboratory
Department of IT Management,
Copenhagen Business School, Denmark
** Mobile Technology Laboratory
Faculty of Technology
Westerdals Oslo School of Arts, Norway
Email: vatrapu@cbs.dk

Kostas Pantazos
Software and Systems Section
IT University of Copenhagen
Copenhagen, Denmark
Email: kopa@itu.dk

Abstract

environment. This transformation [8] demands that teachers
create effective, efﬁcient, enjoyable and sustainable learning
practices, which often requires additional time and effort.
Different tools have been developed to enhance analysis for
various skilled users (e.g. programmers, end-user developers),
but still there is room for improvement [36] towards the
main goal: engage teachers directly. Therefore, researchers are
continuously studying educational data and tools in order to
improve learning.
Learning Analytics (LA) is an emerging research discipline
that investigates ”the collection and analysis of usage data
associated with student learning” and aims at improving education through interventions after observing and understanding
learning behaviors [7]. Vatrapu et al. [47] introduced a new
theoretical approach called teaching analytics, which combines
visual analytics, teaching expertise and design-based research
in a triadic model to support teacher’s diagnostic pedagogical
decision-making in classrooms. Enabling teachers perform
analysis through visual representations aims at facilitating
assessing students’ understandings and knowledge [9], [42].
Teachers can identify new learning insights and study concepts
and ideas that were almost unfeasible to investigate within
the allocated time. Therefore, the goal is to enhance teachers’
experience and ability through teaching analytics and visual
representations.
In this respect, we endeavor towards designing, developing
and evaluating teaching analytics dashboards, and focus our
attention on visual representation of the repertory grid data.
Repertory grid data can indicate to teachers the knowledge
students have on speciﬁc topics [46] and visual representations
aim to enhance human cognition [9]. Therefore, we utilized
the repertory grid and visual techniques to augment teachers’
vision. We used design principles from the visualization area
([5], [42]) and developed visual representations (i.e. dash-

This paper reports on a study of the design, development and evaluation
of two teaching analytics dashboards that visualize students’ repertory grid
exercise data. The technical objective of the dashboards is to support teachers
to investigate and compare personal constructs and element ratings by
students for given topics of study at the individual student, group or classroom
levels of analysis. The pedagogical objective of the dashboards is to facilitate
formative assessment feedback to students and feed-forward reﬁnements to
ongoing instructional practices. The teaching analytics dashboards were
evaluated with six university teachers in a bio-metric usability study with
integrated physiological and performance measures. Eye-tracking data from
the two dashboards showed the relative importance of dashboard areas in
each case in terms of aggregate gaze allocation. Findings informed the
iterative design of the dashboards leading to teachers not being adversely
affected by the visual clutter of the less important dashboard areas in their
pedagogical decision-making. The results also showed that the dashboards
were efﬁcient and effective from a task performance perspective and were
rated to be pleasant from a subjective satisfaction perspective. Regarding
education technology innovation, teachers reported that such dashboards are
lacking in their regular practice and would recommend their use in formal
business educational settings.

Keywords
Teaching analytics; dashboards; repertory grid; bio-metric usability study;

1. Introduction
Today’s technologies provide great opportunities for generating valuable learning resources. Capitalizing on the development within technology towards data, education can better
utilize information, which can improve learning and teaching.
Unlike several years ago where data collection was sparse and
inadequate to support teachers, nowadays, technology supports
them with easy and systematic collection of data [32]. Consequently, new technologies are shifting education from a scarce
data environment towards an abundant and ﬁne-grained data
1530-1605/16 $31.00 © 2016 IEEE
DOI 10.1109/HICSS.2016.14

41

of learning analytics in enhancing the learning and teaching
practices towards better education is depicted in the increasing
number of studies (e.g. [21], [45], [49]).
Previous research [1], [26], [28] has discussed the challenge of learning and teaching. Marjanovic [28] presents a
modeling language, which can be utilized to articulate new
learning designs. Furthermore, supporting teachers with more
appropriate guidance can allow them to better perform, adjust,
and re-use their teaching procedures [28]. New approaches are
being investigated in the established ﬁeld of learning sciences
and emerging ﬁeld of learning analytics to improve traditional
learning and teaching strategies and practices.

boards) using uVis [35] that visualize personal constructs and
element ratings of students at an individual and/or a group
level. For this purpose we designed and developed dashboards
that are single screen visual displays that utilize concise,
clear and intuitive visual objects to present data in order to
reveal trends, patterns and outliers [17]. These dashboards
were evaluated with six teachers from our faculty.
The ﬁndings showed that current teaching practices of
knowledge diagnostics of students are primarily based on
questions and group/class discussions, where data are not
systematically collected and analyzed. The results also showed
that teachers need tools to explore different types of questions
about declarative and procedural knowledge of their students,
ongoing teaching practices and emergent learning outcomes.
The eye-tracking data from the dashboards showed the importance of screen areas in each case, indicating the need
for high level of perceptibility and simplicity. The emotions
computed from the electroencephalogram (EEG) data did
not denote any preference on performing tasks in different
dashboards. (suggested from the guidelines [51] related to the
overwhelming screen). This paper is a ﬁrst attempt towards
teaching analytics dashboards and provides useful insights for
existing learning practices in educational environments.
The remaining of this paper is organized as follows. First,
we discuss related work on repertory grid, learning analytics
and dashboards. Next, we describe the design of the study.
Results are presented in the next section, followed by the
limitation section. The paper concludes with ﬁnal remarks and
future work.

2.3. Dashboards
Dashboards are single screen visual displays that utilize
concise, clear and intuitive visual objects to present data in
order to reveal trends, patterns and outliers [17]. Furthermore,
Few discusses that a well-designed dashboard aims at presenting data and revealing trends, patterns and outliers, which
in response may guide viewers towards effective decisions.
Dashboard is related tightly to information visualization that
aims to enhance human cognition [9]. As in a pedagogical
environment professionals has to monitor several data (e.g.
questions, mood, ratings, progress, etc.), dashboards become
an important factor to improve and conduct successful teaching.
To design the teaching analytics dashboards, we adhered
to the design principles from information visualization [42],
[10], [5]. Duval [14] highlighted the importance of information
visualization in education. Although considerable work has
been done on interactive visualizations in the information
visualization community, there are still large knowledge gaps
when it comes to visualizations and dashboards for educational
data.
Verbert et al. [49] present the concept of learning analytics
dashboards. Their study introduced a conceptual framework to
support learners and teachers utilize applications that visualize
learning traces. The authors [49] reviewed 15 educational tools
and investigated the use of dashboards by focusing on students,
teachers and tracked data (i.e. time spent, social interaction,
document use, tool appropriation, artifacts produced and exercise results. Their results showed that only Course Signals
tool validated the usefulness of dashboards to learning and
highlighted the need for further studies to investigate the role
of dashboards in learning contexts and educational settings.
In this paper, we utilize the repertory grid data for dashboards and visual analysis. In comparison to previous work
on learning analytics dashboards, our work concentrates on
teaching analytics dashboards that are meant to enhance
teachers’ professional vision. The dashboards provide teachers
with an overview of the teaching topic and details-on-demand
about the personal constructs and element ratings on individual
and/or group level.

2. Related Work
2.1. Repertory Grid
Repertory grid (RG) is a technique to elicit the personal
constructs of individuals to determine the relationships among
concepts of a topic. This technique is based on the personal
construct theory developed by George Kelly [25].
A grid consists of: a topic, a set of elements, a set of
personally elicited constructs, and a set of ratings of elements
of the personal constructs. Vatrapu et al. [46] provide a clear
example of a grid, and discuss how to integrate the repertory
grid technique in ”teaching analytics” as in-class activity or
a take-home exercise. The repertory grid technique can be
used as a pedagogical method by teachers for knowledge
diagnostics of students about a speciﬁc topic of study [46].
In this paper, we aim to support the pedagogical method of
the repertory grid with computational tools (teaching analytics
dashboards) to support teachers professional vision [47].

2.2. Learning Analytics
Learning Analytics aims at improving education through
interventions after observing and understanding learning behaviors [7]. That is, ”the collection and analysis of usage
data associated with student learning” [7]. The importance

42

Measuring the electrical activity of the human brain is a
common practice in cognitive neuro science, and the ﬁrst study
goes back in 1930 when Berger recorded brain activity as an
electroencephalogram [4]. Brain activity depicts information
processing and is used to measure cognitive task performance.
Analyzing EEG data collected during a cognitive task can
determine the level of involvement [12]. Brain activity is also
used to measure emotional states during different situations
[53], [19].
Studies [19], [53] have shown that the frontal lobe activity
has been found to be correlated with emotions. Other studies
[18], [11] have investigated how emotions affects decision
making. These studies showed that emotions are important for
decision making and problem solving.
Moreover, trying to minimize the effects on evaluation, we
employ physiological devices. We incorporate physiological
measures as rigorous and unbiased data collection techniques
[29], [13].
Therefore, we used usability tests because we wanted to
learn what real users believe and enhance our results with
data collected by an eye tracker and an EEG headset. In this
way, this evaluation combines subjective, performance and
physiological measures.

2.4. Evaluation Methods
A considerable amount of work has been done in identifying
evaluation methods, design principles and challenges faced
during evaluation sessions. It is a common practice to employ
usability tests or controlled experiments for evaluations.
A usability test is cheap and provides useful information
of what domain users believe [31]. Usability studies are
subject to biases and according to Hertzum and Jacobsen
[23], the evaluator effect can have an impact on data validity.
Controlled experiment results primarily rely on performance
data: task accuracy and completion time. However, controlled
experiments do not provide the same insights as usability tests
and are also subject to biases. Designing and conducting a
controlled experiment can be biased because it involves several
steps: including deﬁning the hypothesis and the dependent and
independent variables, selecting a random sample, analyzing
the data, etc. These steps have to be designed and performed
in standardized routines [27]. Despite open issues regarding
evaluation biases, it seems to us that evaluations primarily rely
on the case and what you want to learn.
Cognition or cognitive lead is concerned with the mental
effort spent when a particular task is performed [34]. Mental
load, mental effort and performance are three aspects of
cognitive load. In their model Paas and van Merienboer [34]
refer to mental effort as the aspect that is related to cognitive
load, and is recorded while users execute a particular task.
Therefore, in visualization cognitive load can be seen as a
major factor that deﬁnes the success of visualization.
Cognitive Load Theory (CLT) [37], [38], [39] suggests
that humans have limited cognitive processing capacity to
utilize while they learn and solve a particular task, and their
working memory can be affected by the presentation (in our
case by the visualization). According to CLT, task failures
occur when task demands overload cognitive capacity, provide
insufﬁcient distribution of cognitive sources, or both [33]. It
is recommended that cognitive load caused by an instructional
design (visual representation in this case) should not exceed
the working memory [33].
Information visualization enhances human cognition [9],
thus it is important to investigate cognitive process involved
in the perception and appropriation of the affordances [48]
of multiple view dashboards. More speciﬁcally we refer to
the relevance of cognitive load and CLT to understanding the
performance of users with respect to dashboards in teaching
analytics.
Eye movements provide important information regarding
users’ gaze behaviors while they perform visual-cognitive
tasks [22]. Therefore, it can be said that well-designed visual
representations can enhance human cognition [9].
Fixations are eye movements that occur when the visual
gaze is stabilized in an area of interest [22]. High number of
ﬁxations imply additional cognitive effort in visual search, and
may result to low performance level [52], [43], [24]. According
to Tatler et al. [41], scan paths and the number of ﬁxations
vary on task type.

3. Teaching Analytics Dashboards
3.1. Dashboard Design
Inspired by [47], we developed teaching analytics dashboards. They combine visual representations, teaching expertise and design-based research in to support teacher’s diagnostic pedagogical decision-making in classrooms. Therefore, we
performed an iterative process and designed and developed a
pedagogical visual representation for teachers for knowledge
diagnostics of students about a speciﬁc topic of study.
We utilized the repertory grid technique and several visual
design principles to create teaching analytics dashboard. Figure 1 shows how repertory grid data from a Social Media
Analytics course were used to design and develop the teaching
analytics dashboard. The dashboard contains: a word cloud, a
bubble graph, a track bar for frequency, and two check boxes
for selecting triads and students.
The data were collected from the Social Media Analytics
course using the Repertory Grid for Formative Assessment
(RGFA) website. The topic for the repertory grid exercise was
social media. The eight elements were: facebook, youtube,
vimeo, ﬂickr, foursquare, linked in, twitter and digg. The
elements were selected to range from popular to potentially
not popular websites, personal taste vs. social inﬂuences and
ranging from functionality involving little or great consideration time. The teacher made the design decision to select
ﬁve triads to include in the exercise. The order of presentation
of the triads was randomized to control for practice effects.
Students ﬁlled their preferences and knowledge on eight social
media sites. These data were exported in an MS Access ﬁle.

43

Fig. 1. The repertory grid dashboard for comparative analysis among students. The top shows an overview and
details-on-demand of one group. The bottom view facilitates comparative analysis between two groups. Due to space
limitations full sized images can be found here [16].
The dashboard uses this MS Access ﬁle and generates the
representation.

no need for having more than two dashboards in a screen.
Therefore, we decided to utilize only two dashboards for
comparative tasks. Furthermore, the dashboard is scalable in
terms of students and triads used in a study.
The dashboard is an interactive environment that allows
teachers narrow down their analysis to speciﬁc triads and
students. It provides an overview and details-on-demand of the
collected data from students. The dashboard goal is to assist
teachers conduct easier analysis than in traditional tabular
formats, assist them in better assessment, and obtain easier
knowledge on students’ education on the topic.
The dashboard provides several interaction mechanisms:

3.2. Dashboard Development
The dashboard was developed using uVis [35] uVis has
an integrated development environment, where designers can
create visualizations by dragging and dropping controls (e.g.
triangle, rectangle, etc.) in the design panel, and specifying
spreadsheet-like formulas in the property grid. By means of
formulas, controls can be bound to data, refer to other controls,
deﬁne the appearance, and the behavior of each control.
The formula language and the integrated development environment allowed us to rapidly create several visualization
prototypes. Initially we aimed for a scalable user interface
that can have more than two dashboards in a screen. However,
during development we noticed that space efﬁciency was an
issue and our teaching experience indicated that there was

•

44

Teachers can interact using the track bars, as shown in
Figure 1. In this way they can select students and set the
frequency for the word cloud. Using the tack bars will
have an effect on the word-cloud and the triads. In this
way the teacher can view the most representative word(s)

and in which triad these words are mainly used.
They can also interact with a concept (word) and investigate it in details. In this way, they can seek answers
related to questions of how, who and why a particular
concept is elicited more than others and the relative
distribution of concepts in the classroom as a whole.
• Teacher can also customize the screen by choosing the
triads they are interested in. They use the check-boxes to
ﬁlter down the presented information.
The dashboard inherits principles and techniques from the
visualization ﬁeld. It employs the word-cloud technique and a
custom bubble-chart approach.
• The word-cloud provides an overview of all elicited
constructs (i.e. opposite and similarity constructs).
• The bubble chart provides details on demand aggregating
and showing element ratings for each triad.
Filtering techniques are employed to support targeted noticing and to increase users’ satisfaction. Small letters (i.e. Ssimilar and D-different) are positioned above bubbles that
represent triad’s elements. The purpose is to enhance teachers’
ability in relating and distinguishing opposite from similar
personal constructs and corresponding elements.
Additionally, color and size encodings are used to also
denote similarity and/or opposite constructs. The green color
decrease as the rating number goes from 5 to 1. The same
feature is implemented for the bubble size. It decreases as
the value decreases. The small letter S (similar) is colored in
green, while the D (different) letter is colored in red. This
pattern was inherited from the trafﬁc lights example, familiar
to most of us. Teacher can also click on a bubble and a
box with speciﬁc information regarding similar and different
constructs is shown. This aims at increasing understandability
by providing context data.
Figure 1 (bottom) also shows a dashboard that utilize
the same design principles and functionalities as the above
dashboard, but it supports better comparison tasks. The scope
of this dashboard is to make comparison easier between
groups of students in the same class, between pre- and postteaching knowledge diagnostics and between different classes
that answered the same repertory grid exercise.
The dashboard provides an overview of the results aiming
at providing better information than a tabular view. In case
a teacher needs more details regarding the content used, the
teacher can double click on a single triad, expand it, and click
on a bubble. As shown in the ﬁgure 1, a rectangle will be
opened showing the different and similar constructs. In this
way the teacher can obtain an overview but also explore and
get detail-on demand regarding triads and students.

topics [46], our design-based research hypothesis was that
visual analytics in general and dashboards in particular can
enhance teachers’ professional vision of knowledge diagnostics. We conducted a study to investigate our design on the
usefulness and relevance of the teaching analytics dashboards.
We conducted an heuristic evaluation of the two dashboards
based on the visual analytics design guidelines formulated by
Ware [50].

•

4.2. Participants
The lab study sample was drawn from university colleagues
that were not familiar or involved with the research study.
Six authentic users (university teachers) participated and completed in the eye-tracking study. All of them had teaching
experience of at least two full courses at the university level.

4.3. Apparatus
The pilot and ﬁnal study were performed in a usability
lab. We used an eye-tracking device, an electroencephalogram
(EEG) headset, and iMotions Attention tool [3] to integrate and
collect data. The EEG data were collected using the Emotiv
EPOC research EEG neuroheadset [15]. The SMI iView X
Red eye-tracker device was used to collect eye activity data.

4.4. Procedure and Tasks
The study was divided in ﬁve parts and lasted one hour
on average. First, the participant was asked a few general
questions such as ”how many courses have you taught, do
you know repertory grid technique, etc.”
As they may have not used the repertory grid technique,
in the second part, we described to the participants how the
repertory grid technique works, and asked them to reply to a
training grid (similar but identical to the study with the grid
that the students ﬁlled in class) using the RGFA tool [46]. The
purpose was to create or enforce participant’s understanding
on the repertory grid technique. Moreover, as they were asked
to perform several tasks using the dashboard, it was important
to explain them and provide a good understanding of the data
used. For example they used triads and constructs to ﬁll the
grid (data used in the dashboard).
In the third part, the participant was asked perceive a
teaching analytics dashboard, and tell us what sense they can
make out of this screen. After their reﬂections and comments,
the instructor explained how the dashboard is used to perform
analytical tasks.
The fourth part of the study consisted of a list of two
different visual task types: ﬁnd and compare. The participant
had to perform the tasks on their own using the teaching analytics dashboards. As an example, the participant used the ﬁrst
dashboard (Figure 1, top) to answer to tasks such as: ”which
triad/s show/s that students share the same understanding?,
”which concepts (words) are most popular in the topic?” and

4. Method
4.1. Design
Given that the repertory grid technique can inform teachers
on the knowledge concepts that their students have on speciﬁc

45

5. Results and Discussion

”looking at the word-cloud and the bubble-chart, what sense
can you make out of this screen?”.
In the ﬁfth part, task where more comparative, and the
participant was asked to create two groups and compare what
it was shown in the screen (Figure 1, bottom).
We positioned participants in front of a screen at a distance
of about 70 cm. All participants were calibrated with the
eye tracker (SMI iView X Red), which used a nine-point
calibration mode. Data were collected on the rate of 60 Hz.
The eye-tracker was connected with the iMotions Attention
Tool 4.8. Dashboards were imported in the study created with
Attention Tool.

Six participants were recruited: one associate professor,
three assistant professors and two PhD students. We report on
participants’ satisfaction, performance, eye-tracking data and
usability issues.

5.1. Preference and Performance Data
Table 1 summarizes their teaching experience utilizing different technologies in order to facilitate teaching. Students’
assessment is primarily based on questions and group/class
discussions, where data are not systematically collected and
analyzed. Thus, teaching data is analyzed in an ad hoc
manner and using non-standard ways rather than systematic
and standardized approaches.
All participants were able to answer correctly to questions
that related to identifying the concepts that are the most
popular in a topic, compare students ratings, etc. Table 2 and 3
provides an overview of the questions used in each dashboard.
In the ﬁrst dashboard, the word-cloud made teachers reﬂect
more as evidenced by their think aloud comments. Further,
three participants pointed out the popularity of these social
networks and looked into the details. Looking at the opposite
and similarity construct, participants were able to identify why
a student rated differently a triad than the other.
In the second dashboard, participants were asked to perform
comparative analysis between two groups of students. Table
2 and 3 shows the questions, instructions and answers. As an
example, one of the participants said ”the two groups have
used the concept of sharing (facebook, youtube, vimeo are
understood the same, while the other shows some differences).
If I go into details, I assume I will ﬁnd out more.”
Rather than investigating tabular data, participants appreciated the visual approach for collecting and analyzing data.
We observed that reading and comparing concepts in the
word-cloud between groups was easier than understanding the
triads. These observations provide preliminary evidence for
our design hypothesis that teaching analytics dashboards can
help teachers better analyze the data and reason pedagogically.

4.5. Dataset
We used data collected during a Social Media Analytics
course at our university. Students were asked to use the RGFA
website [46] and answer to a grid designed for the course.
The responses provided information regarding their knowledge
related to the social media websites. Eight popular social
networks (i.e. facebook, youtube, vimeo, ﬂickr, foursquare,
linked in, twitter and digg) were used as grid elements.
Personal constructs of students were elicited using the
triadic sorting method. The teacher of the course designed 8
elements and conﬁgured ﬁve triads (groups of three elements).
For each triad, students described why one of the elements is
different from others (opposite construct) and why the other
two are similar (similarity construct). Then, the students were
asked to rate the rest of elements using a ﬁve-point Likert
scale, from their own personal constructs of opposite (1) to
similar (5). For example, some rated with 5 youtube and vimeo
as similar sites. Facebook was rated with 1 as an opposite site
to the others.
Furthermore, students used several keywords to describe
similarity and construct. These keywords were manually ﬁltered and used in our analysis to create the word-cloud in the
dashboard. In order to produce a realistic word-cloud several
random words (e.g. the, is, a, etc.) were removed by our textanalysis program. In addition, to preserve acceptable levels of
realism as well as anonymity, student data from the classroom
were de-identiﬁed and random unrelated names were used in
the lab study.

5.2. Physiological Measures: Eye-Tracking Data
The study was performed with six participants, but we
decided to reject participant 5 because the quality of data
gathered from the device was lower than 85%. Therefore, we
report eye-tracking data from the other ﬁve participants.
We deﬁned areas of interest (AOI) and investigated the time
spent and number of ﬁxations. Figure 2 shows the averaged
participants’ results for each AOI. More speciﬁcally, participants’ ﬁxation number and time spent is higher in the bubblechart and word-cloud, as we expected. However, the other
areas of the dashboard that occupy valuable screen real estate
do not get the same attention as evidenced from aggregate gaze
behavior distribution. This resulted in the design implication
that these areas could be hidden.

4.6. Data Collection
The study was performed in a usability lab. All participants were informed to follow the think-aloud protocol [6].
The aforementioned devices were connected with iMotions
Attention Tool 4.8 [3]. This tool supports synchronized data
collection of EEG and Eye-Tracking activity. Furthermore,
it facilitates data gathering and data analysis. Performance
measures, subjective ratings, EEG and eye-tracking data were
analyzed. The instructor observed, kept notes and screen-voice
recorded each study.

46

TABLE 1. Participants’ experience with teaching, assessment, data collection and repertory grid technique.
Participant
1

No. of
Courses
50

No. of
Students
40

Technology to facilitate teaching
BPM, E-learning,
Beer Game

2

3

30

PowerPoint

3

2

25

PowerPoint,
Laptops
research

for

4

3

60

Laptops / Smartphones to interact,
clickers and projector

5

4

35

6

12

10

SAP,
process
models,
PowerPoint,
blackboard,
simulation games,
YouTube
PowerPoint,
Java and Python
examples

Student’s
Assessment
Summative
examination,
Questions
in
class
Discussions,
Asking
questions
Group
discussions,
Asking
questions
Group
discussion,
reading
individual
comments
Quiz, Questions,
Participation,
Group exercises

Data collection during a class
No data collected

Knowledge on the RG

No data collected

To some extent

Natural feedback

No

Coded questions. Individual comments

No

Nothing formal, Read
something about it
but would use MS Excel to categorize if so

No

Exercises at the
end

No

No

To some extent

TABLE 2. Questions (Q), instructions (I), participants and their answers using the dashboard in the fourth part.
Question /
Instruction
I1

Description
Create two groups of students (using the ﬁlters to the left and right of the screen) of 5 students:
Group 1 All the students, excluding Polo Group 2 Only Polo
Which concepts (words) show that Pola shares the same understanding with the others?

Answers
6

Which triad or triads show that Pola share the same understanding with the others?

6

Which triad or triads show that Pola does not share the same understanding with the others?

6

Create two groups of students (using the ﬁlters to the left and right of the screen) of 7 students:
Group 1 Top 7 Group 2 Last 7
Select ”All” in each word-cloud and set frequency to 2. Looking at the word-cloud and the bubble
chart, what sense (same as above) can you make for each group?
Select ”Sharing” in each word-cloud. Looking at the bubble chart, what sense (same as above) can
you make for each group?

6

6

Q1
Q2
Q3
I2

Q5
Q6

6
6

TABLE 3. Questions (Q), instructions (I), participants and their answers using the dashboard in the ﬁfth part .
Question /
Instruction
I1
Q1
Q2
Q3
I2
Q5
Q6

Description
Look at the default view of the dashboard: all students 6 selected and all triads selected and frequency
= 2.
Which concepts (words) are most popular in the topic?
Which triad or triads show that students share the same understanding?
Which triad or triads show that students do not share the same understanding?
Look at the default view of the dashboard: all students selected and all triads selected and frequency
= 2.
Which student or students show similar and different understanding in triad’s ratings?
Can you ﬁnd out why Stella’s rating differs from Brad’s rating?

47

Answers
6
6
6
6
6
6
6

The same observation was made when teachers performed
compression task. The eye-tracking ﬁndings reveal which
dashboard areas are mostly used in visual analysis and provide
design directions for future iterations, indicating the importance for perceptibility and simplicity.
This behavior was also noticed during the studies from our
observations where the bubble chart and the word cloud were
mainly used. These areas took mainly their attention. This can
be explained because the answers of the tasks were in these
areas. Therefore, it is crucial to highlight these areas more
and provide less space or the possibility of minimizing them.
This will enhance teacher analysis and provide a more pleasant
experience.

Another usability issue relates to reducing the number
of clicks. We noticed that participants found interesting to
observe similar or different construct, but cumbersome to click
over the bubble to view details. This relates to the known
question faced in visualization areas on how to integrate and
present an overview with details.
Performances, space issues, more visualization and interaction types, are also some of the ﬁndings and recommendations
that are being investigated and will be addressed in the next
version of the teaching analytics dashboards.
The space-efﬁcient issue was also pointed out by the eye
tracking data. This result complies with our observation and
yields for a more space efﬁcient solution. While the EEG
data did not yield any important result, they showed that the
teachers were almost similarly affected by the dashboard. This
was also observed from the instructor during the study.

5.3. Physiological Measures: EEG Measures
The study was performed with six participants, but we
decided to reject participant 5 because the quality of data
gathered from the device was lower than 85%. We report on
EEG data from ﬁve participants.
Using EEG data, we investigated emotions, which are
automatically generated by EEG headset. More precisely, the
EPOC framework generates data that relates to levels of
frustration, engagement, meditation and excitement. The fact
that these measures are generated automatically does pose
questions regarding data provenance, construct validity and
reliability. That said, the purpose of the paper is not to verify
and validate the EPOC output but to illustrate the potentials
of this approach in teaching analytics.
We selected the average values of emotions produced during
task execution of two parts: dashboard 1 and 2. Figure 3
shows that, in general, participants had almost the same level
of engagement and frustration in each dashboard. We would
expect that the second dashboard would cause more frustration
due to its overwhelming screen. From the visualization ﬁeld,
guidelines suggest the use of minimum views in a visual
representation [51] to avoid cognition impact. However, the
results did not yield this.
Our results showed that teachers perform almost equally in
both dashboards. Also, it seems that four participants were
more exited using the second than the ﬁrst dashboard. On
the other hand, the level of meditation increased when they
used the second dashboard. This can be explained because
participants had to compare groups. However, this observation
needs more research in order to determine which how a task
inﬂuences human meditation.

5.5. Reﬂections
Understanding and enhancing teachers’ professional vision
[2] is an essential but understudied research topic within
learning analytics. Our research program on teaching analytics
[47] seeks to address this research gap by involving teachers
in the design, development and evaluation of visual analytics
tools and services that enhance their dynamic diagnostic
pedagogical decision-making. We believe that this paper can
serve as an illustrative case study of the need for and value of
integrating physiological measures into the traditional evaluation of performance and preference measures for better
understanding teachers’ professional vision with respect to
their classrooms, students, learning resources and, teaching
and learning data.

6. Limitations
This study provides data and results for enhancing the professional vision of teachers using teaching analytic dashboards.
However, it has several limitations that are worth mentioning.
We utilized simple visual search tasks, which we believe are
common tasks in daily practice. While we expect our ﬁndings
to generalize to other tasks, more tasks and visual types need
to be tested in future investigations.
As participants were not used to dashboards, we observed
that guidance and basic training on visual aspects and types
are required. A methodical explanation on how a dashboard
and its views work was noticed in the studies. We think
that an explanation can be part of the training, which will
enhance their performance and subjectivity. Furthermore, our
observations regarding their use act in accordance with the
design guidelines described in [44].
Recruiting real domain users it is challenging due to their
tight schedule. Moreover, there is no suitable number for
conducting usability studies as researchers debate regarding
the number and their opinions differ [30], [20], [40] Also, we
are aware that the number is limited, but usability tests with
5 participants [30] can provide good indications. In our case

5.4. Usability Issues
Several usability issues were identiﬁed from the participants’ think aloud comments, the study facilitator’s notes
and the data analysis of study sessions. For instance, the
dashboards provided participants with an overview, which
pointed out differences but did not inform them on similarity
and/or opposite constructs.

48

Fig. 2. Average ﬁxation count and time spent for each AOI.

Fig. 3. Average values for engagement and frustration.

perspective and pleasant from a subjective satisfaction perspective. Regarding innovation, teachers said that similar tools
are lacking in their regular pedagogical practice and would
recommend their use in formal educational settings. Eyetracking data from the two dashboards showed that teachers
will not be adversely affected by less important areas in
their pedagogical decision-making. The emotions computed
from the EEG data showed similar levels of engagement and
frustration for both dashboards.
We presented evaluation results towards teaching analytics
dashboards and provided useful insights for existing learning practices in educational environments. However, research
should concentrate and investigate further approaches for
educational settings. More speciﬁcally future work includes:
conducting longitudinal studies of dashboard use in real
classrooms, investigating teaching analytics dashboards for
students engaged in peer-tutoring, and enriching the variety
of supported visualizations and interaction techniques.

we evaluated a new concept for the teachers: how teaching
is performed and how the dashboards are perceived by real
teachers. This information provided useful insights towards a
better understanding of their needs and allowed us to collect
data for the next versions of teaching analytics dashboard.
Teachers from elementary and high school may produce
different results, have other requirements and so forth. Therefore, we believe that this is a ﬁrst step towards investigating
teaching analytics. Employing larger number of teachers from
different institution (e.g. elementary school) is another milestone towards effective learning analytics.
Another scenario for formative assessment would involve
real-time feedback from the students. This means that the
students reply to a grid and the teacher investigates how the
dashboard changes during time. Therefore, conducting perspective analysis may discover other issues that retrospective
analysis does not. This issue will be addressed in the future.

7. Conclusion and Future Work
8. Acknowledgments

This paper describes teaching analytics dashboards for
repertory grid data to enable teachers to conduct systematic
visual analysis of classroom learning data for formative assessment purposes. The teaching dashboards of repertory grid
data can assist teachers in identifying student competencies,
strengths, and weaknesses, planning future learning, and can
support teachers in obtaining an overview of the whole classroom as well drill-down into details about individual and/or
groups of students.
The teaching analytics dashboards were evaluated with six
teachers. The results showed that the teaching analytics dashboards were efﬁcient and effective from a task performance

We would like to thank all participants of this study and the
reviewers of the paper. This work was partially supported by
the NEXT-TELL project, co-funded by the European Union
under the ICT theme of the 7th Framework Programme for
R&D (FP7).

References
[1] Agostinho, S.: Learning design representations to document, model, and
share teaching practice. Handbook of Learning Design and Learning
Objects: Issues, Applications, and Technologies 1, 1–19 (2009)

49

[2] Goodwin, C. Professional vision. American Anthropologist, 96, 3 1994),
606-633.
[3] Attention tool. http://www.imotionsglobal.com/, accessed
August, 2014
[4] Berger, H.: Über das elektrenkephalogramm des menschen. Archiv
für Psychiatrie und Nervenkrankheiten 87, 527–570 (1929),
http://dx.doi.org/10.1007/BF01797193
[5] Bertin, J.: Graphics and graphic information processing. de Gruyter
(1981)
[6] Boren, T., Ramey, J., Thinking aloud: Reconciling theory and practice.
Professional Communication, IEEE Transactions on, 2000, 43.3: 261278.
[7] Brown, M.: Learning analytics: The coming third wave. EDUCAUSE
Learning Initiative Brief (2011)
[8] E. D. Cassidy, A. Colmenares, G. Jones, T. Manolovitz, L. Shen, S.
Vieira, Higher education and emerging technologies: Shifting trends in
student usage, The Journal of Academic Librarianship, Elsevier, 2014.
[9] Card, S.K., Mackinlay, J.D., Shneiderman, B. (eds.): Readings in
information visualization: using vision to think. Morgan Kaufmann
Publishers Inc., San Francisco, CA, USA (1999)
[10] Cleveland, W.S., McGill, R.: Graphical perception: Theory, experimentation, and application to the development of graphical methods. Journal
of the American Statistical Association 79(387), 531–554 (1984)
[11] Damasio, A.: Descartes’ error: Emotion, reason, and the human brain.
Penguin Books (2005)
[12] Davidson, R.J., Jackson, D.C., Larson, C.L.: Human electroencephalography. Handbook of psychophysiology 2, 27–52 (2000)
[13] Dimoka, A., et al. On the use of neurophysiological tools in IS research:
developing a research agenda for NeuroIS. (2010).
[14] Duval, E.: Attention please!: learning analytics for visualization and
recommendation. In: Proceedings of the 1st International Conference
on Learning Analytics and Knowledge. pp. 9–17. ACM (2011)
[15] Emotiv. http://www.emotiv.com, accessed March, 2013
[16] Print screens from the Teaching Analytics Dashboard using social data.
https://goo.gl/SIOKKH, accessed September, 2015
[17] Few, S.: Information Dashboard Design: The Effective Visual Communication of Data. O’Reilly Media, Inc. (2006)
[18] Forgas, J.P.: Mood and judgment: the affect infusion model (aim).
Psychological bulletin 117(1), 39 (1995)
[19] Frantzidis, C., Bratsas, C., Klados, M., Konstantinidis, E., Lithari, C.,
Vivas, A., Papadelis, C., Kaldoudi, E., Pappas, C., Bamidis, P.: On the
classiﬁcation of emotional biosignals evoked while viewing affective
pictures: An integrated data-mining-based approach for healthcare applications. Information Technology in Biomedicine, IEEE Transactions
on 14(2), 309–318 (2010)
[20] Hwang, W., Gavriel S., Number of people required for usability evaluation: the 102 rule. Communications of the ACM 53.5 (2010): 130-133.
[21] Haythornthwaite, C., Gruzd, A.: Exploring patterns and conﬁgurations
in networked learning texts. In: System Science (HICSS), 2012 45th
Hawaii International Conference on. pp. 3358–3367 (Jan 2012)
[22] Henderson, J.M., Hollingworth, A.: Eye movements during scene viewing: An overview. Eye guidance in reading and scene perception 11,
269–293 (1998)
[23] Hertzum, M., Jacobsen, N. E., The evaluator effect: A chilling fact about
usability evaluation methods, Int. J. Hum. Comput. Interaction 13 (4)
(2001) 421–443.
[24] Hyökki, S.: Eye tracking in user research. Interdisciplinary Studies
Journal 1(4), 65 (2012)
Hertzum, M., and Jacobsen, N. E. The evaluator effect: A chilling fact
about usability evaluation methods. Int. J. Hum. Comput. Interaction
13, 4 (2001), 421–443.
[25] Kelly, G.A.: A theory of personality. The psychology of personal
constructs. NY: Norton (1963)
[26] Koper, R., Tattersall, C.: Learning Design: A Handbook on Modelling
and Delivering Networked Education and Training. Springer (2005)
[27] Lazar, J. Feng, H. Hochheiser, Research methods in human-computer
interaction, John Wiley & Sons, 2010.
[28] Marjanovic, O.: Sharing and reuse of innovative teaching practices in
emerging business analytics discipline. In: System Sciences (HICSS),
2013 46th Hawaii International Conference on. pp. 50–59 (Jan 2013)
[29] Mehler, B, Reimer, B., Coughlin, J. F. Sensitivity of physiological
measures for detecting systematic variations in cognitive demand from a
working memory task an on-road study across three age groups. Human

[30]

[31]
[32]
[33]
[34]
[35]

[36]
[37]
[38]
[39]
[40]
[41]
[42]
[43]
[44]
[45]

[46]
[47]

[48]

[49]
[50]
[51]

[52]
[53]

50

Factors: The Journal of the Human Factors and Ergonomics Society,
54(3):396–412, 2012.
Nielsen, J., Landauer, T.K.: A mathematical model of the ﬁnding of
usability problems. In: Proceedings of the INTERACT ’93 and CHI ’93
conference on Human factors in computing systems. pp. 206–213. CHI
’93, ACM (1993)
Nielsen, J. Usability Engineering. Morgan Kaufmann Publishers Inc.,
San Francisco, CA, USA, 1993.
Expanding evidence approaches for learning in a digital world. Ofﬁce
of Educ. Technology, U.S. Dept. Education, http://www.ed.gov. (2013)
Paas, F., Tuovinen, J.E., Tabbers, H., Van Gerven, P.W.: Cognitive load
measurement as a means to advance cognitive load theory. Educational
psychologist 38(1), 63–71 (2003)
Paas, F.G., Van Merriënboer, J.J.: Instructional control of cognitive load
in the training of complex cognitive tasks. Educational Psychology
Review 6(4), 351–371 (1994)
Pantazos, K., Kuhail, M., Lauesen, S., Xu, S.: uVis Studio: An integrated
Development Environment for Visualization, pp. 15–30. Proceedings
of S P I E - International Society for Optical Engineering, SPIE International Society for Optical Engineering (2013)
Pantazos, K., Lauesen, S., Vatrapu R., End-User Development of Information Visualization, End-User Development, Springer Berlin Heidelberg, 2013, 104-119.
Sweller, J.: Cognitive load during problem solving: Effects on learning.
Cognitive science 12(2), 257–285 (1988)
Sweller, J.: Cognitive load theory, learning difﬁculty, and instructional
design. Learning and Instruction 4(4), 295–312 (1994)
Sweller, J., Van Merrienboer, J.J., Paas, F.G.: Cognitive architecture
and instructional design. Educational psychology review 10(3), 251–296
(1998)
Schmettow, Martin., Sample size in usability studies. Communications
of the ACM 55.4 (2012): 64-70.
Tatler, B.W., Wade, N.J., Kwan, H., Findlay, J.M., Velichkovsky, B.M.:
Yarbus, eye movements, and vision. iPerception 1(1), 7–27 (2010)
Tufte, E.R., Graves-Morris, P.: The visual display of quantitative information, vol. 2. Graphics press Cheshire, CT (1983)
Van Orden, K.F., Nugent, W., La Fleur, B., Moncho, S.: Assessment
of variable coded symbology using visual search performance and eye
ﬁxation measures. Tech. rep., DTIC Document (1998)
M. Elias and A. Bezerianos. Exploration views: Understanding dashboard creation and customization for visualization novices. In HumanComputer Interaction–INTERACT 2011, pages 274–291. Springer, 2011.
Vatrapu, R., Reimann, P., Bull, S., Johnson, M.: An eye-tracking study
of notational, informational, and emotional aspects of learning analytics
representations. In: Proceedings of the Third International Conference
on Learning Analytics and Knowledge. pp. 125–134. ACM (2013)
Vatrapu, R., Reimann, P., Hussain, A., und Beratung, M.P.F.: Towards
teaching analytics: Repertory grids for formative assessment. In: Proc.
International Conference of the Learning Sciences (ICLS) 2012 (2012)
Vatrapu, R., Teplovs, C., Fujita, N., Bull, S.: Towards visual analytics
for teachers’ dynamic diagnostic pedagogical decision-making. In: Proceedings of the 1st International Conference on Learning Analytics and
Knowledge. pp. 93–98. LAK ’11, ACM, New York, NY, USA (2011),
http://doi.acm.org/10.1145/2090116.2090129
Vatrapu, R.K.: Explaining culture: An outline of a theory
of socio-technical interactions. In: Proceedings of the 3rd
International Conference on Intercultural Collaboration. pp.
111–120. ICIC ’10, ACM, New York, NY, USA (2010),
http://doi.acm.org/10.1145/1841853.1841871
Verbert, K., Duval, E., Klerkx, J., Govaerts, S., Santos Odriozola,
J.L.: Learning analytics dashboard applications. American Behavioral
Scientist 57(10), 1500–1509 (Feb 2013)
Ware, C., Information visualization: perception for design. Elsevier,
2012.
Wang Baldonado, M.Q., Woodruff, A., Kuchinsky, A.: Guidelines for
using multiple views in information visualization. In: Proceedings of
the working conference on Advanced visual interfaces. pp. 110–119.
ACM (2000)
Zelinsky, G.J., Rao, R.P., Hayhoe, M.M., Ballard, D.H.: Eye movements
reveal the spatiotemporal dynamics of visual search. Psychological
Science pp. 448–453 (1997)
Zhang, Q., Lee, M.: Analysis of positive and negative emotions in natural
scene using brain activity and gist. Neurocomputing 72(4), 1302–1306
(2009)

