Hindawi
Computational Intelligence and Neuroscience
Volume 2019, Article ID 5627156, 13 pages
https://doi.org/10.1155/2019/5627156

Research Article
Motor Imagery EEG Classification Based on Decision Tree
Framework and Riemannian Geometry
Shan Guan , Kai Zhao , and Shuning Yang
School of Mechanical Engineering, Northeast Electric Power University, 132012 Jilin, China
Correspondence should be addressed to Shan Guan; guanshan1970@163.com
Received 24 September 2018; Revised 20 December 2018; Accepted 3 January 2019; Published 21 January 2019
Guest Editor: Anastassia Angelopoulou
Copyright © 2019 Shan Guan et al. This is an open access article distributed under the Creative Commons Attribution License,
which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.
This paper proposes a novel classiﬁcation framework and a novel data reduction method to distinguish multiclass motor imagery
(MI) electroencephalography (EEG) for brain computer interface (BCI) based on the manifold of covariance matrices in a
Riemannian perspective. For method 1, a subject-speciﬁc decision tree (SSDT) framework with ﬁlter geodesic minimum distance
to Riemannian mean (FGMDRM) is designed to identify MI tasks and reduce the classiﬁcation error in the nonseparable region of
FGMDRM. Method 2 includes a feature extraction algorithm and a classiﬁcation algorithm. The feature extraction algorithm
combines semisupervised joint mutual information (semi-JMI) with general discriminate analysis (GDA), namely, SJGDA, to
reduce the dimension of vectors in the Riemannian tangent plane. And the classiﬁcation algorithm replaces the FGMDRM in
method 1 with k-nearest neighbor (KNN), named SSDT-KNN. By applying method 2 on BCI competition IV dataset 2a, the kappa
value has been improved from 0.57 to 0.607 compared to the winner of dataset 2a. And method 2 also obtains high recognition rate
on the other two datasets.

1. Introduction
Brain computer interface (BCI) based on motor imagery
(MI) is used to analyze human intention by electroencephalogram (EEG) signals generated by human brain
electrophysiological activity [1, 2]. Based on BCI technology,
exoskeletons can be used to help people with physical disabilities regain their motor ability, and BCI also has wide
applications in smart home, entertainment, military, and
other ﬁelds [3–6].
Common spatial pattern (CSP) is widely used in motor
imagery to extract EEG features [7]. CSP has excellent
performance in two classiﬁcation tasks, but the drawback is
that it needs a lot of electrodes [8].
Despite its short history, the use of the Riemannian
geometry in BCI decoding is currently attracting increasing
attention [9–13]. Covariance matrices lie in the space of
symmetric positive deﬁnite (SPD) matrices, which can be
formulated as a Riemannian manifold [14]. In the BCI ﬁeld,
the connections of the CSP algorithm and the tools of
information geometry have been investigated, considering

several divergence functions in alternative to the Riemannian distance [15–18]. Barachant et al. proposed a
simple data augmentation approach for improving the
performance of the Riemannian mean distance to mean
(MDM) algorithm [13]. Kumar et al. propose a single band
CSP framework for MI-BCI that utilizes the concept of
tangent space mapping in the manifold of covariance
matrices, and the proposed method obtains good results
when compared to other competing methods [19]. A hierarchical MDM classiﬁer for multiclass problem has been
tested in [20].
Advanced classiﬁers based on the tangent space on the
Riemannian manifold of positive matrices are also receiving
increasing attention. Barachant et al. map the covariance
matrices in the tangent space and apply feature selection and
linear discriminate analysis (LDA) in the tangent space [10].
For the application of the classiﬁer in the tangent space, the
problem is that the curse of dimensionality. Traditional data
dimensionality reduction methods include two categories:
linear dimensionality reduction (LDR) and nonlinear dimensionality reduction (NLDR). Since most of the actual

2

Computational Intelligence and Neuroscience

data are nonlinear, NLDR techniques such as locally linear
embedding (LLE) [21], isometric mapping (ISOMAP) [22],
maximum variance unfolding (MVU) [23], and t-distributed
stochastic neighbor embedding (t-SNE) [24, 25] are used to
tackle problems widely. Lee et al. used discrete wavelet
transform (DWT) and continuous wavelet transform
(CWT) to extract features of MI tasks, and Gaussian mixture
model (GMM) was used to construct GMM supervectors;
this method accelerates the speed of training and improves
the accuracy of motor imagery [26]. Sadatnejad et al. propose a new kernel to preserve the topology of data points in
the feature space, and the proposed kernel is strong, particularly in the cases where data points have a complex and
nonlinear separable distribution [8]. Xie et al. proposed a
framework for intrinsic submanifold learning from a highdimensional Riemannian manifold; the proposed method
exhibited strong robustness against a small training dataset
[27].
There is still another approach for overcoming the
problem of high dimensionality in SPD manifolds. And this
method maps from a high-dimensional SPD manifold to a
lower dimensional one while the geometry of SPD manifolds
is preserved. And there are only two works of this way.
Davoudi et al. [14] proposed distance preservation to local
mean (DPLM) as dimensionality reduction technique,
combined with FGMDM, the best performance of this article
in terms of kappa value is 0.60. Harandi et al. [28] learned a
mapping that maximizes the geodesic distances between
interclass and simultaneously minimizes the distances between intraclass, and it is done via an optimization on
Grassmann manifolds.
In this paper, we proposed a novel SSDT-FGMDRM and
SSDT-KNN for the classiﬁcation of multiclass MI tasks by
designing a simple yet eﬃcient subject-speciﬁc decision tree
framework. Method 1 contains SSDT-FGMDRM to improve
the performance of FGMDRM. For each individual, method
1 ﬁrst separates the two most discriminative classes from the
group. Furthermore, the remaining categories including the
misclassiﬁcation samples of the previous nodes are reclassiﬁed in the last node. Method 2 contains SSDT-KNN and a
NLDR method named SJGDA. SJGDA combines the advantage of semi-JMI and GDA, and method 2 performed
well on diﬀerent datasets. The aims of this article are as
follows:
(1) To verify the eﬀectiveness of the proposed SSDT
framework through dataset 1
(2) To verify the superiority of SJGDA in feature extraction, compared with semi-JMI and GDA
(3) To validate the generalization ability of method 2
through diﬀerent datasets, in this paper
The rest of the paper is organized as follows: Section 2
introduced the mathematical preliminaries of the Riemannian geometry. Section 3 discussed the proposed methods in
detail. Three datasets are introduced in Section 4. The results
of our work are discussed in Section 5. And in Section 6, we
compared our methods with the state of the art. This paper
concludes in Section 7.

2. Geometry of SPD Matrices
Let Xi represent a short segment of continuous EEG signals,
and Xi can be denoted as follows:
Xi � 􏽨Xt+Ti · · · Xt+Ti +Ts −1􏽩 ∈ Rn×Ts ,

(1)

where Xi corresponds to the ith trail of imaged movement
starting at time t � Ti. Ts denotes the number of sampled
points of the selected segment.
For the ith trail, the spatial covariance matrix (SCM)
Pi ∈ Rn×n can be calculated as follows:
1
X XT .
Pi �
(2)
Ts − 1 i i
Based on the SCM, there are two ways to classify MI tasks
in the Riemannian manifold.
2.1. Filter Geodesic Minimum Distance to the Riemannian
Mean. The Riemannian distance between two SPD matrices
P1 and P2 in P(n) is given by [29]
δR

1/2
n
��
��
−1
2 ⎤
�
�
⎣
⎦
⎡
�
�
P1 , P2 􏼁 � �log􏼐P1 P2 􏼑� � 􏽘log λi .
F

(3)

i�1

Given m SPD matrices P1, . . . , Pm, the geometric mean in
the Riemannian sense is deﬁned as
m

I P1 , . . . , Pm 􏼁 � arg min 􏽘δ2R P, Pi 􏼁.

(4)

P∈P(n) i�1

For algorithm mean Riemannian distance to Riemannian mean (MDRM), we compute the Riemannian distance
between unknown class P to the Riemannian mean point of
each class and classify the unknown class into categories
corresponding to the shortest distance. Inspired by the
principal geodesics analysis (PGA) method [30], the literature [31] ﬁnds a set of ﬁlters by applying an extension of
Fisher linear discriminant analysis (FLDA) named Fisher
geodesic discriminant analysis (FGDA). And then, apply
these ﬁlters to MDRM to form ﬁlter geodesic minimum
distance to Riemannian mean (FGMDRM). More details can
be seen from [31].
2.2. Tangent Space Mapping. As shown in Figure 1, the SPD
matrix of P is denoted by a diﬀerentiable Riemannian
manifold Z. Each tangent vector Si can be seen as the derivative at t � 0 of the geodesic Γ(t) between P and the exponential mapping Pi � EXPP(Si), deﬁned as follows:
ExpP Si 􏼁 � Pi � P1/2 exp􏼐P−1/2 Si P−1/2 􏼑P1/2 .

(5)

The inverse mapping is given by the logarithmic mapping and can be deﬁned as follows:
logP Pi 􏼁 � Si � P1/2 log􏼐P−1/2 Pi P−1/2 􏼑P1/2 .

(6)

Using the Riemannian geodesic distance, the Riemannian mean of I > 1 SPD matrices by

Computational Intelligence and Neuroscience

3
between the test point and the RMP, it caused a wrong
classiﬁcation. Figure 3(b) shows the example of the classiﬁcation results obtained by using the ﬁrst node of the SSDTFGMDRM framework. It can be seen that the error classiﬁcation is corrected by using the decision tree framework.
Method 1 is used to classify four types of MI tasks directly. The training and testing diagram is shown in Figure 4.

TP
LogP (Pi)

P

Si
Γ(t)
ExpP (Si)
Pi

Z

Figure 1: The tangent space at point P, and the geodesic Γ(t)
between P and Pi.

3.3. Feature Extraction Algorithm Based on the Riemannian
Tangent Space. In this paragraph, we propose a novel data
reduction method which combines semi-JMI and GDA,
namely SJGDA, to solve the dimension disaster problem
after tangent space mapping.

I

I P1 , . . . , PI 􏼁 � arg min 􏽘 δ2R P, Pi 􏼁.

(7)

P∈P(n) i�1

Using the tangent space located at the geometric mean of
the whole set trials, PI � I(Pi , i � 1 . . . I), and then, each
SCM Pi is mapped into this tangent space, to yield the set of
m � n(n + 1)/2 dimensional vectors:
−1/2
Si � upper􏼐P−1/2
I log Pi 􏼁PI 􏼑.

(8)

Many eﬃcient classiﬁcation algorithms can be implemented in the Riemannian space [10].

3. Methods
3.1. Subject-Speciﬁc Decision Tree Framework. Decision tree
is a common machine learning method. Each node of decision tree can be deﬁned as a rule. Guo and Gelfand [32]
proposed classiﬁcation trees with neural network, and this
method embeds multilayer neural networks directly in
nodes. In the decision tree, one of the most important things
is to construct a proper binary tree structure; the upper
nodes have the greater impact of the accuracy of the whole
samples [33]. In order to solve the multiclassiﬁcation
problem in this paper, we constructed a subject-speciﬁc
decision tree (SSDT) classiﬁcation framework as shown in
Figure 2 according to the best separating principle [34]. As
can be seen from Figure 2, the SSDT proposed in this paper
trains a diﬀerent classiﬁcation model at diﬀerent nodes of
the decision tree.
The advantages of the SSDT framework are as follows:
(1) This model separates the two MI tasks (e.g., C.1 and
C.2) with the highest recognition rate as far as
possible
(2) At the last node, we reclassify some samples to
enhance the classiﬁcation ability of the classiﬁer
3.2. Method 1: A Direct Classiﬁcation Method Based on SSDTFGMDRM. Firstly, we point out one problem of the multiclass FGMDRM by using an example. Figure 3 gives a threeclass classiﬁcation problem. Figure 3(a) shows the classiﬁcation progress by FGMDRM. We can see that three Riemannian mean points (RMPs) are located on the manifold. Since
the classiﬁcation criterion is decided by the distance calculated

3.3.1. Semisupervised Joint Mutual Information.
Semisupervised dataset D � D{DL ∪ DU} consists of two
N
N
parts, DL � 􏼈xi , yi 􏼉i�1L are labelled data and DU � 􏼈xNL +i 􏼉i�1U
are unlabelled data. A binary random variable S is introduced to determine the distribution of labelled dataset
and unlabelled dataset. When s � 1, we record the value of y,
otherwise not. In this way, the labelled set DL comes from
the joint distribution p(x, y|s � 1), while the unlabelled set
DU comes from the distribution p(x|s � 0). The underlying
mechanism S turns out to be very important for feature
selection.
Feature selection method based on mutual information
theory is a common feature selection method [35]. In these
methods, we rank the features according to the score and
select the features with higher scores. For example, by
ranking the features according to their mutual information
with the labels, we get the sort of correlation that is related to
class labels. The characteristics of the score are deﬁned as
follows:
Y
JJMI Xk 􏼁 � 􏽘 􏽢I􏼠Xk ; 􏼡,
X
j
X ∈X
j

(9)

θ

where Xθ represents the set of the features already selected
and Xk is the feature ranked by scores. Y represents the label
corresponding to feature Xk.
Semi-JMI is a method of using a semisupervised dataset
as a training set for JMI. More details can be seen from
Reference [36]. In this paper, the missingness mechanism is
class-prior-change semisupervised scenario (MAR-C) [37].
After feature ranking, we can obtain a feature vector as
follows:
f � 􏼂f1 , f2 , . . . , fn 􏼃,

(10)

where n is the length of the tangent vectors Si. Since information redundancy exists in f, we select the best vector
length m (m < n) of each subject by the classiﬁcation recognition rate:
fSJ � 􏼂f1 , f2 , . . . , fm 􏼃.

(11)

3.3.2. Generalized Discriminant Analysis. After variable selection, this paper uses generalized discriminant analysis

4

Computational Intelligence and Neuroscience
Training classifier in an OVR scheme. According to the ranking of classification
recognition rate, the highest recognition rate model is placed in this node

N.1
C.1

The second high recognition rate model is placed at this node

N.2

The classifier is trained in an OVO scheme this node

N.3

C.2

C.1 C.2 C.3 C.4

Figure 2: SSDT based on the best separating principle for four class. N. i represents node i, and C. i represents class i.

150
RMP
100
50
0
–50
–100
RMP
–150
120 100
80 60
40 20

150
RMP
100
50
0
–50
–100
–150
120 100
80 60
40

RMP

0 0
(a)

100 120
60 80
20 40

RMP
20

0 0
(b)

100 120
60 80
20 40

Figure 3: Three classiﬁcation problems classiﬁed by FGMDRM (a); and a subjectspeciﬁc decision tree FGMDRM model (b).

K-fold cross validation is used to classify the data set into training set and test set

Four FGMDRM models are obtained by training
FGMDRM in OVR way
Classify with
SSDT-FGMDRM
By sorting the correct rate, they are named F.x (x = 1, 2, 3, 4)

Train FGMDRM for 4 class MI tasks

F.1

N.1

The framework of SSDT-FGMDRM
F.2

N.2
C.1

Multiclass FGMDRM

N.3
C.2
C.1 C.2 C.3 C.4

Obtain the test set label

Figure 4: Block diagram for method 1.

(GDA) [38, 39], which is a nonlinear feature reduction
technique based on kernels to reduce the length of the feature
vectors fSJ and their redundancies. Mapping X (fSJ) into a
high-dimensional space F through a kernel function Φ:
Φ: Rd ⟶ F,
Φ

x | ⟶ (x).

(12)

The linear Fisher decision is performed in the F space,
and the criterion function for its extension is
􏼌
􏼌􏼌
Φ 􏼌􏼌􏼌
􏼌􏼌􏼌 WΦ 􏼁T SΦ
W
􏼌⎠
B
⎝􏼌􏼌
􏼌􏼌⎞
(13)
,
J􏼐WΦ 􏼑 � arg max⎛
T
􏼌
􏼌
Φ
Φ
Φ
Φ
W
􏼌􏼌(W ) ST W 􏼌􏼌
where WΦ ∈ F and SB and SW are between-class scatter and
within-class scatter, respectively.

Computational Intelligence and Neuroscience

5

For the convenience of the numerical calculation, kernel
functions are introduced to solve the problem:
k(x, y) � (Φ(x) · Φ(y)).

(14)

Gauss kernel, poly kernel, and sigmoid kernel are widely
used in GDA [40]. For test data z, its image Φ(z) in F space
projects on WΦ is as follows:
N

N

Φ
􏼐Wi · Φ(z)􏼑 � 􏽘 αij 􏼐Φ􏼐xj 􏼑 · Φ(z)􏼑 � 􏽘 αij k􏼐xj , z􏼑.
j�1

j�1

(15)
This paper uses ploy kernel to reduce the dimension.
After GDA, we can get a vector fG as follows:
fG � 􏼂f1 , f2 , . . . , fd 􏼃,

(16)

where d of fG is decided by the actual needs, and in this paper
we set d � 1. And then, SJGDA is applied to the dataset of this
paper, and the ﬁnal feature vectors are constructed as
follows:
fSJGDA � 􏽨fG , fSJ 􏽩.

DRL), and the electrode placement follows the international
10–20 standard. Equipment and the Emotiv 14 electrodes are
located over 10–20 international system positions as shown in
Figure 6. This experiment collected three kinds of EEG signals
of one joint: imagination of shoulder ﬂexion (F), extension (E),
and abduction (A), as shown in Figure 7.
Seven subjects participated in this experimental study.
These subjects were in good health. During the experiment,
subjects were naturally placed with both hands, trying to avoid
body or head movement. During the experiment, subjects
carried out motor imagery under the outside cue, a single
experiment collected EEG signal for 5 seconds, and then took
5–7 seconds to have rest, each action repeated acquisition 20
times. The experimental process is shown in Figure 8.

(17)

3.4. Method 2: SJGDA and Subject-Speciﬁc Decision Tree
k-Nearest Neighbor. Method 2 is used to classify four types
of MI tasks after tangent space mapping. The training and
testing diagram is shown in Figure 5.

4. Description of Data
4.1. Dataset 1. BCI competition IV dataset 2a is used to
evaluate the performance of the proposed two methods [41].
Dataset 2a collects 22 channel EEG data and 3 EOG channel
data. Four types of motor imagery were collected: left hand,
right hand, foot, and tongue. The dataset contains nine
healthy subjects and each subject has two sessions, one
training session and one test session. Each session has 288
trails of MI data with 72 trails for each MI task. The EEG
signals are bandpass ﬁltered by a 5-th order Butterworth
ﬁlter in the 8–30 Hz frequency band. The selection of trial
period is important in MI classiﬁcation; we select 2 s data
(0.5 s and 2.5 s) after the cue, instructing the user to perform
the MI tasks by the winner of the competition.
4.2. Dataset 2. BCI competition III dataset IIIa is used to
evaluate the performance of method 2. BCI III dataset IIIa
contains 3 subjects: K3b, K6b, and L1b, and collects 64
channel EEG data. The EEG was sampled with 250 Hz. Four
types of motor imagery were collected: left hand, right hand,
foot, and tongue. More details about this dataset can be seen
at Reference [42].
4.3. Dataset 3. In our own dataset, Emotiv Epoc+ is used to
collect EEG data of motor imagery. It is a portable EEG acquisition device with a sampling rate of 128 Hz. It has fourteen
electrode channels (AF3, F7, F3, FC5, T7, P7, O1, O2, P8, T8,
FC6, F4, F8, and AF4), two inference electrodes (CMS and

5. Results
5.1. Results of Method 1. We use SSDT-FGMDRM to classify
multiclass MI tasks as introduced in Section 3.1. Since there are
four classes, we can have four pairs of MI tasks: left vs rest (L/
RE), right vs rest (R/RE), foot vs rest (F/RE), and tongue vs rest
(T/RE). For each subject, the pair with the highest accuracy is
used to train N.1, and the pair with the second highest accuracy is to train N.2. Table 1 gives the ten-folder crossvalidation results obtained using FGMDRM in OVR scheme.
Table 2 displays the kappa values obtained by method 1.
Compared with other methods, ﬁve subjects (A03, A06, A07,
A08, and A09) achieved higher kappa value of nine without
exploring the frequency domain information by method 1.
In the case of ﬁxed frequency window, we have improved the
mean kappa value of 0.069 than MDRM (p � 0.4683), and
0.139 than FGMDM_ﬁxed (p � 0.1423). Our approach also
shows signiﬁcant improvement than FGMDM (p � 0.6607),
which has exploited subject-speciﬁc frequency information,
in terms of the kappa value of 0.039.
5.2. Results of Method 2. The results in Figure 9 show the T/
RE feature distribution of the ﬁve features of subject A09.
Figure 9(a) shows the ﬁrst ﬁve ranked features with semiJMI. After applying the semi-JMI, the ﬁrst ﬁve best features
extracted have shown statistically signiﬁcant improvement
in the separability with p values <0.05 except feature 2 with p
value 0.77. In Figure 9(b), the ﬁrst ﬁve features extracted
from primitive feature vectors with p value of 0.13, 0.05,
0.87, 0.05, and 0.13. The p values indicate that the pair T/RE
have no signiﬁcance in the primitive feature vectors. The
results show that with our semisupervised feature ranking
algorithm, the separable degree of the feature has been
greatly improved.
Figure 10 shows the evolution of the classiﬁcation accuracy with KNN (k � 5 in this paper) against the number of
ranked variables in OVR scheme. L/RE and T/RE are the two
pairs with the highest recognition rate, and they achieved the
highest recognition rate in 100 variables. But this is still a
curse of dimensionality for classiﬁers; GDA is used to analyze the ﬁrst 100 sorted variables in our study.
As the separation of characteristics cannot meet our
requirements, GDA is used to get more obvious variables.

6

Computational Intelligence and Neuroscience
K-fold cross validation is used to classify the data set into training set and test set

Tangent space mapping

SJGDA is used to construct
training set features

SJGDA is used to construct
test set features

Four KNN models are obtained by training KNN in OVR way

Classify with
SSDT-KNN

By sorting the correct rate, they are named K.x (x = 1, 2, 3, 4)

The combined classifier KNN is obtained in OVO scheme,
which contains k (k – 1)/2 subclassifiers

N.1

K.1

The framework of SSDT-KNN
K.2

N.2
C.1

The combined classifier KNN

N.3
C.2
C.1 C.2 C.3 C.4

Obtain the test set label

Figure 5: Block diagram for method 2.

Emotiv epoc+

AF3 AF4
F7
T7

F3
FC5

T8

CMS
P7

DRL
P8

O1

(a)

F8

F4
FC6

O2

(b)

Figure 6: (a) Emotiv Epoc+ and (b) Emotiv 14 electrodes located over 10–20 international system positions.

90°

Extension

Flexion

Abduction

30°

(a)

90°

(b)

(c)

Figure 7: Three movements of shoulder joint: (a) ﬂexion, (b) extension, and (c) abduction.

Computational Intelligence and Neuroscience

7

Rest 5–7 seconds

Rest 5–7 seconds

Collect 3 s EEG

Collect 3s EEG

Figure 8: Timing for experimental process.
Table 1: Ten-folder cross-validation classiﬁcation accuracy (%) for FGMDRM with OVR scheme applied on BCI competition dataset 2A.
Subject
L/RE
R/RE
F/RE
T/RE

A01
84.40
89.26
77.80
88.21

A02
67.39
75.33
83.44
68.76

A03
93.05
95.16
89.20
90.59

A04
77.64
78.56
80.88
79.95

A05
66.96
63.92
71.56
71.85

A06
74.34
71.81
78.22
76.72

A07
86.33
81.18
88.89
90.64

A08
93.83
93.39
79.18
94.43

A09
94.09
82.92
84.4
94.38

Mean
82.00
81.28
81.51
83.95

Std
10.90
10.24
5.65
9.82

Table 2: Kappa value comparison by SSDT-FGMDRM with other published results.
Subject
Method 1
MDRM [10]
FGMDM [14]
FGMDM_ﬁxed [14]

A01
0.66
0.75
0.72
0.69

A02
0.39
0.37
0.50
0.35

A03
0.78
0.66
0.64
0.60

A04
0.47
0.53
0.38
0.28

A05
0.25
0.29
0.28
0.21

A06
0.41
0.27
0.34
0.30

2

A07
0.72
0.56
0.64
0.46

A08
0.79
0.58
0.68
0.62

A09
0.83
0.68
0.75
0.53

Mean
0.589
0.52
0.55
0.45

p value
0.4683
0.6607
0.1423

1.5
1

1

First five features

First five selected features

1.5

0.5
0
–0.5
–1

0.5
0
–0.5

–1.5
Tongue Rest Tongue Rest Tongue Rest Tongue Rest Tongue Rest

Tongue Rest Tongue Rest Tongue Rest Tongue Rest Tongue Rest

(a)

(b)

Figure 9: The box plot of ﬁve features of subject A09. (a) One-way ANOVA analysis on the ﬁrst ﬁve features after applying semi-JMI. (b) Oneway ANOVA analysis on the ﬁrst ﬁve features from primitive vectors.
Classification accuracy in 10-fold cross validation

100
Classification accuracy (%)

Figure 11 illustrates distributions for the ﬁrst ﬁve most
discriminant variables with GDA and semi-JMI. It can be
seen from Figure 11 that L/RE is separated equally well by
using GDA.
Table 3 displays ten-folder cross-validation results obtained using SJGDA and KNN in OVR scheme. It can be
seen that the vectors which are mapped to the tangent space
have better classiﬁcation performance than that in the
Riemannian manifold directly.
Table 4 presents the results obtained by SJGDA in
pairwise way for multiclass MI tasks. We have six pairs of MI
tasks: left and right (L/R), left and foot (L/F), left and tongue
(L/T), right and foot (R/F), right and tongue (R/T), and foot
and tongue (F/T).
Table 5 displays the comparison of classiﬁcation accuracy using SJGDA and KNN for L/R task in 10-folder cross
validation. References [8, 43–45] contain the classiﬁcation of

95
90
85
80
75
70
65
60
55

0

50
L/RE
R/RE

100
150
Number of ranked variables
F/RE
T/RE

200

250

Figure 10: Classiﬁcation accuracy corresponding the number of
selected variables of Subject A09.

8

Computational Intelligence and Neuroscience
100

Frequency

80

L/RE with semi-JMI

60
40

L/RE with GDA

20
0
–8

–6
–4
–2
0
2
Trial values on the discriminant variables

4

Figure 11: Feature distribution for the most discriminant variables in tangent space of Subject A09.

Table 3: Ten-folder cross-validation results (%) obtained using SJGDA and KNN in OVR scheme applied on BCI competition dataset 2A.
Subject
L/RE
R/RE
F/RE
T/RE

A01
90.29
91.34
85.06
88.89

A02
74.62
77.11
83.70
76.72

A03
93.05
94.47
84.77
89.25

A04
75.02
77.07
77.11
80.91

A05
75.31
73.61
75.36
78.51

other publications. We have improved the accuracy compared with Reference [44] (p � 0.85) and Reference [45]
(p � 0.45). Gaur et al. [43] (p � 0.95) explored the speciﬁc
frequency information for each subject, and Sadatnejad and
Shiry Ghidary [8] (p � 0.90) used a novel kernel for dimensionality reduction which is similar to SJGDA. Although
the results in the paper are not as high as those in Reference
[43], it can be concluded that there is no diﬀerence between
the results in Reference [43] and those in this paper because
of p � 0.95.
Table 6 presents the results in terms of the kappa value.
The proposed method 1 achieved a mean performance of
0.589 which ranks this method to the ﬁrst place of the
competition. And with our proposed method 2, we have
achieved a mean performance of 0.607, which makes
method 2 to acquire the best performance of the state of the
art.
Dataset 2 is used to verify the eﬀect of method 2, and the
classiﬁcation results are given directly in this paper. The
results are shown in Table 7. As can be seen from Table 7,
method 2 obtained the second highest recognition rate in the
comparative literature. Compared with the recent reference
[47], method 2 achieved good classiﬁcation results.
5.3. Results of Dataset 3. Dataset 3 is used to evaluate the
performance of method 2. Figure 12 shows the classiﬁcation
error with KNN against the number of ranked variables in
OVR scheme. A/RE and F/RE are the two pairs with the
lowest classiﬁcation error, and they all achieved the highest
recognition rate within 60 variables. In this paper, the ﬁrst 60
ranked variables are used for the next analysis.
Figure 13 displays 5-folder cross-validation results obtained by using SJGDA and KNN in OVR and OVO scheme.
This Figure 13(a) illustrates three possible pairs of MI tasks
(F/RE, E/RE, and A/RE) for each subject. It can be learned

A06
78.15
75.74
77.09
76.04

A07
85.08
81.58
88.17
89.27

A08
92.35
93.41
82.30
94.08

A09
95.14
87.53
85.44
93.41

Mean
84.33
83.54
82.11
85.23

Std
8.12
7.75
4.24
6.76

from the ﬁgure that ﬂexion and abduction are the easiest
movement to distinguish in six subjects of seven, and the six
subjects are S1, S3, S4, S5, S6, and S7. However, due to
individual diﬀerences, the highest recognition rate of each
subject is diﬀerent.
We also compared three possible pairs (F/E, F/A, and E/
A) in OVO scheme of seven subjects. Figure 13(b) depicts
the comparison results for each subject, and it can be seen
that the pair of F/A obtained the highest recognition rate in
seven subjects. Combined with the analysis results of
Figures 13(a) and 13(b), it can be considered that ﬂexion and
extension are more obvious in the three MI tasks.
As SJGDA is a new method proposed in this paper, we
also compared the feature distribution of SJGDA, GDA, and
semi-JMI to illustrate the eﬀectiveness of SJGDA. Figure 14
depicts the feature distribution of F/E MI tasks of seven
subjects. The blue and red circles represent the two diﬀerent
feature classes. As shown in Figure 14, the F/E MI tasks
learned by SJGDA have high separability than GDA and
semi-JMI.
The performance of the proposed method 2 is evaluated
by using classiﬁcation accuracy. Since there are three classes,
the chance level is 33.33%. Figure 15 demonstrates that the
proposed method achieves higher performance for six
subjects (S1, S2, S3, S4, S5, and S6) out of seven except S7
compared to semi-JMI and GDA methods. In addition, it
also can be seen that GDA obtains a better classiﬁcation
accuracy for four subjects of seven (S1, S2, S5, and S7)
compared with semi-JMI. The reasons for this phenomenon
can be attributed to as follows: In the process of feature
selection, we manually select feature dimensions suitable for
classiﬁers, which results in partial information loss. As a
feature dimensionality reduction technique, GDA is suitable
for the preservation of useful information from the primitive
vectors. And the proposed method SJGDA in this paper not
only preserves the advantages of GDA but also adds some

Computational Intelligence and Neuroscience

9

Table 4: Ten-folder cross-validation results (%) obtained using SJGDA and KNN in OVO scheme applied on BCI competition dataset 2A.
Subject
L/R
L/F
L/T
R/F
R/T
F/T

A01
90.95
95.19
96.52
95.90
99.33
83.94

A02
67.29
87.43
64.24
87.52
79.82
84.17

A03
94.52
93.93
96.57
95.05
96.48
86.62

A04
63.87
81.84
83.24
84.84
79.12
75.10

A05
64.48
68.10
73.67
66.71
72.22
62.62

A06
70.33
77.14
69.91
72.88
71.48
74.21

A07
70.29
98.57
97.95
97.24
97.24
88.31

A08
97.95
88.29
97.23
93.18
95.05
93.10

A09
95.00
93.81
99.29
87.43
92.29
90.33

Mean
79.41
87.14
86.51
86.75
87.00
82.04

Std
13.85
9.29
13.16
10.01
10.61
9.10

Table 5: Comparison of classiﬁcation accuracy (%) for L/R task with other published results using OVO scheme applied on BCI competition
dataset 2A.
Subject
Method 2
Reference [43]
Reference [8]
Reference [44]
Reference [45]

A01
90.95
91.49
88.89
88.89
90.28

A02
67.29
60.56
59.03
51.39
54.17

A03
94.52
94.16
90.28
96.53
93.75

A04
63.87
76.72
78.47
70.14
64.58

A05
64.48
58.52
62.50
54.86
57.64

A06
70.33
68.52
75.00
71.53
65.28

A07
70.29
78.57
72.92
81.25
62.50

A08
97.95
97.01
93.06
93.75
90.97

A09
95.00
93.85
87.50
93.75
85.42

Mean
79.41
79.93
78.63
78.01
73.84

Std
13.85
14.13
11.63
16.04
15.02

p value
0.95
0.90
0.85
0.45

Table 6: Kappa value comparison with other published results.
Subject
Method 2
Method 1
Reference [43]
TSLDA [10]
Winner 1 [46]
Reference [8]
Reference [14]

A01
0.77
0.66
0.86
0.74
0.68
0.71
0.75

A02
0.38
0.39
0.24
0.38
0.42
0.46
0.49

A03
0.76
0.78
0.70
0.72
0.75
0.76
0.76

A04
0.47
0.47
0.68
0.50
0.48
0.44
0.49

A05
0.27
0.25
0.36
0.26
0.40
0.26
0.34

A06
0.42
0.41
0.34
0.34
0.27
0.37
0.36

A07
0.73
0.72
0.66
0.69
0.77
0.79
0.68

A08
0.81
0.79
0.75
0.71
0.75
0.75
0.76

A09
0.85
0.83
0.82
0.76
0.61
0.61
0.76

Mean
0.607
0.589
0.60
0.567
0.57
0.57
0.60

p value
0.8632
0.9586
0.6894
0.7051
0.7245
0.9353

Table 7: Five-folder cross validation by method 2 applied on BCI III dataset IIIa.
Subject
k3b
k6b
l1b
Mean
p value

Method 2
91.67
75.00
81.67
82.78

Reference [48]
86.67
81.67
85.00
84.44
0.42

Reference [49]
94.20
69.00
78.60
80.60
0.94

Reference [50]
94.44
62.50
78.33
78.42
0.73

high ranking features to strengthen the expressive ability of
the features.

Classification error in 5-fold cross validation

50
Classification error (%)

Reference [47]
90.00
76.25
77.91
81.38
0.96

40

6. Discussions

30
20
10
0

0

10

20

30
40
50
60
70
80
Number of ranked variables

90

100

F/RE
E/RE
A/RE

Figure 12: Classiﬁcation accuracy corresponding to the number of
selected variables of S01.

In this paper, we proposed a novel SSDT framework
combined with classiﬁers to improve the performance of
classiﬁers for multiclass MI tasks. We also proposed a novel
NLDR method named SJGDA, and this NLDR method
performs better than both semi-JMI and GDA on diﬀerent
datasets. In the following paragraphs, we have discussed the
two methods in detail.
Method 1 indicates the drawback of FGMDRM, and
then the novel SSDT framework is used to improve the
accuracy for each individual. As shown in Table 2, compared
with other published results, method 1 gets a quite good
result in the case of processing the EEG signals of ﬁxed
frequency segment (8–30 Hz).

Computational Intelligence and Neuroscience
110

110

100

100
Best accuracy (%)

Best accuracy (%)

10

90
80
70

90
80
70
60

60
50

S1

S2

S3

S4

S5

S6

S7

50

Mean

S1

S2

S3

S4

S5

S6

S7

Mean

F/E
F/A
E/A

F/RE
E/RE
A/RE
(a)

(b)

FIGURE 13: The comparison of three MI tasks in OVR scheme of seven subjects: (a) F/RE, E/RE, and A/RE; (b) F/E, F/A, and E/A.

Subject 1

1
0
–1
–2
–5

0
SJGDA
F

4

Subject 2

1

5

E
Subject 1

0.5

0.5

0.5

0

0

0

0

–0.5

–0.5

–0.5

–0.5

–0.5

–0.5

–1
–5

0
SJGDA

5

E
Subject 2

0
GDA
F

0
–2
–4
–1

0
Semi-JMI
F

E

0
GDA
F

2

1

0
SJGDA
F

4

5

E
Subject 3

5

–2
–10

–2
–2

1
0.5

0

0

–0.5

–0.5
0
Semi-JMI
F

E

0
GDA
F

1

1

2

E
Subject 4

–1
–5

0
SJGDA
F

5

5

E
Subject 5

10

E
Subject 3

0
GDA
F

F

E

1

0
SJGDA

5

E
Subject 6

2

–5
–5

E
Subject 4

0
Semi-JMI
F

–5
–5

E

0
GDA
F

5

5

–1
–2

2
0

–2

–1

–4
–5

0
GDA
F

5

–2
–5

E
Subject 6

2

1

1

1

0.5

0

0

0

–1

–1

–0.5

0
Semi-JMI
F

E

2

E
Subject 7

–2
–1

0
Semi-JMI
F

0
GDA
F

2

–2
–2

2

1

0

E
Subject 5

0
SJGDA
F

2

0

0
Semi-JMI

–1
–5

4

0

5

–1
–1

0.5

F

2
0

E
Subject 2

0
SJGDA

4

0

0.5

–1
–1

–1
–2
F

2

–10
5
–5

E
Subject 1

–1
–5

Subject 7

1

0

–5

–4
–5

Subject 6

1

0.5

0

–2

Subject 5

1

0

5

0

Subject 4

1

0.5

F

2

Subject 3

1

1

–1
–1

E

E
Subject 7

0
Semi-JMI
F

5

1

E

Figure 14: Feature distribution of F/E MI tasks extracted by SJGDA, GDA, and semi-JMI in dataset 2.

As shown in Table 6, Gaur et al. [43] proposed SSMEMDBF to select the subject-speciﬁc frequency to obtain
enhanced EEG signals which represent MI tasks related to µ
and β rhythms, then classiﬁcation with the Riemannian
distance directly. TSLDA was proposed by Barachant et al.
[10], and the covariance matrices are mapped onto a higher
dimensional space where they can be vectorized and treated
as Euclidean objects. Ang et al. [46] is the winner of the
competition, FBCSP and multiple OVR classiﬁers were
used for MI tasks, and achieved the mean kappa value of
0.57. Sadatnejad and Shiry Ghidary [8] proposed a new
kernel for NLDR over the manifold of SPD matrices, the

kappa value is 0.576. Davoudi et al. [14] considered the
geometry of SPD matrices and provides a low-dimensional
representation of the manifold with high-class discrimination, and the best result of this method in terms of the
kappa value is 0.60.
In method 2, SJGDA is used to get more obvious vectors
from the tangent vectors, and a SSDT-KNN classiﬁer is used
to identify diﬀerent MI tasks. Combined with SJGDA and
SSDT-KNN, we have achieved a better performance compared with method 1 (p � 0.8632), Reference [43]
(p � 0.9586), TSLDA (p � 0.6894), winner 1 (p � 0.7051),
Reference [8] (p � 0.7245), and Reference [14] (p � 0.9353).

Best accuracy (%)

Computational Intelligence and Neuroscience

11

110

References

100

[1] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller,
and T. M. Vaughan, “Brain-computer interfaces for communication and control,” Clinical neurophysiology, vol. 113,
no. 6, pp. 767–791, 2002.
[2] N. Birbaumer, “Breaking the silence: brain?computer interfaces (BCI) for communication and motor control,” Psychophysiology, vol. 43, no. 6, pp. 517–532, 2006.
[3] K. K. Ang, C. Guan, K. S. Phua et al., “Transcranial direct
current stimulation and EEG-based motor imagery BCI for
upper limb stroke rehabilitation,” in Proceedings of 2012
Annual International Conference of the IEEE Engineering in
Medicine and Biology Society (EMBC), pp. 4128–4131, IEEE,
San Diego, CA, USA, August 2012.
[4] B. Xu, S. Peng, A. Song, R. Yang, and L. Pan, “Robot-aided
upper-limb rehabilitation based on motor imagery EEG,”
International Journal of Advanced Robotic Systems, vol. 8,
no. 4, p. 40, 2011.
[5] F. Wang, X. Zhang, R. Fu, and G. Sun, “Study of the homeauxiliary robot based on BCI,” Sensors, vol. 18, no. 6, p. 1779,
2018.
[6] F. Wang, H. Wang, and R. Fu, “Real-Time ECG-based detection of fatigue driving using sample entropy,” Entropy,
vol. 20, no. 3, 2018.
[7] Z. J. Koles, M. S. Lazar, and S. Z. Zhou, “Spatial patterns
underlying population diﬀerences in the background EEG,”
Brain Topography, vol. 2, no. 4, pp. 275–284, 1990.
[8] K. Sadatnejad and S. Shiry Ghidary, “Kernel learning over the
manifold of symmetric positive deﬁnite matrices for dimensionality reduction in a BCI application,” Neurocomputing, vol. 179, pp. 152–160, 2016.
[9] M. Congedo, A. Barachant, and R. Bhatia, “Riemannian geometry for EEG-based brain-computer interfaces; a primer
and a review,” Brain-Computer Interfaces, vol. 4, no. 3,
pp. 155–174, 2017.
[10] A. Barachant, S. Bonnet, M. Congedo, and C. Jutten, “Multiclass brain-computer interface classiﬁcation by riemannian
geometry,” IEEE Transactions on Biomedical Engineering,
vol. 59, no. 4, pp. 920–928, 2012.
[11] A. Barachant, S. Bonnet, M. Congedo, and C. Jutten,
“Common spatial pattern revisited by riemannian geometry,”
in Proceedings of 2010 IEEE International Workshop on
Multimedia Signal Processing (MMSP), pp. 472–476, IEEE,
Saint Malo, France, October 2010.
[12] A. Barachant, S. Bonnet, M. Congedo, and C. Jutten, “A brainswitch using riemannian geometry,” in Proceedings of 5th
International Brain-Computer Interface Conference 2011 (BCI
2011), pp. 64–67, Graz, Austria, September 2011.
[13] A. Barachant, S. Bonnet, M. Congedo, and C. Jutten, “Classiﬁcation of covariance matrices using a riemannian-based
kernel for BCI applications,” Neurocomputing, vol. 112,
pp. 172–178, 2013.
[14] A. Davoudi, S. S. Ghidary, and K. Sadatnejad, “Dimensionality
reduction based on distance preservation to local mean
(DPLM) for spd matrices and its application in BCI,” 2016,
https://arxiv.org/abs/1608.00514.
[15] S. Brandl, K.-R. Müller, and W. Samek, “Robust common
spatial patterns based on Bhattacharyya distance and gamma
divergence,” in Proceedings of 2015 3rd International Winter
Conference on Brain-Computer Interface (BCI), pp. 1–4, IEEE,
Gangwon-Do, South Korea, January 2015.
[16] W. Samek, M. Kawanabe, and K.-R. Muller, “Divergencebased framework for common spatial patterns algorithms,”

90
80
70
60
50

S1

S2

S3

S4

S5

S6

S7

Mean

Semi-JMI
GDA
SJGDA

Figure 15: The comparison of the three diﬀerent feature selection
methods (semi-JMI, GDA, and SJGDA) with the same classiﬁer
(DT-KNN) of three MI tasks classiﬁcation accuracy for each subject
is depicted. The results are obtained by 5-folder cross validation.

It is clear that the proposed method in this paper is eﬀective
for MI tasks in a BCI system.
In order to prove the eﬀectiveness of the proposed
method 2, we tested it on two other datasets. As shown in
Table 7 and Figure 15, method 2 achieves good classiﬁcation
results on two datasets.

7. Conclusion
The experimental results of method 1 show that the
proposed classiﬁcation framework signiﬁcantly improves
the classiﬁcation performance of the classiﬁer. The experimental results of method 2 show that the SJGDA algorithm proposed in this paper is superior to GDA and
semi-JMI in feature extraction, and method 2 has the
highest recognition rate in this paper. However, as the
classiﬁers in the SSDT framework is substitutable, the focus
of the next work is to combine more advanced classiﬁers
with SSDT to increase the recognition rate of the BCI
systems.

Data Availability
The dataset 1 and dataset 2 used to support the ﬁndings of
this study are available from http://bnci-horizon-2020.eu/
database/data-sets. The dataset 3 used to support the ﬁndings of this study is available from the corresponding author
upon request.

Conflicts of Interest
The authors declare no conﬂicts of interest.

Acknowledgments
Northeast Electric Power University (Grant number
BSJXM-201521) and Jilin City Science and Technology
Bureau (Grant number 20166012).

12

[17]

[18]

[19]

[20]

[21]

[22]

[23]

[24]
[25]

[26]

[27]

[28]

[29]

[30]

[31]

Computational Intelligence and Neuroscience
IEEE Reviews in Biomedical Engineering, vol. 7, pp. 50–72,
2014.
W. Samek and K.-R. Müller, “Information geometry meets
BCI spatial ﬁltering using divergences,” in Proceedings of 2014
International Winter Workshop on Brain-Computer Interface
(BCI), pp. 1–4, IEEE, Gangwon, South Korea, February 2014.
W. Samek and M. Kawanabe, “Robust common spatial patterns by minimum divergence covariance estimator,” in
Proceedings of 2014 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), pp. 2040–2043,
IEEE, Florence, Italy, May 2014.
S. Kumar, K. Mamun, and A. Sharma, “CSP-TSM: optimizing
the performance of riemannian tangent space mapping using
common spatial pattern for mi-BCI,” Computers in biology
and medicine, vol. 91, pp. 231–242, 2017.
C. Lindig-León, N. Gayraud, L. Bougrain, and M. Clerc,
“Comparison of hierarchical and non-hierarchical classiﬁcation for motor imagery based BCI systems,” in Proceedings
of The Sixth International Brain-Computer Interfaces Meeting,
Paciﬁc Grove, CA, USA, May-June 2016.
S. T. Roweis and L. K. Saul, “Nonlinear dimensionality reduction by locally linear embedding,” Science, vol. 290,
no. 5500, pp. 2323–2326, 2000.
O. Kramer and D. Lückehe, “Visualization of evolutionary
runs with isometric mapping,” in Proceedings of 2015 IEEE
Congress on Evolutionary Computation (CEC), pp. 1359–1363,
IEEE, Sendai, Japan, May 2015.
K. Q. Weinberger, F. Sha, and L. K. Saul, “Learning a kernel
matrix for nonlinear dimensionality reduction,” in Proceedings of the Twenty-First International Conference on
Machine Learning, p. 106, ACM, Banﬀ, Canada, July 2004.
L. Van Der Maaten, “Learning a parametric embedding by
preserving local structure,” RBM, vol. 500, p. 26, 2009.
L. Van Der Maaten, “Accelerating t-SNE using tree-based
algorithms,” Journal of Machine Learning Research, vol. 15,
pp. 3221–3245, 2014.
D. Lee, S.-H. Park, and S.-G. Lee, “Improving the accuracy
and training speed of motor imagery brain-computer interfaces using wavelet-based combined feature vectors and
Gaussian mixture model-supervectors,” Sensors, vol. 17,
no. 10, p. 2282, 2017.
X. Xie, Z. L. Yu, H. Lu, Z. Gu, and Y. Li, “Motor imagery
classiﬁcation based on bilinear sub-manifold learning of
symmetric positive-deﬁnite matrices,” IEEE Transactions on
Neural Systems and Rehabilitation Engineering, vol. 25, no. 6,
pp. 504–516, 2017.
M. T. Harandi, M. Salzmann, and R. Hartley, “From manifold
to manifold: Geometry-aware dimensionality reduction for
SPD matrices,” in Proceedings of European Conference on
Computer Vision, pp. 17–32, Springer, Zurich, Switzerland,
September 2014.
M. Moakher, “A diﬀerential geometric approach to the
geometric mean of symmetric positive-deﬁnite matrices,”
SIAM Journal on Matrix Analysis and Applications, vol. 26,
no. 3, pp. 735–747, 2005.
P. T. Fletcher and S. Joshi, “Principal geodesic analysis on
symmetric spaces: statistics of diﬀusion tensors,” in Computer
Vision and Mathematical Methods in Medical and Biomedical
Image Analysis, M. Sonka, I. A. Kakadiaris, and J. Kybic, Eds.,
pp. 87-98, Springer, Berlin, Heidelberg, 2004.
A. Barachant, S. Bonnet, M. Congedo, and C. Jutten, “Riemannian geometry applied to BCI classiﬁcation,” in Proceedings of International Conference on Latent Variable

[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]
[41]

[42]

[43]

[44]

[45]

[46]

[47]

[48]

Analysis and Signal Separation, pp. 629–636, Springer, St.
Malo, France, September 2010.
H. Guo and S. B. Gelfand, “Classiﬁcation trees with neural
network feature extraction,” IEEE Transactions on Neural
Networks, vol. 3, no. 6, pp. 923–933, 1992.
S. R. Safavian and D. Landgrebe, “A survey of decision tree
classiﬁer methodology,” IEEE Transactions on Systems, Man,
and Cybernetics, vol. 21, no. 3, pp. 660–674, 1991.
Y.-H. Shao, W.-J. Chen, W.-B. Huang, Z.-M. Yang, and
N.-Y. Deng, “The best separating decision tree twin support
vector machine for multi-class classiﬁcation,” Procedia
Computer Science, vol. 17, pp. 1032–1038, 2013.
G. Brown, A. Pocock, M.-J. Zhao, and M. Luján, “Conditional
likelihood maximisation: a unifying framework for information theoretic feature selection,” Journal of Machine
Learning Research, vol. 13, pp. 27–66, 2012.
K. Sechidis and G. Brown, “Simple strategies for semisupervised feature selection,” Machine Learning, vol. 107,
no. 2, pp. 357–395, 2017.
J. G. Moreno-Torres, T. Raeder, R. Alaiz-Rodrı́guez,
N. V. Chawla, and F. Herrera, “A unifying view on dataset
shift in classiﬁcation,” Pattern Recognition, vol. 45, no. 1,
pp. 521–530, 2012.
G. Baudat and F. Anouar, “Generalized discriminant analysis
using a kernel approach,” Neural Computation, vol. 12, no. 10,
pp. 2385–2404, 2000.
M. Haghighat, S. Zonouz, and M. Abdel-Mottaleb, “Cloudid:
trustworthy cloud-based and cross-enterprise biometric
identiﬁcation,” Expert Systems with Applications, vol. 42,
no. 21, pp. 7905–7916, 2015.
V. Vapnik, The Nature of Statistical Learning Theory, Springer
Science & Business Media, Berlin, Germany, 2013.
C. Brunner, R. Leeb, G. Müller-utz, A. Schlögl, andG.
Pfurtscheller, BCI Competition 2008–Graz Data Set A, vol.16,
Institute for Knowledge Discovery (Laboratory of BrainComputer Interfaces) Graz University of Technology, Graz,
Austria, 2008.
B. Blankertz, K. R. Muller, D. J. Krusienski et al., “The BCI
competition III: validating alternative approaches to actual
BCI problems,” IEEE Transactions on Neural Systems and
Rehabilitation Engineering, vol. 14, no. 2, pp. 153–159, 2006.
P. Gaur, R. B. Pachori, H. Wang, and G. Prasad, “A multi-class
EEG-based BCI classiﬁcation using multivariate empirical
mode decomposition based ﬁltering and riemannian geometry,” Expert Systems with Applications, vol. 95, pp. 201–211,
2018.
F. Lotte and C. Cuntai Guan, “Regularizing common spatial
patterns to improve BCI designs: uniﬁed theory and new
algorithms,” IEEE Transactions on Biomedical Engineering,
vol. 58, no. 2, pp. 355–362, 2011.
H. Raza, H. Cecotti, Y. Li, and G. Prasad, “Adaptive learning
with covariate shift-detection for motor imagery-based braincomputer interface,” Soft Computing, vol. 20, no. 8,
pp. 3085–3096, 2015.
K. K. Ang, Z. Y. Chin, C. Wang, C. Guan, and H. Zhang,
“Filter bank common spatial pattern algorithm on BCI
competition IV datasets 2a and 2b,” Frontiers in Neuroscience,
vol. 6, p. 39, 2012.
H. Baali, A. Khorshidtalab, M. Mesbah, and M. J. E. Salami, “A
transform-based feature extraction approach for motor imagery tasks classiﬁcation,” IEEE Journal of Translational
Engineering in Health and Medicine, vol. 3, pp. 1–8, 2015.
A. Schlögl, F. Lee, H. Bischof, and G. Pfurtscheller, “Characterization of four-class motor imagery EEG data for the

Computational Intelligence and Neuroscience
BCI-competition 2005,” Journal of Neural Engineering, vol. 2,
no. 4, pp. L14–L22, 2005.
[49] M. Grosse-Wentrup and M. Buss, “Multiclass common spatial
patterns and information theoretic feature extraction,” IEEE
Transactions on Biomedical Engineering, vol. 55, no. 8,
pp. 1991–2000, 2008.
[50] I. Koprinska, “Feature selection for brain-computer interfaces,” in Prooceedings of Paciﬁc-Asia Conference on
Knowledge Discovery and Data Mining, pp. 106–117, Springer,
Bangkok, Thailand, April 2009.

13

Advances in

Multimedia
Applied
Computational
Intelligence and Soft
Computing
Hindawi
www.hindawi.com

Volume 2018

The Scientific
World Journal
Hindawi Publishing Corporation
http://www.hindawi.com
www.hindawi.com

Volume 2018
2013

Mathematical Problems
in Engineering
Hindawi
www.hindawi.com

Volume 2018

Engineering
Journal of

Hindawi
www.hindawi.com

Volume 2018

Hindawi
www.hindawi.com

Modelling &
Simulation
in Engineering
Hindawi
www.hindawi.com

Volume 2018

Advances in

Artificial
Intelligence
Hindawi
www.hindawi.com

Volume 2018

Volume 2018

Advances in
International Journal of

Reconfigurable
Computing

Hindawi
www.hindawi.com

Fuzzy
Systems

Submit your manuscripts at
www.hindawi.com

Hindawi
www.hindawi.com

Volume 2018

Volume 2018

Journal of

Hindawi
www.hindawi.com

International Journal of

Advances in

Scientific
Programming

Engineering
Mathematics

Human-Computer
Interaction
Volume 2018

Hindawi
www.hindawi.com

Computer Networks
and Communications

Volume 2018

Hindawi
www.hindawi.com

Advances in

Civil Engineering
Volume 2018

Hindawi
www.hindawi.com

Volume 2018

Hindawi
www.hindawi.com

Volume 2018

International Journal of

Biomedical Imaging

International Journal of

Robotics
Hindawi
www.hindawi.com

Journal of

Computer Games
Technology

Journal of

Volume 2018

Hindawi
www.hindawi.com

Electrical and Computer
Engineering
Volume 2018

Hindawi
www.hindawi.com

Volume 2018

Hindawi
www.hindawi.com

Volume 2018

Computational Intelligence
and Neuroscience
Hindawi
www.hindawi.com

Volume 2018

