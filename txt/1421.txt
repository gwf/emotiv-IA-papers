Wybrane aspekty informatyki biomedycznej

Wyższa Szkoła Biznesu w Dąbrowie Górniczej

WYBRANE ASPEKTY INFORMATYKI
BIOMEDYCZNEJ

Praca zbiorowa pod redakcją
Pawła Kostki

Dąbrowa Górnicza 2014

Prace naukowe Wyższej Szkoły Biznesu w Dąbrowie Górniczej
Recenzent:
Dr hab. inż. Bartłomiej Zieliński

ISBN 978-83-62897-82-7

Wydawca:
Wyższa Szkoła Biznesu w Dąbrowie Górniczej
ul. Cieplaka 1c
41-300 Dąbrowa Górnicza
Tel/fax. (32) 262 28 05
e-mail: info@wsb.edu.pl
www.wsb.edu.pl

Druk i oprawa:
Drukarnia GS Sp. z o.o.

Publikacja współfinansowana ze środków Unii Europejskiej
w ramach Europejskiego Funduszu Społecznego.
Człowiek – najlepsza inwestycja
Projekt „Innowacyjne specjalności na kierunku
Informatyka w Wyższej Szkole Biznesu w Dąbrowie
Górniczej” realizowany w ramach Poddziałania 4.1.1
„Wzmocnienie potencjału dydaktycznego uczelni”
Programu Operacyjnego Kapitał Ludzki
Publikacja dystrybuowana bezpłatnie

© Copyright by Wyższa Szkoła Biznesu w Dąbrowie Górniczej 2014

Spis treści
Wstęp ......................................................................................................... 7

Rozdział 1
Zastosowanie filtru odpornego na bazie Lp-normy
w przetwarzaniu sygnałów biomedycznych .............................................. 9
Tomasz Pander, Tomasz Przybyła

Rozdział 2
Hybrydowe systemy falkowo-neuronowe jako struktury
klasyfikatorów sygnałów biomedycznych ............................................ 27
Paweł Kostka

Rozdział 3
Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne
na błędy „bazy dużych danych” ........................................................... 67
Maciej Rostański

Rozdział 4
Wykorzystanie systemów teleinformatycznych i pomiarowych
do dystrybucji danych na potrzeby prognozowania infekcji
roślin uprawnych ................................................................................... 81
Paweł Buchwald

Rozdział 5
Wykorzystanie metod biometrycznych w realizacji interfejsów
człowiek – komputer ............................................................................ 103
Paweł Buchwald

Wstęp
Informatyka medyczna, tak rozległa dziedzina wiedzy przeżywa w ostatnich dziesięcioleciach a obecnie szczególnie wyraźnie widoczny, bardzo
gwałtowny rozwój w wyniku efektu synergii – połączenia procedur medycznych z technologią informatyczną. To pole badań naukowych i prac aplikacyjnych trudne do krótkiego zdefiniowania. Wydaje się jednak, że można
przedstawić je dość trafnie jako dyscyplinę, która zajmuje się akwizycją,
przechowywaniem i przetwarzaniem różnego typu danych biomedycznych
i wiedzy w celu opracowania urządzeń i metod dla szeroko rozumianego
wspomagania diagnostyki i terapii medycznej.
Projekty z tej liczącej ok. 50 lat dziedziny, która ciągle ewoluuje to multi-dyscyplinarne platformy, łączące specjalistów lekarzy, biologów, przedstawicieli nauk podstawowych oraz inżynierów, którzy działają w środowisku
ogromnej, często nadmiarowej liczby danych, opisujących złożone, niepowtarzalne, trudne do testowania in-vivo zjawiska zachodzące w organizmach żywych.
Oddawana do rąk czytelnika praca zbiorowa: „Wybrane aspekty informatyki medycznej”, to zbiór kilku prac, ulokowanych w różnych polach zarówno
badawczych jak i aplikacyjnych, informatyki medycznej – „próbkujących”
tę dyscyplinę i jej aktualny stan, obrazując jej dużą rozległość i wielo-obszarowość. Dwa pierwsze rozdziały przedstawiają prace dotyczące aspektów
przetwarzania informacji biomedycznych, z użyciem nowatorskich narzędzi cyfrowego przetwarzania sygnałów: odpornych filtrów dla tłumienia zakłóceń impulsowych oraz hybrydowych systemów klasyfikatorów falkowo-neuronowych, gdzie przed strukturą sieci neuronowej, pełniącej rolę klasyfikatora sygnałów przeprowadzana jest ekstrakcja najistotniejszych cech
z wykorzystaniem analizy falkowej sygnału zmienności rytmu serca (Heart
Rate Variability). Rozdział trzeci poświęcony jest opisowi systemów przechowywania i przetwarzania dużych ilości danych (określanych jako „Big
Data Problem”), z przedstawieniem ich struktury oraz przykładem modelu
przełożenia relacyjnej bazy danych na formę hipertabeli. Dwa ostatnie rozdziały pracy (czwarty i piąty) maja charakter aplikacyjny i bardziej przeglądowy, przedstawiając rozproszone sieciowe systemy akwizycji i przesyłania
danych pomiarowych na przykładzie monitorowania i predykcji infekcji wi7

Wstęp
rusowej roślin oraz w rozdziale piątym rozwiązania sprzętowo-programowe
interfejsu człowiek-komputer w akwizycji sygnałów biometrycznych bazujących na sygnałach EEG, EMG i okulograficznych.
Publikacja ta powstała w ramach projektu „Innowacyjne specjalności na
kierunku Informatyka w Wyższej Szkole Biznesu w Dąbrowie Górniczej”
w ramach Poddziałania 4.1.1 Programu Operacyjnego Kapitał Ludzki i stanowi w swym zamierzeniu również materiał wspomagający proces dydaktyczny studentów specjalności „Bioinformatyka”.

8

Rozdział 1
Zastosowanie filtru odpornego na bazie Lp-normy
w przetwarzaniu sygnałów biomedycznych
Tomasz Pander
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
okolica90@gmail.com
Tomasz Przybyła
Wyższa Szkoła Biznesu w Dąbrowie Górniczej
tprzybyla@polsl.pl

Streszczenie
Celem pracy jest przedstawienie problemu tłumienia zakłóceń
impulsowych w cyfrowym przetwarzaniu sygnałów biomedycznych. Przykładem filtrów odpornych są filtry miriadowy lub
meridianowy, które należą do grupy filtrów będących estymatorami największej wiarygodności parametru położenia, tzw.
M-estymatory. Funkcja kosztu tych filtrów ma podobną postać.
W tej pracy zostanie przedstawiony uproszczony filtr na bazie funkcji kosztu Cauchy’ego i Lp-normy. Zaproponowany filtr
pozwala na skuteczne tłumienie zakłóceń o charakterze impulsowym dzięki możliwości zmiany parametru p w Lp-normie.
Działanie filtru jest zaprezentowane na przykładzie filtracji sygnałów biomedycznych, a w szczególności tłumienia zakłóceń
o charakterze impulsowym i mięśniowych w sygnale elektrokardiograficznym. Otrzymane wyniki świadczą o przydatności
i skuteczności działania uproszczonego filtru na bazie funkcji
kosztu Cauchy’ego i Lp-normy.

9

Tomasz Pander, Tomasz Przybyła

1. Wprowadzenie
W ostatnich latach wiele uwagi poświęcono tzw. metodom odpornym, które
są reprezentowane m.in. przez estymatory największej wiarygodności parametru położenia (tzw. M-estymatory) jak miriada, meridiana czy mediana.
Odporne M-filtry są stosowane w wielu dziedzinach cyfrowego przetwarzania sygnałów. Ich zastosowanie w obecności zakłóceń impulsowych jest
przydatne w przetwarzaniu sygnałów radarowych, akustyce podwodnej,
komunikacji bezprzewodowej i z wykorzystaniem sieci zasilających i inne
[1, 3, 5, 9, 19, 25, 28, 29]. Zakłócenia o charakterze impulsowym spotykane
są również w inżynierii biomedycznej, np. w przypadku elektrochirurgii, rejestracji sygnałów elektrofizjologicznych (zakłócenia mięśniowe), zmiany
trybu pracy urządzenia, przełączanie napięcia zasilania z zasilania sieciowego na zasilanie awaryjne [21, 23]. Przedostanie się zakłóceń o charakterze
impulsowym do toru przetwarzania, w którym dokonuje się klasyfikacji lub
podejmuje decyzje, może być niebezpieczne dla życia i zdrowia pacjenta.
W systemach biomedycznych, pierwszym etapem przetwarzania sygnałów
musi być efektywna filtracja, ponieważ dokładność i precyzja dalszych etapów, takich jak pomiary czy klasyfikacja, zależą od filtracji [17]. Nieliniowe, odporne M-filtry pozwalają osiągnąć lepsze wyniki w tłumieniu zakłóceń o rozkładach niegaussowskich niż filtry liniowe. Znajomość a priori
rozkładu prawdopodobieństwa zakłóceń w przypadku użycia odpornych,
nieliniowych filtrów jest użyteczna, ale w wielu praktycznych sytuacjach
rozkład zakłóceń jest nieznany, a ponadto może zmieniać się w czasie. Ten
fakt motywuje do poszukiwania bardziej ogólnej formuły dla filtrów odpornych, które dzięki możliwości zmiany parametrów, mogą efektywnie tłumić
szeroką gamę rozkładów gęstości prawdopodobieństwa zakłóceń [18].
Wymienione wcześniej filtry korzystają z metryk odpornych, takich jak metryka miriadowa lub meridianowa. W [13] wprowadzono metrykę Hubera lub bardziej ogólną Lp-normę [11, 26, 27]. Tym samym Lp-norma działa
prawidłowo dla rozkładów prawdopodobieństwa z tzw. ciężkimi ogonami.
Jednak określenie prawidłowej wartości parametru p dla Lp-normy jest trudne, ponieważ nie są znane odpowiednie ku temu metody [18]. W [5] zaproponowano iteracyjną metodę estymacji parametru p. Podczas projektowania
filtrów, często spotyka się pewną funkcję kryterialną będącą normą błędu,
która powinna przyjąć wartość minimalną. Możliwe jest stosowanie w takim przypadku minimaksowej normy Czebyszewa L lub normy L2, której
minimalizacja prowadzi do analitycznego rozwiązania metodą najmniejszych kwadratów. Jednak głównym problemem pozostaje dobór odpowiedniej wartości parametru p w taki sposób, by została zachowana odporność
filtru na zakłócenia impulsowe [32].
10

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...
Modelowanie procesów o charakterze impulsowym za pomocą rozkładów
α-stabilnych, jest często wykorzystywane w przetwarzaniu sygnałów [7, 25,
30]. Przyczyną początkowego niewielkiego zainteresowania rozkładami
stabilnymi, jest brak analitycznej postaci funkcji gęstości prawdopodobieństwa. Natomiast istnieje postać analityczna funkcji charakterystycznej φ(t)
rozkładów stabilnych wyrażona jako [8, 25]:



 ( t )  exp j t   t



1 

j sign(t) (t, ) 



(1)

gdzie:

oraz



tan
, dla   1


2
( t , )  
 2 log t , dla   1



(2)

 1, dla t  0

sign( t )   0 , dla t  0
1, dla t  0


(3)

oraz
 α – jest parametrem (ang. characteristic exponent) spełniającym następujący warunek 0    2 . Parametr  kontroluje opadanie „ogona”
funkcji gęstości prawdopodobieństwa. Wolne opadanie „ogona” funkcji
gęstości prawdopodobieństwa odbywa się dla małych wartości  , wtedy zakłócenia mają charakter bardzo impulsowy, podczas gdy dla większych wartości  „ogon” funkcji gęstości prawdopodobieństwa opada
szybciej i charakter impulsowy maleje,
 λ – jest parametrem odpowiadającym za przesunięcie rozkładu i   .
Parametr ten odpowiada wartości średniej dla 1    2 , oraz medianie
dla 0    1 .
 γ – jest parametrem skali. Zachowuje się w analogiczny sposób jak wariancja rozkładu gaussowskiego, jest miarą rozrzutu wokół wartości
średniej,
 θ – jest parametrem asymetrii (θ  [-1, 1]). Gdy θ = 0, rozkład gęstości
prawdopodobieństwa jest symetryczny względem λ.
W przypadku, gdy α = 2 oraz θ = 0 otrzymuje się rozkład gaussowski z wariancją 2γ, a dla α = 1 oraz θ = 0 otrzymuje się rozkład Cauchy`ego, który
jest jedynym rozkładem w rodzinie rozkładów α-stabilnych o znanej postaci
funkcji gęstości prawdopodobieństwa [5]. W celu określenia parametrów roz11

Tomasz Pander, Tomasz Przybyła
kładów α-stabilnych, można skorzystać z metod zaproponowanych w [8, 25,
30, 31]. W tej pracy wykorzystywana jest metoda opisana w [8].
Filtry miriadowe i meridianowe są zaprojektowane do optymalnego działania, gdy zakłócenia można opisać znaną funkcją gęstości prawdopodobieństwa. W zaproponowanym w niniejszej pracy podejściu, zakłócenia są
opisane przez uogólniony rozkład Cauchy’ego. W takim przypadku zaprezentowana jest uproszczona postać funkcji kosztu filtru opartej o ideę metryki Lp. Celem pracy jest zaprezentowanie uproszczonego odpornego filtru
opartego na Lp-normie oraz jego właściwości. Przedstawiona funkcja kosztu
filtru, składa się z pewnej stałej ξ oraz metryki Lp. Jest udowodnione, że
zaprezentowany filtr ma rozwiązanie dla 1  p < .
Organizacja pracy jest następująca. W kolejnej części zostanie przedstawiona rodzina M-estymatorów i filtrów należących do tej grupy. W części
trzeciej, po zaprezentowaniu uogólnionego rozkładu Cauchy’ego, zostanie
przedstawiony uproszczony filtr, oparty na Lp-normie oraz jego podstawowe
właściwości. Sposób oceny działania filtru zostanie przedstawiony w następnej części. W kolejnej części zostanie przedstawione praktyczne wykorzystanie filtru w tłumieniu zakłóceń impulsowych i mięśniowych w sygnale EKG.

2. Rodzina filtrów opartych o M-estymatory
Techniki wykorzystujące M-estymatory, należą do jednych z najpopularniejszych metod odpornych w cyfrowym przetwarzaniu sygnałów. Podstawa działania estymatora największej wiarygodności parametru położenia
(M-estymatora) może zostać przedstawiona w następujący sposób. Niech
dany jest zbiór N próbek sygnału x1, x2,…, xN, gdzie xi = β + νi oraz 1  i  N.
Zadaniem jest estymowanie wartości parametru β w obecności zakłóceń νi.
Parametr β odpowiada położeniu środka funkcji gęstości prawdopodobieństwa na osi odciętych, utworzonej przez próbki sygnału xi. Dokładny rozkład zakłóceń νi nie jest znany. Jedynym założeniem jest, że ν1, ν2,…,νN są
symetryczne, niezależne i mają jednakowy rozkład [12, 14]. Estymator największej wiarygodności, został zaproponowany w celu poprawy odporności
statystycznych estymatorów w przypadku małych odchyleń.
Jeżeli funkcja kosztu ρ(x, β) jest wybrana w taki sposób, że ρ(x, β) = log f(x,
β), wtedy M-estymator zwraca estymator największej wiarygodności, gdzie
x to obserwowane zmienne losowe z funkcją gęstości prawdopodobieństwa
f(x) i parametr β, który ma być estymowany. W praktyce, nieznana funkcja
gęstości prawdopodobieństwa jest trudna do określenia i zwykle funkcja
12

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...
kosztu ρ(x,β) jest wybierana jako pewna funkcja reszt z = x  β taka, że ρ(x,
β) = ρ(x  β). Ogólna postać M-estymatora jest następująca:
N

ˆ ˆ arg min   ( xi   )
   i 1

(4)

gdzie ρ() jest nazywana funkcją kosztu. Estymator największej wiarygodności parametru położenia jest zdefiniowany jako parametr β, który minimalizuje wartość wyrażenia (4). Właściwości M-estymatora ściśle zależą od
właściwości funkcji kosztu ρ() [6, 9, 14].
W przypadku rozkładu Cauchy’ego, parametr położenia β jest nazywany
miriadą. Dla danych N niezależnych i jednakowo prawdopodobnych próbek, których rozkład jest zgodny z rozkładem Cauchy’ego z danym parametrem skali K, miriada jest argumentem, dla którego następujące wyrażenie
osiąga wartość minimalną:
N

2
2
ˆ K  arg min  log[K  (xi   ) ]
  i 1

(5)

Przez przyporządkowanie nieujemnych wartości wag wi do próbek sygnału
xi otrzymuje się ważoną miriadę
N

2
2
ˆ K  arg min  log[K  wi (xi   ) ]
  i 1



N

 miriada wi  xi i 1 ; K



(6)

Filtr miriadowy jest filtrem ruchomym o długości N. Wartość ważonej miriady (6) jest uzależniona od zbioru próbek sygnału x, wartości wag w oraz
od parametru liniowego K, który kontroluje odporność filtru miriadowego.
Dla małych wartości K, wartość wyjściowa filtru jest w przybliżeniu taka,
jak najbardziej liczebne skupisko próbek sygnału wejściowego. Gdy K dąży
do nieskończoności i α = 2 (rozkład gaussowski), wyjście filtru miriadowego zachowuje się jak wyjście filtru ruchomej średniej [2, 9]. Jednak nie istnieje prosta zależność, która pozwoliłaby określić dokładną wartość K. Aby
dobrać wartość tego parametru, by filtr miradowy jak najlepiej tłumił zakłócenia impulsowe, konieczna jest informacja a priori o rozkładzie funkcji
gęstości prawdopodobieństwa zakłóceń [24].
Innym przypadkiem szczególnym, w którym funkcja kosztu ρ(x, β) jest podobna do funkcji kosztu filtru miriadowego, jest filtr meridianowy. Jeżeli
zmienna losowa jest opisana jako stosunek dwóch niezależnych zmiennych
losowych o rozkładzie Laplace’a z zerową wartością średnią, to wtedy
zmienna losowa ma rozkład meridianowy [3]. Analogicznie jak w przypad13

Tomasz Pander, Tomasz Przybyła
ku miriady, meridiana jest zdefiniowana dla zbioru N niezależnych i o jednakowym rozkładzie próbek xi (gdzie 1 ≤ i ≤ N), których rozkład jest zgodny
z rozkładem meridianowym, w następujący sposób:
N

ˆ   arg min  log[  xi   ]
  i 1

(7)

gdzie β jest parametrem położenia, δ jest parametrem skali. Przez przyporządkowanie nieujemnych wartości wag wi do próbek sygnału xi otrzymuje
się ważoną meridianę
N

 log[  w
 

ˆ   arg min



 meridiana wi  x i

N
i 1

xi   ]

i

i 1

;



(8)

Więcej właściwości filtru meridianowego jest przedstawionych w [3].

3. Uproszczona funkcja kosztu oparta na uogólnionym
rozkładzie Cauchy’ego
Zaprezentowane wcześniej funkcje kosztu mają wspólny mianownik, którym jest rozkład Cauchy’ego. Uogólniona postać rozkładu Cauchy’ego ma
następującą postać [5]:



f GC  z   a  p  z



p 2/ p

(9)

gdzie a  p  2 / p  / 2   1 / p   , z = x − β, β jest parametrem położenia,
σ jest parametrem skali, Γ() to funkcja gamma a p jest parametrem ogonym rozkładu. Rodzina uogólnionych rozkładów Cauchy’ego zawiera, jako
przypadki szczególne, rozkład meridianowy (p = 1) i rozkład Cauchy’ego
(p = 2) [5]. Funkcja kosztu powstała na bazie uogólnionego rozkładu Cauchy’ego ma postać:
2



  z   log  p  z

p

 , dla   0, 0  p  2

(10)

oraz estymator największej wiarygodności parametru położenia jest zdefiniowany jako:
N

p
ˆ   arg min  log[  wi  xi    ]
  i 1

14

p

(11)

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...
Równanie (11) jest wykorzystane do zdefiniowania filtru odpornego na bazie uogólnionego rozkładu Cauchy’ego.
Porównując właściwości wcześniej przedstawionych funkcji kosztu dla filtrów miriadowego i meridianowego, można zauważyć wspólne cechy. Jedną z nich jest zachowanie się funkcji kosztu ρ() w przypadku, gdy wyraz
wolny, tj. K, σ, δ w wyrażeniu logarytmowanym dąży do zera. W takim
przypadku dla takich samych danych x, wartość ważonej miriady jest równa
wartości ważonej meridiany. Inną wspólną cechą jest to, że obie funkcje
kosztu mają zbliżoną postać, ale funkcja kosztu ważonej miriady wykorzystuje normę L2, a funkcja kosztu ważonej meridiany wykorzystuje normę L1.
Niech Lp norma zostanie zdefiniowana w następujący sposób:
1/ p

u

p

 s
p
   ul 
 l 1


(12)

gdzie u jest s wymiarowym wektorem, tj. u s . Stosując tak zdefiniowaną
Lp normę w wyrażeniach na funkcję kosztu ważonej miriady (6) lub ważonej
meridiany (8), otrzymuje się uproszczoną postać funkcji kosztu na bazie
rozkładu Cauchy’ego z Lp-normą:
N

( p )      log   wi xi   p 

(13)

i 1

gdzie  p jest Lp normą podniesioną do potęgi p, natomiast parametr ξ
jest odpowiednikiem parametru liniowego K dla p = 2 (miriada) i wówczas
  K oraz parametru δ dla p = 1 (meridiana) i wtedy    .
Dla stałej wartości parametru ξ oraz dla uporządkowanych danych  x[ i ] i 1
zdefiniowana w (13) funkcja kosztu ma następujące właściwości [3, 5, 16, 23]:
N

a) funkcja ( p )    jest ściśle malejąca dla   x[1] oraz ściśle rosnąca dla
  x[N ] ,
b) wszystkie lokalne ekstrema funkcji (p )    znajdują się w przedziale
x1 ,x N  ,
c) jeżeli 0  p  1 , to rozwiązaniem równania (p )     0 są wartości xi
(tzw. filtr selekcyjny),
d) jeżeli 1  p   , to (p )    ma przynajmniej  pN  1 lokalnych ekstremów i dlatego ma również skończony zbiór lokalnych minimów (dla
całkowitych, dodatnich wartości p).
Na rysunku 1 przedstawiono przebieg funkcji kosztu dla różnych wartości
p i ξ = 1.
15

Tomasz Pander, Tomasz Przybyła
W przypadku, gdyξ = 1 otrzymuje się przebieg funkcji kosztu taki sam jak
dla funkcji kosztu uogólnionego rozkładu Cauchy’ego [5].

Rys. 1. Przebieg funkcji kosztu (p )    dla różnych wartości p (0.5, 1, 1.5, 2, 2.5, 3,
3.5, 4) and ξ = 0.1 (próbki danych wejściowych x = {8.1, 9.0, 1.2, 9.1, 6.3, 0.9, 2.7})

Na podstawie wcześniejszych analiz można zdefiniować dla zbioru danych
N
N
 xi i 1 oraz przypisanych im odpowiednio wag wi i 1 , argument ˆ  , który
będzie minimalizował następujące wyrażenie funkcji kosztu z (13):
N

ˆ   arg min  log   wi xi   p 
  i 1

(14)

Zależność (14) definiuje wartość wyjściową zaproponowanego uproszczonego, ruchomego filtru, zbudowanego na bazie uogólnionego rozkładu Cauchy’ego z p-normą. Filtr ten ma kilka przypadków szczególnych. Dla p = 1
otrzymuje się ważoną meridianę, a dla p = 2 otrzymuje się ważoną miriadę.
Zachowanie zaproponowanego filtru w zależności od parametrów p oraz ξ
przedstawiono na rysunku 2. Wykresy te pokazują również zachowanie się
filtru dla różnych danych, w których nie ma próbek o charakterze impulsowym (y1, y2) lub takie próbki są (zbiory y3, y4).
16

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...

a)

p=1

80

b)

70

p=2

70

y1
y2
y3
y4

60

60
50

y1
y2
y3
y4

40

40

(p)


(p)


50

30

30
20
20

10

10

0
-1
10

c)

0

10

1

10

2

10



3

4

10

10

p=3

11

d)

y1
y2
y3
y4

10

9

0
-1
10

5

10

0

10

1

10

2

10



3

10

10

5

4

10

p=4

5.8

y1
y2
y3
y4

5.6

5.4

5.2

8

4

10

(p)


(p)


5

4.8

7

4.6
6
4.4
5
4.2

4
-1
10

0

10

1

10

2

10



3

10

4

10

5

10

4
-1
10

0

10

1

10

2

10



3

10

10

5

Rys. 2. Wartości uproszczonego filtru zbudowanego na bazie uogólnionego
rozkładu Cauchy’ego z p-normą dla różnych wartości p oraz różnych danych
y1 = [0 2 4 5 6 9] (linia ciągła), y2 = [0 2 4 5 6 9 50] (linia kropkowana), y3 = [0 2
4 5 6 9 50 500] (linia kreskowa), y4 = [-250 0 2 4 5 6 9 50 800] (linia kreskakropka) a) p = 1.0, b) p = 2.0, c) p = 3.0, d) p = 4.0

Dla p = 1 oraz p = 2 zaproponowany filtr zachowuje się odpowiednio, jak
filtry meridianowy i miriadowy. Ponadto dla danych y1 i y2 filtr zachowuje się bardzo podobnie, gdyż wraz ze wzrostem wartości ξ, wartość filtru
zmierza do wartości średniej. Jedynie dla zbioru danych y3 i y4 wartości
filtru rosną, jednak gdy wartość ξ > 1000, wówczas wartość funkcji (p )   
ustala się na stałym poziomie. Dla p > 2 i danych zawierających próbki wyraźnie o charakterze impulsowym (y2, y3, y4) filtr osiąga wartość minimalną dla porównywalnych wartości ξ (ok. 10 dla p = 3, oraz ok. 80 dla p = 4).
Właściwy dobór parametru ξ jest istotny dla działania filtru, jednak problem
ten jest dobrze znany i opisany w literaturze [9, 24]. Kolejnym ważnym
problemem jest dobór wag filtru wi. W niniejszej pracy dla uproszczenia
przyjęto, że wagi wi = 1/N dla i = 1,…, N i w dalszej części pracy ich dobór
nie będzie rozważany.
17

Tomasz Pander, Tomasz Przybyła
Metodą obliczenia wartości wyjściowej zaproponowanego filtru w ruchomym oknie sygnału, jest metoda stałego punktu zaprezentowana w [15, 16].
Istnieją również inne metody wyznaczania wartości wyjściowej opisane
w [4, 19, 20].

4. Ocena działania filtru
4.1. Procedura oceny działania filtru
Filtracja sygnału w dziedzinie czasu prowadzi do zmian w oryginalnym
widmie częstotliwościowym. Najczęściej zmiany te dotyczą niepożądanych
komponentów w sygnale wejściowym. Jednak filtracja nie powinna wprowadzać zniekształceń sygnału. Filtrami, które nie wprowadzają zniekształceń do sygnału, są filtry liniowe o prawdziwie liniowej charakterystyce
fazowej. Problemem są natomiast filtry nieliniowe, a do tej grupy zalicza
się filtr prezentowany w niniejszej pracy. Jednak filtry liniowe nie są zoptymalizowane do tłumienia zakłóceń o charakterze impulsowym i w takich
sytuacjach zawodzą. Dlatego przed zastosowaniem w systemie filtru nieliniowego konieczne jest, aby sprawdzić w jaki sposób filtr ten deformuje
sygnał wejściowy. W celu oceny działania prezentowanego filtru, zostaną
użyte wskaźniki średniego błędu średniokwadratowego (MSE) i średniego błędu bezwzględnego (MAE). Wskaźnik MSE pozwoli ocenić względną moc zniekształceń resztowych, wprowadzonych do sygnału po filtracji
przez filtr nieliniowy. Natomast wskaźnik MAE jest średnią miarą wielkości
błędu bez względu na jego znak. Filtrowane sygnały są wyrównane względem sygnałów wzorcowych i posiadają identyczne indeksy czasowe.
Z dostępnej bazy danych został wybrany pojedynczy cykl sygnału EKG
(rys. 3) o długości 1560 próbek, próbkowany z częstotliwością 2000 Hz.
Zakłócenia o charakterze impulsowym są modelowane z wykorzystaniem
rozkładów symetrycznych α-stabilnych dla   1.6,1.8,2.0 i γ = 1 [22].
Do sygnału o bardzo wysokim stosunku sygnał-szum dodawane są zakłócenia o znanej wartości współczynnika GSNR. Ponieważ dla zakłóceń o charakterze impulsowym nie istnieją momenty wyższych rzędów (np. nie można określić wartości wariancji) [Shao1993] w pracy wykorzystuje się tzw.
geometryczny SNR zdefiniowany w następujący sposób [Gonzalez2006]:
GSNR 

1 A
 
2Cg  S0 

(15)

gdzie: Cg  eC  1.78 jest eksponentą stałej Eulera, A jest amplitudą sygnału
modulowanego sygnału, S0 moc geometryczna dodawanych zakłóceń. Stała
e

18

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...
normalizacyjna 2Cg jest użyta w celu zapewnienia zgodności ze standardową definicją SNR [10].

Rys. 3. Przykładowy cykl sygnału EKG (sygnał deterministyczny – obniżony)
z zakłóceniami o charakterze impulsowym

Moc geometryczna jest zdefiniowana w następujący sposób:
1/ N

 N

S0    xi 
 i 1 

1 N

log xi 

 N i 1


lub S0  exp 

(16)

Wyniki są wyznaczane dla wartości GSNR  5, 0, 5, 10, 20 dB. Wybrany
cykl EKG jest 200 razy zakłócany wg powyższych kryteriów. Po każdej
filtracji badanymi filtrami, obliczane są wartości MSEi i MAEi a następnie
te wartości są uśrednione:
MSE sr 

1 200
 MSEi
200 i1

(17)

W analogiczny sposób obliczana jest wartość MAEsr. Filtrami referencyjnymi są filtry miriadowy (K = 1) i meridianowy (δ = 1). Długość okna filtrów
wynosi N = 25. Wszystkie obliczenia są wykonywane w środowisku Matlab.

19

Tomasz Pander, Tomasz Przybyła

4.2. Wyniki filtracji
W tabeli 1 są przedstawione średnie wartości wskaźnika MSE dla symulowanych zakłóceń impulsowych modelowanych symetrycznymi rozkładami
α-stabilnymi. Filtracja zaproponowanym filtrem jest dokonywana dla następujących wartości parametru p to: 1.5, 2.5, 3.0 i 4.0. Dla zakłóceń impulsowych o wartości α = 1.6 i 1.8 oraz wartości GSNR < 10 dB, najmniejsza
wartość MSE jest osiągana przez zaproponowany filtr dla wartości parametru p = 2.5. W sytuacji, gdy poziom zakłóceń GSNR > 5 dB, to lepsze rezultaty filtracji osiąga się dla większej wartości parametru p = 3. Dla zakłóceń
o charakterze gaussowskim najmniejsze wartości wskaźnika MSE osiągane
są również dla p = 2.5 oraz p = 3. Dla tych wartości parametru p, filtracja
zaproponowanym filtrem wprowadza mniejsze zniekształcenia do sygnału
niż filtry referencyjne.
Tabela 1. Wartości MSEsr dla proponowanego filtru i różnych wartości p oraz
filtrów referencyjnych dla zakłóceń modelowanych symetrycznymi
rozkładami α-stabilnymi

α
1.6
1.8
2.0

20

meridiana p = 1.5
0.2541
0.1809
0.1857
0.1598
0.1428
0.1435

1.6
1.8
2.0

0.1028
0.0719
0.0536

0.074
0.0618
0.0529

1.6
1.8
2.0

0.0536
0.0331
0.0253

0.0394
0.0289
0.0244

1.6
1.8
2.0

0.0259
0.0195
0.0164

0.0206
0.0173
0.0155

1.6
1.8
2.0

0.0143
0.0131
0.0126

0.0127
0.012
0.0116

GSNR = -5 dB
miriada
p = 2.5
0.1619
0.1584
0.152
0.1551
0.1469
0.1563
GSNR = 0 dB
0.0633
0.0583
0.0569
0.0542
0.0519
0.0515
GSNR = 5 dB
0.0347
0.0315
0.0261
0.0238
0.023
0.0214
GSNR = 10 dB
0.0177
0.0153
0.0153
0.0133
0.014
0.0122
GSNR = 20 dB
0.011
0.009
0.0104
0.0085
0.0101
0.0082

p=3
0.1684
0.1694
0.1758

p=4
0.213
0.226
0.2462

0.0577
0.0552
0.0534

0.0714
0.0711
0.0697

0.0307
0.0233
0.0212

0.0429
0.0351
0.0331

0.0144
0.0127
0.0117

0.0256
0.0245
0.0237

0.0081
0.0077
0.0074

0.0192
0.0192
0.0189

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...
W tabeli 2 są przedstawione średnie wartości wskaźnika MAE dla zakłóceń
impulsowych modelowanych symetrycznymi rozkładami α-stabilnymi. Dla
zakłóceń impulsowych o α = 1.6, najmniejsze wartości wskaźnika MAE są
osiągane przez proponowany filtr dla p = 2.5 i p = 3 w całym zakresie zmian
GSNR. Gdy poziom impulsowości maleje (α = 1.8), wartości wskaźnika
MAE osiągają podobne wartości jak dla α = 1.6. Dla zakłóceń gaussowskich, tylko gdy GSNR = 5 dB, do najmniejszych wartości MAE prowadzi filtracja filtrem meridianowym. Dla wyższych wartości GSNR, wartości
wskaźnika MAE osiągają porównywalne wartości.
Tabela 2. Wartości MAEsr dla proponowanego filtru i różnych wartości p oraz
filtrów referencyjnych dla zakłóceń modelowanych symetrycznymi
rozkładami α-stabilnymi

1.6
1.8
2

meridiana
0.3837
0.3355
0.2975

1.6
1.8
2

0.2374
0.2029
0.1761

1.6
1.8
2

0.1542
0.1267
0.1092

1.6
1.8
2

0.1001
0.0826
0.0723

1.6
1.8
2

0.0508
0.0447
0.0408

α

GSNR = -5 dB
p = 1.5
0.3266
0.3127
0.2983

miriada

p = 2.5

0.3054
0.3004
0.3041
0.3059
0.3017
0.3107
GSNR = 0 dB
0.2053
0.1892
0.181
0.19
0.1821
0.178
0.1753
0.1744
0.1741
GSNR = 5 dB
0.1354
0.124
0.1169
0.1202
0.1146
0.1101
0.1081
0.1062
0.1039
GSNR = 10 dB
0.0916
0.0848
0.0791
0.0791
0.0752
0.0711
0.0711
0.0689
0.0657
GSNR =2 0 dB
0.0484
0.0453
0.0409
0.0432
0.0407
0.0367
0.0395
0.0372
0.0334

p=3

p=4

0.3066
0.3175
0.3276

0.3391
0.3609
0.3816

0.1785
0.1781
0.1766

0.1852
0.1883
0.1902

0.1141
0.1088
0.1039

0.1205
0.1169
0.114

0.0769
0.07
0.0654

0.0855
0.0803
0.077

0.0397
0.036
0.0329

0.0511
0.0482
0.0453

Do oceny skuteczności działania zaproponowanego filtru są również wykorzystane prawdziwe zakłócenia mięśniowe, które dodawane są do sygnału
zgodnie z równaniem (23). Sygnał EMG został zarejestrowany na przedramieniu z częstotliwością próbkowania 2000 Hz. Wyniki są przedstawione
w tabeli 3 i 4.
21

Tomasz Pander, Tomasz Przybyła
Tabela 3. Wartości MSE dla rzeczywistych zakłóceń mięśniowych
i zaproponowanego filtru dla różnych wartości p

Wskaźnik MSE
GSNR
meridiana
[dB]
-5
0.6999
0
0.2225
5
0.0807
10
0.0369
20
0.0174

p = 1.5

miriada

p = 2.5

p=3

p=4

0.6992
0.2218
0.0797
0.0358
0.0163

0.7166
0.2230
0.0785
0.0342
0.0145

0.7766
0.2316
0.787
0.0327
0.0123

0.88
0.2526
0.0842
0.0339
0.0116

1.1127
0.3127
0.1119
0.0533
0.0263

Tabela 3. Wartości MAE dla rzeczywistych zakłóceń mięśniowych
i zaproponowanego filtru dla różnych wartości p

Wskaźnik MAE
GSNR
meridiana
[dB]
-5
0.6433
0
0.3623
5
0.2138
10
0.1326
20
0.0605

p = 1.5

miriada

p = 2.5

p=3

p=4

0.6433
0.3617
0.2129
0.1315
0.0593

0.6498
0.3622
0.2117
0.1298
0.0572

0.6712
0.3665
0.2115
0.1280
0.0545

0.7066
0.3764
0.2149
0.1289
0.0535

0.7868
0.4038
0.2285
0.1415
0.0661

Otrzymane wartości wskaźników MSE i MAE pokazują, że najmniejsze
zniekształcenia są wprowadzane do sygnału po filtracji, przez proponowany
w pracy filtr dla p = 1.5 w przypadku, gdy GSNR = 5 dB i 0 dB. Dla wyższych wartości, tj. GSNR = 5 dB i 10 dB, najlepsze wyniki filtracji są osiągane przez proponowany filtr dla p = 2.5. W przypadku, gdy GSNR = 20 dB
do najskuteczniejszej sytuacji prowadzi proponowany filtr dla p = 3. Przykład filtracji proponowanym filtrem dla p = 2.5 sygnału z symulatora sygnałów firmy Fluke ProSim 4 (rysunek 4).

22

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...

Rys. 4. Przykład filtracji sztucznego sygnału z symulatora Fluke ProSim4 za
pomocą prezentowanego filtru dla p = 2.5 (górny sygnał – sygnał z zakłóceniami,
dolny sygnał – sygnał po filtracji)

5. Wnioski
W pracy został zaprezentowany uproszczony filtr odporny, na bazie rozkładu Cauchy’ego z wykorzystaniem Lp-normy. Przedstawiony filtr należy do
grupy odpornych estymatorów największego prawdopodobieństwa parametru położenia, podobnie jak filtr miriadowy czy filtr meridianowy. Funkcja
kosztu zaproponowanego filtru powstała na bazie uogólnionego M-estymatora Cauchy’ego, ale jest odmianą uproszczoną. Działanie filtru opiera
się o p-tą potęgę Lp-normy. Natomiast wyraz wolny funkcji kosztu nie jest
podnoszony do potęgi p. Poziom impulsowości jest kontrolowany przez parametr α symetrycznych rozkładów α-stabilnych. Do wyznaczenia wartości
wyjściowej proponowanego filtru z okna o szerokości N, wykorzystywana
jest tzw. metoda „stałego-punktu”, która również jest używana w przypadku
filtrów referencyjnych. Złożoność obliczeniowa badanych filtrów jest porównywalna.
Działanie filtru jest ocenione w różnych warunkach zakłóceń impulsowych.
Skuteczność tłumienia zakłóceń impulsowych uproszczonego filtru na bazie uogólnionego rozkładu Cauchy’ego z Lp-normą, jest przetestowana na
23

Tomasz Pander, Tomasz Przybyła
przykładzie filtracji sygnału elektrokardiograficznego. Statystyczna analiza
otrzymanych wyników prowadzi do wniosku, że zaproponowany filtr pozwala na silne tłumienie zakłóceń o charakterze impulsowym, nie wprowadzając do filtrowanego sygnału znacząco dużych zniekształceń, co jest
szczególnie istotne w przypadku przetwarzania i analizy sygnałów biomedycznych. Lepszą efektywność działania filtru można uzyskać poprzez właściwy dobór potęgi p w Lp-normie, co będzie przedmiotem dalszych badań
nad przedstawionym w pracy filtrem.
Potencjalne zastosowanie zaproponowanego filtru nie obejmuje tylko aplikacji biomedycznych, ale również przetwarzanie sygnałów telekomunikacyjnych, np. przesył danych za pomocą sieci zasilających.

Literatura
1.

2.

3.
4.

5.

6.

7.

8.

9.

24

Acciani G., Amoruso V., Fornarelli G., Giaquinto A.: A supervised method for the
automatic detection of impulsive noise in naval powerline communications, IEEE
International Symposium on Power Line Communications and Its Applications
(ISPLC), 2011, s. 90-95.
Arce G.R., Kalluri S.: Robust Frequency-Selective Filtering Using Weighted
Myriad Filters admitting Real-Valued Weights, IEEE Trans. on Signal Proc.
vol. 49, 2001, s. 2721-2733.
Aysal T.C., Barner K.E.: Meridian Filtering for Robust Signal Processing, IEEE
Transactions on Signal Processing, vol. 55, 2007, s. 3939-3962.
Benny Ming Kai Goh, Heng Siong Lim: Sequential Algorithms for Sample
Myriad and Weighted Myriad Filter, IEEE Transactions On Signal Processing,
vol. 60, 2012, s. 6047-6052.
Carrillo R.E., Aysal T.C., Barner K.E.: A generalized Cauchy distribution framework for problems requiring robust behavior, EURASIP Journal on Advances in
Signal Processing, 2010, doi:10.1155/2010/312989.
Chan S., Zou Y.: A Recursive least M-estimate algorithm for robust adaptive
filtering in impulsive noise: fast algorithm and convergence performance analysis,
IEEE Trans. on Signal Process. vol. 52, 2004, s. 975-991.
Friedmann J., Messer H., Cardoso J.F.: Robust parameter estimation of a deterministic signal in impulsive noise, IEEE Trans. on Signal Processing, vol. 48,
2000, s. 935-942.
Georgiou P.G., Tsakalides P., Kyriakakis Ch.: Alpha-stable modeling of noise and
robust time-delay estimation in the presence of impulsive noise, IEEE Trans. on
Multimedia, vol. 1, 1999, s. 291-301.
Gonzalez J.G., Arce G.R.: Statistically-Efficient Filtering in Impulsive
Environments: Weighted Myriad Filters, EURASIP Journal on Applied Signal
Processing, vol. 1, 2002, s. 4-20.

Zastosowanie filtru odpornego na bazie Lp-normy w przetwarzaniu...
10. Gonzalez J.G., Paredes J.L., Arce G.R.: Zero-order statistics: a mathematical
framework for the processing and characterization of very impulsive signals,
IEEE Trans. on Signal Processing, vol. 54, 2006, s. 3839-3851.
11. Hathaway R.J., Bezdek J.C., Hu Y.: Genalized Fuzzy c-Means Clustering
Strategies Using Lp Norm Distances, IEEE Trans on Fuzzy Sys., vol. 8, 2000,
s. 576-582.
12. Hong X., Chen S.: M-estimator and D-optimality model construction using
orthogonal forward regression, IEEE Trans. on Systmes, Man and Cybernetics,
vol. 35, 2005, s. 1-7.
13. Huber P.: Robust statistics, Wiley, New York, 1981.
14. Li S.Z.: Robustizing robust M-estimation using deterministic anealing, Pattern
Recognition, vol. 29, 1996, s. 159-166.
15. Kalluri S., Arce G.R.: Fast algorithms for weighted myriad computation by fixed
point search, IEEE Trans. on Signal Processing, vol. 48, 2000, s. 159-171.
16. Kalluri S., Arce G.R.: Robust frequency-selective filtering using weighted myriad
filters admitting real-valued weights, IEEE Trans. on Signal Processing, vol. 49,
2001, s. 2721-2733.
17. Łęski J.: Robust Weighted Averaging, IEEE Trans. on Biomedical Engineering,
vol. 8, 2002, s. 796-804.
18. Nasri A., Nezampour A., Schober R.: Adaptive coherent Lp-norm combining,
IEEE ICC 2009 Proceedings.
19. Núñez R.C., Gonzalez J.G., Arce G.R.: Fast and Accurate Computation of the
Myriad Filter via Branch-and-Bound Search, IEEE Transactions on Signal Processing, vol. 56, 2008, s. 3340-3346.
20. Pander T.: New polynomial approach to myriad filter computation, Signal Processing, vol. 90, 2010, s. 1991-2001.
21. Pander T.: Robust filtering of ECG signal with the myriad filter, Polish Journal of
Medical Physics and Engineering, vol. 9, 2003, s. 17-30.
22. Pander T.: Application of weighted myriad filters to suppress impulsive noise in
biomedical signals, TASK Quarterly, vol. 8, 2004, s. 199-216.
23. Pander T., Przybyła T.: Impulsive noise cancelation with simplified Cauchy-based
p-norm filter, Signal Processing, vol. 92, 2012, s. 2187-2198.
24. Roenko A.R., Lukin V.V., Djurovic I.: Two approaches to adaptation of sample
myriad to characteristics of S-S data, Signal Processing, vol. 90, 2010, s. 2113-2123.
25. Shao M., Nikias Ch.L.: Signal processing with fractional lower order moments:
stable processes and their applications, Proc. of The IEEE, vol. 81, 1993,
s. 986-1009.
26. Shao H., Beaulieu N.C.: Analysis of a novel p-order metric UWB Receiver Structure with improved performance in multiple access interference, In Proceedings of
the IEEE Global Telecommun. Conf., 2007, s. 4112-4117.

25

Tomasz Pander, Tomasz Przybyła
27. Shevlyakow G., Kim K.: Robust minimax detection of a weak signal in noise
with a bounded variance and density value at the center of symmetry, IEEE Trans.
Inform. Theory, vol. 52, 2006, s. 1206-1211.
28. Sivaneasan B., Gunawan E., So P.L.: Modeling and performance analysis of
automatic meter-reading systems using PLC under impulsive noise interference,
IEEE Trans. Power Delivery, vol. 25, 2010, s. 1465-1475.
29. Surender V.P., Ganguli R.: Adaptive myriad filter for improved gas turbine
condition monitoring using transient data, Journal of Eng. for Gas Turbines and
Power, vol. 127, 2005, s. 329-339.
30. Tsihrintzis G.A., Shao M., Nikias Ch.L.: Recent results in applications and
processing of α-stable-distributed time series, J. Franklin Inst. 333B, 1996, s. 467-497.
31. Tsihrintzis G.A., Nikias Ch.L., Fast estimation of the parameters of alpha-stable
impulsive interference, IEEE Transactions on Signal Processing, vol. 44, 1996,
s. 1492-1503.
32. Vargas R.A., Burrus C.S.: Adaptive Iterative Reweighted Least Squares Design of
Lp FIR Filters, Proceedings of the ICASSP, III, 1999, s. 2396.

26

Rozdział 2
Hybrydowe systemy falkowo-neuronowe
jako struktury klasyfikatorów sygnałów
biomedycznych
Paweł Kostka
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
pkostka@wsb.edu.pl

Streszczenie
Przedstawiono strukturę oraz przykład wykorzystania hybrydowych systemów bazujących na metodach inteligencji obliczeniowej-sieciach neuronowych, połączonych ze wstępną
ektrakcją cech w warstwie realizującej transformatę falkową.
Efektywność działania systemów falkowo-neuronowych przetestowano w strukturze klasyfikatora sygnałów zmienności rytmu
serca HRV.

1. Idea użycia systemów hybrydowych w analizie
sygnałów biomedycznych
„ ... Przyszłość należy do systemów złożonych, stanowiących połączenie metod należących do grupy określanej jako Computational Intelligence – teorii
sieci neuronowych, zbiorów rozmytych, algorytmów genetycznych oraz innych metod przetwarzania sygnałów, w celu opracowania technik zdolnych
do rozwiązywania coraz bardziej skomplikowanych problemów otaczającego nas świata ... ” – te słowa wypowiedziane przez prof. Lofti Zadeha,
twórcę teorii zbiorów rozmytych [1Zad65], wyrażają ogólną tendencję, jaka
pojawiła się w rozwoju dziedzin związanych z szeroko rozumianą problematyką przetwarzania informacji w dekadzie lat 90-tych. Ogromny wzrost
27

Paweł Kostka
ilości dostępnych danych, szybkości ich transmisji, zasobów do ich przechowywania oraz mocy obliczeniowych, gwarantujących szybkie porządkowanie i przetwarzanie zgromadzonej wiedzy, to czynniki, które umożliwiają coraz szersze i pełniejsze rozumienie otaczającej nas rzeczywistości.
Techniki należące do grupy metod sztucznej inteligencji, reprezentują powiązanie nauk ścisłych i technicznych z biologiczno-medycznymi, które ma
charakter oddziaływania dwukierunkowego [2], [3]. Każda z gałęzi tej grupy metod posiada określone, unikatowe cechy, które mogą być użyteczne do
rozwiązania określonego problemu. Od początku lat 90-tych obserwować
można szybki rozwój systemów hybrydowych, łączących w sobie możliwości różnych dziedzin [4], [5]. Wynikiem tego procesu jest powstanie ewolucyjnych sieci neuronowych [6], układów rozmyto-neuronowych [7],[8]
czy modularnych sieci neuronowych [9][10]. Tematyka pracy, opisującej
struktury i możliwości hybrydowych systemów falkowo (ang. wavelet)
-neuronowych (SFN), nawiązuje do przedstawionej powyżej idei łączenia
różnych technik przetwarzania sygnałów, w celu powstania systemów skupiających w sobie zalety metod wchodzących w ich skład. Sieci neuronowe
posiadają zdolność modelowania badanych zjawisk i zależności, poprzez
tworzenie jednokierunkowych lub rekurencyjnych struktur, zawierających
liniowe lub nieliniowe elementy przetwarzania danych z, możliwością połączenia zarówno lokalnymi jak globalnymi sprzężeniami [11]. Często jednak
problemem systemów wykorzystujących struktury SN podczas przetwarzania sygnałów dyskretnych, jest zbyt duża liczba cech sygnału wejściowego,
reprezentowanych przez jego próbki. Wynikiem tego jest znaczny stopień
złożoności systemu, mający wpływ na jego proces uczenia i działania.
Systemy falkowo-neuronowe (SFN) stanowią stosunkowo młode narzędzie
przetwarzania sygnałów, przełomu XX i XXI wieku, w którym sygnał przed
podaniem na wejście sieci neuronowej zostaje poddany transformacie falkowej (TF) (ang. wavelet transform) mającej na celu wydobycie i poddanie
dalszej obróbce tylko wyselekcjonowany wektor cech, najbardziej charakterystyczny i znamiennych dla analizowanego problemu, często o zredukowanym wymiarze [12],[13]. Etap realizacji TF tworzy dodatkową warstwę
SN, której parametry podlegają modyfikacji podczas procesu uczenia sieci, podobnie jak parametry pozostałych warstw systemu, w celu uzyskania
optymalnego rozwiązania. Dekompozycja falkowa realizowana jest przez
rozwinięcie analizowanego sygnału w bazę funkcji, utworzonych w wyniku
przeskalowania i przesunięcia podstawowej funkcji falkowej (PFF) wybranej dla badanego problemu. TF określana jest w literaturze jako narzędzie
pełniące funkcję „matematycznego mikroskopu”, który umożliwia „patrzenie” na przetwarzany sygnał w różnej skali poprzez zmianę „ogniskowej”,
czyli wartości parametru skali a. Właściwości TF wynikające z wieloskalo28

Hybrydowe system falkowo-neuronowe jako struktury...
wej dekompozycji sygnału pozwalają na jej stosowanie do analizy sygnałów
zarówno stacjonarnych, jak i niestacjonarnych [14]. Sieci neuronowe, dzięki posiadaniu struktur o różnych architekturach oraz funkcjach aktywacji są
narzędziem umożliwiającym analizę liniowych lub nieliniowych zależności
między przetwarzanymi sygnałami. Przedstawione powyżej właściwości
tych technik, połączone w hybrydowych SFN, stały się czynnikiem motywującym opracowanie struktur i algorytmów ich uczenia odpowiednich do
zagadnień opisywanych w pracy, ze względu na charakter reprezentujących
je sygnałów biologicznych. Wykorzystanie SFN w charakterystycznych
dla SN polach klasyfikacji [15] i identyfikacji [16], [17], ma również na
celu ocenę wpływu wprowadzenia warstwy realizującej TF, na szybkość
i zbieżność procesu uczenia tych układów oraz na jakość ich działania w porównaniu do struktur SN, jak również w odniesieniu do innych metod stosowanych w realizacji wyżej wymienionych zagadnień. Specyficzne i nowatorskie pole zastosowań SFN dotyczy analizy zdarzeń biomedycznych
– klasyfikacji pacjentów z wielonaczyniową chorobą wieńcową [18].

1.1. Potrzeby klasyfikacji sygnałów biomedycznych
Klasyfikacja zdarzeń biomedycznych, jest jednym z końcowych etapów
procesu rozpoznawania i przyporządkowywania obserwowanego zdarzenia
do danej klasy. Procedura ta jest uniwersalna dla różnych form realizacji
tego procesu, reprezentowanych w postaci sygnałów jedno- lub wielowymiarowych. Duda i Hart zdefiniowali pojęcie rozpoznawania i klasyfikacji obrazów, jako dziedzinę automatycznego rozpoznawania znaczących
regularności w złożonym i często zakłóconym środowisku [19], podczas
gdy Bezdek określił je jako poszukiwanie znamiennej struktury istniejącej
w analizowanych danych [20].
Precyzując ogólne pojęcie użytego w tytule podrozdziału sygnału biomedycznego do zagadnień klasyfikacji, określić go można, jako stan lub zmiana stanu badanego systemu, która możliwa jest do detekcji w wyniku próbkowania przenoszącego je medium, przy użyciu czujników biomedycznych.
SFN, których struktury i algorytmy uczenia proponowane są w pracy, realizują etapy ekstrakcji i selekcji cech w warstwie transformaty waveletowej
(falkowej) oraz klasyfikacji w podstrukturze sieci neuronowej. Ilustracją
działania tych struktur, jest problem klasyfikacji pacjentów z chorobą naczyń wieńcowych serca (CNWS) różnego stopnia, na podstawie sygnałów
zmienności rytmu serca HRV (ang. heart rate variability).
Jedną z głównych przyczyn patologicznego zmniejszania się przepływu
wieńcowego jest arterioskleroza. Schorzenie to wywołane jest zarówno
procesami biochemicznymi, związanymi głównie z zaburzeniami metabo29

Paweł Kostka
lizmu cholesterolu i lipidów, jak również czynnikami hemodynamicznymi,
odgrywającymi znaczną rolę w tworzeniu i lokalizacji płytek miażdżycowych [21].
CNWS jest wiodącą przyczyną śmierci pacjentów cierpiących na choroby
układu krążenia we współczesnym świecie. Raport Instytutu Serca Europejskiej Akademii Nauk [22], powstały na bazie danych ośrodków rządowych oraz naukowych 35 państw Europy w roku 2000, oddaje skalę tego
problemu. W kontekście opisywanego w pracy systemu klasyfikacji stopnia CNWS istotne dane, związane z tym schorzeniem umieszczone w ww.
raporcie wskazują na to, że operacje chirurgiczne naczyń wieńcowych
stanowią 65.25% wszystkich procedur kardiochirurgicznych, a ich liczba
zwiększyła się w okresie 7 lat ponad dwukrotnie. Tak duża liczba przeprowadzanych zabiegów pomostowania naczyń wieńcowych, stanowiących
ostateczną metodę leczenia tego schorzenia, wskazuje na istnienie potrzeb
detekcji i klasyfikacji CNWS na wczesnym etapie ich rozwoju, co mogłoby
zmniejszyć liczbę koniecznych do przeprowadzenia operacji, a tym samym
ograniczyć związane z ich przeprowadzaniem ryzyko.
Potrzeby klasyfikacji pacjentów z CNWS można podzielić na dwie grupy:
–

Potrzeby diagnostyczne – opracowana metoda może pełnić funkcję
nieinwazyjnego, szeroko dostępnego, wstępnego badania przesiewowego, stosowanego równolegle z badaniami wysiłkowymi EKG na etapie
kwalifikacji pacjentów do bardziej zaawansowanych, czasochłonnych
i droższych metod diagnozowania CNWS – badań koronarograficznych.

–

Potrzeby terapeutyczne – metoda po implementacji sprzętowej, stanowiąc dodatkowy moduł kardiomonitora, może zostać wykorzystana
do oceny stanu naczyń wieńcowych pacjentów, poddanych wcześniej
zabiegowi udrażniania lub pomostowania tętnic wieńcowych w czasie
okresu rekonwalescencji.

Metoda klasyfikacji CNWS oparta na przedstawianych w pracy SFN, może
stać się uzupełnieniem innej nieinwazyjnej opisanej w literaturze metody
oceny stopnia zaawansowania tego schorzenia. Bazuje ona na wykrywaniu
metodami cyfrowego przetwarzania sygnału akustycznego, miejsc zmian
miażdżycowych naczyń, dzięki towarzyszącym im lokalnym turbulencjom przepływu wieńcowego [23]. Idea wykorzystania sygnału HRV do
oszacowania stopnia CNWS wynika z opisanych w literaturze właściwości tego sygnału [24], [25], pozwalającego w wyniku analizy w dziedzinie
czasu i częstotliwości odzwierciedlić stan mechanizmów sterujących pracą
serca, pozostającego pod pierwotnym wpływem autonomicznego układu
30

Hybrydowe system falkowo-neuronowe jako struktury...
nerwowego [26], [27], [28]. Transformacja sygnału HRV w widmowej gęstość mocy (PSD) (ang. power spectral density) jest często używana jako
nieinwazyjny test, pozwalający rozróżnić wpływ układu sympatycznego
i parasympatycznego, na proces regulacji częstością pracy serca [29]. Ze
względu na bezpośrednie powiązanie funkcjonowania układu oddechowego
i układu krążenia, czynnik respiracyjny, określany jako zatokowa arytmia
oddechowa (RSA) (ang. Respiratory sinus arrhythmia) stanowi podstawowy składnik wpływający na HRV [30]. Pozostałe znaczące komponenty,
mające wpływ na częstość pracy serca to ciśnienie krwi, termoregulacja,
system regulacji naprężenia ścian i średnicy naczyń krwionośnych poprzez
stężenie enzymu reniny oraz objętość wyrzutowa serca [31], [25]. Pętla
sprzężenia baroreceptorowego, przekazuje informacje dotyczącą rytmu
pracy serca oraz ciśnienia krwi, do ośrodka sterującego, umiejscowionego w mózgu [32]. Analiza sygnału HRV przy zastosowaniu PSD wymaga
ignorowanego w wielu publikacjach założenia statystycznej stacjonarności
sygnału i wykazuje niezdolność do separacji nakładających się na siebie
komponentów wynikających z różnorodnych mechanizmów fizjologicznych [26]. Wyznaczanie parametrów mających stanowić wskaźniki fizjologiczne w oparciu o sztywno określone wartości przedziałów częstotliwości
z uwagi na brak wzorca i niepowtarzalność systemów biomedycznych może
prowadzić do uzyskania nieścisłych wyników [33]. Alternatywą dla sposobu analizy sygnału HRV w oparciu o widmo Fourierowskie, może być
proponowane w rozprawie zastosowanie wstępnego przetworzenia sygnału
HRV pacjentów z CNWS, przy użyciu transformaty falkowej. Wynikiem
działania proponowanych systemów, jest przeniesienie analizy HRV w adaptacyjnie określane podpasma częstotliwościowe [18], [34]. Potwierdzone
w wielu publikacjach [35], [36] właściwości wieloskalowej dekompozycji
sygnału przy pomocy TW, stanowią przesłankę stosowania tego narzędzia
do analizy sygnałów niestacjonarnych.

2. Struktury systemów falkowo-neuronowych
Systemy falkowo-neuronowe SFN, stanowią złożenie dwóch narzędzi przetwarzania sygnałów oraz metod tzw. Inteligencji Obliczeniowej: transformaty falkowej (waveletowej) oraz sztucznych sieci neuronowych.

2.1. Sieci falkowe (SF)
W 1992 r. autorzy pracy „Wavelet networks” Q. Zhang i A. Benveniste [37],
zaproponowali po raz pierwszy połączenie zdolności aproksymacji nieznanej ciągłej funkcji przez SN z czasowo-częstotliwościową reprezentacją sy31

Paweł Kostka
gnału, poddanego przekształceniu waveletowemu, przedstawiając strukturę
SW (rys. 1), której wyjście y(x) wyrażone jest zależnością (1)
N
 x  bi
y( x )   wi 
i 1
 ai


  w0


(1)

Wyjście SF y(n) jest sumą ważoną wartości podstawowych funkcji falkowych ψ(x), przeskalowanych i przesuniętych odpowiednio o parametry ai
i bi, przesuniętą o wartość stałej b0 , wprowadzonej dla sygnałów o niezerowej wartości średniej. Zastąpienie funkcji aktywacji neuronów WWP, bazą
przeskalowanych i przesuniętych funkcji waveletowych, stanowi istotę SW
zaproponowanych w ww. pracy. Zarówno struktura SF (rys. 1), jak i opisująca ją formuła (1), są analogiczne do budowy i zależności sieci neuronowych typu wielowarstwowy perceptron, a pojęcie neuronu zostało zastąpione określeniem wavelon. Właściwości różnych typów funkcji falkowych,
które mogą być stosowane w strukturach SF i cechy baz funkcji tworzonych
przy dekompozycji falkowej, przyczyniły się do szybkiego rozwoju tych
struktur, algorytmów ich uczenia oraz pól zastosowań. Własności nieredundancyjnych baz ortogonalnych tworzonych przez niektóre funkcje falkowe,
§ x  b1 ·
¸¸
© a1 ¹

\ ¨¨
x

§ x  b2 ·
¸¸
© a2 ¹

\ ¨¨

w0

w1
w2

6

y(x)

...
§ x  bM
\ ¨¨
© aM

·
¸¸
¹

wM

Rys. 1. Struktura sieci waveletowej zaproponowanej przez Q. Zhanga
i A. Benveniste w pracy „Wavelet networks” [ZhBen92]

umożliwiły tworzenie struktur, o możliwie najmniejszym stopniu skomplikowania, przeznaczonych do rozwiązywania określonych problemów [38].
Stąd szerokie zastosowanie SW, zarówno jako narzędzi aproksymacji nieznanych funkcji opisujących działanie modelowanych obiektów [39], [40],
[41], jak również systemów klasyfikacji, np. sygnałów EEG [42], sygnałów
32

Hybrydowe system falkowo-neuronowe jako struktury...
mowy [43], w dziedzinie radiolokacji [44] lub detekcji i klasyfikacji niestacjonarnych procesów związanych z przesyłem energii elektrycznej na duże
odległości [45].

2.2. Sieci falkowe (SF)
Idea przedstawionych w poprzednim podrozdziale sieci waveletowych, polegała na zastąpieniu w strukturze SN zbioru funkcji aktywacji neuronów
danej warstwy, rodziną funkcji waveletowych, powstałych przez przeskalowanie i przesunięcie funkcji podstawowej, a wyjście układu stanowiła analogicznie jak w strukturze WWP SN, suma ważona wartości tych funkcji dla
określonego pobudzenia. SFN stanowiące przedmiot przedstawianej pracy,
posiadają strukturę (rys. 2), w której przekształcenie waveletowe realizowane

x(n)

Wstępne
przetworzenie
i normalizacja
sygnału x(n)

Zastosowanie TW
Dekompozycja x(n)
w podpasma częst.

Struktura
sieci
neuronowej

Rys. 2. Ogólna struktura systemów falkowo-neuronowych

jest po wstępnym przetworzeniu i normalizacji sygnału wejściowego x(n),
w dodatkowej wstępnej warstwie SN, w celu dokonania czasowo-częstotliwościowej analizy przetwarzanego sygnału, wydobycia i poddania dalszej
obróbce w SN tylko tej jego części, która jest najbardziej charakterystyczna dla badanego problemu. Wyjściem warstwy realizującej TW, jest nowy
wektor cech, opisujący analizowany sygnał, ukształtowany w procesie uczenia w ten sposób, aby zapewnić optymalny wektor wejściowy dla struktury
SN. Połączenie omówionych w poprzednich rozdziałach cech zarówno TW
jak i SN, czyni z SFN uniwersalne narzędzie do rozwiązywania złożonych
problemów. Własności TW wynikające z wieloskalowej dekompozycji sygnału, pozwalają na jej stosowanie do analizy sygnałów zarówno stacjonarnych, jak i niestacjonarnych, natomiast SN, dzięki możliwości posiadania
różnych struktur oraz funkcji aktywacji, znane są jako narzędzie umożliwiające analizę liniowych lub nieliniowych zależności między przetwarzanymi sygnałami. Istotnym zagadnieniem, związanym z tworzeniem SFN,
wynikającym zarówno z danych literaturowych, jak i doświadczeń autora
pracy jest fakt, iż struktura systemu warunkowana sposobem wykorzystania TW, zależy i może być różna dla konkretnego zastosowania. Dlatego
też, rys. 2 przedstawia ogólny schemat przetwarzania sygnału wejściowego
33

Paweł Kostka
x(n) realizowanego przez SFN, natomiast następne rozdziały poświęcone
ich zastosowaniom w klasyfikacji sygnałów HRV pacjentów z chorobą naczyniową oraz identyfikacji obiektów protez zastawek serca, zawierać będą
propozycje i porównanie działania różnych struktur, dedykowanych tym polom aplikacyjnym. Istotną cechą stanowiącą o odmienności proponowanych
systemów, jak zostanie to przedstawione w dalszej części pracy, jest sposób
zastosowania TW w celu ekstrakcji i selekcji cech analizowanego sygnału,
przed wprowadzeniem go na wejście SN. Jednak zarówno w systemach klasyfikacji sygnałów, jak i identyfikacji obiektów, w bloku SN SFN ze względu na znane własności odwzorowywania nieznanych funkcji, zastosowano
i sprawdzono działanie następujących struktur jedno-, dwu- i trzy-warstwowych:
–

SN jednowarstwowa o liniowej FA neuronów,

–

SN jednowarstwowa o tangsoidalnej FA neronów,

–

SN dwuwarstwowa o tangsoidalnej i liniowej FA neuronów kolejnych
warstw,

–

SN dwuwarstwowa o radialnych funkcjach bazowych i liniowej funkcji
aktywacji neuronów kolejnych warstw,

–

SN trzywarstwowa o 1. i 2. tangsoidalnej oraz 3. liniowej funkcji aktywacji.

Jak można się domyśleć, nie jest możliwe skonstruowanie fali podstawowej spełniającej jednocześnie wszystkie użyteczne, wymienione powyżej
własności. Dlatego podczas jej wyboru dla konkretnej aplikacji musi zostać
dokonana ich selekcja.
Do zastosowania w SFN wybrano trzy typy PFW:
– Morlet (rys. 3.11a),

34

–

Daubechies 4: Db4 (rys. 3.11b),

–

Biortogonal-Spline: Bior 2.4 (rys. 3.11c), o różnych własnościach zebranych w tab. 1.

Hybrydowe system falkowo-neuronowe jako struktury...

a)

b)

c)
Rys. 3. Wykresy falki typu – Morlet (a), Daubechies: Db4 (b), Biortogonal-Spline:
Bior 2.4
Tabela 1. Podstawowe cechy wybranych falek podstawowych

Cechy PFW
Morlet
1. Ortogonalność
2. Biortogonalność
3. Ograniczony czas trwania
4. Symetria
+
5. Arbitralna regularność
6. Określona liczba momentów zanikających
7. Zapis analityczny
+
8. Filtr FIR
9. Istnienie PFS φ(t)
10. Zapewnienie dokładnej rekonstrukcji
11. Możliwość przeprowadzenia CTW
+
12. Możliwość przeprowadzenia DTW
+
13. Możliwość przeprowadzenia STW
-

Db 4
+
+
+
+
+
+
+
+
+
+
+

Bior 2.4
+
+
+
+
+
+
+
+
+
+
+

Tab. 1 prócz przedstawionych na wstępie rozdziału cech PFW i PFS, zawiera dodatkowe własności związane z praktyczną realizacją TW, które nie
wymagają opisu.

35

Paweł Kostka

2.2.1. Algorytm uczenia Systemów Falkowo-Neuronowych (SFN)
W strukturach SFN, prócz parametrów SN, takich jak wagi połączeń oraz
wartości stałych przesunięć, optymalizacji podlegają również parametry dodatkowej warstwy systemu realizującej transformatę falkową. . W odróżnieniu od sieci falkowych omówionych na wstępie tego rozdziału, w których
uczeniu podlegają zarówno parametr skali a jak i przesunięcia b, a wyjście
warstwy stanowi jedna wartość współczynnika dekompozycji TW, w SFN
przedstawianych w pracy, uczeniu podlega tylko wartość parametru skali,
gdyż celem warstwy falkowej jest wydzielenie optymalnej składowej dekompozycji analizowanego sygnału i poddanie jej dalszemu przetwarzaniu
w strukturze SN. Istotnym czynnikiem warunkującym zastosowaną metodę uczenia, jest typ falki podstawowej wybranej do przeprowadzenia TF.
W tym rozdziale pracy przedstawiono dwa algorytmy uczenia SFN, różniące się sposobem doboru wartości parametrów warstwy realizującej TW dla
różnych PFW. Pierwszy z proponowanych algorytmów, przeznaczony jest
dla struktur wykorzystujących w warstwie wstępnej falkę podstawową typu
Morlet, jedyną spośród trzech wybranych funkcji podstawowych, opisaną
zależnością analityczną. Druga z metod uczenia, zaproponowana jest dla
dwóch pozostałych przedstawionych w poprzednim podrozdziale PFW –
Db4 oraz Bior 2.4.

2.2.2. Algorytm uczenia systemów falkowo-neuronowych
Algorytmy bazujące na metodzie największego spadku gradientu błędu
wyjść struktury jak, np. algorytm wstecznej propagacji błędu, wymagają istnienia zapisu analitycznego FA neuronów, celem wyznaczenia koniecznych
pochodnych cząstkowych funkcji kosztu. Rys. 4 przedstawia uzupełniony
o etapy związane z optymalizacją i inicjalizacją parametru skali a warstwy
TW, schemat blokowy zmodyfikowanego, stosowanego dla struktur SN
typu WWP algorytmu wstecznej propagacji błędu, którego podstawowe założenia przedstawiono w poprzednim podrozdziale.
Wzór (2) wyraża zależność opisującą rzeczywistą PFW typu Morlet, która
została zastosowana w SFN przedstawianych w pracy, uczonych zgodnie ze
zmodyfikowanym algorytmem wstecznej propagacji błędu.
ψ Morlet (t)  cos( 0 )exp( 0.5 2 )

gdzie: τ 

36

t b
, 0  2 f 0
a

(2)

Hybrydowe system falkowo-neuronowe jako struktury...
Uzupełnieniem zależności opisujących sposób modyfikacji parametrów SN
– wag połączeń (3.33) oraz stałych przesunięć jest wyznaczenie przyrostu
parametru skali zgodnie z regułą delta. Wyraża się on podobnie jak dla pozostałych parametrów podlegających optymalizacji, jako zanegowana wartość gradientu funkcji błędu (4) wyjść warstwy TF SFN (5).
a  

E
a

Ns

E     i2 

(4)
(5)

i 1

Ponieważ warstwa ta jest realizacją CTW wartości jej wyjść y(TW) określone
są zależnością (3.57)

y( TW )  s( t ), Morlet ( t ) 


1
t b
*  s( t )*  (
)dt
a
a 

(6)

Z formuły (2) i (6) po zamianie całkowania na sumowanie, ze względu na
dyskretny charakter analizowanych sygnałów, wartość przyrostu Δa można określić równaniem (7), a zależność (8) wyraża nową, zmodyfikowaną
wartość parametru skali a warstwy falkowej w kolejnym n+1 cyklu uczenia
SFN, bazującą na wartości poprzedniej oraz przyroście Δa, wyznaczonym
jako wartość pochodnej funkcji błędu wyjść SFN względem tego parametru
Ns
 Morlet (  i )
E
 2  ( i )s( i ) i
a
b
i 1

(7)

a( n  1 )  a( n )   a a

(8)

Wymagana wartość pochodnej funkcji opisującej falkę podstawową typu
Morlet (2) obliczana względem czasu t wyrażona jest zależnością (9)
ψ Morlet (t) 1
 0 sin( 0 )exp( 0.5 2 )   (  )
b
a

(9)

37

Paweł Kostka
Algorytm wstecznej propagacji błędu należy do grupy metod optymalizacji lokalnej. Stwarza to możliwość zakończenia procesu uczenia w jednym z minimów lokalnych powierzchni funkcji błędu, zamiast osiągnięcia
minimum globalnego. Prócz znanych z literatury modyfikacji algorytmu,
np. wprowadzenie dodatkowego członu momentum oraz adaptacyjny dobór współczynnika uczenia, SFN prezentowane w pracy podlegały uczeniu
wielokrotnemu, dla różnego losowo dobieranego zestawu wartości początkowych wag połączeń, w celu osiągnięcia najlepszej jakości ich działania.
Wartości inicjalizujące parametry warstwy TF systemu, określone zostały
na podstawie analizy specyficznych właściwości badanych procesów.
W podsumowaniu podrozdziału poświęconego omówieniu części teoretycznej leżącej u podstaw struktur stanowiących ideę przedstawianej pracy,
podstawowe cechy transformaty falkowej oraz sieci neuronowych skupione w hybrydowych systemach falkowo-neuronowych, można przedstawić
schematycznie w poniższym zestawieniu.
TRANSFORMATA FALKOWA
– wieloskalowa dekompozycja
sygnału

SIECI NEURONOWE
– równoległe przetwarzanie danych

– czasowo-częstotliwościowa
reprezentacja sygnału

– różne typy funkcji aktywacji

– analiza sygnałów stacjonarnych
i niestacjonarnych

– możliwość analizy zarówno
liniowych jak i nieliniowych
zależności między sygnałami
wyjściowymi i wejściowymi

SYSTEMY WAVELETOWO-NEURONOWE
Przedstawione informacje o kolejnych etapach tworzenia struktur i doboru
ich parametrów, z uwzględnieniem najważniejszych związanych z tym zagadnień można przedstawić w następującym zestawieniu:
I

Przeprowadzić wstępną analizę zadanego problemu z określeniem, jeśli
to możliwe, pasma częstotliwości opisujących go sygnałów.

II Dobrać odpowiedni typ PFW ψ(t) oraz dla przeprowadzenia algorytmu
WDS również odpowiadającej jej PFS φ(t) zastosowanej w warstwie

38

Hybrydowe system falkowo-neuronowe jako struktury...
TW SFN, na podstawie cech reprezentowanych przez różne funkcje
podstawowe.
III Określić sposób wykorzystania transformaty falkowej w SFN determinujący strukturę warstwy realizującej TF systemu.
IV Dokonać wyboru typów struktur SN zastosowanych w SFN, zgodnie
z ich własnościami znanymi z teorii SN, z uwzględnieniem natury rozwiązywanego problemu.
V Na podstawie przeprowadzonych badań obiektu bądź zjawiska utworzyć bazę uczącą i weryfikującą system, złożoną z par odpowiadających
sobie sygnałów wejściowych i wyjściowych systemu, stanowiących
materiał dla procesu doboru parametrów SFN.
VI Przed rozpoczęciem procesu uczenia SFN, przeprowadzić inicjalizację
parametrów podlegających optymalizacji oraz określić liczbę poziomów dekompozycji falkowej, niezbędną dla algorytmów uczenia, opartych na siatce dyskretnych wartości parametru skali a , przeznaczonych
dla PFW, nieopisanych zależnością analityczną, bazując na określonym
paśmie częstotliwości sygnałów lub korzystając z kryterium entropii dla
kolejnych składowych rozkładu falkowego.
VII Opracować algorytm uczenia utworzonego systemu z uwzględnieniem
wybranej falki podstawowej oraz struktury sieci neuronowej, w celu
optymalizacji wartości parametrów wag połączeń, stałych przesunięć
sieci oraz parametru skali falki, podczas procesu uczenia nadzorowanego, korzystając z bazy sygnałów uczących.
VIII Dokonać analizy wpływu stopnia złożoności SFN na jakość jej działania, co może prowadzić do redukcji liczby neuronów struktury.
IX Za pomocą zbioru weryfikującego sygnałów wejść i wyjść badanego
obiektu, przeprowadzić sprawdzenie i dokonać oceny działania SFN.
Struktury SFN i techniki doboru wartości ich parametrów, jak zostało zasygnalizowane to w kilku miejscach tego rozdziału, nie posiadają charakteru
uniwersalnego, lecz powinny być określone zgodnie z charakterem rozwiązywanego problemu i reprezentujących go sygnałów. Dotyczy to głównie
sposobu zastosowania analizy falkowej we wstępnej warstwie SFN, celem
wyboru najbardziej optymalnej dla dalszej analizy w sieci neuronowej reprezentacji sygnału, dopasowanej do rozwiązania zadanego zagadnienia.
Rozwiązanie takie zostanie przedstawione w kolejnym rozdziale, poświęconym zastosowaniu SFN do klasyfikacji sygnałów zmienności rytmu serca
(HRV) pacjentów z chorobą naczyniową serca.
39

Paweł Kostka

3. Klasyfikator sygnałów biomedycznych.
Przegląd wybranych metod klasyfikacji ZB
Proces klasyfikacji zdarzeń biomedycznych może być przedstawiony jako
ciąg etapów wstępnego przetwarzania dopasowującego, ekstrakcji i selekcji
cech oraz klasyfikacji optymalnej reprezentacji sygnału. W zależności od
sposobu ich realizacji, typu analizowanego zdarzenia oraz jego formalnego
opisu, metody klasyfikacji podzielić można na trzy podstawowe grupy [46]:
–

metody statystyczne,

–

metody syntaktyczne,

–

metody bazujące na technikach sztucznej inteligencji.

Przyjęcie upraszczających założeń dotyczących charakteru rozpoznawanych obrazów pozwala stwierdzić, że opracowane metody [46] zdolne są
w takich warunkach do przeprowadzenia skutecznej klasyfikacji. Jednak
właściwości rzeczywistych sygnałów takie jak:
–

niestacjonarność,

–

nieliniowość,

–

obecność zakłóceń,

–

niepełna wiedza dotycząca analizowanego problemu.

charakterystyczne dla zdarzeń biomedycznych w tym i sygnałów HRV,
utrudniają proces klasyfikacji, wykluczając często możliwość stosowania
znanych metod.
Przedstawione w dalszej części rozdziału wybrane typy klasyfikatorów, będące reprezentantami trzech wymienionych wcześniej podstawowych grup,
omówione zostaną z uwzględnieniem ograniczeń dla analizowanych sygnałów, występujących przy ich stosowaniu.

3.1. Klasyfikatory statystyczne
Statystyczne metody rozpoznawania sygnałów bazują na wektorach cech
powstałych w wyniku pomiarów lub transformacji struktur analizowanych
przebiegów. Przyporządkowanie obrazu do danej klasy następuje na podstawie oceny funkcji gęstości prawdopodobieństwa odpowiadającej kolejnym
składnikom wydzielonego zbioru cech [47].
Cztery podstawowe kategorie klasyfikatorów statystycznych bazują na różnych kryteriach oceny przynależności analizowanego obrazu od danej klasy:
–

40

Metody minimalizacji odległości sygnału od prototypu klasy w przestrzeni cech.

Hybrydowe system falkowo-neuronowe jako struktury...
–

Estymatory maksymalnego podobieństwa, optymalizujące parametry
na podstawie oceny podobieństwa.

–

Metody bazujące na liniowym dyskryminatorze Fishera, redukujące
rozmiar wektora cech w celu poprawy jakości klasyfikacji.

–

Klasyfikatory bazujące na pomiarze entropii sygnału.

Klasyfikator Bayesa należący do pierwszej z przedstawionych powyżej kategorii jest często stosowany w zagadnieniach klasyfikacji sygnałów biomedycznych. Powstał on w oparciu o minimalizację średniej funkcji strat rj(X)
(5.1), klasyfikacji wektora X do klasy vj :



c

rj ( X )   Li , j p( vi / X )  E Li , j / X



5.1.

i 1

gdzie: Li,j – strata wywołana klasyfikacją obrazu j-tej klasy do klasy i,
c – liczba klas,
p(vi / x ) – funkcja gęstości prawdopodobieństwa.
Wykorzystując wzór Bayesa i przeprowadzając kolejne przekształcenia, klasyfikator Bayesa dla przypadku dwóch klas v1 i v2 o gaussowskim rozkładzie
funkcji gęstości prawdopodobieństwa, można opisać zależnością (5.2) [48]:
1
( X m1 )T
2

1
1

( X m1 )

ln

1
( X m2 )T
2

p( v1 / X )
p( v2 / X )

x

1
1

( X m2 )

v1
v2

1 |
ln
2 |

|
2|
1

5.2.

gdzie: mi – wektor wartości średnich klasy vi

 i1

– macierz kowariancji klasy vi.

Zastosowanie klasyfikatora Bayesa w odniesieniu do analizy wartości testów układu krążenia opisane zostało m.in. w pracy [48], gdzie wykorzystane zostały one do oceny stanu wybranych elementów tego sygnału oraz
w pracy [49], przedstawiającej wyniki analizy sygnałów EKG zarejestrowanych podczas testów wysiłkowych pacjenta. Prowadzone były również
41

Paweł Kostka
badania poświęcone wykorzystaniu tej metody do klasyfikacji pacjentów
z CNWS, np. na podstawie analizy wyników badań angiograficznych [50]
oraz oceny w wyniku analizy statystycznej, stopnia ryzyka przeprowadzenia operacji pomostowania naczyń wieńcowych zajętych stenozą [51].
Stosowanie klasyfikatora Bayesa ograniczone jest koniecznością spełnienia
założeń dotyczących analizowanych zdarzeń. Podstawowe z nich to:
–

niezależność cech,

–

znajomość rozkładu funkcji gęstości prawdopodobieństwa sygnałów
wejściowych, który najczęściej przyjmowany jest jako rozkład normalny,

–

podzielony na kategorie typ danych wejściowych klasyfikatora.

Eliminuje to możliwość użycia tej metody w odniesieniu do złożonych systemów biomedycznych, w tym również do przedstawianego w pracy problemu klasyfikacji pacjentów z CNWS na podstawie analizy sygnałów HRV.
Ograniczeń tych pozbawiony jest należący również do grupy metod statystycznych, klasyfikator k – najbliższych sąsiadów (k-NN) (ang. k – nearest neighbours), który nie wymaga znajomości rozkładu statystycznego
populacji każdej klasy. Jest to metoda nieparametryczna, przeznaczona do
klasyfikacji wektora cech Y wydzielonego z wektora wejściowego X, na
podstawie oceny wyznaczonego prawdopodobieństwa P (5.2), że wektor Y,
znajduje się w danej podprzestrzeni cech R.
P   p( Y )dY  p( y )V
R

gdzie: V

– hyper-objętość podprzestrzeni cech R

p(Y) – funkcja gęstości prawdopodobieństwa Y, która może być
estymowana zależnością (5.3):
p̂( Y )

k
mV

k

– indeks wektora cech y

m

– liczba wszystkich wektorów y .

Dla kolejnego nieznanego wektora cech podlegającego klasyfikacji, w zbiorze sygnałów wzorcowych wyznaczana jest grupa k obrazów, o najbliższej,
ograniczonej zadaną wartością progową odległości od analizowanego przebiegu [52], [53]. Następnie zgodnie z algorytmem k-NN, predykowana jest
klasa, do której zostaje przyporządkowany klasyfikowany obraz. Sposób
42

Hybrydowe system falkowo-neuronowe jako struktury...
działania klasyfikatora k-NN zależy w znaczący sposób od przyjętego typu
miary, zgodnie z którą wyznaczana jest odległość obrazu od prototypów
kolejnych klas. W wielu dziedzinach szeroko stosowanej metody [54], [55],
proponowano zastosowanie różnych typów miar [56], wśród których występujące najczęściej to odległość Euklidesowa, Mińkowskiego oraz różnic
bezwzględnych (miara Manhattanowa) (5.4).



DY

(R)

,Y

(k )

  Y
c

i 1

i

(R)

 Yi k

(5.5.)

gdzie: Y(k) – nieznany wektor cech,
Y(R) – element zbioru wzorców,
c

– liczba cech opisujących klasyfikowany sygnał .

Istotnym zagadnieniem warunkującym działanie klasyfikatora k-NN jest
również sposób uczenia systemu, czyli algorytm wyboru prototypów
klas [57]. Proponowane jest wykorzystanie w tym celu również technik rozmytych [58], algorytmów genetycznych [59] oraz innych metod wstępnego
grupowania danych [60].
W literaturze dotyczącej metody k-NN podkreślany jest duży nakład obliczeniowy związany z koniecznością porównania kolejnych składników
wektora cech analizowanego obrazu i wszystkich wzorców [61]. Stosowane
są następujące algorytmy zwiększające szybkość działania metody:
–

technika wyznaczania odległości cząstkowej (ang. sequential decision
technique), pozwalająca wyeliminować w czasie wyznaczania miar podobieństwa część obrazów już w fazie wstępnej oceny, na podstawie ustawionych wartości progowych, przed porównaniem wszystkich cech,

–

wstępne przeorganizowanie zbioru prototypów w odpowiednio sformatowaną strukturę, której analiza przyspiesza proces klasyfikacji (np.
struktury drzewiaste) [62],

–

redukcja zbioru uczącego przez selekcję i utworzenie zbioru prototypów klas jako podzbioru wszystkich obrazów uczących.

Klasyfikator k-NN, jeden z najczęściej stosowanych algorytmów klasyfikacji obrazów jedno i wielowymiarowych posiada również ograniczenia, które
utrudniałyby jego użycie w odniesieniu do przedstawianego w pracy problemu grupowania pacjentów z CNWS na podstawie sygnałów HRV. Pomimo
zalety jaką jest użyteczność metody dla różnego nawet nieznanego rozkładu
43

Paweł Kostka
gęstości prawdopodobieństwa klasyfikowanych sygnałów, konieczne jest
jednak założenie stałości prawdopodobieństwa warunkowego w obrębie
klasy [63]. Warunek ten staje się trudny do spełnienia dla wielowymiarowej
przestrzeni cech reprezentowanej przez ciąg próbek sygnału o skończonej
długości, przy przyjętej stałej mierze odległości obrazu od wzorca.

3.2. Klasyfikatory syntaktyczne
Działanie tej grupy klasyfikatorów oparte jest na analizie i opisie kształtu
analizowanych sygnałów za pomocą łańcuchów symboli, zaczerpniętych
z opracowanego wcześniej słownika, połączonych zgodnie z regułami przyjętej składni. Zawartość słownika symboli stanowią najprostsze jednostki
niosące informację o strukturze analizowanego obrazu, takie jak np. linie
proste, łuki itp. które nie mogą być poddane dalszemu podziałowi na znaczące składniki. Każdy symbol scharakteryzowany jest przez kilka podstawowych statystyk – atrybutów, wśród których występujące najczęściej
to stopień krzywizny, długość i symetria łuku oraz długość cięciwy [64].
Wartości atrybutów związanych z długością podstawowych jednostek decydują o rozdzielczości zbioru symboli, co ma wpływ na długość łańcuchów
opisujących analizowany sygnał. Zbiór wszystkich symboli tworzy alfabet
klasyfikatora syntaktycznego, który zgodnie z regułami przyjętej gramatyki
określającymi strukturę możliwej składni służy do generowania poprawnych pojęć języka. W literaturze przedstawione są dwa podstawowe typy
klasyfikatorów syntaktycznych wykorzystujących algorytmy:

44

–

Dopasowania łańcuchów (ang. string matching) – podobnie jak w przypadku statystycznej metody rozpoznawania obrazów k-NN, dla utworzonego łańcucha charakteryzującego dany sygnał wyznaczana jest
jego miara podobieństwa (dopasowania) do łańcuchów syntaktycznych,
tworzących przygotowaną bibliotekę wzorców. Warunkiem rozpoczęcia porównywania łańcuchów jest sprawdzenie czy ciąg symboli charakteryzujący nieznany obraz, należy do języka danego klasyfikatora.
Podstawowa metoda posiada wiele modyfikacji mających na celu poprawę efektywności działania [65].

–

Analiza składniowa (ang. parsing) – celem tej metody jest określenie
czy obraz wejściowy podlegający klasyfikacji może być wygenerowany
przy pomocy istniejącego języka i reguł składni. Realizowane to jest
często w wyniku wstępnego opisu kolejnych fragmentów sygnału za
pomocą symboli danego słownika, a następnie wyznaczenia łańcucha
charakteryzującego sygnał jako fragmentu struktury drzewiastej, reprezentującej wszystkie możliwe pojęcia języka. Metoda ta może być zaklasyfikowana jako wariant programowania dynamicznego [65].

Hybrydowe system falkowo-neuronowe jako struktury...
Zaletą klasyfikatorów syntaktycznych jest możliwość reprezentacji sygnału
wejściowego, scharakteryzowanego przez zbiór kolejnych próbek, za pomocą łańcucha o skończonej długości, złożonego z podstawowych i stosunkowo łatwych do interpretacji symboli. Liczne przykłady stosowania metod
tej grupy występują dla sygnałów, które pozwalają na wydobycie znaczącej
ilości informacji dotyczącej reprezentowanych przez nie procesów, bezpośrednio na podstawie kształtu zarejestrowanych przebiegów. Dotyczy to takich obszarów jak automatyczne rozpoznawanie liter [66] i innych znaków
graficznych – np. wzorów matematycznych [67] lub zapisów nutowych [68],
automatyczna analiza dokumentów (np. rysunków technicznych ) a w dziedzinie przetwarzania sygnałów biomedycznych np. klasyfikacja sygnałów
EKG o charakterystycznym, odzwierciedlających kolejne etapy czynności
pracy serca kształcie przebiegu [69], [70], [71]. Skuteczność działania tych
metod zależy w dużym stopniu od sposobu doboru punktów stałych analizowanej krzywej, które stanowią granice kolejnych fragmentów opisanych
za pomocą symboli podstawowych. Jednak dla przedstawianego w pracy
problemu klasyfikacji CNWS na podstawie sygnałów HRV, dla których
w celu wydobycia istotnych informacji zawartych w sygnale, konieczne jest
przeprowadzenia transformacji w inną dziedzinę rozważań (rozdiał 1.4.4)
klasyfikatory syntaktyczne wydają się być mało przydatne.

3.3. Klasyfikatory wykorzystujące metody sztucznej
inteligencji (SI)
Wykorzystanie metod sztucznej inteligencji w procesie klasyfikacji można
interpretować jako realizację trzech kolejnych etapów:
–

poszukiwanie właściwej dla rozwiązywanego problemu techniki jego
rozwiązania,

–

reprezentacja wiedzy – wybór formalnego sposobu zapisu symptomów
analizowanego procesu,

–

uczenie systemu na podstawie znanych sygnałów wejściowych i wyjściowych obiektu (uczenie nadzorowane) lub dobór parametrów systemu w wyniku znajdowania określonych zależności istniejących
w sygnałach wejściowych, pozwalających je grupować (uczenie bez
nauczyciela).

Cechą charakterystyczną metod SI i jednocześnie wynikiem poprawnego
przeprowadzenia przedstawionych etapów, powinno być uzyskanie struktury zdolnej do znalezienia zadawalającego rezultatu zarówno w warunkach,
dla których następował proces tworzenia systemu, jak i w pewnym zakresie
pobudzeń spoza zbioru sygnałów uczących.
45

Paweł Kostka
Techniki sztucznej inteligencji, stosowane w strukturach klasyfikatorów
można podzielić na dwie podstawowe kategorie: metody symboliczne oraz
algorytmy bazujące na strukturach połączeń wagowych (ang. connectionist
techniques). Klasyfikatory symboliczne służą do organizowania i przetwarzania wiedzy w sytuacji, gdy struktura klas reprezentowana jest w sposób
symboliczny. Jednym z często stosowanych w dziedzinie przetwarzania sygnałów biomedycznych typów tej grupy metod są systemy ekspertowe (SE).
Posiadają one zdolność interpretacji na podstawie wprowadzonej do nich
wiedzy, danych różnego pochodzenia, zapewniając jednocześnie sprzężenie
z ekspertem medycznym [72]. Systemy tego typu spotykane są w ocenie
stanu układu krążenia pacjentów, w tym również w diagnozowaniu przedstawianej w pracy CNWS [72], [73]. Prezentują one jednak odmienne podejście do problemu. W odróżnieniu od opisywanych w pracy systemów
falkowo-neurunowych (SFN), bazujących na sygnale HRV, wejście systemu
ekspertowego stanowi zbiór wyników różnego typu badań, testów i danych
wywiadu lekarskiego pacjenta.
Druga grupa klasyfikatorów bazujących na technikach sztucznej inteligencji, do których należą również przedstawione w pracy SFN, posiada struktury oparte na połączeniach wagowych. Podstawowym reprezentantem tej
kategorii są sieci neuronowe, których struktura odzwierciedlająca połączenia biologicznych neuronów umożliwia przeprowadzanie procesu klasyfikacji. Sztuczne sieci neuronowe mogą być scharakteryzowane przez typ
podstawowej jednostki – neuronu, topologię połączeń oraz strategię uczenia
systemu (rozdział 3.2). W odróżnieniu od metod statystycznych i syntaktycznych, struktury oparte na SN w systemach klasyfikacji muszą posiadać
zdolność zapamiętywania wprowadzonej do nich podczas fazy uczenia informacji \. Pamięć systemu stanowią wektory wag i przesunięć odpowiadające kolejnym neuronom wszystkich warstw sieci. Klasyfikatory bazujące
na SN, ze względu na przedstawione powyżej cechy, zalecane są do stosowania w odniesieniu do problemów, dla których wiedza a priori opisująca
zależności między sygnałami wejściowymi sieci a klasami przynależności,
jest minimalna. W literaturze przedstawione są przykłady skutecznego stosowania sieci neuronowych w przypadku, gdy dane opisujące analizowany
proces były znacznie zakłócone, niekompletne, wieloźródłowe oraz częściowo wzajemnie sprzeczne [74].
Dynamicznie rozwijającą się grupę klasyfikatorów bazujących na metodach
SI tworzą systemy hybrydowe (rozdział 1), wśród których wyróżnić można
układy rozmyto-neuronowe. Prace [75] przedstawiają wykorzystanie metod
wnioskowania rozmytego w fazie uczenia klasyfikatorów wykorzystujących struktury SN.
46

Hybrydowe system falkowo-neuronowe jako struktury...
Działanie SN o parametrach dobranych w procesie uczenia może być interpretowane jako realizacja reguł diagnostycznych if – then [76]. Jednak
przewaga metod bazujących na strukturach SN w odniesieniu do klasyfikatorów innych typów w wybranych zastosowaniach wynika z następujących
podstawowych przyczyn [77]:
–

ze względu na fakt, że SN nie wymagają znajomości rozkładu statystycznego cech opisujących klasyfikowane obrazy mogą być stosowane
dla sygnałów o nieznanych funkcjach rozkładu lub posiadających rozkład niegaussowski,

–

klasyfikatory bazujące na SN pozwalają na przetwarzanie danych wzajemnie skorelowanych, pozwalając na eliminację niezbędnego dla metod statystycznych warunku niezależności cech.

Zakres stosowania klasyfikatorów opartych na SN w dziedzinie przetwarzania sygnałów biomedycznych jest bardzo szeroki. Wykorzystanie metod tego
typu w procesie wspomagania diagnostyki medycznej, zarówno w odniesieniu do obrazów dwu jak i jedno wymiarowych, odzwierciedlone jest bogatym
zbiorem pozycji literatury [78]. Istnieje duża liczba publikacji przedstawiających wyniki stosowania SN w zagadnieniach dotyczących oceny stanu wybranych elementów układu krążenia. Najliczniejszą grupę metod stanowią klasyfikatory sygnału EKG [79], natomiast prace powstałe w grupie prof. Akaya
(Dep. of Biomedical Engineering, Rutgers University) [80], opisują tematykę
bezpośrednio związaną z przedstawianym w pracy problemem klasyfikacji
pacjentów z CNWS. Jednak w odróżnieniu od podejścia przedstawianego
w pracy, przedstawiane metody bazują na analizie cech wydzielonych z zarejestrowanych na klatce piersiowej sygnałów akustycznych, towarzyszących
przepływowi krwi przez naczynia wieńcowe zajęte stenozą.
Poprawne działanie klasyfikatorów tej grupy uwarunkowane jest właściwym
doborem struktury SN. Przykłady literaturowe zwracają uwagę na fakt, że
trudno jest budować systemy uniwersalne przeznaczone do rozwiązywania
zagadnień z różnych dziedzin. Zgodnie z hipotezą izomorficzną [RumCle86],
stanowiącą pomoc w doborze optymalnego rozwiązania, topologia SN powinna być uzależniona od natury rozwiązywanego problemu. Systemy neuronowego rozpoznawania obrazów są określane są w literaturze jako mało
odporne (ang. not robust) na znaczne zmiany warunków przetwarzania informacji i dlatego wymagane jest często testowanie struktur różnego typu, celem wyboru optymalnego rozwiązania dla specyficznego zagadnienia. Zgodnie z powyższymi wnioskami, w procesie doboru najbardziej odpowiedniej
struktury przedstawianych w pracy SFN, zastosowanych do klasyfikacji ZB,
przeprowadzono badania dla różnych realizacji zarówno części wstępnego
przetwarzania z użyciem transformaty falkowej (TF) jak i podsystemu SN.
47

Paweł Kostka

3.4. Klasyfikator sygnałów zmienności rytmu serca HRV
w oparciu o struktury SFN zmienności rytmu serca
HRV w oparciu o struktury SFN
W przedstawianym w pracy klasyfikatorze zastosowano i przetestowano
dwie proponowane struktury warstwy wstępnej SFN. Obydwie bazują na
przeprowadzaniu dekompozycji falkowej wejściowego sygnału HRV, jednak sposób realizacji i dalszego przetwarzania otrzymanych wyników jest
różny, co prowadzi do uzyskania odmiennych wektorów cech podawanych
dalej na wejście struktury klasyfikatora neuronowego.

Rys. 5. Dwie struktury warstwy wstępnej SFN. Wydzielenie składowej
dekompozycji falkowej dla współczynnika skali aOPT określonego w fazie uczenia
(a); Wyznaczenie wektora cech na podstawie N składowych dekompozycji
falkowej, dla wstępnie określonego zbioru wartości parametru skali a (b)

W pierwszym rozwiązaniu (rys. 5a) wyjście warstwy falkowej stanowi wynik
przeprowadzenia falką sygnału wejściowego X2 otrzymany dla współczynnika skali aOPT . Ta wartość parametru transformaty falkowej reprezentuje skalę
dekompozycji waveletowej, określoną w procesie uczenia SFN, jako optymalną dla problemu klasyfikacji sygnałów HRV pacjentów z CNWS. Trak48

Hybrydowe system falkowo-neuronowe jako struktury...
tując analizę falkową jako narzędzie przeprowadzania dekompozycji sygnału
w podpasma częstotliwościowe w wyniku analizy przez bank filtrów, współczynnik skali aOPT jednoznacznie określa parametry filtru i własności podpasma częstotliwościowego, które w wyniku uczenia SFN, zostało określone
jako zawierające najistotniejsze informacje pozwalające dyskryminować klasyfikowane grupy. Typ I struktury warstwy falkowej przeznaczony został dla
zastosowania falki podstawowej typu Morlet, zdefiniowanej różniczkowalną
funkcją (9), której współczynnik skali a razem z parametrami SN poddany został uczeniu w SFN, zgodnie z algorytmem wstecznej propagacji błędu. Jako
wynik działania tej warstwy, uzyskany został nowy wektor cech X3 o liczności
równej liczbie próbek sygnału wejściowego HRV – X2.
Działanie drugiego typu struktury warstwy wstępnej omawianych systemów, przedstawionych na rys. 5b podzielone jest na dwa etapy. W pierwszym, wejściowy sygnał HRV – X2 zostaje N-krotnie poddany dekompozycji
falkowej, dla zbioru N współczynników skali ai, określonych na podstawie
szerokości użytecznego pasma częstotliwościowego analizowanego sygnału. Jako wynik uzyskiwane jest N składowych falkowych, odpowiadających
podpasmom częstotliwościowym zawartym w przedziale ograniczonym
krańcowymi wartościami zadanej siatki wartości parametrów skali ai. Następnie dla każdego wektora X3i wyznaczany jest jeden charakteryzujący go
parametr – energia i-tej składowej, obliczany jako suma kwadratów współczynników dekompozycji falkowej na poziomie określonym przez ai (10)
M

Fi    X 3i ( j )

2

(10)

j 1

W wyniku otrzymywany jest N elementowy wektor cech X3 poddawany następnie procesowi klasyfikacji w strukturze SN. Ten typ warstwy wstępnej
SFN przeznaczono i przetestowano dla dwóch pozostałych użytych w pracy
PFW: Bior 2.4 i Db4, stosując algorytm doboru parametrów tej warstwy,
bazujący na wstępnie określonej siatce wartości parametru skali a.

3.4.1. Dobór parametrów struktury klasyfikatora bazującego na
Systemach Falkowo-Neuronowych
SFN opisywane w pracy, przy omawianiu systemów identyfikacji, nie należą
do grupy metod uniwersalnych lecz posiadają strukturę, której typ i parametry silnie zależą od cech opisujących problem podlegający analizie. Różnice
budowy warstwy falkowej, uwarunkowane właściwościami przetwarzanego w nich sygnału HRV, zostały przedstawione w poprzednim podrozdziale. W tym miejscu pracy, opisany zostanie sposób przeprowadzania doboru
parametrów, zarówno struktur części falkowej, jak i neuronowej systemu.
49

Paweł Kostka

Ustalenie parametrów warstwy wstępnej systemu: określenie warunków uczenia bloku realizującego analizę falkową
Warunki przeprowadzania procesu uczenia dla warstwy falkowej systemu,
można ustalić, wykorzystując częściowo wiedzę dotyczącą cech charakterystycznych sygnału HRV. Na podstawie zebranych informacji, zawartych
w widmie częstotliwościowym tego sygnału, dobór parametrów warstwy
wstępnej SFN przeprowadzono w następujących dwóch etapach:
–

ustalenie zakresu zmienności parametru skali a podczas uczenia systemu,

–

inicjalizacja wartości współczynnika skali a.

Przyjęty zakres zmienności aTW (11), odpowiada założonemu przedziałowi
częstotliwości: 0.01-0.5 [Hz], w którym wg przedstawionych w literaturze
wyników badań znajdują się wszystkie istotne informacje zawarte w widmie sygnału HRV
aTW  aTW _ MIN ;aTW _ MAX  5; 250

(11)

Granice przedziału (5.13) zostały określone w odniesieniu do częstotliwości
próbkowania przetworzonego wstępnie sygnału HRV: fP = 5 [Hz]. Przedział
ten ogranicza wartości przyjmowane przez aTW podczas uczenia SFN, zgodnie ze zmodyfikowanym algorytmem wstecznej propagacji błędu dla falki
typu Morlet.
Dla dwóch pozostałych falek podstawowych zastosowanych w SFN, określone wartości graniczne aTW_MIN i aTW_MAX pozwoliły ustalić siatkę wartości
aTW , na podstawie której, w wyniku uczenia systemu, określana jest wartość
optymalna współczynnika skali aTW_OPT . Dla sygnału HRV, podawanego na
wejście SFN, w wyniku zastosowania schematu wielopoziomowej dekompozycji Mallata, uzyskano składowe sygnału w podpasmach częstotliwościowych przedstawionych w tab. 1. Operacja zmiany skali, polegająca na
jej zwiększaniu na każdym poziomie dekompozycji przez 2, jest realizowana w wyniku decymacji składowej otrzymanej po etapie filtracji. Zestawienie przedstawione w tab. 1, zawiera tylko te przedziały częstotliwości, które
mieszczą się w zakresie określonym przez szerokość użytecznego pasma
sygnału HRV (11). Analiza entropii kolejnych składowych dekompozycji,
przeprowadzona zgodnie z algorytmem przedstawionym w poprzednim
3.3.4.2, potwierdziła słuszność przyjętych założeń. Największą wartość entropii uzyskano dla przedziału odpowiadającego składowej d4 , co mogłoby
wskazywać na położenie istotnych dla rozwiązywanego problemu składni50

Hybrydowe system falkowo-neuronowe jako struktury...
ków, właśnie w tym przedziale częstotliwości. Wyniki te zostaną porównane z wartościami współczynnika skali TW, uzyskanymi w wyniku uczenia
SFN.
Tabela 2. Zestawienie szerokości podpasm częstotliwościowych odpowiadających
kolejnym poziomom dekompozycji (PD) waveletowej sygnału, zgodnie
z algorytmem Mallata.

Składowa dekompozycji
sygnału
d3
d4
d5
d6
d7
d8

Wartość ai
8
16
32
64
128
256

Granice i-tego podpasma
[Hz]
0.3125  0.6250
0.1563  0.3125
0.0781  0.1563
0.0391  0.0781
0.0195  0.0391
0.0098  0.0195

Po określeniu zakresu zmienności współczynnika skali aTW, drugim etapem
doboru parametrów warstwy falkowej systemu, było wyznaczenie wartości
początkowej a0 . Procedura ta została przeprowadzona dla struktury warstwy TW typu I, gdzie zastosowano PFW typu Morlet i algorytm uczenia,
bazujący na wstecznej propagacji błędu .
Z powodu braku przesłanek pozwalających określić, w którym charakterystycznym dla sygnału HRV paśmie częstotliwości, mogą znajdować się
składniki widma, determinujące w największym stopniu przeprowadzenie
klasyfikacji pacjentów z CNWS, wartość inicjalizującą aTW_POCZ określono
na podstawie testów. Wykres 5.3 przedstawia przebieg zmienności parametru a, przy różnych wartościach początkowych a0, w kolejnych iteracjach
procesu uczenia. W wyniku przeprowadzonych prób, wyznaczono przedział
wartości początkowych aTW_POCZ (5.14), dla których można było zaobserwować zbieżność wartości tego parametru podczas fazy uczenia SFN do
wartości aTW_OPT ≈ 10.5, co odpowiada częstotliwości fOPT ≈ 0.24 [Hz]

aTW _ POCZ  aTW _ POCZ _ MIN ;aTW _ POCZ _ MAX  8.6;14
co odpowiada:

f  0.18; 0.29

[Hz]

(12)

Inicjalizacja parametru a wartościami spoza przedziału (11), prowadziła do uzyskania nieznacznych jego zmian podczas procesu uczenia, zarówno dla wartości mniejszych, jak i większych od granic wyznaczonego zakresu (rys. 6).
51

Paweł Kostka

Rys. 6. Wykres zmian wartości parametru skali, a warstwy falkowej SFN,
podczas uczenia zgodnie z algorytmem WPB, dla czterech różnych wartości
inicjalizujących a0. Typ SFN: Falki typu Morlet, SN typu 3WTangTangLinFA

Wyznaczenie optymalnego stopnia złożoności struktury klasyfikacyjnej
SN
W celu określenia stopnia złożoności SN typu wielowarstwowy perceptron
(WWP), podobnie jak w SFN zastosowanych w układach identyfikacji procesów przeprowadzono redukcję systemu zgodnie z algorytmem eliminacji nieistotnych węzłów struktury (NP) (node pruning), przedstawionym
w rozdziale 3.3.6. Wykres 7a jest wynikiem zastosowania tego algorytmu,
prezentującym przyrost funkcji kosztu wyjść SN – ΔE wyznaczony dla prób
usunięcia kolejnych neuronów struktury.
Zakładając pierwotną liczbę neuronów warstwy ukrytej 2 warstwowej SN
typu WWP równą NHL_POCZ = 50, przeprowadzona analiza pozwoliła po eliminacji 18, zredukować tę wielkość do NHL = 32 neuronów (rys. 7b), bez
znaczącej zmiany błędu wyjść systemu E, otrzymanego w wyniku procesu
uczenia.

52

Hybrydowe system falkowo-neuronowe jako struktury...

Rys. 7. Wartość przyrostu funkcji kosztu ΔE dla prób usuwania kolejnych neuronów
warstwy ukrytej SFN (a). Wykres zmian wartości błędu wyjść E w kolejnych iteracjach
procedury NP (b); E=1 – wartość dla pełnej struktury

Weryfikacja jakości działania klasyfikatora opartego na systemach falkowo-neuronowych dla sygnałów zmienności rytmu serca (HRV)
Utworzenie bazy danych
Badanie elektrokardiograficzne pacjentów z podejrzeniem choroby naczyń
wieńcowych serca (CNWS), na podstawie którego wyznaczono sygnał
HRV, jest jednym z podstawowych testów przesiowowych, mających na
celu ocenę stanu tętnic wieńcowych pacjenta. Materiał badawczy pochodził
z ogólnodostępnej, benchmarkowej bazy sygnałów biomedycznych Physionet.org .
Podział bazy danych
Baza danych złożona z zarejestrowanych sygnałów HRV została poddana
klinicznej diagnozie eksperta. Na podstawie przeprowadzonych badań wyodrębniono cztery grupy pacjentów, które wraz z liczbą należących do nich
przypadków są następujące:
53

Paweł Kostka
I

Grupa kontrolna (0N)

– 12

II Grupa z dysfunkcją 1 naczynia wieńcowego (1N) – 13
III Grupa z dysfunkcją 2 naczyń wieńcowych (2N)

– 15

IV Grupa z dysfunkcją 3 naczyń wieńcowych (3N)

– 22

Razem:

62

W celu wykorzystania materiału zgromadzonego w bazie, zarówno w procesie uczenia jak i weryfikacji działania przedstawianych klasyfikatorów,
wszystkie sygnały podzielono na zbiór uczący i weryfikujący. Ze względu
na stosunkowo małą liczność bazy, zbiór uczący utworzono z 20 sygnałów
(po 5 sygnałów z każdej grupy), które następnie powielono 4-krotnie uzyskując zestaw 80 przebiegów HRV. Wielokrotne wprowadzanie tych samych
sygnałów na wejście sieci neuronowej (SN) podlegającej uczeniu, zalecane
jest jako sposób mogący wpłynąć korzystnie na proces doboru parametrów
tych struktur, w przypadku zbiorów małolicznych [11]. Zbiór weryfikujący
utworzony został z pozostałych 42 sygnałów HRV.
Wyniki klasyfikacji
Rezultaty zastosowania systemów falkowo-neuronowych do klasyfikacji
sygnałów HRV pacjentów z chorobą naczyń wieńcowych serca (CNWS)
różnego stopnia, zostaną przedstawione w dwóch grupach:
I.

Wyniki związane z szybkością i zbieżnością procesu uczenia różnych
struktur SFN oraz dla odniesienia SN.

II. Porównanie jakości klasyfikacji tych systemów, mierzonej przez parametry określające czułość i specyficzność metody.
Zestawienie czynników warunkujących analizę działania klasyfikatorów zbudowanych na bazie systemów hybrydowych i samodzielnych
sieci neuronowych
Przedstawione poniżej trzy grupy czynników określają budowę i działanie
analizowanych systemów. Elementy zawarte w grupie II i III są wzajemnie
zależne.
I

Typ struktury SN stanowiącej część SFN:
1. SN typu wielowarstwowy perceptron (WWP):
A. SN 1. warstwowa o liniowej funkcji aktywacji (FA) – 1WLinFA
B. SN 1. warstwowa o tangsoidalnej FA – 1WTangFA

54

Hybrydowe system falkowo-neuronowe jako struktury...
C. SN 2. warstwowa o tangsoidalnej i liniowej FA – 2WTangLinFA
D. SN 3. warstwowa o tangsoidalnej, tangsoidalnej i liniowej FA –
3WTangTangLinFA
2. SN o radialnych funkcjach bazowych (RFB):
A. SN 2. warstwowa o RFB i liniowej FA – RBFLinFA
II Typ podstawowej funkcji falkowej warstwy SFN realizującej analizę
falkową:
1. Morlet
2. Db 4
3. Bior 2.4
III Typ struktury warstwy falkowej:
1. Struktura typu I, dająca na wyjściu składową dekompozycji waveletowej dla wyznaczonego w procesie uczenia optymalnego współczynnika skali aWT_OPT; przeznaczona dla falki Morlet.
2.

Struktura typu II, bazująca na przeprowadzaniu dekompozycji sygnału wejściowego zgodnie ze schematem Mallata; zastosowana
dla falki typu Bior 2.4 i Db 4.

Analiza szybkości i zbieżności procesu uczenia struktur hybrydowych i klasycznych sieci neuronowych
Przebieg uczenia nadzorowanego systemów falkowo-neuronowych (SFN)
oraz sieci neuronowych (SN) scharakteryzowany został, podobnie jak dla
przedstawionego w rozdziale 4 zagadnienia wykorzystania tych struktur
w procesie identyfikacji PZS, przez wartości błędu średniokwadratowego E
wyjść struktur w kolejnych iteracjach lIT. Wartości funkcji kosztu EN przedstawiane w tym podrozdziale, poddane zostały normalizacji w odniesieniu
do struktury, dla której otrzymano w wyniku uczenia najmniejszą wartość
błędu E (SFN – PFW Morlet, typ SN: 3WTangTangLinFa). Znormalizowana wartość EN , przyjęta jako miara szybkości i zbieżności fazy uczenia
analizowanych systemów stanowiła kryterium porównawcze wpływu badanych czynników na otrzymane wyniki.

55

Paweł Kostka
WYKRES ZMIAN ZNORM. BŁĘDU ŚREDNIOKW. – EN DLA RÓŻNYCH TYPÓW STRUKTUR CZĘŚCI SN SWN

SN: 3WTangTangLinFA
SN: 2WTangLinFA
SN: RBFLinFA
SN: 1WTangFA

Rys. 5.5. Wykres zależności znormalizowanego błędu uczenia EN (lIT) SFN dla
różnych typów struktur bloku SN systemu. PFW typu Morlet

Analiza wpływu typu struktury SN na przebieg procesu uczenia
SFN
Wyniki przedstawione na rys. 5.5 obrazują porównanie wpływu typu struktury SN stanowiącej część klasyfikacyjną SFN, na wartości EN w kolejnych
iteracjach procesu uczenia. Wyznaczone zależności charakteryzują fazę
uczenia SFN z warstwą waveletową typu I (falka typu Morlet), dla której
otrzymano najmniejsze wartości E.
Dane opisujące przebieg doboru parametrów SFN dla pozostałych dwóch
PFW (typu Bior 2.4 i Db4) jak również dla porównania, klasyfikatora wykorzystującego tylko część SN z pominięciem warstwy waveletowej, zawarte
zostały tab. 5.2.

56

Hybrydowe system falkowo-neuronowe jako struktury...
Analiza wpływu zastosowania dwóch struktur warstwy waveletowej na
przebieg procesu uczenia SFN.
WYKRES ZMIAN ZNORM. BŁĘDU ŚREDNIOKW. – EN DLA RÓŻNYCH TYPÓW PFW WARSTWY TW SWN

PFW typu Morlet
PFW typu Db4
PFW typu Bior 2.4
Brak TW, tylko SN

Rys. 5.6. Wykres zależności znormalizowanego błędu uczenia EN (lIT) SFN dla
oceny wpływu typu PFW warstwy wstępnej SFN na proces uczenia. Typ struktury
SN: 3WTangTangLinFA

Dla oceny wpływu przeprowadzenia adaptacyjnej TW na proces uczenia
systemu, rys. 5.6 przedstawia zależności EN (lIT) uzyskane dla różnych PFW
warstwy wstępnej SFN oraz dla odniesienia krzywą uczenia struktury SN
bez dodatkowej warstwy realizującej TW. Rodzaj falki podstawowej związany jest z typem warstwy falkowej SFN. Wykres dla funkcji podstawowej
Morlet charakteryzuje przebieg procesu uczenia dla SFN ze strukturą warstwy TW typu I, natomiast zależności uzyskane dla PFW Db4 i Bior 2.4
odpowiadają strukturze typu II.
Wyniki zobrazowane na rys. 5.6 wyznaczone zostały dla podsystemu SN
o topologii sieci 3-warstwowej: 3WTangTangLinFA, dla której uzyskano
najmniejsze wartości EN.

57

Paweł Kostka
Tab. 5.2. Zestawienie wartości znormalizowanego błędu EN w kolejnych etapach
procesu uczenia SFN o różnych typach warstwy waveletowej i strukturach SN

Nr cyklu
Typ SN
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA

PFW typu Morlet (warstwa TF – typ I)
1
80
200
500
1000
1.000
0.236
0.086
0.081
0.080
1.492
0.193
0.141
0.136
0.134
3.550
2.341
1.890
1.579
1.576
2.230
0
0
0
0
PFW typu DB4 (warstwa TF – typ II)
1.408
0.482
0.416
0.401
0.400
1.689
0.522
0.501
0.423
0.418
3.889
2.521
2.145
1.988
1.923
2.356
0
0
0
0
PFW typu Bior 2.4 (warstwa TF – typ II)
1.511
0.637
0.582
0.521
0.519
1.698
0.613
0.554
0.505
0.487
3.921
2.632
2.267
1.894
1.890
2.376
0
0
0
0
Struktura SN (brak warstwy TF)
2.202
1.447
1.406
1.389
1.384
2.311
1.513
1.401
1.400
1.400
4.367
3.499
3.232
3.190
3.179
2.879
0
0
0
0

5.3.1. Weryfikacja działania przedstawionych systemów
klasyfikacji sygnałów HRV pacjentów z chorobą wieńcową
Weryfikacja działania klasyfikatorów wykorzystujących opisywane w pracy
SFN, przeprowadzona została w oparciu o miary czułości (C) (5.15) i specyficzności (SP) (5.16), wyznaczone na podstawie oceny wyników układu
C

Total  FalseNeg
100%
Total

pobudzanego sygnałami HRV ze zbioru testowego,

gdzie: Total – liczba wszystkich sygnałów testowych,
SP 

Total  FalsePos
100%
Total

FalseNeg – liczba przypadków należących do danej klasy sklasyfikowanych
błędnie jako nie należące,
58

Hybrydowe system falkowo-neuronowe jako struktury...
FalsePos – liczba przypadków nie należących do klasy sklasyfikowanych
błędnie jako należące.
Klasyfikacja sygnałów HRV przeprowadzona została dwuetapowo.
W pierwszej części, ze względu na założenie, że proponowana metoda
stanowić może element wstępnych badań przesiewowych w diagnostyce
CNWS, dokonano klasyfikacji pacjentów na dwie grupy: grupę kontrolną
(ON) oraz zagrożonych chorobą bez rozróżniania stopnia jej zaawansowania (kN). Następnie przeprowadzono próbę rozróżnienia poziomu CNWS
(3 stopnie), zgodnie ze strukturą bazy danych stanowiącej materiał badań
(grupy patologiczne: 1N, 2N, 3N).
Tab. 5.3 i 5.4 przedstawiają zestawienie wartości wprowadzonych powyżej parametrów, uzyskanych dla różnych typów struktur SFN, w przypadku
klasyfikacji do dwóch (tab. 5.3) oraz czterech (5.4) grup. Dla oceny wpływu wprowadzenia warstwy waveletowej, zaprezentowano również wyniki otrzymane dla klasyfikatorów bazujących tylko na klasycznych sieciach
neuronowych.
Tab. 5.3. Zestawienie wartości czułości(C) i specyficzności (SP) testowanych
metod, przy podziale na dwie grupy (kontrolna – 0N i patologiczna – kN), dla
SFN i SN o różnych badanych strukturach

Typ PFW
Typ SN
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA

C

SP

PFW typu DB4 (warstwa TF – typ II)
92.8
92.8
90.4
92.8
80.9
90.4
85.7
90.4
73.8
88.1
PFW typu Bior 2.4 (warstwa TF – typ II)
90.4
92.8
90.4
90.4
80.9
90.4
83.3
90.4
71.4
88.1
PFW typu Morlet (warstwa TF – typ I)
88.1
92.8
88.1
90.4
76.2
90.4
85.7
92.8
69.4
90.4

59

Paweł Kostka

3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA

Struktura SN (brak warstwy TF)
83.3
90.4
85.7
88.1
71.4
88.1
85.7
88.1
69.4
88.1

Tab. 5.4. Zestawienie wartości czułości(C) i specyficzności (SP) testowanych
metod, przy podziale na cztery grupy (kontrolna – 0N i 3 patologiczne – 1N, 2N,
3N), dla SFN i SN o różnych badanych strukturach

Typ PFW
Typ SN
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA
3WTangTangLinFA
2WTangLinFA
1WTangFA
2RBFLinFA
1WLinFA

C

SP

PFW typu DB4 (warstwa TF – typ II)
88.1
83.3
85.7
80.9
78.6
76.2
80.9
76.2
69.0
66.6
PFW typu Bior 2.4 (warstwa TF – typ II)
85.7
83.3
83.3
78.8
76.2
76.2
83.3
76.2
69.0
64.2
PFW typu Morlet (warstwa TF – typ I)
83.3
80.9
83.3
78.8
73.8
73.8
83.3
76.2
69.0
64.2
Struktura SN (brak warstwy TF)
76.2
76.2
73.8
73.8
64.3
64.3
78.6
73.8
64.3
61.9

W kolejnych dwóch, ostatnich fragmentach rozdziału 5, przedstawiono zestawienie wybranych wyników zawartych w tab. 5.3 (klasyfikacja do dwóch
grup: ON i kN), dla zobrazowania wpływu typu struktury zarówno warstwy
falkowej jak i części SN na wartość parametrów C i SP klasyfikatora SFN.
60

Hybrydowe system falkowo-neuronowe jako struktury...
Przedstawienie wpływu zastosowania warstwy TF w SFN, dla różnych
typów falki podstawowej.
Dane przedstawione w postaci wykresów słupkowych na rys. 5.7 stanowią porównanie wyników zastosowania różnych typów falki podstawowej
w warstwie wstępnej SFN, z odpowiednim klasyfikatorem bazującym tylko
na SN, bez wstępnej dekompozycji sygnału HRV za pomocą falek.

Rys. 5.7. Wykres wartości C i S [%] dla trzech PFW w warstwie TW SFN oraz dla struktury SN, (typ części SN: 3WTangTangLinFA)

Wykres 5.7 przedstawia zestawienie wartości parametrów C i SP dla testowanej 3-warstwowej SN oznaczonej symbolem: 3WTangTangLinFA, której
zastosowanie pozwoliło uzyskać najwyższą jakość klasyfikacji.

LITERATURA
1. Akay Y.M., Akay M., Welkowitz W., Kostis J.B.: “Noninvasive acoustical
detection of coronary artery disease”, IEEE Engineering in Medicine and Biology,
pp. 761-764, 1994
2. Akay Y.M., Akay M., Welkowitz W., Kostis J.: “Noninvasive detection of coronary
artery disease”, IEEE Engineering in medicine and biology, 11/12, pp. 761-764,
1994.
3. Akay Y.M., Akay M., Welkowitz W., Semmlow L., Kostis J.B.: “Noninvasive
acoustical detection of coronary artery disease: a comparative study of signal
processing methods”, IEEE Transaction on Biomedical Engineering, 40:6:571-578.

61

Paweł Kostka
4. Akay M.: “Time-Frequency and wavelet analysis”, IEEE EMB Magazine 14(2),
1995.
5. Akselrod S., Gordon D., Ubel F.A., Shannon D.C., Barger A.C., Conen R.J.:
“Power spectrum analysis of heart rate fluctuation: a quantitative probe of beat-tobeat cardiovascular control”, Science, 213(10):220-222, 1981.
6. Alimoglu F., Alpaydin E.: „Combining multiple representation and classifiers for
pen-based handwritten digit recognition”, Proc. of Conf. on Document Analysis
and Recognition, (ICDAR’97), 1997
7. Anand R., Mohan C.K., Mehrotra K., Ranka S.: „Efficient classification for
multiclass problems using modular neural networks“, IEEE Trans. on Neural
Netoworks, 6:117-124, Jan. 1995.
8. Anderson R.H.: “Syntax directed recognition of hand printed two-dimensional
mathematics”, In M. Klerer & J. Reinfelds, editors: Interactive systems for
experimental applied mathematics, Academic Press, New York, 1968
9. Anrep G.V., Pascual W., Rossler R.: “Respiratory variations of the heart rate I – The
reflex mechanism of the respiratory arythmia”, Proceedings of the Royal Society
of London; 119(B):191-217, 1936.
10. Baumann S.: „A simplified attributed graph grammar for high-level music
recognition”, In proceedings of third international conference on document
analysis and recognition, Montreal, pp.1080-83, Aug. 1995.
11. Bezdek J.: IEEE Trans. on Neural Networks – Special Issue on Fuzzy Logic and
Neural Networks, vol. 3, 1992.
12. Bezdek J.C.: “Pattern recognition with fuzzy objective function algorithms”,
Plenum Press, New York, 1981.
13. Birman K.P.: “Rule based learning for more accurate ECH analysis”, Technical
Report 001, University of California, Berkeley, Computer Science Division, June
1981.
14. Bortolan G., Degani R., Willens J.L.: “Neural networks for ECG classification”,
Computers in Cardiology 1990, Proceedings, pp:1269-272, 1991.
15. Caelli T., Guan L., Wen W.: “Modularity in neural computing”, Proc. of the IEEE,
87:9:1497-1518.
16. Cerrito P.B., Koenig S.C., VanHimbergen D.J., Jaber S.F., Ewert D.L., Spence P.A.:
“Neural network pattern recognition analysis of graft flow characteristics improves
intra-operative anastomotic error detection in minimally invasive CABG”,
European Journal of Cardio Thoracic Surgery, 16:88-93, 1999.
17. Cerutti S., Alberti M., Baselli G., Rimoldi O., Malliani A., Merri M., Pagani M.:
“Automatic assessment of the interaction between respiration and heart rate
variability signal”, Medical Progress through Technology, 14:7-19, 1988.
18. Cha S.H., Srihari S.N.: “Nearest neighbor search using additive binary tree”, IEEE
Conference on Computer Vision and Pattern Recognition, 1:782 –787, 2000.

62

Hybrydowe system falkowo-neuronowe jako struktury...
19. Chen W., Qian Q., Wang X.: „Wavelet neural network based transient fault signal
detection and identification”, Internationale Conference on Inf. Com. and Sig.
Proc. ICICS’97, pp. 13771381, Singapore, Sep 1997.
20. Chui C.K.: “An introduction to wavelets; Wavelets – a tutorial in theory and
applications”, Academic Press, San Diego 1992.
21. Ciaccio E.J., Dunn S.M., Akay M.: “Biosignal pattern recognition and interpretation
systems – part 2: Methods of classification”, IEEE Engineering in Medicine and
Biology, pp. 89-94, September 1993.
22. Cover T.M., Hart P.E.: “Nearest neighbor pattern classification”, IEEE Trans. on
Information Theory, pp.21-27, 1967
23. Domenikoni C., Peng J., Gunopulos D.: „Adaptic metric nearest neighbor
classification”, IEEE Conference on Computer Vision and Pattern Recognition, 1:
517-522, 2000.
24. Duda R.O., Hart P.E.: “Pattern classification and scene analysis”, John Wiley &
Sons, New York, 1973.
25. Favata J.T., Srikantan G., Srihari S.N.: “Hand-printed character/digit recognition
using a multiple feature/resolution philosophy”, In IWFHR-IV, pp.54-66, 1994
26. Fukunaga K., “Introduction to statistical pattern recognition”, Academic Press,
NY, 1991.
27. Fukunaga K., Narendra P.: “A branch and bound algorithm for computing k-nearest
neighbors”, IEEE Trans. On Computers, 24:743-750, 1975.
28. Ghosh P., Unger F.: “Cardiac activist care. Cardiac surgery and catheter based
procedures in Europe in 1988. Report of European Heart Institute of the EAS&A”,
Cor Europaeum – European Journal for Cardiac Interventions, Vol.8, No.3, 2000.
29. Guillermo M., Shroyer L.W., Grover F.L., Hammermeister K.E.: “Bayesian-logit
Model for risk assessment in coronary artery bypass grafting”, Ann. Thoracic
Surgery, 57:1492-500, 1994.
30. Gurgen F.: “Neural-network based decision making in diagnostic applications”,
IEEE Engineering in Medicine and Biology, 7/8:89-93, 1999.
31. Hendler J.: “Developing hybrid symbolic/connectionist models”, in Advances in
Connectionist and Neural Computations Theory, J. Barnden & J. Pollack Eds.,
Ablex, 1991.
32. Herrmann C.S.: “Wavelet networks for EEG analysis”, pp. 443-447 EUFIT 1997,
Aachen, Germany, 1997.
33. Hyndman B.W., Kitney R.I., B.McSayers A.: “Spontaneous rhythms in
physiological control systems”, Nature, 233:339-341, 1971.
34. Kangas J.: „Prototype search for a nearest neighbor classifier by a genetic
algorithm”, Proceedings of Third International Conference on Computational
Intelligence and Multimedia Applications – ICCIMA ‘99, pp: 117-121, 1999.
35. Kasabov N.: „Foundations on Neural Networks, Fuzzy Systems and Knowledge
Engineering”, Cambridge, MA: MIT Press, 1996.

63

Paweł Kostka
36. Kim B.S.: Park S.B.: „A fast k-nearest neighbor finding algorithm based on the
ordered partition”, IEEE Trans. On PAMI, 8(6):761-766, 1986.
37. Kocur C.M., Rogers S.K., Myers L. at all, “Using neural networks to select
wavelet features for breast cancer diagnosis”, IEEE EMB, 5/6:95-102, 1996.
38. Koepchen H.P.: “History of studies and concepts of blood pressure waves”, In:
Miyakawa K, Mechanisms of blood pressure waves. Tokyo: Springer-Verlag,
pp.3-23, 1984.
39. Kostka P., Tkacz E., Nawrat Z.: „The comparison of different methods of
modelling and identification of circulation system elements, based on various type
heart valves prostheses”, 15th Eurasip Conference, Biosignal 2000, June 21 to23,
Brno, Czech Republic.
40. Koulouris A., Papakonstantinou G., Tsanakas P.: “A parallel approach to ECG
processing”, Engineering in Medicine and Biology Society, Bridging Disciplines
for Biomedicine, 18th Annual International Conference of the IEEE , 5: 22712272, 1996.
41. Lee T.T., Chang Y.C.: “Approximating nonlinear functions via neural network
based on discrete affine wavelet transformations”, IEEE Symposium on Emerging
Technologies and Factory Automation, pp.174-181, 1994.
42. Liu Ch.L., Nakagava M.: „Prototype learning algorithms for nearest neighbor
classifier with application to hand-written character recognition”, Proceedings
of the Fifth International Conference on Document Analysis and Recognition –
ICDAR ‘99, pp: 378 –381, 1999.
43. McLachlan G.J.: „Discriminant analysis and statistic pattern recognition”, New
York, Wiley, 1992
44. Nagel J., Tkacz E., Reddy S.: „Continuous representation of unevenly sampled
signals – an application to the analysis of heart rate variability”, The 1st Medical
Engineering Week of the World, September 1994, Taipei, Taiwan.
45. Narendra K.S., Parthasarathy K.: „Control of dynamic systems using neural
networks, part II“, IEEE Transactions on Neural Networks, 1(7):4-27, Jan 1996.
46. Narendra K.S., Parthasarathy K.: „Identification an control of dynamic systems
using neural networks“, IEEE Transactions on Neural Networks, 1:4-27, Mar 1990.
47. Pagani M., Lombardi F., Guzzette S.: „Power spectral analysis of heart rate and
arterial pressure variabilities as a marker of sympatho-vagal interaction in man
and conscious dog”, Cir. Res. 59:178-184, 1986.
48. Pal S.K., Pal N.R.: “Soft computing: goals, tools and feasibility”, J. IETE,
42(4,5):195-204, 1996.
49. Pati Y.C., Krishnaprasad P.S.: “Analysis and synthesis of feed-forward neural
networks using discrete affine wavelet transformations”, IEEE Trans. on Neural
Networks, 4:73-85, Jan. 1993.
50. Rasiah A.I., Attikiouzel Y.: “Syntactic recognition of common cardiac
arrhythmias”, Engineering in Medicine and Biology Society, Engineering

64

Hybrydowe system falkowo-neuronowe jako struktury...

51.
52.
53.

54.
55.
56.

Advances: New Opportunities for Biomedical Engineers, Proceedings of the 16th
Annual International Conference of the IEEE, 1: 155 –156, 1994
Ray K.S., Ghoshal J.: “Neuro-fuzzy approach to pattern recognition”, J. Neural
Networks, 10(1):161-182, 1997.
Reul H.: „Blood pumps – general design considarations”, Advances cardiovascular
physics, vol.5, part IV, pp. 55-71, Karger & Basel Edition, 1983.
Ricci F., Avesani P.: “Data compression and local metrics for nearest neighbor
classification”, IEEE Trans. On Pattern Recognition and Machine Intelligence,
21:(4):380-385, 1999.
Rifkin R.D., Hood W.B.: “Bayesian analysis of electrocardiographic exercise
stress testing”. N Engl J Med, 297:681– 6, 1977.
Ripley B.D.: “Pattern recognition and neural networks”, Cambridge University
Press, 1996.
Saxena S.C., Tandon V.K., Gupta S.: “Patient monitoring in ICCU using syntactic
approach”, Engineering in Medicine and Biology Society, 14th Conference
of the Biomedical Engineering Society of India. An International Meeting,

Proceedings of the First Regional Conference, IEEE, pp. 2/1-2/2, 1995.
57. Schalkoff R.: “Pattern recognition: statistical, structural and neural approaches”,
John Wiley & Sons, NY, 1992.
58. Schmermund A., Bailey K.R., Rumberger J.A., Reed J.E., Sheedy F., Schwartz R.S.:
“An Algorithm for Noninvasive Identification of Angiographic Three-Vessel and/
or Left Main Coronary Artery Disease in Symptomatic Patients on the Basis of
Cardiac Risk and Electron-Beam Computed Tomographic Calcium Scores”,
Journal of the American College of Cardiology, 33(2), Elsevier Science Inc. 1999.
59. Tadeusiewicz R.: „Sieci neuronowe”, Akademicka Oficyna Wydawnicza, 1993.
60. Tkacz E.: “Nowe możliwości diagnostyczne analizy zmienności rytmu pracy
serca (HRV)”, Prace Instytutu Biocybernetyki i Inżynierii Biomedycznej, Nr 45,
Warszawa 1996.
61. Tkacz E., Kostka P., Kopała M., Komorowski D.: „An application of wavelet
transform and adaptive filters for decomposition of the hrv signals in case of
patients with coronary artery disease”, World Congress on Medicala Physics and
Biomedical Engineering, Nice, Sep 1997.
62. Tkacz E., Kostka P.: „Zastosowanie systemów waveletowo-neuronowych do
klasyfikacji pacjentów z chorobą naczyniową serca”, XI Konferencja IBIP,
Warszawa, Grudzień 1999.
63.. Tkacz E., Kostka P., Kopała M.: “Ciągła reprezentacja nierównomiernie
spróbkowanych sygnałów, zastosowana do analizy sygnału HRV”, IV Krajowa
Konferencja Sekcji Elektrokardiografii Nieinwazyjnej Polskiego Towarzystwa
Kardiologicznego, Zakopane Kwiecień ’97.
64. Tkacz E.J., Kostka P.: “An Application of Wavelet Neural Network for Classification
Patients with Coronary Artery Disease Based on HRV Analysis”, World Congress
on Medical Physics and Biomedical Engineering – Chicago 2000.

65

Paweł Kostka
65. Tollig C., Hoffman A.J.: “Wavelet neural network for classification of transient
signals”, IEEE, pp. 161-166, 1997.
66. Vetterli M.: “Wavelets and filter banks”, IEEE Transaction on Signal Processing,
9(40):2207-2232, Sep 1992.
67. Weissler A.M.: “Assessment and use of cardiovascular tests in clinical prediction”,
In: Giuliani ER et al, eds. Mayo Clinic Practice of Cardiology. 3rd ed. St. Louis:
Mosby, 400-21, 1996.
68. Wolberg G.: „A syntactic omni-font character recognition system”, International
Journal of Pattern Analysis and Artificial Intelligence, 1(3&4):303-322, 1987.
69. Wrześniowski A., Tkacz E.J., Kostka P., Domider T.: “A new approach to the
holter events classification using wavelet neural network”, International Journal
of Health Care Monitoring, vol. 2, pp. 740-741, May 2001, pp. 301-303.
70 Zadeh L.A.: “Fuzzy logic and soft computing”, issues contensions and
rd
perspectives”, Proc. of 3 International Conference on Fuzzy Logic, Neural Nets
and Soft Computing, pp:1-2, Izuka, Japan, 1994.
71. Zadeh L.A.: “Fuzzy sets”, Information and Control, 8:338-353, 1965.
72. Zhang Q.: “Using wavelet network in nonparametric estimation”, IEEE
Transaction on neural networks, 2(8):227-235, Mar 1997.
73. Zhang Q., Benveniste A.: “Wavelet networks”, IEEE Transaction on Neural
Network 3(6):889-898, Sep 1992.
74. Zhang J., Walter G.G., Miao Y., Wayne Lee W.N.: „Wavelet neural networks for
function learning“, IEEE Transaction on Signal Processing, 6(43):1485-1497,
1995.
74. Zhang Y., Jiao L.: „An efficient method of automatically feature extraction and
target classification“, IEEE, pp. III 179 – III 182, 1998.
76. Yang M.S., Chen C.H.: „On the editet fuzzy K-nearest neighbor rule”, IEEE Trans.
On Systems, Man and Cybernetics, part B: Cybernetics, 28(3):461-467, 1998.
77. Yu D., Zhang A.: “ClusterTree: integration of cluster representation and nearest
neighbor search for image data based”, IEEE International Conference on,
Multimedia and Expo, 3: 1713 –1716, 2000.

66

Rozdział 3
Big Data, NoSQL i Hadoop – masywne,
bezpieczne, odporne na błędy
„bazy dużych danych”
Maciej Rostański
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
mrostanski@wsb.edu.pl
Streszczenie
W rozdziale przedstawiono podstawowe zagadnienia dotyczące
wydajnego przetwarzania dużych ilości danych. Pokazano więc
koncepcję, rozwój i obecną postać problemu określanego w świecie jako „Big Data”. W dalszej części zawarto charakterystykę
systemów, których używa się do wytwarzania, przechowywania
i przetworzenia dużych ilości danych, a także architektury danych
używanych w tym celu. Opisano pokrótce jedno z najważniejszych
rozwiązań w tym obszarze – platformę Hadoop. Dokonano przełożenia relacyjnej bazy danych na formę hipertabeli (NoSQL) na
prostym przykładzie. Całość zamknięto krótką dyskusją na temat
problemów bezpieczeństwa w tym obszarze.

1. Wprowadzenie
Niewiele nowo powstających technologii jest tak odpowiednim uosobieniem
zarówno potencjału, jak i stopnia skomplikowania obecnych technologii IT,
jak tzw. technologie Big Data. Informacje przechowywane są za pomocą zbiorów danych na całym świecie, w ilościach niewyobrażalnych jeszcze dekadę
temu. Wykładniczy wzrost przetwarzanych danych jest efektem wielu przemian, takich jak wzrost klientów różnorodnych urządzeń mobilnych, czy też
wszechobecność sensorów i urządzeń monitorowania używanych we wszystkich sektorach przemysłu i gospodarki. Znakomitym przykładem takiej dzie67

Maciej Rostański
dziny gospodarki jest bioinformatyka, w której jednym z głównych problemów
jest katalogowanie informacji biologicznych, a jego nieodzownym elementem
jest duża ilość danych do wytworzenia, przetworzenia i przechowywania. Co
więcej, owe dane często wzbogacone są o metadane, które zawierają, np. lokalizację powstania, autora, cel czy też nośnik i czas utworzenia.
Rozdział niniejszy przedstawia koncepcję, rozwój i obecną postać problemu, czy też raczej zagadnienia, zwanego „Big Data” (wielkie dane). Pokazane są systemy, których używa się do przetwarzania owych danych, architekturę, za pomocą której można taki system zrealizować, wreszcie przytoczone są typowe problemy, z jakimi można zetknąć się, realizując system
przetwarzający „wielkie dane”. Wg ekspertów sprawa warta jest głębokiego
rozważenia – w 2014 roku 50% danych na świecie będzie przetwarzane
i przechowywane metodami „Big Data” [1].

2. Zjawisko „Big Data”
Postęp w systemach, metodach i technologiach wytwarzania i przechowywania danych na niespotykaną skalę, a także wyłonienie się metodologii zdolnych
do ich przetwarzania, miał w ostatnich latach ogromne znaczenie. Jedną z konsekwencji, zaznaczoną w raportach takich jak [2], jest wzrost trendu używania
w nauce i medycynie metod empirycznych, wypierających metody doświadczalne, na co pozwalają duże zbiory danych, dostarczające wskazań statystycznych.
Przykładem tutaj może być tłumaczenie językowe, które udaje się dzięki bazowaniu na statystyce występowania szukanych zwrotów w posiadanym już zbiorze danych. Dobrym przykładem funkcjonującego komercyjnie, choć jeszcze
w fazie rozwoju oprogramowania, jest MEDgle [3] – portal wspomagający diagnozę na podstawie podanych zestawów symptomów występujących u chorego.

Rys. 1. Witryna portalu MEDgle, źródło: www.medgle.com

68

Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne na błędy...
Użytkownik podaje symptomy, czas ich występowania, oraz podstawowe
dane – wyniki (diagnoza) otrzymuje w postaci możliwych chorób lub zespołów, stanowiących prawdopodobną przyczynę objawów. Baza wiedzy
wykorzystywana przez system jest uzupełniana z danych aktualnych, rzeczywistych baz medycznych.
Jak pisze Pilch-Kowalczyk w [4]: „Pojawienie się i standaryzacja usług
webowych we wczesnych latach 2000-nych, wdrożonych przez duże firmy Internetowe takie, jak Amazon, eBay, Google, Microsoft i wiele innych,
stworzyło technologiczne podstawy świadczenia usług komputerowych
w Internecie. Ważnym elementem był fakt stworzenia przez te firmy skalowalnej softwarowej infrastruktury, w rodzaju MapReduce, Google File System, BigTable, czy też Dynamo.” Przykładowo, w skład chmury Amazon
wchodzą cztery podstawowe usługi: Simple Storage Service (S3); Elastic
Compute Cloud (EC2); Simple Queuing Service; oraz SimpleDB. Innymi
słowami, Amazon oferuje przechowywanie, przetwarzanie komputerowe,
kolejkowanie komunikatów, oraz system zarządzania bazami danych, dostępne przez Internet.

Definicja
Skąd w ogóle szybki rozwój problemów (i ich rozwiązań) w zakresie Big
Data? Raport Gartner, Inc. z roku 2011 pokazał, że prawie połowa menedżerów centrów danych wskazuje, że problemem najwyższej wagi, jest
szybko rosnąca ilość danych. Należy tu zwrócić uwagę, że wyzwania dotyczące danych przedsiębiorstwa, danych przemysłowych, są wyjątkowo
różne od zagadnień dotyczących zwiększania się ilości danych osobistych,
klienckich, czy też publicznych. IT ma za zadanie nie tylko przetwarzanie
różnorodnych informacji w użyteczne, wartościowe dane, ale musi je również przechowywać w sposób umożliwiający automatyzację wykorzystania,
współdzielenia – słowem, utylizacji przez organizację.
Termin Big Data dotyczy więc wszelkich aspektów, związanych z analizą
(często w czasie rzeczywistym) dużych, różnorodnych, szybko zmieniających się zbiorów danych, które stanowią kluczową wartość dla biznesu1.

Studium przypadku: „biocurator”
Bazy danych dostępne online stały się ważnym sposobem publikacji danych
biologicznych. Danych biologicznych również dotyczy wzrost wykładni1

Za: Tomasz Słoniewski, Business Intelligence Portal, http://bi.pl/keyword/15-bigdata

69

Maciej Rostański
czy; aktywność polegająca na organizacji, prezentacji oraz udostępnianiu
biologicznych informacji zarówno użytkownikom (człowiekowi) jak i systemom (maszynom), stała się niezbędną częścią odkryć biologicznych oraz
badań biomedycznych oraz bioinformatycznych (angielskie określenie na
takie działanie to biocuration) [5].
Biologia, jak większość naukowych dyscyplin, jest w fazie przyspieszonego przyrostu informacji, ale także zwiększającej się zależności postępów
naukowych od dostępności danych pochodzących z innych badań. Ośrodki
sekwencjonowania na wielką skalę, laboratoria, ośrodki analityczne produkują ogromne ilości danych, przykładowo pomiary ekspresji genu, sekwencje proteinowe, nukleotydowe czy struktury białkowe. W bazie PubMed
zindeksowano ponad 22 milionów artykułów biomedycznych, a w bazie sekwencji DNA, zwanej GenBank spoczywa ponad 135 milionów sekwencji
genowych.

Problemy z dużymi ilościami danych
Dane tego rodzaju, niezależnie od wysiłku wymaganego przy ich utworzeniu, są przydatne naukowcom jedynie w przypadku zdolności do ich zlokalizowania, integracji, oraz organizacji dostępu. Niezmiernie istotne są tutaj
dwa czynniki typowe dla parametrów określających właściwości systemów
informatycznych:
–

czas dostępu do danych (wartość, która będzie wypadkową stanu i zaawansowania infrastruktury, która służy do przechowywania danych,
konfiguracji i optymalizacji bazy danych, oprogramowania baz danych,
wreszcie – samego projektu bazy danych i pomysłu na ich przechowywanie i opis),

–

dostępność danych.

Szerzej rozwija problem Słoniewski, pisząc iż, „niejednokrotnie w kontekście Big Data wspomina się o tzw. idei czterech „V”:
–

Volume – duża ilość danych,

–

Variety – duża różnorodność danych,

–

Velocity – duża szybkość pojawiania się nowych danych i ich analizy
w czasie rzeczywistym,

–

Value – znacząca wartość danych dla biznesu” [6].

Dla przykładu, wyniki ankiety przeprowadzanej wśród klientów sieci marketów, zawierającej pytania otwarte typu „Co podoba się Państwu w wystroju naszych sklepów?”, będą cechowały się ogromną różnorodnością, ale
i będzie wymagana analiza ich wartości – zmiany wystroju występują prze70

Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne na błędy...
cież dosyć często. Czy po zmianie wystroju dane są już nieistotne? Niekoniecznie – opinie klientów mają przecież nadal znaczenie, jeśli chodzi o ich
odczucia wobec marki, itp., itd.
Istotnym efektem rozwoju Big Data jest rosnący popyt na:
a) oprogramowanie, które wykorzystuje duże ilości danych i uwzględnia
problemy związane z dużą ilością danych i problemem „czterech V”,
jak również,
b) ekspertów w obszarach, które jednocześnie są czynnikiem rozwoju, jak
i czerpią z technologii Big Data, takich jak Machine Learning, sieci
komputerowe, czy nawet statystyka.

Informatyczne wyzwania Big Data
Praca z wielką ilością danych wymaga rozwiązania co najmniej dwóch
głównych wyzwań programistycznych. Pierwszym z nich jest znalezienie
sposobu na efektywne przetwarzanie ogromnej ilości danych, za pomocą
dostępnej mocy obliczeniowej oraz infrastruktury IT. Drugi problem dotyczy sposobu „wyciągania” danych, który różni się znacząco od obecnego,
typowego podejścia bazodanowego, strukturalnego.
Tymczasem, podobnie jak wiele innych metodologii i narzędzi, rozwiązania
opisanych problemów zostały opracowane przez przedsiębiorstwa związane
z siecią Internet. Za przełomowy wielu analityków uznaje rok 2004 i publikację Google [7], w której dowodzi się, że wspomniany wcześniej algorytm zwany MapReduce, może zostać użyty do rozwiązania „zadziwiającej
liczby problemów” przetwarzania danych, a także, że wzory MapReduce,
mogą pozwalać na dynamiczne skalowanie wykonywania programów używających tego algorytmu w zakresie odpowiednio dużego klastra maszyn.
Powstała na bazie tych dokonań implementacja MapReduce w języku Java,
oparta na rozwiązaniach Apache Foundation, o nazwie Hadoop, szybko
stała się używanym na całym świecie środowiskiem działania programów
przetwarzających wielkie ilości danych.

Przetwarzanie rozproszone Big Data. Platforma Hadoop
Hadoop jest rozwiązaniem, dzięki któremu paradygmat zastosowany przez
Google jest dostępny dla każdego; a co ciekawe, jednym z największych
wspierających i stosujących to rozwiązanie było Yahoo. Projekt Apache
Hadoop polegał i polega nadal na rozwoju otwartego oprogramowania dla
przetwarzania danych w sposób niezawodny, skalowalny i rozproszony.
71

Maciej Rostański
Biblioteka Hadoop jest platformą szkieletową (ang. framework) pozwalającą przede wszystkim na rozproszone przetwarzanie dużych zestawów danych, rozmieszczonych w klastrach składających się z wielu węzłów, przy
użyciu względnie prostego modelu programistycznego. Podstawową cechą
tego systemu jest przygotowanie do skalowania od jednego stanowiska po
tysiące węzłów, każdy oferujący lokalne zasoby obliczeniowe i magazynowe. Zamiast polegać na niezawodności sprzętu, który miałby oferować
wysoką dostępność i odporność na błędy, biblioteka Hadoop zawiera mechanizmy wykrywania i radzenia sobie z awariami. Można powiedzieć, że
na klastrze komputerów, każdym z nich podatnym na awarie, realizuje się
wysoko dostępną usługę, niwelującą ten problem w warstwie aplikacji.
Idea polega na zastosowaniu jako węzłów klastra Hadoop typowych jednostek serwerowych – relatywnie niedrogich dla organizacji, która dzięki temu
łatwo może planować rozrastanie się systemu.
Istnieje parę dystrybucji systemu Hadoop, wśród nich zarówno typowe
open-source, jak i komercyjne rozwiązania:
–

Apache Hadoop,

–

Cloudera [8],

–

Hortonworks [9],

–

MapR [10],

–

GreenplumHD [11],

–

IBM InfoSphere BigInsights [12].

3.2. Specyfikacja działania Hadoop
Apache Hadoop nie jest właściwie pojedynczym produktem. Bardziej właściwym byłoby określenie Hadoop jako „kolekcji” kilku składników, z których podstawowe to:

72

–

MapReduce – platforma do konstrukcji aplikacji przetwarzających dane
strukturalne i niestrukturalne w sposób rozproszony,

–

Hadoop Distributed File System – niezawodny i rozproszony system
plików oparty o Java – pozwala na przechowywanie i szybkie pobieranie plików dużych rozmiarów poprzez klastry dużej liczby typowych
serwerów,

–

Hive – magazyn danych (zachowanych w HDFS), pozwalający na szybkie podsumowania i zapytania poprzez interfejs podobny do SQL,

–

Pig – platforma do przetwarzania i analizy dużych zestawów danych,

Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne na błędy...
–

HBase – system składowania danych NoSQL umożliwiający aplikacjom użytkownika nagły dostęp do zapisu/odczytu do dużych zasobów
danych w czasie rzeczywistym,

–

HCatalog – usługa zarządzania tabelami i metadanymi, która dostarcza
w sposób scentralizowany interfejs systemom przetwarzania danych do
struktury i lokalizacji danych zawartych w systemie Apache Hadoop.

Hadoop jest zarówno rozproszonym systemem plików jak i środowiskiem
wykonywalnym – określa organizację danych, dzieli zadania na wiele podzadań, harmonogramuje je biorąc pod uwagę lokalizacje danych oraz topologię systemu, jak również zajmuje się obsługą błędów. Jako, że z punktu
widzenia programisty wdrażającego odpowiedni kod, nie trzeba zajmować
się efektywnością poszczególnych węzłów klastra, system jest niejako automatycznie skalowalny w poziomie. Do tego, Hadoop stanowi obecnie platformę, dla której dostępne jest wiele rodzajów interfejsów, upraszczających
obsługę algorytmów MapReduce, zwiększając tym samym zdolności odbiorców do wykorzystywania skalowalnej architektury.
Aczkolwiek Hadoop stanowi sprawdzoną platformę do przetwarzania danych, nie jest jednakże zaprojektowany, aby odpowiadać z właściwą szybkością na pojedyncze żądania użytkowników końcowych. „Wyciąganie”,
czy też wybieranie danych, w sposób wymagany przez obecne systemy
i użytkowników końcowych, wymaga innego zestawu rozwiązań.

„Wybieranie danych” – szybki dostęp do Big Data
Krótki czas realizacji odpowiedzi na żądania, jest jednym z najbardziej
krytycznych warunków działania systemów sieciowych, webowych, operujących na wielkich ilościach danych. Biznes wymaga wysoko dostępnych, szybko odpowiadających usług, co de facto przekreśla szanse jakiegokolwiek pojedynczego systemu jako rozwiązania. Jak już wspomniano,
operatorzy usług przyjęli architekturę skalowalną poziomo – zakładającą
mocno rozproszone oprogramowanie w połączeniu z wdrożeniem dużej
ilości podstawowych serwerów owego oprogramowania. Takie rozwiązanie nazywane jest skalowaniem horyzontalnym (ang. scale-out). Szerszą
dyskusję nad rodzajami skalowalności systemów zaprezentowano w [Systemy przetwarzania danych]. System składowania musi posiadać zdolność
nie tylko do powiększania rozmiarów, ale zachować przy tym wydajność
i funkcjonalność. Dodawanie nośników do macierzy dyskowej, nie spełnia
tego warunku – wraz ze zwiększeniem pojemności, nie poprawia się dostępne pasmo oraz moc obliczeniowa.

73

Maciej Rostański

Zagadnienia skalowania horyzontalnego
Systemy składowania skalowalne horyzontalnie typowo składają się z węzłów, wnoszących do systemu własne parametry – pojemność, moc obliczeniową oraz przepustowość wejścia/wyjścia. W momencie dodania węzła,
wszystkie trzy parametry są podniesione w tym samym czasie. Kluczowym
składnikiem jest tutaj oprogramowanie, pozwalające na połączenia węzłów
w taki sposób, by umożliwić odwołania się do nich jako do pojedynczego
obiektu, utworzonego przez klaster. Kolejną główną funkcją owego oprogramowania, jest zdolność do wsparcia bardzo dużych przestrzeni danych.
Celem głównym jest w tym przypadku utworzenie jednej przestrzeni rozciągającej się do praktycznie każdego rozmiaru i wspierającego szereg aplikacji i systemów plików. Oprogramowanie musi być świadome operowania
w klastrze i mieć możliwość wykorzystania różnej mocy obliczeniowej węzłów i ich zasobów wejścia/wyjścia.
Ponieważ tradycyjne bazy danych często nie są zdolne do skalowania
do poziomu wymaganego przez duże ilości danych, pojawiła się pewna
liczba rozwiązań alternatywnych. Może to być, często używane obok tradycyjnego rozproszonego systemu baz danych, wdrożenie rozproszonego
systemu cache, zapamiętujących rezultaty zapytań. Taki system rozproszonych pamięci cache (przykładem jest memcached) jest zaprojektowany, aby znacząco zmniejszyć narzut pracy bazy danych poprzez zmniejszenie ilości zapytań.

Przykłady baz danych NoSQL
Kompletnym i jednym z najbardziej znanych systemów jest open-source’owy produkt Apache Foundation o nazwie Cassandra [http://cassandra.apache.org/]. Opisywana jako „rozproszony system składowania dla zarządzania bardzo dużymi ilościami strukturalnych danych rozłożonych pomiędzy
serwerami”, Cassandra jest bazą danych nowoczesnego typu – skalowalną,
zorientowaną na krótki czas odpowiedzi, trwałą bazą danych. Ciekawym
rozwiązaniem, charakterystycznym dla ery Big Data, jest realizacja w Cassandrze modelu „ewentualnej konsystencji” – oznacza to brak potrzeby stałej spójności, natychmiast osiąganej na wszystkich serwerach. Rozwiązanie
takie jest jednak kwestionowane przez niektórych liderów NoSQL, jak na
przykład HyperTable.

Studium przypadku: Hypertable
HyperTable jest opartą na rozwiązaniu Google BigTable bazą danych z własnościowym mechanizmem wyszukiwania HQL (Hypertable Query Langu74

Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne na błędy...
age). Jest dobrym przykładem innego sposobu zapisu danych niż w typowej, relacyjnej bazie danych.
Cytując dokumentację producenta [http://hypertable.com/documentation/],
Hypertable jest podobny do bazy relacyjnej w tym, iż przedstawia dane jako
tabele z informacjami, z rzędami i kolumnami, ale ta analogia na tym się
kończy. Przykładowe różnice:
–

klucze rzędów są łańcuchami znaków UTF-8,

–

brak wsparcia dla typów danych, wartości są traktowane jako sekwencje bajtów,

–

brak wsparcia dla złączeń,

–

brak wsparcia dla transakcji.

Jak można przedstawić najlepiej ideę przechowywania danych przez Hypertable? Na przykładzie. Tradycyjna baza relacyjna zakłada, że każda kolumna zdefiniowana w schemacie tabeli będzie miała wartość w każdym
rzędzie, który jest obecny w tabeli. Wartości NULL zwykle reprezentowane
są przy użyciu specjalnego znacznika. Klucz główny i identyfikator kolumny, są niejawnie związane z każdą komórką na podstawie fizycznej pozycji
w układzie. Tabela 1 pokazuje rozkład na dysku omawianej tabeli.
Tabela 1. Rozkład na dysku tabeli w relacyjnej bazie danych

Rzecz
Jabłka
Szparagi
Banany
Winogrona
Cebule
Brzoskwinie

Data
2011-10-29
2011-10-30
\N
\N
2011-10-27
\N

Ilość
60
34
\N
\N
66
\N

Dostawca
Figoni
Giusti Farms
\N
\N
Pastorino
\N

Projekt Hypertable spłaszcza strukturę tabeli w posortowaną listę par klucz/
wartość, każda reprezentująca komórkę w tabeli. Klucz zawiera pełen identyfikator rzędu i kolumny, co oznacza, że każda komórka posiada pełną
informację o adresowaniu. Komórki zawierające NULL po prostu nie są
dołączone do listy, co bardzo podnosi przydatność takiego sposobu zapisu
dla danych rozrzuconych. Poniższa tabela ilustruje sposób przechowywania
danych przez Hypertable.

75

Maciej Rostański
Tabela 2. Rozkład na dysku tabeli w Hypertable

Jabłka
Jabłka
Jabłka
Szparagi
Szparagi
Szparagi
Cebule
Cebule
Cebule

KLUCZ
Data
Ilość
Dostawca
Data
Ilość
Dostawca
Data
Ilość
Dostawca

WARTOŚĆ
2011-10-29
60
Figoni
2011-10-30
34
Giusti Farms
2011-10-27
66
Pastorino

Pomimo, iż taki sposób zapisu powoduje znaczną ilość redundantnych danych (klucze rzędów oraz identyfikatorów kolumn), Hypertable uruchamia
mechanizmy kompresji, które znacząco zmniejszają problem.
Do tego podstawowego sposobu przechowywania danych, który można
nazwać spłaszczeniem tradycyjnego dwuwymiarowego modelu tabeli, Hypertable dodaje inny wymiar: znacznik czasowy. Znacznik czasowy (timestamp) można rozumieć jako sposób reprezentowania różnych wersji tej samej komórki tabeli. Podczas zapytania, najpierw zwracane są ostatnie wersje komórek. Przechowywane są jednak wszystkie wersje komórki (chyba,
że baza zostanie skonfigurowana inaczej).

Architektura przetwarzania dużych danych
Jak już wspomniano, Google było jednym z pierwszych przedsiębiorstw
rozpoznających problem dużych danych – i to właśnie w Google stworzono
wewnętrzną architekturę do obliczeń rozproszonych (MapReduce) i rozproszony system plików (Google File System), rozwiązania pozwalające na
niezmiernie równoległe przetwarzanie dużych ilości danych. Hadoop jest
bezpośrednim „potomkiem” tej koncepcji. Ponieważ architektura Hadoop
ma za zadanie rozwiązać dwa podstawowe, opisane problemy, najważniejsze są dwie usługi: warstwy danych (opisywany Hadoop Distributed File
System) oraz warstwy równoległego przetwarzania (MapReduce).
HDFS złożony jest z dwóch składników:
1. NameNode (węzła nazw) – węzeł ten jest odpowiedzialny za wszystkie
operacje na metadanych oraz utrzymywanie metadanych w przestrzeni
nazw. W przypadku awarii węzła NameNode, HDFS staje się niedostępny.
76

Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne na błędy...
2. DataNode (węzła danych) – węzeł ten jest odpowiedzialny za interfejs
wejścia-wyjścia i operacje na właściwych danych. We wdrożeniach fizycznych węzeł ten jest pojedynczą instancją, która może być podatna
na awarie, jednakże w konfiguracji zwirtualizowanej uzyskuje się wysoką dostępność.
Na MapReduce składają się dwie główne części:
1. Job Tracker, będący składnikiem odpowiedzialnym za harmonogramowanie i zarządzanie przydzielaniem zadań i kolejkami. Najczęściej realizowany jako pojedyncza instancja, chyba, że w grę wchodzi realizacja w odpowiednio wysokiej dostępności.
2. Task Tracker, odpowiedzialny za właściwe wykonanie algorytmu
MapReduce oraz zarządzanie zadaniami.
Na rys. 2 pokazano ułożenie opisanych warstw względem siebie oraz sposób interakcji z klientami.

Rys. 2. Architektura Hadoop, źródło: [7]

Studium przypadku: wirtualizacja węzłów
Prostota skalowalności polega na zaprojektowaniu opisywanych rozwiązań
w sposób przewidujący częste „dokładanie” i zmianę ilości węzłów (komputerów) w systemie.
77

Maciej Rostański
Przykładem infrastruktury, w której zastosowano koncepcję wykorzystania
wielu niedrogich jednostek do stworzenia pełnego magazynu danych (ang.
storage), może być Nutanix Complete Cluster. W tym przypadku realizowana jest wielowęzłowa baza NoSQL, a cała struktura jest skalowalna w modelu „scale-out”.

Rys. 3. Architektura wielowęzłowa bez sieci SAN, źródło: [13]

W przytoczonej architekturze (rys. 3) klaster składa się z serwerów, na
których uruchomione są hyperwizory, oraz które zawierają procesory, pamięć oraz lokalne zasoby dyskowe (w tym SSD). Każdy węzeł służy nie
tylko do uruchamiania maszyn wirtualnych, jak w typowym rozwiązaniu
wirtualizacyjnym, ale dodatkowo, lokalne zasoby dyskowe są zwirtualizowane w zunifikowaną pulę dostępną poprzez własnościowy protokół
SOCS (ang. Scale-out Converged Storage). Rezultatem jest osiągnięcie
możliwości sieci SAN dla składowania danych maszyn wirtualnych – maszyny uruchomione w klastrze zapisują dane do SOCS tak, jakby były
zapisane do zasobów sieci SAN. Ponieważ główne zadanie SOCS polega
na trzymaniu danych blisko (lokalnie) maszyny wirtualnej, można stwierdzić dość dużą wydajność w typowych warunkach działania, przy dużo
niższym koszcie. Jest to rozwiązanie skalowalne horyzontalnie – od kilku
do dużej liczby węzłów.

Zagadnienia bezpieczeństwa
Wiele organizacji do zabezpieczania systemów przetwarzania wielkiej ilości danych stosuje produkty bezpieczeństwa IT, przeznaczone do ochrony
„tradycyjnych” systemów IT. Jakkolwiek każda ochrona jest lepsza niż żadna, należy zdawać sobie sprawę, że rozwiązania nie zaprojektowane specjalnie dla Big Data, często wprowadzają ograniczenia w sposobie wdroże78

Big Data, NoSQL i Hadoop – masywne, bezpieczne, odporne na błędy...
nia, w możliwościach skalowania, a także mogą ograniczać funkcjonalność
klastra Big Data na różne sposoby.
Wśród głównie obecnie wykorzystywanych technologii wspomagających
bezpieczeństwo, jednocześnie zaprojektowanych z myślą o rozproszonych,
mocno skalowalnych rozwiązaniach IT, należy wymienić (w dużym skrócie):
–

Kerberos – wspomaga zadanie uwierzytelniania węzłów, z myślą o zapewnieniu ochrony przed niechcianym kopiowaniem danych bądź uruchomieniem niepożądanych zapytań.

–

TLS – zapewnia mechanizmy bezpiecznej komunikacji pomiędzy
wszystkimi węzłami i usługami. Jest to konieczne ze względu na to, że
mechanizmy wbudowane w oprogramowanie klastra, nie muszą takiej
funkcjonalności zapewniać, np. Hadoop pozwala na bezpieczną wymianę transakcji z klientem, ale już nie dla komunikacji pomiędzy węzłami
tego samego klastra.

–

Szyfrowanie w warstwie systemu plików – z założenia zapewnia ochronę zawartości klastra przed tymi, którzy nim administrują. Szyfrowanie
na tym poziomie chroni dane przez podejrzeniem plików, a nawet przed
inspekcją migawki zdjętej z systemu i przeniesionej na inny serwer.

Dodatkowo, zagadnienia zachowania prywatności i ustawodawstwo w tym
zakresie, stanowią istotny problem daleki od satysfakcjonującego rozwiązania. Zagadnienia warte szczegółowego rozpatrzenia dla każdego przypadku
przetwarzania w chmurze to: specyficzne zasady i warunki oferowane przez
usługodawcę; elastyczność uzgodnień z usługodawcą; warunki zakończenia lub wypowiedzenia umowy; oraz prawne i praktyczne komplikacje przy
przemieszczaniu określonych typów danych z obiektu [4].

Literatura
1. Witryna HortonWorks http://hortonworks.com/what-is-apache-hadoop/
2. Big Data – It’s not just for Google Any More. The Software and Compelling
Economics of Big Data Computing.
3. Portal MEDgle – http://medgle.com
4. Rostański M. (red.): Systemy przetwarzania danych, WSB 2012.
5. Howe D. et al., Big data: The future of biocuration, Nature. 2008 September 4;
455(7209): pp. 47-50.
6. Słoniewski T.: Od BI do „Big Data”, Artykuł na portalu Business Inteligence,
http://bi.pl/

79

Maciej Rostański
7. Dean J.: MapReduce: Simplified Data Processing on Large Clusters, Google Inc.,
USENIX Association OSDI ’04: 6th Symposium on Operating Systems Design
and Implementation.
8. Witryna firmy Cloudera – www.cloudera.com
9. Witryna główna firmy HortonWorks – http://hortonworks.com
10. Witryna firmy MapR http://www.mapr.com/
11. Witryna firmy Greenplum http://www.greenplum.com/products/greenplum-hd
12. http://www-01.ibm.com/software/data/infosphere/biginsights/whats_new.html
13. Hadoop on Nutanix: Reference Architecture, ©Nutanix 2012.
14. Magazyn Dark Reading – How do you secure big data environments? November
2012.

80

Rozdział 4
Wykorzystanie systemów teleinformatycznych
i pomiarowych do dystrybucji danych na
potrzeby prognozowania infekcji roślin
uprawnych
Paweł Buchwald
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
pawel@buchwald.com.pl

Streszczenie
Wykorzystanie szeroko pojętych sieci teleinformatycznych w dystrybucji danych telemetrycznych daje nowe możliwości w różnych aspektach działalności człowieka. Rozdział przedstawia
koncepcję akwizycji danych pomiarowych za pomocą sieci teleinformatycznych, oraz urządzeń pomiarowych stosowanych
w meteorologii i ocenie warunków wegetacji roślin. Problematyka akwizycji najświeższych danych pozwalającą na ocenę warunków wegetacji roślin uprawnych jest istotna z punktu widzenia
przewidywania i oceny prawdopodobieństwa infekcji upraw, jak
również oceny rozprzestrzeniania się czynników chorobotwórczych. Rozdział przedstawia koncepcję działania rozproszonego
systemu pozwalającego na akwizycję danych pomiarowych, przy
użyciu ogólnie dostępnej aparatury pomiarowej i sieciowej.

1. Wstęp
Szybki postęp w dziedzinie sieci komputerowych, a zwłaszcza rozwiązań
działających w oparciu o bezprzewodową transmisję danych sprawił, iż
możliwe stało się zastosowanie mechanizmów zdalnej kontroli i akwizy81

Paweł Buchwald
cji danych, również w zastosowaniach monitoringu warunków wegetacji
roślin. Jednym z ważniejszych czynników, które wpływają na prawidłowy
rozwój roślin, są czynniki meteorologiczne. Występowanie sprzyjających
warunków meteorologicznych do rozwoju pewnych czynników chorobowych, powinno zainicjować odpowiednie przeciwdziałania, np. zastosowanie środków chemicznych ochrony roślin. Duże znaczenie ma nie tylko
dobór odpowiedniego środka, ale również czasu jego zastosowania. Prawdopodobieństwo wystąpienia infekcji chorobowej roślin, może być oszacowane na podstawie modeli matematycznych, które uwzględniają istnienie
potencjalnych czynników chorobotwórczych w otoczeniu, oraz sprzyjające
warunki środowiskowe ich rozwoju. Podstawowym warunkiem prawidłowego oszacowania prawdopodobieństwa wystąpienia infekcji, jest dostarczenie aktualnych danych i zastosowanie rozproszonego sytemu teleinformatycznego. Dzisiejsze urządzenia stosowane w telemetrii, oraz systemy
akwizycji danych, dają możliwość praktycznego zastosowania takich modeli. W rozdziale przedstawiono zarys problemu predykcji wystąpienia parcha jabłoni, przy zastosowaniu popularnego modelu oceny wystąpienia tej
choroby. Przedstawiono również koncepcję działania teleinformatycznego
systemu, wykorzystującego dostępne na rynku urządzenia pomiarowe i teletransmisyjne, służące do monitorowania warunków pogody.

2. Problem oceny ryzyka wystąpienia parcha jabłoni na
podstawie warunków meteorologicznych
Parch jabłoni jest chorobą o podłożu grzybicznym. Dotyka większość gatunków drzew z rodzaju jabłoni (Malus Mill). Jest on powodowany przez
grzyb Venturia inaequalis, który należy do typu workowców i rodziny Venturiaceae. Parch jabłoni można zaliczyć do jednej z najpospolitszych chorób, która powoduje znaczne obniżenie plonów, zmniejszenie ich wartości
i osłabienie drzew owocowych. Ponadto metabolity grzyba, które pozostają
w tkankach owoców mogą mieć dla człowieka właściwości kancerogenne.
Ze względu na duże niebezpieczeństwo dla upraw jakie niesie ze sobą ta
choroba, od dłuższego czasu podejmuje się próby opracowania skutecznych modeli, które pomogłyby w trafnej ocenie jej rozwoju, a tym samym
przyczyniłyby się do jej zapobiegania. Jednym z modeli pozwalających na
przewidywanie wystąpienia parcha jabłoni jest model Millsa, który obecnie ma charakter historyczny. Model ten został przedstawiony w rozdziale
na potrzeby zobrazowania samego procesu predykcji prawdopodobieństwa
wystąpienia choroby, oraz ukazanie roli, jaką pełnią aktualne dane o pogodzie w generowaniu prognoz rozwoju choroby.
82

Wykorzystanie systemów teleinformatycznych i pomiarowych...

2.1. Prognozowanie ryzyka wystąpienia parcha jabłoni na
podstawie modelu Millsa
Model Millsa pozwala na sygnalizację okresów krytycznych, w których następuje intensywny rozwój czynników chorobowych parcha jabłoni. W przypadku wystąpienia owych czynników, powinna nastąpić sygnalizacja o zbliżającym się terminie zabiegów zapobiegawczych – takich jak opryski substancjami ochrony roślin. Wyznaczenie sygnalizacji okresów krytycznych,
odbywa się poprzez detekcję warunków koniecznych do zaistnienia infekcji, jak również obserwację wysiewu zarodników workowych. Przewidzenie
początku trwania korzystnych warunków dla czynników chorobotwórczych
powoduje, iż możliwe jest wczesne wdrożenie ochrony chemicznej. Infekcja
chorobotwórcza charakteryzuje się wystąpieniem trzech faz:
–

Faza saprofityczna – zimowanie, rozpoczyna się w chwili opadania
z drzew porażonych liści i trwa do czasu wysiewu pierwszych zarodników workowych.

–

Faza infekcji pierwotnej – rozpoczynająca się w momencie pojawienia
się zarodników workowych.

–

Faza infekcji wtórnych – rozpoczynająca się w momencie pojawienia
się zarodków konidialnych.

Trafne uchwycenie okresu zakończenia trwania korzystnych warunków dla
rozprzestrzeniania się grzyba chorobotwórczego, pozwala na zmniejszenie
lub zaprzestanie stosowania ochrony chemicznej. Wpływa to na oszczędności, oraz jakość plonów. Model Millsa przewiduje zastosowanie danych
dotyczących średniej temperatury powietrza i minimalnego okresu zwilżenia liścia, w ocenie warunków koniecznych dla infekcji parchem jabłoni.
Zależności opracowane przez Millsa przedstawiono w tabeli 1.
Z praktycznych obserwacji wynika, iż nie wszystkie warunki, które są sygnalizowane przez tabelę Millsa jako sprzyjające występowaniu infekcji,
prowadzą faktycznie do wystąpienia zachorowania roślin. W praktyce tylko
część wyznaczonych za pomocą modelu Millsa okresów sprzyjających infekcji, jest związanych z wywołaniem faktycznej choroby. Jest to spowodowane tym, iż tabela Millsa bierze pod uwagę jedynie czynniki wilgotnościowo-temperaturowe. Co ciekawe trafność predykcji zachorowań na podstawie tabeli Millsa, jest różna w zależności od regionu. Trafność wskazań
według tabeli Millsa w porównaniu do rzeczywistego zagrożenia ze strony
parcha jabłoni, zostały przedstawione w tabeli 2.

83

Paweł Buchwald
Tabela 1. Zależności Millsa do wyznaczania prawdopodobieństwa zarażeniem
parchem jabłoni

Średnia temperatura
st. Celsjusza
poniżej 0,5
0,5-5
6
7
8
9
10
11
12-13
14-15
16-24
25
26

Okres zwilżenia liścia
w godzinach
–
ponad 48
25
20
19
15
14
12
11
10
9
11
13

Tabela 2. Trafność wskazań za pomocą tabeli Millsa – źródło [3]

% nietrafnych wskazań
według tabeli Millsa
Kraj
Rok
Autor badań
(brak porażenia, mimo
wskazań o zagrożeniu)
71
Soenen 1956
Belgia
73
1952-1959 Soenen 1960
19
1962-1985 Oberhofer 1985
Włochy
35
1986-1992
Waldner
32
Cesari 1992
(Południowy
52
1991-2000 Beratungsring
Tyrol)
67
2001
Laimburg
Z przedstawionych danych w tabeli 2 wynika, iż trafność wskazań na podstawie tej metody jest niewystarczająca. Z tego względu należało przedsięwziąć badania prowadzące do opracowania efektywniejszych metod z wykorzystaniem dodatkowych czynników.

2.2. Zmodyfikowane metody predykcyjne bazujące na
modelu Millsa
Opracowano również metody detekcji infekcji pierwotnej i wtórnej parchem jabłoni, na podstawie analiz przeprowadzonych przez innych badaczy.
W wyniku tych analiz, zależności podane przez Millsa były modyfikowane, jednak zasada wykorzystania modelu pozostała bez zmian. Modyfikacje
84

Wykorzystanie systemów teleinformatycznych i pomiarowych...
wartości określających zależności pomiędzy zwilżeniem liścia i temperaturą,
a zakażeniem parchem, jabłoni miały na celu zwiększenie trafności oceny
w różnych warunkach regionalnych. Zależności te przedstawiono w tabeli 3.
Tabela 3. Modyfikacje zależności Millsa – źródło [3]

Konieczny czas zwilżenia ilości w godz.
Średnia
infekcja pierwotna
infekcja wtórna
wg MacHardy
wg MacHardy
temperatura wg Mills/Jones
i Gadoury/
i Gadoury
w °C
(1980)
Stensvand (1997)
(1989)
–
48
41
1,0
–
30
2,7
41
28
21
26
5,0
19
13
16
7,8
10,0
16
11
14
14
9
12
11,1
12
8
11
13,0
12-11
7
10
14-15
6
10-7
16-23
9
11
11
8
25,0
Model Millsa powstał w latach 50 ubiegłego stulecia i przeszedł pewne modyfikacje. Okazuje się, iż przy niższej temperaturze, okres zwilżenia liści wymagany do zajścia infekcji, może być krótszy. Ponadto, przy samodzielnym wyznaczaniu okresów infekcji pierwotnych, zalecane jest zliczanie godzin zwilżenia
liści od momentu rozpoczęcia deszczu, gdyż z tym związane jest pojawienie się
zarodników workowych w powietrzu. Przyjmuje się także, że jeśli okres suszy
między dwoma okresami zwilżenia jest krótszy niż 24 godziny, to przy wyznaczeniu okresów krytycznych należy sumować wszystkie godziny zwilżenia
liści, nie należy natomiast wliczać tych godzin, kiedy liście były suche. Według
badań amerykańskich, tabela Millsa również prognozuje dokładniej infekcje,
jeśli czas po wyschnięciu liści, w którym wilgotność względna powietrza jest
równa lub wyższa niż 90%, dodany zostanie do czasu zwilżenia liścia.
Obecnie znane są dodatkowe czynniki, które determinują rozwój grzybów
odpowiedzialnych za zakażenie parchem jabłoni. Wśród nich wyróżnia się
nasłonecznienie. Opracowany przez Millsa model był dostosowany do wykorzystania bez wsparcia infrastruktury teleinformatycznej, która może być
stosowana w chwili obecnej. Z tego względu ma on charakter uproszczony
i nie uwzględnia wszystkich czynników.
Analiza dodatkowych czynników może przynieść lepsze rezultaty. Początek wysiewów zarodników workowych, oraz dynamikę wysiewów można
sprawdzić stosując urządzenie bazujące na zasadzie działania aparatu Bur85

Paweł Buchwald
karda, którego model przedstawia rysunek 1. Jest to urządzenie dokonujące
pomiarów metodą objętościową, wyłapując cząsteczki zawieszone w powietrzu w sposób selektywny, wykazując ograniczoną efektywność w stosunku do zarodników o wymiarach poniżej 5 mikronów [1]. Tym samym
takie pomiary umożliwiają bardzo dobrą ocenę stężenia w powietrzu ziaren
pyłku roślin i zarodników z rodzaju Alternaria, Cladosporium, czy Epicoccum. Pomiary nie są w stanie zapewnić pełnej oceny obecności zarodników,
których rozmiary nie przekraczają 5 mikronów.
Dokładny czas i intensywność zwilżenia liścia, może być również wyznaczony za pomocą dzisiejszych ogólnie dostępnych elektronicznych urządzeń
pomiarowych, wyposażonych w specjalny czujnik zwany sztucznym liściem.
Pozwala on na dokładny pomiar czasów i intensywności zwilżenia liścia,
bez konieczności stosowania metod bazujących na przybliżeniach. Można
stwierdzić, iż zastosowanie infrastruktury pomiarowej i urządzeń do akwizycji danych, przyczynia się do ułatwienia pracy sadownikom, oraz do rozwoju
modeli wspomagających ocenę zagrożenia infekcji roślin uprawnych.

Rys. 1. Model działania aparatu Burkarda (źródło: www.aero.cm-uj.krakow.pl)

86

Wykorzystanie systemów teleinformatycznych i pomiarowych...

3. Architektura systemu pozwalającego na akwizycję
danych pomiarowych dla potrzeb określenia ryzyka
infekcji roślin uprawnych
Modele predykcyjne wspomagające ocenę prawdopodobieństwa zachorowania roślin, są nastawione na możliwość praktycznego i łatwego zastosowania
w warunkach terenowych. Modele opracowane przed erą dynamicznego rozwoju technologii informatycznych, nie mogły brać pod uwagę wielu czynników, których ciągłe monitorowanie było kłopotliwe. Obecnie możliwe jest
wsparcie ze strony ogólnie dostępnych urządzeń pomiarowych, oraz akwizycja danych przy użyciu sieci bezprzewodowych, w tym sieci GSM. Wpływa
to korzystnie na możliwość otrzymania aktualnych danych. Wykorzystanie
nowoczesnej aparatury pomiarowej, daje możliwość kolekcjonowania danych historycznych, oraz opracowywania systemów ekspertowych oceny
zagrożenia chorobami roślin, które będą dostosowane do indywidualnych potrzeb odbiorców i warunków geolokalizacyjnych. Architetura systemu pomiaru i akwizycji warunków klimatycznych została przedstawiona na rysunku 2.

Rys. 2. Architektura systemu akwizycji i składowania danych pomiarowych

87

Paweł Buchwald
Koncepcyjna architektura systemu akwizycji danych pomiarowych ma charakter wielowarstwowy. Dzięki temu możliwe staje się dostosowanie systemu informatycznego do nowych urządzeń pomiarowych. Ze względu na
szerokie spektrum urządzeń pomiarowych, które są stosowane dla potrzeb
kontroli warunków wegetacji roślin oraz warunków meteorologicznych,
jest to jedna z kluczowych cech projektowanego systemu. Hierarchiczny
charakter systemu pozwala również na możliwość integracji rozmaitych heterogenicznych podsystemów, które służą do zadań pomiarowych z jednym
systemem przechowywania i analizy danych. Do tego celu możliwe staje się
wykorzystanie w chwili obecnej standardów komunikacyjnych i urządzeń
używanych powszechnie do budowy sieci komputerowych, oraz intersieci. Koncepcja prezentowanego systemu akwizycji danych zakłada istnienie
trzech podstawowych warstw:
 Warstwa terenowych czujników i urządzeń pomiarowych
W prezentowanej architekturze systemu, warstwę tę będą reprezentowały stacje meteorologiczne, urządzenia kontroli warunków wegetacji roślin, oraz inne dedykowane urządzenia do pomiaru występowania czynników, mających wpływ na infekcje roślin daną jednostką chorobową.
Dla rozpatrywanego w tym rozdziale parcha jabłoni, zastosowanie będą
miały czujniki temperatury, czujniki zwilżenia liścia, deszczomierz,
czujniki wilgotności powietrza, oraz czujniki aparatu Burkarda. Urządzenia pełniące bezpośrednią funkcję pomiarową, są połączone przy
pomocy mikrosieci z lokalnymi stacjami pomiarowymi. Lokalne stacje
pomiarowe mają charakter dedykowanych urządzeń elektronicznych.
Ze względu na konieczność pracy w warunkach terenowych, urządzenia te muszą cechować się małym poborem mocy, oraz koniecznością
zachowania ciągłości działania, niezależnie od warunków meteorologicznych. Ta warstwa systemów pomiarowych, jest często konstruowana w oparciu o gotowe zestawy pomiarowe lub stacje meteorologiczne, wyposażone w dedykowane czujniki. Lokalne stacje pomiarowe
zapewniają konwersję formatów danych, oraz posiadają podstawowe
mechanizmy kontroli błędów transmisyjnych, jak również możliwość
lokalnego składowania danych pomiarowych w pamięci wewnętrznej.
W większości tego typu urządzenia są wyposażone w interfejs RS232,
który za pomocą dedykowanych urządzeń można przekonwertować na
inne protokoły komunikacyjne.
 Warstwa akwizycji danych pomiarowych
Urządzenia pomiarowe wykorzystywane w telemetrycznych systemach
kontroli warunków wegetacji roślin, bazują na dedykowanych standardach transmisji danych oraz własnych protokołach transmisyjnych.
88

Wykorzystanie systemów teleinformatycznych i pomiarowych...
Obecnie w akwizycji danych stosuje się ujednolicone standardy komunikacji takie, jak protokół TCP/IP czy standard sieci bezprzewodowych
802.11. Skonstruowanie warstwy akwizycji danych w oparciu o powszechnie wykorzystywane standardy, pozwoli zastosować urządzenia
i rozwiązania będące w użyciu sieci komputerowej jednostek badawczych lub organizacji prowadzących monitoring warunków wegetacyjnych roślin. W szczególności mogą to być urządzenia typu router klasy
SOHO, które są relatywnie tanie i ogólnie dostępne dla indywidualnych
klientów. Wszędzie tam, gdzie istnieje potrzeba zapewnienia wysokiej
dostępności aktualnych danych, niezawodności działania, zalecane jest
zastosowanie bardziej niezawodnego sprzętu sieciowego. Dla potrzeb
akwizycji danych z odległych urządzeń terenowych, ważnym aspektem
jest możliwość wykorzystania infrastruktury sieciowej telefonii komórkowej GSM. Ze względu na stosunkowo małą objętość transmitowanych danych pomiarowych, transmisja taka może być zrealizowana
za pomocą usługi GPRS. Możliwe jest również skorzystanie z usług
przystosowanych do zadań telemetrycznych, które są oferowane przez
firmy telekomunikacyjne na polskim rynku. Dla potrzeb systemów telemetrycznych, ważnym czynnikiem parametrów łącza realizowanego
w oparciu o sieć GSM, jest niezawodność transmisji, oraz dobry zasięg
sieci danego operatora w miejscu instalacji terenowej stacji pomiarowej. Czynniki te są ważniejsze niż maksymalna przepustowość, jaką
może oferować urządzenie. Należy także zwrócić uwagę na stabilność
jakości usługi wybranego operatora sieci GSM. W sytuacji niskiego poziomu sygnału, można go wzmocnić przez zastosowanie anteny z większym uzyskiem lub w ostateczności anteny kierunkowej. Podstawowym warunkiem jest jednak stabilność usługi. Karty SIM instalowane
w urządzeniach telemetrycznych, dla prawidłowego działania z wykorzystaniem GPRS, wymagają przypisania do struktury sieci pakietowej
poprzez nadanie APN. Moduły telemetryczne pozwalają na współpracę
z prywatnymi i publicznymi APN. Do zadań profesjonalnych wskazane jest jednak korzystanie z prywatnych APN i statycznej adresacji IP.
Prywatna sieć pakietowa (APN) jest z reguły przeznaczona dla wąskiej
grupy użytkowników, często udostępniana jednej firmie. Transmisja danych możliwa jest tylko w obrębie tej małej strefy. Daje to większe bezpieczeństwo użytkowania kart SIM, a co za tym idzie, całego systemu
telemetrycznego. W takiej infrastrukturze akwizycji danych, nie występuje również ryzyko odbioru danych niechcianych (spamu), które mogą
zwiększyć koszty użytkowania systemu, przez zwiększenie w bilingach
operatora sieci ilości danych odebranych. Komunikacja pomiędzy urządzeniami odbywa się na zasadzie przekazania pakietu pod zadeklaro89

Paweł Buchwald
wany stały adres IP. W sytuacjach kiedy możliwe jest udostępnienie
łącza internetowego za pomocą infrastruktury kablowej lub transmisji
bezprzewodowej w standardzie 802.11, możliwe staje się wykorzystanie sieci Internet do akwizycji danych pomiarowych.
 Warstwa składowania i analizy danych
Dane pomiarowe z terenowych stacji są dystrybuowane do centralnej
bazy danych. W celu lepszego opracowania modeli diagnostycznych
i detekcji nowych czynników, które mogą mieć wpływ na zachorowania roślin w centralnej bazie danych, powinny być przechowywane
dane zarówno o wystąpieniu czynników sprzyjających infekcjom, jak
i dane o rzeczywistym występowaniu infekcji. Zagregowanie danych
w centralnej bazie danych i możliwość skorzystania z rzeczywistych
pomiarów prowadzonych w wielu miejscach, może pomóc w dalszej
analizie statystycznej zgromadzonych pomiarów, a przede wszystkim
zapewni taką ich ilość, aby badania statystyczne mogły być prowadzone na reprezentatywnej próbie. Centralizacja danych, którą umożliwia
system informatyczny, może ułatwić pracę specjalistom. W przypadku
zastosowania prostych modeli detekcji infekcji, algorytmy mogą być
wbudowane w same urządzenia pomiarowe, rozmieszczone w terenie.
Bardziej zaawansowane metody będą jednak wymagały oceny specjalistów. Bieżące dane pomiarowe, mogą być przechowywane w oparciu
o relacyjną bazę danych, natomiast archiwalne dane, które nie będą ulegały zmianie, jednak będą służyły do dalszej analizy, powinny być zagregowane w hurtowni danych. Przed zapisaniem danych do centralnej
bazy danych, musi nastąpić ujednolicenie formatów, oraz wyeliminowanie błędów systemowych, które mogą pojawić się w danych podczas
zakłóceń transmisji. Niejednolity format danych, może być związany
również z ustawieniami regionalnymi (np. format daty i czasu), zastosowanymi separatorami dziesiętnymi, lub użytymi formatami zapisu pomiarów, które wynikają ze specyfiki użytego sprzętu. Rozwiązanie tych
problemów, jest jednak typowym zadaniem realizowanym przez moduły ETL, w rozwiązaniach bazodanowych [5]. Centralizacja danych
pomiarowych, pozwala także na zastosowanie większych mocy obliczeniowych w analizie poziomu ufności, dla poszczególnych prognoz
i modeli z podziałem na regiony geograficzne i klimatyczne. Wyniki
uzyskanych analiz mogą być udostępniane zainteresowanym przy pomocy mechanizmów raportowania, które udostępniają dzisiejsze systemy bazodanowe.

90

Wykorzystanie systemów teleinformatycznych i pomiarowych...

4. Przegląd wybranych urządzeń sprzętowych
pozwalających na realizację systemu odczytu
i akwizycji danych pomiarowych
Zaproponowana architektura systemu informatycznego do akwizycji danych pomiarowych, może być zrealizowana w oparciu o gotowe komponenty, stosowane w zakresie pomiarów meteorologicznych, pomiarów warunków środowiskowych oraz urządzeń akwizycji danych. Zastosowanie
proponowanej koncepcji, pozwala również na wykorzystanie standardowych elementów infrastruktury sieciowej. Co prawda wielu producentów
urządzeń telemetrycznych, udostępnia gotowe rozwiązania zgodne z koncepcją prezentowanego systemu, jednak projekt i implementacja własnego
rozwiązania, pozwala na bardziej elastyczne podejście i często umożliwia
osiągnięcie zadowalających efektów dużo niższym kosztem.

4.1. Automatyczne stacje pomiarowe do detekcji danych
meteorologicznych
Jednym z urządzeń pozwalających prowadzić ciągły pomiar warunków klimatycznych, są stacje meteorologiczne firmy Davis. Są stosunkowo tanimi i popularnymi rozwiązaniami, pozwalającymi na elektroniczną detekcję
warunków meteorologicznych. Ich zaletą jest bardzo solidne wykonanie,
gwarantujące długą bezawaryjną pracę oraz precyzyjność pomiarów, na poziomie odpowiednim do zastosowań predykcji infekcji roślin uprawnych.
Wśród modeli stacji oferowanych przez producenta, są dostępne zarówno
urządzenia dla zastosowań podstawowych, jak i zaawansowanych, z możliwością rozbudowy o dodatkowe czujniki. Podstawowe oprogramowanie
dostarczane wraz z urządzeniem, pozwala na integrowanie odczytanych
wartości pomiarowych z innymi systemami, np. strony internetowe, systemy inteligentnych budynków itp. Stacja jest wyposażona w interfejs RS232
pozwalający na jej wykorzystanie w infrastrukturze wielu systemów informatycznych. Nie bez znaczenia pozostaje wykorzystanie przez producenta prostego protokołu tekstowego do transmisji danych, który pozwala na
zaprojektowanie własnych rozwiązań programowych, wykorzystujących to
urządzenie. Wygląd stacji meteorologicznej przedstawiono na rysunku 3.

91

Paweł Buchwald

Rys. 3. Stacja meteorologiczna DAVIS (źródło: http://www.davisnet.com)

Zespół czujników, w które może być wyposażona stacja, gromadzi dane
o warunkach na zewnątrz pomieszczenia i przesyła dane do konsoli. Możliwe jest użycie rozwiązania opartego o przewodowe połączenia konsoli
i czujników, jak również połączenia bezprzewodowe. W przypadku zastosowania przewodowych połączeń, przewód transmisyjny służy jednocześnie
do zapewnienia zasilania dla końcowych czujników pomiarowych. W standardowej wersji wyposażenia, dostępne są czujniki temperatury, wilgotności powietrza, siły i kierunku wiatru, jak również czujnik opadów atmosferycznych. Zestaw można wyposażyć w dodatkowe czujniki takie, jak czujnik
nasłonecznienia, czujnik promieniowania UV, czy czujnik zwilżenia liścia.

Rys. 4. Czujnik zwilżenia liścia i czujnik promieniowania UV dla stacji Vandage Pro
(źródło: http://www.davisnet.com

92

Wykorzystanie systemów teleinformatycznych i pomiarowych...
Ciekawym rozwiązaniem jest również zestaw stacji meteorologicznych
WatchDog firmy Spectrum. Urządzenie jest przystosowane do zastosowań
agrometeorologicznych. Pozwala ono na konfigurację przez użytkownika
czujników pomiarowych, które będą wykorzystywane w procesie monitorowania. Na uwagę zasługuje również możliwość podłączenia dowolnego
czujnika, który jest kompatybilny z pętlą prądową 4-20 mA. Urządzenie pozwala na dystrybucję danych z czujników do konsoli, za pomocą mikrosieci
przewodowej. Konsola jest wyposażona w interfejs RS232 pozwalający na
podłączenie do komputera. Możliwe jest również zintegrowanie urządzenia z infrastrukturą archiwizacji danych producenta. Producent umożliwia
przy pomocy dodatkowego zestawu podłączenie urządzenia do sieci GSM,
z wykorzystaniem karty SIM skojarzonej z dedykowanym APN. Urządzenie pozwala na pracę z interwałem pomiarowym od 1 do 60 minut. Możliwy
jest zapis próbek pomiarowych w samym urządzeniu – możliwość przechowywania pomiarów z ostatnich 183 dni, lub ich dystrybucja na serwer zdalny. Producent dołącza dodatkowo oprogramowanie do zarządzania danymi,
jak również konfigurowania urządzenia. Możliwe jest także wykorzystanie
urządzenia we współpracy z innymi systemami, poprzez implementację
modułów komunikacyjnych. Urządzenie wykorzystuje prosty sposób przesyłu informacji w formacie tekstowym, który ma charakter otwarty. Dzięki
temu możliwe jest wykorzystanie własnego oprogramowania i tym samym
zwiększenie elastyczności systemów budowanych w oparciu o stacje pomiarowe firmy Spectrum. Konsolę stacji pomiarowej z uwidocznionymi interfejsami pozwalającymi na podłączenie zewnętrznych czujników, przedstawiono na rysunku 5.

Rys. 5. Konsola pomiarowa stacji Watchdog firmy Spectrum

93

Paweł Buchwald

4.2. Wybrane urządzenia do akwizycji danych
pomiarowych z zastosowaniem transmisji GSM
Obecnie jedną z sieci o szerokim zasięgu, jest sieć operatorów telefonii
komórkowych GSM, która świadczy również usługi pakietowej transmisji danych. Transmisja GPRS jest odpowiednią metodą, która zapewnia
wystarczającą przepustowość dla akwizycji danych pomiarowych z żądanym interwałem czasowym, dla potrzeb monitorowania warunków
wegetacyjnych roślin uprawnych. Integracja urządzeń pomiarowych
z siecią GSM, jest możliwa przy pomocy dodatkowych urządzeń pełniących funkcje bramy pomiędzy mikrosiecią czujników pomiarowych,
a siecią GSM [4].
Jednym z takich urządzeń jest moduł firmy Telit o nazwie GaTel EZ10.
Oprócz standardowych funkcjonalności bramy pomiędzy sieciami, takich
jak konfigurowanie stosu protokołów TCP/IP, urządzenie wyposażono
w możliwość wykonywania skryptów programowalnych, napisanych w języku Python. Komunikacja z urządzeniem jest możliwa za pośrednictwem
interfejsu RS232 lub RS485. Dzięki temu może ono pełnić funkcję urządzenia dostępowego do mikrosieci lub sieci przemysłowej. Producent urządzenia udostępnia szereg bibliotek programowych, które pozwalają na implementację w języku Python rozwiązań integracyjnych, w oparciu o interfejs
RS232 lub RS485. Wśród udostępnionych bibliotek są również biblioteki
do wysyłania i odbioru wiadomości SMS, oraz obsługi komunikacji z wykorzystaniem protokołu TCP/IP.
Urządzenie posiada również szereg funkcji oferowanych w celu podniesienia bezpieczeństwa transmisji. Zostało wyposażone w funkcjonalność, która pozwala ograniczyć możliwość komunikacji do wybranych
adresów IP. Funkcja ta jest wykorzystywana do blokady komunikacji
z obcymi adresami IP poprzez sieć GPRS. Istnieje możliwość wprowadzenia na listę akceptowalnych adresów IP odpowiednich wpisów, za
pomocą standardowych komend konfiguracyjnych AT. W celu umożliwienia komunikacji urządzenia ze zdalnym węzłem sieciowym o danym adresie IP, konieczne jest jego jawne dodanie do listy bezpiecznych
urządzeń. Dla przykładu dodanie możliwości nawiązania połączenia
z urządzeniem o adresie IP 81.219.113.111 jest możliwe po wykonaniu
następującej komendy AT:
AT#FRWL =1,”81.219.113.111”,”255.255.255.255”
Komenda FRWL oznacza konfiguracje ustawień firewalla. Parametr 1 oznacza odblokowanie transmisji, następny parametr określa adres IP, natomiast
ostatni parametr reprezentuje tzw. Maskę blankietową. Domyślnie włączo94

Wykorzystanie systemów teleinformatycznych i pomiarowych...
ne, są restrykcyjną listę kontroli dostępu, która blokuje komunikacje ze
wszystkimi adresami IP, z tego względu konieczne jest uwzględnienie na
liście odpowiednich adresów, w celu umożliwienia komunikacji za pośrednictwem protokołu TCP/IP, poprzez pakietową transmisję danych.
Za pomocą funkcji języka Python dostępnych w bibliotekach przeznaczonych do urządzenia można obsługiwać komunikację z serwerem FTP
lub poczty elektronicznej. Umożliwia to w łatwy sposób dystrybuowanie
pomiarów do innych systemów, z wykorzystaniem standardowych usług
sieciowych. Interesującą funkcją dla potrzeb akwizycji danych pomiarowych jest również możliwość programowego wysyłania wiadomości SMS
z poziomu samego urządzenia. Możliwe jest również wysyłanie smsów do
grup odbiorców, jeśli operator sieci GSM obsługuje tę funkcję. Jeśli bogaty zestaw komend oferowanych przez standardowe biblioteki okaże się
niewystarczający dla realizacji zadań integracyjnych, możliwe jest również
posługiwanie się standardowymi poleceniami AT z poziomu języka Python.
Urządzenie daje także możliwość wysłania żądania wykonania odpowiedniego skryptu, za pomocą wiadomości SMS o danej treści. Z tego względu
posiada ono wsparcie dla filtrowania numerów telefonów, które mogą komunikować się z urządzeniem w ten sposób. Można przyjąć, iż numer telefonu jest przypisany do osoby uprawnionej i stosować tę informację jako
element weryfikacyjny autoryzowanego dostępu. Podobne założenie jest
poczynione w wielu systemach bankowych, w których do potwierdzenia
zlecenia przelewu, stosuje się kody jednorazowe, wysyłane na numer telefonu zleceniodawcy. Przy weryfikacji zaufanych numerów telefonów również
obowiązuje zasada domyślnego blokowania numerów. Oznacza to, iż numer
telefonu, za pośrednictwem którego będą inicjowane wykonania skryptów,
musi być jawnie wprowadzony na listę.
Firma Telit wprowadziła na rynek również moduł ME50-868, który dodatkowo udostępnia możliwość szyfrowania transmisji. Moduły tej rodziny,
są oparte o europejskie standardy Wireless M-Bus, który jest stosowany dla akwizycji danych ze zdalnych mierników. W warstwie aplikacji,
moduł ten obsługuje szyfrowanie AES. Dzięki temu moduły Telit mogą
współpracować ze wszystkimi urządzeniami kompatybilnymi ze standardem M-bus, co ułatwia zadania integracji systemów informatycznych
i pomiarowych rozwiązań. Modułową budowę urządzenia firmy Telit
przedstawia rysunek 6.

95

Paweł Buchwald

Rys. 6. Modułowa budowa urządzenia Telit EZ10

Rys. 7. Architektura modułu telemetrycznego i kontrolera protokołów iMod.
(źródło [11])

Innym rozwiązaniem, jest stosowany dla potrzeb integracji urządzeń automatyki przemysłowej, moduł telemetryczny iMod 9300. Jest to urządzenie oparte o platformę sprzętową komputerów przemysłowych NPE
Linux. Dzięki temu, istnieje możliwość wykorzystania wielu możliwości tej platformy, jak dostępność kompilatorów języka C, C++, Java oraz
96

Wykorzystanie systemów teleinformatycznych i pomiarowych...
możliwość dostępu do urządzenia, za pomocą bezpiecznego kanału VPN.
Urządzenie jest przystosowane do ciągłej pracy w trudnych warunkach
terenowych i przemysłowych. Zastosowanie procesora z rodziny ARM
wpłynęło korzystnie na mały pobór mocy urządzenia. Rozwiązanie posiada bogaty zestaw interfejsów wejścia wyjścia w tym interfejs pozwalający
na współpracę z magistralą 1-wire. Urządzenie może również służyć do
buforowania danych pomiarowych. Dzięki możliwości instalacji na urządzeniu oprogramowania wysokiego poziomu takiego, jak baza danych,
z możliwością zadawania zapytań w języku SQL, rozwiązanie może nie
tylko przechowywać aktualne próbki pomiarów, ale udostępniać je innym systemom. Urządzenie umożliwia uruchomienie innych usług, które są powszechnie stosowane w rozwiązaniach typu serwer aplikacyjny.
Przydatną cechą urządzenia, jest obecność serwera http. W standardowej
konfiguracji jest on używany do udostępniania stron WWW, służących do
kastomizacji urządzenia, jednak przy pomocy języka PHP, można w oparciu o to rozwiązanie stworzyć interfejs integracyjny. Na prezentowanym
urządzeniu, z pomocą serwera http, można zrealizować warstwę logiki
biznesowej, udostępniającą dane pomiarowe z zastosowaniem Web Services i protokołu SOAP, lub standardu JSON, popularnego zwłaszcza
w rozwiązaniach implementowanych w języku Java. Standardowym przeznaczeniem urządzenia, jest zastosowanie w systemach integracji rozwiązań automatyki przemysłowej. Realizacja podstawowych funkcjonalności
z zakresu konwersji protokołów, buforowania danych pomiarowych lub
świadczenia usług dostępu do sieci czujników pomiarowych za pośrednictwem transmisji pakietowej GPRS, nie wymagają zastosowania rozwiązań
programistycznych i mogą być zrealizowane w oparciu o udostępniane
opcje konfiguracyjne. Możliwość współpracy z usługami warstwy aplikacyjnej uruchomionymi na urządzeniu i zastosowanie języków programowania wysokiego poziomu, takich jak Java i PHP do tworzenia indywidualnych rozwiązań aplikacyjnych, czynią urządzenia bardzo elastycznym
i przydatnym dla celów integracji systemów pomiarowych z innymi systemami informatycznymi.

4.3. Zastosowanie magistrali 1-wire jako mikrosieci
czujników pomiarowych
1-wire jest interfejsem elektronicznym, oraz protokołem komunikacyjnym
służącym do wymiany danych pomiędzy urządzeniami firmy Dallas Semiconductor. Jego nazwa wywodzi się z faktu, iż do komunikacji jest używana
jedna linia danych, oraz jedna linia zasilająca, jednak odbiornik może być
zasilany bezpośrednio z linii danych wykorzystując zasilanie pasożytnicze,
97

Paweł Buchwald
co jest zaletą tego interfejsu. Odbiorniki współpracujące z tym interfejsem
posiadają kondensator, który jest ładowany bezpośrednio z linii danych,
a następnie zgromadzona w nim energia może być użyta do zasilania urządzenia. Przepustowości osiągane przez protokół 1-wire mieszczą się w zakresie od 16 kb/s do 115,2 kb/s. Jest to wystarczająca przepustowość do
zapewnienia akwizycji danych pomiarowych dla celów kontroli warunków
meteorologicznych i wegetacyjnych. Interfejs 1-wire jest używany najczęściej do komunikacji z małymi urządzeniami typu termometry cyfrowe,
instrumenty meteorologiczne, sterowniki ładowania akumulatorów, zamki elektroniczne iButton itp. Listę dostępnych urządzeń dystrybuowanych
przez firmę Dallas przedstawia tabela 4.
Tabela 4. Wybrane urządzenia pracujące z magistralą 1-wire.

Symbol
urządzenia

Zastosowanie

Urządzenie pełniące funkcję mostu pomiędzy magistralą
I2C, oraz 1-wire. Umożliwia konwersję protokołu
DS2484
i przesyłanie danych, pomiędzy masterem I2C
a dowolnym urządzeniem 1-wire pracującym jako slave
Moduł pamięci EEPROM do zastosowania z magistralą
DS28E05
1-wire
Cyfrowy czujnik temperatury z wbudowaną pamięcią
MAX31826
EEPROM wielkości 1Kb
Konwerter 1-wire <-> USB. Urządzenie jest szczególnie
DS9481R-3C7 przydatne do bezpośredniej integracji sieci 1-wire
z komputerem przemysłowym lub serwerem
Pamięć EEPROM o wielkości 20Kb przystosowana do
DS28EC20
współpracy z magistralą 1-wire
Termometr cyfrowy pracujący z rozdzielczością do
DS1822
12 bitów, charakteryzujący się zakresem temperatur od
-10 do +80 st. C
DS9097U,
DS9097U-009, Urządzenia pełniące funkcję konwertera 1-wire <->
DS9097U-E25, RS232
DS9097U-S09

98

Wykorzystanie systemów teleinformatycznych i pomiarowych...

DS2450

DS2415

Konwerter analogowo cyfrowy czterokanałowy,
który pozwala na współpracę z czterema wejściami
napięciowymi. Każde wejście jest skojarzone z własnym
rejestrem, który pozwala na zapamiętanie bieżącej
wartości pomiarowej. Możliwa jest również obsługa
przekroczenia wartości krytycznych i ustawienia
alarmów. Układ wyposażony jest w generator sumy
kontrolnej CRC16, dzięki czemu zapewnia podstawową
ochronę kontroli błędów odczytu wartości pomiarowych
Jest urządzeniem dla magistrali 1-wire, które zapewnia
obsługę zegara czasu rzeczywistego. Informacje z tego
urządzenia mogą być używane jako stempel czasowy
dla pomiarów

W przypadku chęci rozszerzenia możliwości pomiarowych budowanego
systemu, możliwe jest skorzystanie z magistrali 1-wire celem budowy sieci
pozwalającej na rozmieszczenie znacznej liczby czujników pomiarowych
w żądanej lokalizacji. Zastosowanie magistrali 1-wire obniża koszty implementowanej infrastruktury, oraz pozwala na uniknięcie kłopotliwego
rozwiązania zasilania czujników pomiarowych. Dodatkowym atutem magistrali 1-wire, może być również możliwość współpracy z wieloma urządzeniami analogowymi i cyfrowymi. Dla pomiarów temperatury, można wykorzystać cyfrowe termometry, które udostępnia w swojej ofercie firma Dallas
Maxim. Przy zastosowaniu konwerterów analogowo cyfrowych przeznaczonych dla magistrali 1-wire możliwe jest ponadto skorzystanie z chipów
pomiarowych innych producentów. Znane są implementacje umożliwiające
przy pomocy magistrali 1-wire, pomiar ciśnienia powietrza, wilgotności,
nasłonecznienia, kierunku i siły wiatru, oraz wielu innych czynników atmosferycznych. Przykład umożliwiający integrację czujnika ciśnienia firmy
Philips o symbolu MPX4115 z magistralą 1-wire, z zastosowaniem przetwornika analogowo cyfrowego, przedstawiono na rysunku 8.

99

Paweł Buchwald

Rys. 8. Realizacja czujnika ciśnienia powietrza dla magistrali 1-wire
(autor David Bray – www.davidbray.org)

5. Podsumowanie
Monitorowanie czynników sprzyjających infekcjom roślin, jest ważnym
aspektem wpływającym na utrzymanie zdrowych, pozbawionych szkodliwych dla człowieka czynników upraw roślinnych. Jest to nie tylko problem
związany z ekonomicznymi aspektami produkcji żywności, ale należy do
zagadnień, od których zależy utrzymanie zdrowia konsumentów. Efektywne prognozowanie możliwości zachorowań roślin uprawnych, może przynieść pozytywne skutki w zakresie zmniejszenia ilości wykorzystywanych
środków chemicznych do ochrony plantacji. W przedstawionym rozdziale
pokazano możliwości wykorzystania systemów informatycznych, w zakresie wspomagania monitoringu warunków wegetacji roślin. Dane dostarczane przez tego typu systemy, są podstawą prognozowania infekcji, oraz przy100

Wykorzystanie systemów teleinformatycznych i pomiarowych...
czyniają się do opracowania pewniejszych modeli predykcji wystąpienia
infekcji. Ciągły rozwój urządzeń wykorzystywanych w teletransmisji, oraz
miniaturyzacja urządzeń służących do budowy systemów informatycznych
powodują, iż do implementacji rozwiązań z zakresu monitorowania warunków wegetacji roślin, mogą być użyte urządzenia stosowane do niedawna
jedynie w systemach automatyki przemysłowej, czy sieciach komputerowych przedsiębiorstwa. Daje to możliwość projektowania systemów pomiarowych ściśle dopasowanych do potrzeb odbiorców, jak również zwiększa
zakres ich zastosowań. Obecne systemy monitoringu warunków wegetacyjnych i meteorologicznych, mogą działać w trudnych i niedostępnych warunkach. Są one coraz częściej ważnym aspektem, pozwalającym na przeciwdziałanie chorobom roślin uprawnych, oraz ciągłe monitorowanie ich
warunków wegetacyjnych.

Literatura
1. Buchwald P.: Monitorowanie ciągłości działania sieci lokalnych z zastosowaniem
urządzeń GSM. [w:] P. Pikiewicz: Zastosowania Internetu; Wydawnictwo WSB
Dąbrowa Górnicza 2012.
2. Buchwald P.: Mechanizmy Integracji Danych i Logiki Biznesowej w Kooperacyjnych Systemach Sterowania Procesami Przedsiębiorstwa [w:] R. Knosala: Komputerowo Zintegrowane Zarządzanie Tom 1; Oficyna Wydawnicza PTZP Opole
2011.
3. Domański A., Domańska J., Grzywak A.: Sieci komputerowe; Wydawnictwo
WSB 2007 r.
4. Latałowa M., Góra M.: Aeropalinologia – Badania zawiesiny pyłku w powietrzu.
Wiadomości Botaniczne 40(2):29-37, 1996.
5. Sas R.: Skuteczna ochrona przed parchem jabłoni w Biuletyn ZZO Warka –
Najnowsze technologie i doradztwo w dziedzinie sadownictwa 3/2011. www.
zzowarka.pl
7. Specyfikacja 1-wire – www.maxim-ic.com
8. Specyfikacja techniczna stacji meteorologicznej WatchDog – www.specmeters.
com
9. Specyfikacja techniczna stacji meteorologicznej Davis – www.davisnet.com
10. Specyfikacja techniczna urządzenia GaTel EZ10 – www.telit.com
11. Specyfikacja techniczna urządzenia iMod 9300 www.telemetria-gprs.pl.
12. Strach A., Kasprzyk I.: Metodyka badań zawartości pyłku roślin i zarodników
grzybów w powietrzu z zastosowaniem aparatu Hirsta. Wydawnictwo Politechniki
Rzeszowskiej 1989.

101

Rozdział 5
Wykorzystanie metod biometrycznych
w realizacji interfejsów człowiek – komputer
Paweł Buchwald
Wyższa Szkoła Biznesu – Katedra Informatyki
pawel@buchwald.com.pl

Streszczenie
Rozdział dotyczy zastosowania rozwiązań sprzętowych pozwalających na monitorowanie sygnałów biometrycznych w realizacji
interfejsów komunikacyjnych człowieka z systemami komputerowymi. W opracowaniu przedstawiono ogólne koncepcje działania
systemów śledzenia sygnałów biometrycznych na bazie rozwiązań
stosowanych w medycynie, głównie w zadaniach diagnostycznych. Dynamiczny rozwój sprzętu elektronicznego, oraz systemów
komputerowych sprawiły, iż systemy monitorowania sygnałów
biometrycznych znalazły szerszy obszar zastosowań. Obecnie
można zauważyć ich rosnąca popularność w ogólnie dostępnych
dla użytkowników systemów komputerowych urządzeniach przeznaczonych także dla celów rozrywkowych. Rozdział przedstawia
przegląd tego typu systemów z uwzględnieniem specjalistycznych
rozwiązań, jak i prostych interfejsów człowiek – komputer przeznaczonych dla systemów rozrywkowych i multimedialnych.

1. Problematyka wykorzystania interfejsów człowiek –
maszyna
Jednym z problemów stawianych przed nowoczesnymi systemami komputerowymi, jest zapewnienie efektywnej współpracy z człowiekiem. Z tego
względu zaczęto rozwijać coraz bardziej zaawansowane interfejsy czło103

Paweł Buchwald
wiek – maszyna, które pozwalają na dostosowanie rozwiązań informatycznych nie tylko do potrzeb użytkownika, ale również do jego kondycji psychofizycznej oraz emocji w danej chwili. W kontekście prawidłowej pracy
urządzeń informatycznych, oraz zapewnienia ciągłości działania systemu
informatycznego należy pamiętać, iż poczynania człowieka, który obsługuje system informatyczny, również mają znaczący wpływ na zachowanie
poprawności działania systemu, jako całości. Problem ciągłości działania
systemów informatycznych z świetle normy BS25999, jest traktowany jako
zadanie strategiczne przedsiębiorstwa czy organizacji. Nie można zapewnić
poprawnej pracy systemu informatycznego nie mając pewności, czy osoby go obsługujące, są w danej chwili zdolne do współpracy z systemem.
Dostępność systemu informatycznego jest na ogół zdeterminowana współczynnikiem dostępności najmniej pewnego elementu, którym bardzo często
jest zasób osobowy [2]. Z tego względu możliwość wykonywania prawidłowej pracy przez użytkownika współpracującego z systemem informatycznym, powinna być monitorowana w podobnym stopniu, co poprawna praca
samego systemu informatycznego. To, w jakim stanie znajduje się człowiek,
może wpływać na jego zdolności pozyskiwania wiedzy, zdolności percepcji
i analizy sytuacji, a nawet możliwości podejmowania racjonalnych decyzji. Nowoczesne interfejsy komunikacji człowieka z komputerem oferują
techniczne możliwości, które pozwalają na sprostanie takim zadaniom. Dlatego nowoczesne interfejsy człowiek – maszyna, powinny zapewniać nie
tylko możliwość wprowadzania danych wejściowych i prezentacji wyników
działania dla użytkownika, ale również informacji o stanie emocjonalnym
i zdrowotnym człowieka obsługującego system. Jednym ze sposobów detekcji stanu użytkowników systemów komputerowych, jest wykonywanie
periodycznych pomiarów EEG, EMG i śledzenie ruchów gałek ocznych
użytkownika.

Interfejsy mózg – komputer
Interfejsem mózg – komputer (ang. Brain computer interface) można nazwać
takie rozwiązanie, które pozwala na przesyłanie informacji wejściowych do
systemu komputerowego bez konieczności użycia aparatu mięśniowego
użytkownika. Tak więc interfejsy mózg – komputer są oparte o metody biometryczne, które pozwalają na detekcję bodźców wprost z ludzkiego mózgu. Interfejsy tego typu są dużym udogodnieniem dla osób z dysfunkcjami
ruchowymi, ale są coraz powszechniej wykorzystywane w rozrywkowych
systemach komercyjnych, zamiast klasycznych manipulatorów. Po raz
pierwszy pojęcia interfejsu mózg – komputer użyto w publikacji wyników
104

Wykorzystanie metod biometrycznych w realizacji interfejsów...
prac nad takim rozwiązaniem w latach siedemdziesiątych na Uniwersytecie
Kalifornijskim w Los Angeles. Obecnie tego typu urządzenia są szeroko
rozwijane, zwłaszcza dla potrzeb medycznych i neuroprotetycznych, ale
również w celach monitoringu użytkowników systemów technicznych i zachowania bezpieczeństwa (np. bezpieczeństwo ruchu drogowego). Działanie typowych interfejsów mózg – komputer, odbywa się dzięki możliwości
detekcji elektroencefalogramu. Istnieją również inne rozwiązania, takie jak
implantowane elektrody domózgowe, które ze względu na swój inwazyjny
charakter, są stosowane rzadko i nie mają zastosowania w powszechnych
systemach detekcji stanu użytkowników systemów komputerowych. Inne
systemy BCI oparte są o fMRI (funkcjonalny magnetyczny rezonans jądrowy). Metoda ta została uznana przez psychologów za jedyną technikę, która
w obiektywny sposób pomaga w obrazowaniu stanu psychicznego osoby
badanej. Niestety ta metoda badawcza obarczona jest dość dużym opóźnieniem i wymaga zastosowania drogiego tomografu. Detekcja zachowania
ludzkiego mózgu w oparciu o NIRS (ang. Near infrared spectroscopy) oparta jest o badanie przepływu krwi w różnych częściach mózgu za pomocą
wiązki laserowej światła podczerwonego. Ze względu na nierównomiernie
pochłanianie światła podczerwonego przez krew o różnym stopniu zawartości tlenu, możliwe jest śledzenie przepływu krwi na podstawie emisji
światła odbitego od różnych części mózgu. Technika ta jest również nazywana tomografią optyczną światła rozproszonego. Metody tomograficzne
są obarczone kilkusekundowym opóźnieniem akwizycji informacji o intencjach użytkownika i wymagają bardzo drogiej i specjalistycznej aparatury
badawczej, która ze względu na swoje gabaryty, nie może być powszechnie
stosowana. Z tego względu powszechniej stosuje się systemy pracujące na
podstawie EEG. Wśród takich systemów największą popularność zyskały
rozwiązania zbudowane na trzech podstawowych zasadach:
–

SSVEP (ang. steady state visual evoked potentials) – metoda opiera się
na koncentrowaniu uwagi przez użytkownika na symbolu graficznym,
który mruga z określoną częstotliwością. Obrazy mrugające z określoną
częstotliwością wpływają na zmiany fal mózgowych,

–

P300 – opiera się na wyróżnieniu symbolu graficznego, np. przez podświetlenie, na który patrzy użytkownik. Powoduje to pewien potencjał
wywołany, który może być wykryty podczas pomiaru EEG. Metoda ta
jest niestety niedokładna i wymaga kilku powtórzeń dla prawidłowej
interpretacji danych,

–

Techniki oparte na intencji ruchem kończyn. W tej metodzie system
oparty o detekcję EEG próbuje odczytać wyobrażenie użytkownika
o ruchu ,np. prawą lub lewą ręką. Dane te są następne odwzorowane
105

Paweł Buchwald
na zdarzenia wywołujące funkcje oprogramowania. Metoda ta wymaga
dość dużej ilości elektrod, oraz jest najbardziej skomplikowana w realizacji.
Są również realizowane tzw. zależne interfejsy mózg – komputer, które do
swego działania wykorzystują również inne aspekty biometrii, takie jak
pomiar szczątkowej aktywności mięśniowej, czy detekcja ruchów gałek
ocznych. Są to metody powszechnie stosowane w diagnostyce klinicznej
wielu chorób, a coraz częściej wykorzystywane do realizacji systemów
przekazywania informacji do komputerów. Systemy tego typu, są dla wielu
chorych z poważnymi dysfunkcjami aparatu mięśniowego i chorobami neurologicznymi, jedyną szansą na komunikację z otoczeniem.

Biometryczne metody pomocne w realizacji interfejsów
człowiek – komputer
W celu detekcji zachowania użytkownika, jego stanu fizycznego i aktywności biologicznej, stosuje się różne metody biometryczne, pozwalające na
odczytanie impulsów związanych z aktywnością człowieka. Ze względu na
szeroki, rozmaity charakter danych, jakie można pozyskać w ten sposób,
oraz różne typy urządzeń, które są wykorzystywane do dokonywania pomiarów, zakres wykorzystania metod biometrycznych do oceny aktywności użytkowników, zdobywa coraz szersze zastosowania. Początkowo takie
metody pomiarowe były stosowane do diagnostyki klinicznej, jednak z czasem znalazły zastosowanie w technologiach informatycznych. Do najczęściej wykorzystywanych w tym zakresie metod należą pomiary EEG, EMG
oraz analiza ruchów gałek ocznych.

Elektroencefalografia (EEG)
Historia badań EEG sięga roku 1875. Za prekursora prac nad metodą diagnostyki EEG, uważa się Anglika Richarda Catona. Jako pierwszy zarejestrował on prądy zmierzone na powierzchni kory oraz w głębi mózgu
zwierząt. Podobne prace były prowadzone w tym samym czasie przez naukowców z Polski, między innymi Napoleon Nikodem Cybulski i Adolf
Beck. Za odkrywcę elektroencefalogramu u ludzi, powszechnie uważany
jest niemiecki lekarz i uczony Hans Berger (1873-1941). Początkowe badania Bergera dotyczyły potwierdzenia w sposób doświadczalny wyników
prac. Od 1920 roku, przy użyciu galwanometru strunowego Einthovena,
Edelmena, a następnie firmowego urządzenia Siemensa badał czynność bioelektryczną ludzkiego mózgu. W 1929 roku opublikował pierwsze zapisy
elektroencefalograficzne rejestrowane z powierzchni czaszki, początkowo
106

Wykorzystanie metod biometrycznych w realizacji interfejsów...
rejestracja aktywności elektrycznej mózgu była dokonywana nad ubytkami
kostnymi. W swym pierwszym doniesieniu Berger przedstawił podstawowe rytmy cechujące elektroencefalogram. Berger dokonał prezentacji wielu
zapisów EEG człowieka, wykonanych w stanach fizjologicznych, jak i patologicznych. Przedstawił również EEG charakterystyczny dla chorych na
epilepsję. Obecnie EEG jest jedną z najbardziej podstawowych metod diagnostycznych, w przypadku badania chorób neurologicznych. Elektroencefalografia jest nieinwazyjną metodą diagnostyczną, która służy do badania
bioelektrycznej czynności ludzkiego mózgu. Odpowiednio rozmieszczone
na skórze głowy elektrody, pozwalają na pomiar potencjału elektrycznego,
które po wzmocnieniu, tworzą zapis elektroencefalograficzny.
Podczas wykonywania standardowego badania, umieszcza się 19 elektrod
rozmieszczonych po osiem elektrod nad każdą półkulą, oraz trzy elektrody
w linii środkowej. Elektrody te są oznaczone odpowiednio:
–

trzy elektrody na granicy płatów ciemieniowych i czołowych: C3, C4, Cz,

–

trzy elektrody nad płatami ciemieniowymi: P3, P4, Pz,

–

cztery elektrody nad płatami skroniowymi: T3, T4, T5, T6,

–

wie elektrody nad płatami potylicznymi: O1, O2.

Rozmieszczenie elektrod na głowie badanego pokazano na rysunku 1.

Rysunek 1. Rozmieszczenie elektrod podczas wykonywania badania EEG

Fale mózgowe, które są wykrywane na podstawie badania EEG, można podzielić na kilka typów. Typy fal mózgowych zaprezentowano w tabeli 1.
107

Paweł Buchwald
Tabela 1. Typy fal mózgowych

Nazwa fal
Delta
Fala o częstotliwości
do 4 Hz

Theta
Fala o częstotliwości
4-8 Hz
Alfa
Fala o częstotliwości
8-13 Hz
Beta
Fale o częstotliwości
12-35 Hz
SMR
Fale z grupy beta
o częstotliwości
12-15Hz

Beta 1
Fala o częstotliwości
15-18 Hz
Beta 2
Fala o częstotliwości
18-35 Hz
Gamma
Fale o częstotliwości
36-44 Hz

108

Opis
Fala mózgowa delta jest generowana przez mózg
podczas głębokiego snu. Pewne częstotliwości
należące do fal mózgowych typu delta, są
generowane również podczas uwalniania
hormonu wzrostu. Fale te pojawiają się również
podczas wysiłku umysłowego
Generowana jest w stanie zwiększonej
kreatywności, podczas uczenia się człowieka.
Wykrywane u ludzi będących w stanie głębokiej
medytacji
Fale charakterystyczne dla stanu głębokiego
odpoczynku, odprężenia jednak nie snu
Są wykrywane w stanie logicznego
i analitycznego myślenia, intelektualnego
zaangażowania oraz werbalnej komunikacji.
Wyższe częstotliwości fal beta są związane ze
stresem, agresją i obawami
Jest nazywana niską falą beta. Jest
charakterystyczna dla stanu emocjonalnego
odprężenia i lekkiego skupienia. Fala SMR
o niskiej amplitudzie, może być wykryta w stanie
braku dostatecznego skupienia uwagi. Fala
SMR o wysokiej amplitudzie jest związana
z czujnością i stanem fizycznej ciszy
Charakteryzuje umysłowe pobudzenie i czujność.
Zadania w tym stanie są wykonywane szybko,
badany pracuje z pełną uwagą
Mogą być wykryte podczas subiektywnych
stanów emocjonalnych takich, jak gotowość do
działania, poruszenie, zdenerwowanie, irytacja,
ekscytacja, stres, strach.
Fale o tej częstotliwości zostały wykryte
we wszystkich obszarach mózgu. Są
charakterystyczne dla takich subiektywnych
stanów emocjonalnych, jak myślenie, procesy
skojarzeniowe, występują podczas wydajnej
pracy umysłowej i twórczej

Wykorzystanie metod biometrycznych w realizacji interfejsów...
Podczas generowania określonych fal mózgowych, w organizmie są wytwarzane pewne substancje chemiczne zwane neurotransmiterami. Do najbardziej znanych neurotransmiterów należą adrenalina, noradrenalina, dopamina, serotonina i endorfiny. Zmiany w aktywności generowania poszczególnych fal mózgowych, zachodzą podczas przechodzenia ze stanu czuwania
w stan snu i odwrotnie. Są również zauważalne podczas odbioru i analizy
bodźców zewnętrznych. Wszystkie składowe fal mózgowych, są generowane w sposób ciągły, jednak niektóre z nich mają charakter dominujący.
Podstawą treningu EEG biofeedback, jest możliwość wpływania w sposób
świadomy na zwiększanie aktywności pewnych fal mózgowych, a zmniejszanie innych. Takie podejście stosują również techniczne urządzenia do
kontroli aktywności użytkowników systemów komputerowych, które są
oparte o detekcje EEG.

Koncepcja działania urządzenia do detekcji EEG
Ze względu na amplitudę sygnału EEG w granicach kilku mikrowoltów. Zanim sygnał zostanie wykorzystany musi być wzmocniony kilka tysięcy razy.
Fale mózgowe charakteryzują się dużą podatnością na zakłócenia. Akwizycja sygnału do aparatury pomiarowej, jest dokonywana poprzez czułe elektrody bardzo dobrej jakości wykonane ze srebra, złota lub stali. Dużym problemem w detekcji tego sygnału jest szum, który może pochodzić zarówno
z samej aparatury pomiarowej jak i otoczenia. Koncepcję detekcji sygnału
EEG pokazuje rysunek 2.

Rysunek 2. Koncepcja detekcji sygnału EEG

Sygnał wejściowy jest wstępnie wzmocniony za pomocą wzmacniacza wysokiej częstotliwości, który jednocześnie dokonuje pomiaru różnicy potencjałów pomiędzy dwoma punktami na skórze badanego. Powoduje to eliminację dużej części szumu pochodzącego z przewodów, gdyż jest on taki sam
dla obydwu punktów. Następnie sygnał jest dodatkowo wzmacniany, oraz
przepuszczany przez filtr pasmowo przepustowy, filtr niskiej częstotliwości.
109

Paweł Buchwald
Eliminuje on zakłócenia powstałe podczas konwersji sygnału na postać cyfrową (tzw. aliasing). Po konwersji na postać cyfrową sygnał EEG jest przesyłany do komputera. Urządzenie do detekcji sygnału EEG jest dodatkowo
odseparowane izolacją optoelektryczną od komputera, w celu ochrony użytkownika przed błędami pochodzącymi ze źródeł zasilania.
Ze względu na coraz większą dostępność i spadek cen elementów elektroniki, pozwalających na praktyczną realizację urządzenia do detekcji EEG,
tego typu konstrukcje są wykorzystywane nie tylko w diagnostyce medycznej i terapii, ale również w budowie interfejsu człowiek – komputer
i wykorzystywane do sterowania urządzeniami oraz aplikacjami. Obecnie
urządzenia tego typu są stosowane także w metodzie biofeedback EEG, oraz
w zastosowaniach rozrywkowych (sterowanie aplikacjami typu gry komputerowe).

3.

Elektromiografia EMG

Działanie elektromiografu jest oparte na rejestracji czynności elektrycznych
mięśni. Proces skurczu mięśnia, polega na superpozycji czynności poszczególnych jednostek ruchowych pod wpływem impulsów z ośrodków nerwowych. Obraz czynności elektrycznej jednostki ruchowej mięśnia, powstaje
w wyniku sumowania w czasie i przestrzeni potencjałów włókien mięśniowych. Elektryczna czynność mięśni, jest związana ze zdolnością przenikania jonów sodu i potasu przez błonę komórkową. Na skutek nierównomiernego rozmieszczenia jonów sodu oraz potasu w obrębie komórek mięśniowych, dochodzi do spolaryzowania ładunku elektrycznego wewnątrz
komórki w stosunku do błony komórkowej. W stanie spoczynku potencjał
polaryzacji wynosi około 80mV. Potencjał ten zmienia się w zależności od
stanu czynnościowego mięśni. Przy bardzo dużym wysiłku fizycznym, potencjałów różnych jednostek ruchowych jest tak dużo, że nie jest możliwe
rozróżnienie poszczególnych wartości. Urządzenie zwane elektromiografem rejestruje zmienne potencjały elektryczne w spoczynku raz podczas
czynnościowej pracy mięśni. Jednym z badań EMG jest bolesne i inwazyjne
badanie, wykonywane za pomocą elektrod igłowych. Stosowane jest ono
w diagnostyce chorób zaburzenia aktywności mięśni, oraz chorób nerwowo-mięśniowych (np. miastenii, stwardnienia rozsianego).
Inny rodzaj EMG to badanie powierzchniowe, które jest wykonywane za
pomocą samoprzylepnych elektrod. Tą metodą możliwe jest badanie mięśni
położonych powierzchniowo, oraz grup mięśniowych, takich jak zginacze,
prostowniki, mięśnie oddechowe, mięśnie dna miednicy. Jest ono całkowicie bezbolesne i nieinwazyjne. Za pomocą tej metody można dokonać oceny
czynnościowej pracy mięśni na podstawie bieżącego monitoringu. Elektro110

Wykorzystanie metod biometrycznych w realizacji interfejsów...
niografie wykorzystują najczęściej reumatolodzy, ortopedzi i rehabilitanci,
którzy w ten sposób mogą stwierdzić, czy obserwowane objawy chorobowe
są wynikiem uszkodzenia mięśni czy układu nerwowego. Miniaturyzacja
aparatury elektromiograficznej do badań powierzchniowych, zaowocowała
zwiększeniem dostępności EMG nie tylko do celów oceny klinicznej i kinezjologicznej, ale także znalazła zastosowanie w wielu aplikacjach wykorzystujących biofeedback.

Urządzenia do detekcji sygnałów elektromiograficznych
Obecnie aparatura stosowana w badaniach elektromiograficznych wykorzystuje technikę cyfrową. Dzięki temu przetworzenie sygnału wygenerowanego przez potencjały elektryczne pochodzące z mięśni, nie przysparza
większego problemu. Schemat akwizycji sygnałów bioelektrycznych i ich
konwersje na postać cyfrową dla potrzeb bieżącej analizy czynnościowej
pracy mięśni przedstawia rysunek 3.

Rysunek 3. Koncepcyjny schemat aparatury badawczej EMG

Czynnościowe potencjały mięśni są odczytywane za pomocą elektrody.
W diagnostyce medycznej wykorzystuje się elektrody igłowe, natomiast
w zastosowaniach realizacji interfejsów człowiek – maszyna, konieczne jest
zastosowanie nieinwazyjnej metody, polegającej na zastosowaniu elektrod
powierzchniowych. Stosunkowo słaby elektryczny sygnał, musi podlegać
filtracji, oraz wzmocnieniu, jak również konwersji z postaci analogowej na
postać cyfrową. Dane w postaci cyfrowej, mogą być przetworzone przez
oprogramowanie. Narzędzia diagnostyczne sa wyposażane również w stymulator prądu elektrycznego o natężeniu od 0 do 100 mA, który jest wykorzystywany w procesie diagnostyki przewodzenia włókien nerwów obwodowych. Podstawowym elementem aparatury pomiarowej do badań EMG,
jest wzmacniacz bioelektryczny. Powinien on cechować się niskim poziomem szumów, wysoką impedancją wejściową, oraz pasmem przenoszenia
w zakresie od 2 Hz do minimum 10000 Hz. Dla potrzeb prawidłowej akwizycji sygnału bioelektrycznego, konieczne jest zastosowanie przetwornika
111

Paweł Buchwald
AC o dużej częstotliwości próbkowania, która powinna być co najmniej
dwukrotnie wyższa, niż najwyższa częstotliwość sygnału EMG ( 1000 Hz).
Pomiar czynności elektrycznej mięśni, jest zrealizowany za pomocą detekcji różnicy napięcia, występującej pomiędzy dwoma elektrodami. Znaczne zainteresowanie elektromiografią w ostatnich latach spowodowało powstanie licznych modyfikacji elektrod powierzchniowych. Jedną z nich jest
tzw. elektroda liniowa, która swym zasięgiem może objąć większą część
powierzchni mięśnia. Multielektroda liniowa zawiera do kilkunastu typowych elektrod dwubiegunowych, które są połączone razem i rozmieszczone
liniowo względem siebie. Przykłady takich elektrod pokazuje rysunek 4.

..
Rysunek 4. Multielektrody stosowane w badaniu EMG

Sygnał EMG ma postać nisko amplitudowego przebiegu rzędu mikrowoltów
(jest on mierzyny w skali od kilku do kilkuset mikrowoltów). Jest on bardzo
podatny na zakłócenia pochodzące zarówno z otoczenia, jak i z samej aparatury pomiarowej. W celu uzyskania rejestracji sygnału satysfakcjonującej
jakości, dokonuje się eliminacji powstałych podczas badania artefaktów.
Jednym ze sposobów eliminacji artefaktów ruchowych, jest zastosowanie
filtrów górnoprzepustowych. Obecnie stosowane przewody doprowadzające w aparaturze EMG są wyposażone w elementy przedwzmacniające, które
zmniejszają ich wrażliwość na zakłócenia, związane z polami magnetycznymi otoczenia, oraz zakłócenia generowane przeddodatkowe ruchy osoby
badanej.

112

Wykorzystanie metod biometrycznych w realizacji interfejsów...

5. Okulografia
Okulografia, czyli technika śledzenia ruchów gałek ocznych, jest metodą
stosowaną od ponad stu lat w diagnostyce klinicznej. Technika śledzenia
ruchów gałek ocznych, jest użyteczna także przy realizacji interfejsów człowiek – maszyna, oraz w dziedzinach takich, jak psychologia, marketing i reklama czy bezpieczeństwo ruchu drogowego.
Prekursorem prac nad detekcją ruchów gałki ocznej był Edmund Huey,
który zbudował pierwsze urządzenie mierzące ruch gałki ocznej. Urządzenie było zbudowane w oparciu o pewnego rodzaju soczewkę kontaktową
z otworem na źrenicę. Soczewka była podłączona z aluminiowym wskaźnikiem, który informował o ruchu gałki. Pierwszy, nieinwazyjny przyrząd do
badań okulograficznych został zbudowany przez Guy Thomasa Buswella.
Jego działanie opierało się na użyciu promieni świetlnych, które odbijając
się od powierzchni oka, były rejestrowane na taśmie filmowej.
Istotne badania mające wpływ na rozwój okulografii ,zostały przeprowadzone w 1950 roku przez Alfreda L. Yarbus’ea. Badania te wykazały, iż
duży wpływ na ruchy gałek ocznych, ma zadanie wykonywane przez badanego o obszar na którym skupia on uwagę w danym momencie. Badania dotyczyły także możliwości predykcji. W świetle przeprowadzonych
eksperymentów Yarbus wywnioskował, iż zapisy ruchów oka wskazują, że
obserwator zwraca szczególną uwagę tylko na pewne elementy występujące
na obrazie. Badacz doszedł do wniosku, iż procesy myślowe człowieka są
w pewnym stopniu odzwierciedlone poprzez ruch gałek ocznych. Z tego
względu wnioski na temat skupienia uwagi i emocji badanych, mogą po
części opierać się na analizie ruchu gałek ocznych. Na podstawie zapisów
badań okulograficznych, można bez problemu ustalić, które z elementów
obrazu przyciągają skupienie obserwatora i z jaką częstotliwością.

Metody badania ruchów gałek ocznych
Jedna z kategorii aparatury okulograficznej bazuje na wykorzystaniu potencjału elektrycznego, który jest mierzony przy użyciu elektrod umieszczonych wokół oczu. Oczy w tej metodzie są źródłem stałego potencjału
pola elektrycznego. Zaletą tego rozwiązania jest możliwość dokonywania
pomiaru w całkowitej ciemności oraz przy zamkniętych oczach badanego.
Jeśli badany skieruje oczy z centralnej pozycji w kierunku obwodowym,
siatkówka zbliży się do jednej z tych elektrod, a rogówka, w opozycji do
niej zbliży się do drugiej elektrody. Powoduje to zmianę potencjału elektrycznego, a urządzenie umożliwia jego interpretację jako ruch pionowy lub
poziomy oka. Metoda ta pozwala badać znacznie większy zakres ruchów
113

Paweł Buchwald
poziomych i pionowych, niż przy zastosowaniu innych technik, jednak ze
względu na swój inwazyjny charakter, jest stosowana głównie w obszarach
medycznych, a nie w realizacji interfejsów człowiek – maszyna, czy systemów śledzenia zachowania użytkowników komputerów.
Wysoki poziom oprogramowania komputerowego, a także kamer video
spowodował przełom w badaniach okulograficznych. Dzięki temu, wyniki
badań mogły być wykorzystywane w badaniach rynku, a także w testowaniu
jakości i ergonomii korzystania ze stron internetowych. Stosowany obecnie
okulograf fotoelektryczny, może rejestrować ruchy gałek ocznych poprzez
zmiany w odbiciu światła na rogówce oka konkretnego mówcy-słuchacza,
podczas procesu tłumaczenia. Teksty wyświetlane są na monitorze LCD,
zaś na obudowie ekranu znajduje się miniaturowa kamera video, która rejestruje ruchy obu gałek ocznych. Uzyskany w wyniku badań obraz cyfrowy
charakteryzuje się bardzo wysoką dokładnością, czyli pomiarem z częstotliwością do 500 Hz.

Rysunek 5. Przenośne urządzenie okulograficzne Red-M firmy SensoMotoric
Instrumentation

Badania okulograficzne są wykorzystywane w medycynie nie tylko do detekcji jednostek chorobowych, ale także w zadaniach protetycznych. Przykładem tego typu rozwiązania, może być generator mowy, sterowany ruchem gałek ocznych, który wykorzystywał Stephen Hawking.

114

Wykorzystanie metod biometrycznych w realizacji interfejsów...

Interpretacja sygnałów bioelektrycznych
Przekonwertowany do postaci cyfrowej sygnał EEG, może być użyty bezpośrednio do detekcji subiektywnych stanów emocjonalnych użytkownika systemu komputerowego. Również sygnał EMG, na podstawie którego
można domniemywać o pracy ludzkich mięśni, może być użyty do sprawdzenia zachowania użytkownika systemów komputerowych. Na podstawie
dominującego typu sygnału EEG, można wykryć stopień skupienia użytkownika, jego zdolność do podjęcia racjonalnej decyzji, stopień zdenerwowania oraz zdolność do pozyskiwania wiadomości. Bezpośrednia detekcja
sygnału EEG, bez jego głębszej interpretacji, jest jednak niewystarczająca
w przypadku wykorzystania tej metody do sterowania oprogramowaniem
komputerowym lub urządzeniami. Postać fal mózgowych generowanych
podczas współpracy użytkownika z komputerem, jest charakterystyczna
osobniczo. Oznacza to, iż sygnał EEG musi zostać zinterpretowany w kontekście danego użytkownika. Odczytany sygnał EMG można powiązać
bezpośrednio z pracą danej grupy mięśniowej. Sygnał EEG o podobnej
charakterystyce, może oznaczać wywołanie różnych funkcjonalności oprogramowania, dla różnych użytkowników. Z tego względu konieczne jest
wstępne nauczenie systemu komputerowego prawidłowych wzorców EEG,
które mogą odpowiadać wybraniu odpowiedniej funkcji oprogramowania,
przez danego użytkownika. Ze względu na wysoki stopień skomplikowania sygnał EEG, który musi być zinterpretowany jako wzorzec dla danej
funkcjonalności oprogramowania jest trudny do wyznaczenia klasycznymi
algorytmami. Z tego względu do wyselekcjonowania cech EEG, które odpowiadają wywołaniu przez użytkownika odpowiedniej funkcji oprogramowania należy zastosować rozwiązanie z zakresu sztucznej inteligencji. Zadowalające efekty daje zastosowanie sieci neuronowych, które po przejściu
fazy uczenia się mogą służyć do klasyfikacji sygnałów EEG, oraz ich przetwarzania na wybrane funkcje użytkowników. Zmierzony sygnał EEG oraz
EMG, jest wstępnie przetworzony poprzez odseparowanie znaczących jego
elementów od składowych nieprzydatnych podczas klasyfikacji. Na podstawie znaczących elementów sygnału są określane jego cechy, które pozwalają na dokonanie klasyfikacji i odwzorowanie przeanalizowanego sygnału
na wybraną funkcjonalność systemu informatycznego. Zaletą takiego podejścia jest wykorzystanie nieinwazyjnej dla użytkownika metody badawczej, pozwalającej na detekcję sygnałów generowanych przez ludzki mózg,
czy potencjałów generowanych podczas pracy mięśni, oraz stosunkowo
niski koszt aparatury pomiarowej. Wadą metody jest konieczność wstępnego nauczenia sieci neuronowej wykorzystywanych wzorców w przypadku
wykorzystania EEG, oraz w wielu przypadkach trudne opracowanie metod
115

Paweł Buchwald
klasyfikacji. Pomimo tych trudności metoda interpretacji sygnału EEG została wykorzystana jako interfejs komunikacji z systemami informatycznymi w rozwiązaniach komercyjnych i jest stosowana w wielu aplikacjach
dostępnych dla szerokiego grona odbiorców. Również wykorzystanie EMG
do sterowania oprogramowaniem, urządzeniami elektroniczymi – zwłaszcza przeznaczonymi dla osób z dysfunkcjami ruchowymi, staje się coraz
bardziej powszechne.
Analizowanie danych biometrycznych, które są związane z występowaniem
pewnych częstotliwości składowych – tak jak ma to miejsce w przypadku
EEG, może odbywać się poprzez zastosowanie funkcji gęstości widmowej
mocy. Pomiary sygnałów biologicznych są dokonywane periodycznie w odstępach czasowych. Ze względu na charakter sygnału pomiarowego, który
jest zapisany w dziedzinie czasu, a nie w dziedzinie częstotliwości, konieczne jest zastosowanie odpowiedniej transformaty. W tym zakresie pomocna
jest transformata Fouriera przedstawiona wzorem 1

Wzór 1. Transformata Fouriera z dziedziny czasowej do dziedziny
częstotliwościowej

Zastosowanie ma również transformata odwrotna z dziedziny częstotliwościowej do dziedziny czasu. Jest ona określona wzorem 2

Wzór 2. Odwrotna transformata Fouriera

Analiza Fouriera sygnału EEG została przeprowadzona po raz pierwszy
w 1932 w sposób ręczny. W 1938 roku Grass i Gibbs zaproponowali swoją
metodę zastosowania transformaty Fouriera do analizy widmowej sygnału EEG z użyciem elektromechanicznego analogowego integratora Grassa. Uzyskali oni wyniki nieróżniące się od uzyskiwanych współcześnie.
W 1965 roku dokonano użycia analizy widmowej sygnału EEG w oparciu
o szybką dyskretną transformatę Fouriera. Istota użycia transformaty Fouriera w analizie sygnału EEG polega na rozkładzie analizowanego przebiegu na zbiór składowych funkcji sinusoidalnych, z których każda posiada
swoją amplitudę, częstotliwość oraz przesunięcie fazowe. Dla sygnału EEG
pozwala to ocenić występowanie poszczególnych fal mózgowych w badanym EEG, a tym samym dokonać oceny stanu badanego.
116

Wykorzystanie metod biometrycznych w realizacji interfejsów...

4. Przykłady dostępnych interfejsów człowiek –
maszyna zbudowanych na bazie metod
biometrycznych
Szybki postęp w dziedzinie informatyki oraz elektroniki, przyczynił się do
powstawania licznych implementacji interfejsów człowiek – komputer, które pozwalają na rejestracje jego pomiarów biometrycznych, oraz ich interpretacje, w celu detekcji stanu emocjonalnego, psychofizycznego lub oceny
aktywności użytkownika. Wśród licznych przykładów dostępnych interfejsów człowiek – maszyna, znajdują się rozwiązania przystosowane dla celów badawczych, jak i zaprojektowane dla potrzeb rozrywkowych, których
cena nie przekracza kilkuset dolarów. Większość z dostępnych rozwiązań,
oferuje również bogaty interfejs programowy, który umożliwia tworzenie
własnych rozwiązań programowych, oraz integrację nowego interfejsu komunikacyjnego człowiek – komputer, z już istniejącym oprogramowaniem.
W opracowaniu przedstawiono wybrane dostepne systemy tego typu.

Urządzenie Emotiv
Urządzenie Emotiv EEG zostało utworzone do zastosowań badawczych, jak
i deweloperskich. Występuje ono w dwóch podstawowych wersjach. Dla
celów samodzielnej interpretacji odczytanego EEG, przeznaczona jest wersja urządzenia Emotiv EEG. Jeśli nie istnieje konieczność dokładnej analizy
EEG, można skorzystać z wersji, która nie umożliwia pełnego dostępu do
analizowanego sygnału EEG, ale udostępnia dane poddane analizie i zinterpretowane w taki sposób, aby odzwierciedlały zachowanie użytkownika,
jego poziom koncentracji oraz stan emocjonalny. Ta wersja urządzenia nosi
nazwę Emotiv EPOC. Rodzaje udostępnianych przez producenta interfejsów programistycznych w poszczególnych wersjach dystrybuowanego systemu przedstawiono w tabeli 2.
Systemy Emotiv są oparte o tę samą koncepcję sprzętową i różnią się dostarczonymi komponentami programowymi. Zasada działania jest oparta
na systemie do odczytu EEG pozwalającym na prace z wieloma kanałami i jednocześnie oferującym zalety przenośnego rozwiązania. Urządzenie
w postaci neuro-hełmu składa się z 14 elektrod, używanych do odczytu fal
mózgowych. Urządzenie jest łatwiejsze w obsłudze i mniej kłopotliwe od
klasycznych elektroencefalografów. Urządzenie Emotiv nie wymaga przymocowywania elektrod za pomocą specjalnego żelu zwiększającego przewodność. Wystarczy, aby elektrody zostały umieszczone na głowie użytkownika w odpowiedni sposób. Kolejną cechą, jest brak konieczności kalibracji urządzenia z wykorzystaniem dodatkowych elektrod. Dzięki temu,
117

Paweł Buchwald
producentom udało się obniżyć ilość elektrod niezbędnych do odczytu sygnału EEG i obniżyć koszty rozwiązania.
Tabela 2. Rodzaje SDK dystrybuowane dla neuro-hełmów EMOTIV

SDK

Rodzaj
kompatybilnego
Urządzenia
sprzętowego

Developer
Edition

Emotiv EPOC

Research
Edition

Emotiv EEG

Enterprise
Edition
Educational
Edition
Lite Edition

Emotiv EEG

Dodatkowa informajca
Przeznaczona do budowy
oprgramowania dystrybuowanego
w ramach Emotiv Store
Przeznaczone do celów
badawczych oferuje pełny dostęp
do odczytów EEG
Brak restrykcji dystrybucji
oprogramowania

Emotiv EEG

Licencja dla wielu urządzeń

Symulator
programowy
urządzenia

Darmowa wersja SDK

Rysunek 7. Wygląd neuro-hełmu Emotiv

Oprócz samego urządzenia producent oferuje interfejs API, który pozwala
na odczyt danych EEG, oraz ich interpretacje w celach detekcji stanu psychofizycznego użytkownika, lub zastosowania do sterowania aplikacjami
komputerowymi. Typy bibliotek oferowane przez producenta to:
–
118

Expresiv

Wykorzystanie metod biometrycznych w realizacji interfejsów...
Biblioteka pozwala na interpretację sygnału EEG, odczytanego przez
neuro-hełm w taki sposób, aby na podstawie odczytów EEG określić
ekspresje użytkownika, jego mimikę twarzy oraz emocje.
–

Afectiv
Biblioteka przeznaczona do interpretacji odczytanego sygnału EEG,
w celu detekcji emocji użytkownika. Na podstawie tej biblioteki można
odczytać, jakie odczucia powoduje u użytkownika reakcja na bodźce
generowane przez system komputerowy. Dla lepszego efektu, metoda
ta jest łączona z systemem kontroli ruchu gałek ocznych użytkownika.

–

Cognitiv
Biblioteka przeznaczona do wykorzystania urządzenia Emotiv do generowania sygnałów wejściowych dla systemu komputerowego, które mogą
zastępować tradycyjną mysz lub klawiaturę. Za pomocą tego interfejsu
programowego, możliwe jest sterowanie przez użytkownika obiektami
i funkcjonalnością aplikacji jedynie poprzez zmiany zachodzące w elektroencefalogramie. Pełne wykorzystanie tego komponentu programowego,
wymaga wcześniejszego nauczenia interpretacji EEG użytkownika i przypisania pewnych charakterystycznych cech EEG do typowych akcji, takich
jak ruch myszką, kliknięcie, itp. Konfiguracja komponentu w tym zakresie odbywa się poprzez wybór odpowiedniej akcji, oraz rejestrację EEG
związanego z akcją. Podczas personalizacji odwzorowania akcji na dany
odczyt sygnału EEG, użytkownik może na bieżąco sprawdzać efektywność
metody uczenia na podstawie prostej aplikacji testowej, polegającej na poruszaniu sześcianem w oparciu o zdarzenie wymuszone poprzez odczytany
elektroencefalogram. Prosty graficzny interfejs komunikacyjny udostępniany przez oprogramowanie Cognitiv Suite przedstawiono na rysunku 8.

Rysunek 8. Interfejs graficzny użytkownika Cognitiv Suite (źródło: dokumentacja
techniczna Emotiv)

119

Paweł Buchwald
W ramach zestawu Emotiv Software Development Kit dostarczono szereg
narzędzi, które mogą być pomocne podczas tworzenia, uruchamiania i testowania własnych rozwiązań aplikacyjnych wykorzystujących neuro-hełm
do odczytu i interpretacji sygnału EEG. Do aplikacji dostarczonych wraz
z pakietem Emotiv SDK należą:
–

EmoKey – narzędzie pozwala na przypisanie odpowiednich skrótów klawiszowych po detekcji konkretnego zachowania użytkownika, umożliwiające na przypisanie odpowiednich kombinacji. Narzędzie EmoKey
pozwala na skonfigurowanie odpowiednich reguł sprawdzania odczytu
EEG, które pozwalają na odwzorowanie zapisu EEG na odpowiednie
zdarzenia. Reguły mogą być zapisane w profilu danego użytkownika,
przez co możliwe jest dostosowanie ich w sposób indywidualny. Możliwe jest interpretowanie sygnałów z aplikacji EmoComposer, oraz Control Panel. Daje to możliwość wstępnego przetestowania poprawności
reguł za pomocą aplikacji, pozwalającej na wymuszenie odpowiednich
danych wejściowych, jak również współpracy z danymi o zapisie EEG
z rzeczywistego urządzenia. Konfiguracja reguł jest możliwa za pomocą bardzo prostego i intuicyjnego interfejsu graficznego. Za jego pomocą możliwe jest zdefiniowanie, jakie warunki muszą być spełnione
dla danej reguły, oraz jaka kombinacja klawiszowa powinna być wygenerowana przy zaistnieniu zdefiniowanych warunków. Konfigurator
daje również możliwość zdefiniowania, jak długo ma być przytrzymana dana kombinacja przycisków, oraz pozwala na określenie czasu jej
powtarzania. Dla każdej reguły, możliwe jest zdefiniowanie z którego
neuro-hełmu powinien być odczytany sygnał EEG uruchamiający regułę, wartości dla danego pola odczytanego przez hełm, nazwy stanu jaki
ma zainicjować działania, włączenie lub wyłączenie reguły. Zdefiniowane reguły można zapisać w pliku XML, co pozwala na dynamiczne
zarządzanie regułami i dopasowywanie ich do aplikacji lub wymogów
użytkownika,

–

EmoComposer – narzędzie będące programowym emulatorem neuro – hełmu. Jest przeznaczone głównie do testowania oprogramowania
współpracującego z neuro-hełmem. Za jego pomocą możliwe jest testowanie zachowania oprogramowania przy zajściu określonych modyfikacji sygnału EEG. Za jego pomocą można dokonać ręcznego ustawienia
przebiegu EEG i wysłanie odpowiednich komunikatów o zdarzeniach
o zmianie parametrów EEG do aplikacji. Narzędzie to może pracować
w dwóch trybach: interaktywnym, oraz w trybie uruchamiania skryptów.

W trybie pracy interaktywnej EmoComposer pozwala na bieżącą kontrolę
i ustawienia parametrów symulowanego EEG, oraz pozwala zasymulować
120

Wykorzystanie metod biometrycznych w realizacji interfejsów...
zaistnienie zdarzeń związanych ze zmianą stanu emocjonalnego użytkownika. Podobnie jak rzeczywiste urządzenie, aplikacja symulująca hełm przesyła do testowanego oprogramowania na bieżąco.
Tryb uruchamiania skryptu pozwala na uprzednie zdefiniowanie interakcji
z aplikacją, poprzez przygotowanie skryptu opisującego zdarzenia w specjalnie przygotowanym do tego celu języku EML (ang. Emo composer Markup Language). Język ten jest oparty na znacznikowym zapisie XML i może
być zinterpretowany przez narzędzie EmoComposer.
–

Emotiv Control Panel – narzędzie pozwalające na uczenie odpowiedniej interpretacji EEG dla celów kontroli aplikacji, pozwala na testowanie zachowania systemu, oraz ocenę prawidłowego odwzorowania EEG
na akcje związane ze sterowaniem. Pozwala użytkownikowi na wybór
własnego profilu w czasie współpracy z urządzeniem i aplikacjami.

Emotiv Control Panel jest używany jako warstwa pośrednicząca pomiędzy oprogramowaniem i neuro-hełmem lub emulatorem. Takie podejście
pozwala uniknąć kłopotliwego podczas testowania rozwiązania programowego konfigurowania fizycznego urządzenia. Aplikacja Control Panel jest
instalowana jako usługa w systemie operacyjnym i nasłuchuje na porcie
3008. Dzięki temu możliwe jest korzystanie z danych odczytanych przez
neuro-hełm poprzez każde oprogramowanie, które posiada możliwość korzystania z protokołu TCP/IP.

7.1.1. Możliwości w zakresie tworzenia oprogramowania
wykorzystującego neuro-hełm Emotiv
Wraz z aplikacjami do obsługi neuro-hełmu dostarczane są interfejsy programistyczne, pozwalające na tworzenie własnego oprogramowania, które
używa neuro-hełmu jako interfejsu wprowadzania danych i kontroli stanu emocjonalnego użytkownika. W ramach interfejsu programistycznego
dostarczone sa dwa pliki: edk.dll, oraz edk_util.dll jak również pliki nagłówkowe, pozwalające na wykorzystanie dostarczonych bibliotek w programach napisanych w języku C++. Istnieją również komponenty pośredniczące, pozwalające na użycie bibliotek współpracujących z urządzeniem dla
platformy .Net oraz Java.
Bibliteka edk.dll dostarcza klas pozwalających na połączenie oprogramowania ze sprzętowym detektorem EEG, oraz pozwala na odczyt wstępnie zinterpretowanych danych, które są przechowywane w specjalnie w tym celu
przygotowanych strukturach danych. Struktury te pozwalają na odczytywanie
danych o EEG, jak również określonych na ich podstawie stanów emocjonalnych użytkownika, czy stanów związanych z jego koncentracją i stopniem
121

Paweł Buchwald
skupienia. Na podstawie dostarczonych przez interfejs programowy struktur danych możliwe jest odczytanie kompleksowej informacji pochodzącej
z czujników neuro-hełmu. Modułową architekturę udostępnianej biblioteki
pozwalającej na komunikacje z urządzeniem przedstawia rysunek 9.

Rysunek 9. Architektura komunikacji Emotiv API z aplikacjami zewnętrznymi
(źródło: dokumentacja techniczna firmy Emotiv)

EmoState jest zestawem klas pozwalających na odczytanie danych o emocjach użytkownika. Na ich podstawie można zinterpretować poziom zadowolenia użytkownika, uśmiech itp. Wszystkie struktury danych eksponowane poprzez moduł EmoState mają przedrostek ES_. Podczas tworzenia
aplikacji, możliwe jest periodyczne odczytywanie danych o emocjach użytkownika, lub korzystanie z delegatów, które umożliwiają reakcje na zdarzenie zmiany stanu. Zdarzenia obsługiwane przez dostarczony komponent
programistyczny EmoState można podzielić na trzy grupy:
–

Zdarzenia związane z samym urządzeniem – mają miejsce podczas zainicjowania komunikacji z neuro-hełmem, lub przerwania połączenia ze
względu na jego wymuszenie albo utratę sygnału,

–

Nowe zdarzenie zmiany stanu – zachodzi podczas detekcji przez wbudowany w moduł programowy algorytm zmiany stanu emocjonalnego
użytkownika, lub zmiany stanu jego uwagi,

–

Rozszerzone zdarzenia użytkownika – zdarzenia pochodzące z modułów Expresiv i Cognitiv, które wymagają uprzedniego nauczenia się
charakterystycznej dla użytkownika interpretacji zmian jego EEG.

Oprócz typowych klas związanych z obsługą metod funkcjonalnych, dostarczanych przez interfejs programowy w ramach udostępnianego interfejsu programistycznego, udostępniany jest zestaw klas związanych z obsługą błędów.
122

Wykorzystanie metod biometrycznych w realizacji interfejsów...

Dostęp do odczytów EEG z poziomu udostępnianego
interfejsu API
Dostęp do danych opisujących odczytane EEG, wymaga uprzedniego zainicjowania połączenia ze sprzętem. Jest to realizowane poprzez wywołanie instrukcji EE_EngineConnection(). W przypadku użycia emulatora
urządzenia Emo Composer, połączenie powinno odbywać się poprzez wywołanie metody EE_EngineRemoteConnected(). Po zainicjowaniu połączenia konieczna jest obsługa zdarzenia EE_UserAdded, które pozwala
na rejestracje użytkownika, jeśli domyślny użytkownik nie został skonfigurowany. Po zalogowaniu użytkownika i możliwości wykorzystania jego
profilu, możliwe jest wywołanie metody DataAquisitionEnable. Pozwala
ona na bezpośredni dostęp do odczytanych danych o EEG i wykonanie samodzielnej interpretacji. Podczas takiego monitorowania odczytów EEG,
muszą być utworzone bufory, które są używane do zapisywania odczytanych danych. Najprostsza metoda odczytu danych EEG, odbywa się poprzez periodyczne wywołanie metody DataUpdateHandle(). Po każdorazowym wywołaniu funkcji, odbywa się przepisanie buforowanych danych
do miejsca pamięci, które jest dostępne poprzez uchwyt hData. Istnieje
możliwość zwrócenia liczby możliwych do odczytania próbek, za pomocą
funkcji DataGetNumberOfSample(). Sam odczyt interesujących próbek
jest realizowany poprzez wywołanie funkcji EE_DataGet(). Funkcja ta
umożliwia wybranie jednego z kanałów, powiązanego z konkretnym czujnikiem neuro-hełmu. Dla przykładu pobranie pierwszej odczytanej próbki z czujnika AF3 możliwe jest poprzez wywołanie funkcji EE_DataGet
w następującej postaci:
EE_DataGet(hData, ED_AF3, buffer, 1).
Po odczytaniu wartości obrazujących odczytany sygnał EEG, konieczne
jest zadbanie o prawidłowe zamknięcie połączenia i wyczyszczenie buforów danych, poprzez wywołanie funkcji EE_EngineDisconect() lub
EE_EmoStateFree(). Przy użyciu tej metody odczytu danych EEG należy
pamiętać, iż jest ona możliwa do wykorzystania w wersjach urządzenia
z komponentami SDK, umożliwiającymi bezpośredni dostęp do parametrów EEG. Inne wersje SDK pozwalają jedynie na odczytanie danych
o interpretacji wartości EEG, które są odzwierciedlone na konkretny stan
emocjonalny użytkownika, jego wyraz twarzy lub zdarzenie reprezentujące daną czynność. Przykładem tego jest edycja deweloperska komponentów SDK.

123

Paweł Buchwald

OCZ – The Neural Impulse Actuator
Neural Impulse Actuator wykorzystuje do „odczytywania myśli” trzy mechanizmy, każdy skupiający się na innym obszarze odzwierciedlenia przez
człowieka swoich świadomych i ukrytych zamiarów i reakcji. Urządzenie
bazuje na detekcji fal mózgowych EEG w zakresie analizy fal alfa, oraz
beta. Wykorzystuje także czujniki detekcji aktywności mięśni twarzy, oraz
detekcję ruchów gałek ocznych.
Wadą rozwiązania OCZ jest wysoka czułość urządzenia, a tym samym możliwość łatwego zakłócenia jego działania, poprzez urządzenia emitujące fale
elektromagnetyczne, takie jak ładowarki czy kuchenki mikrofalowe. Urządzenie wymaga także każdorazowej kalibracji przed jego zastosowaniem do
sterowania systemem komputerowym. Zaletą urządzenia są niskie koszty,
które udało się obniżyć dzięki programowej realizacji modułu interpretacji
aktywności neuro-elektrycznej.

Rysunek 10. Wygląd urządzenia OCZ NIA

Sprzętowy moduł urządzenia OCZ NIA pełni funkcję odczytu sygnału aktywności neuroelektrycznej, oraz przekazuje sygnał do komputera za pomocą interfejsu USB. Interpretacja sygnału jest realizowana przez moduł
programowy, co w przypadku komputerów o słabszej mocy obliczeniowej,
może negatywnie wpływać na wydajność systemu. Producent systemu OCZ
nie dostarcza żadnego interfejsu API, który umożliwiłby tworzenie własnych rozwiązań programistycznych, budowanych w oparciu o to urządzenie. Możliwe jest jednak wykorzystanie programowych interfejsów komunikacyjnych firm trzecich. Przeznaczenie tego rozwiązania jest wyłącznie
rozrywkowe, gdyż nie daje ono satysfakcjonującej elastyczności dla zastosowań badawczych i bardziej zaawansowanej kontroli stanu użytkownika.
124

Wykorzystanie metod biometrycznych w realizacji interfejsów...

NeuroSky
Z kolei kalifornijska firma NeuroSky zbudowała hełm, który wykorzystuje
tylko jeden czujnik, umieszczony w okolicach skroni. Dzięki niemu urządzenie potrafi stwierdzić, czy użytkownik jest skupiony, zrelaksowany,
przestraszony, czy niepewny. Firma NeuroSky oferuje swoje rozwiązania
w czterech wersjach: MindSet, MindWave, MindWave Mobile oraz MindBand. Są to fizyczne urządzenia bezprzewodowe, które pozwalają na odczytywanie sygnału EEG, wykorzystujące suchą elektrodę do akwizycji zmiennych potencjałów elektrycznych. Urządzenie MindSet dokonuje przesyłu
danych do komputera w oparciu o interfejs bluetooth, natomiast urządzenie
MindWave używa w tym celu interfejsu IRDA. Urządzenie MindSet jest
kompatybilne z systemami operacyjnymi rodziny Windows, Mac OS, oraz
Android, natomiast MindWave umożliwia pracę jedynie z komputerami wyposażonymi w system operacyjny Windows lub Mac OS.
Tabela 3. Porównanie urządzeń firmy NeuroSky

Rodzaj
System
Interfejs
Zasilanie
urządzenia
operacyjny
komunikacyjny
MindWave Windows, MacOS,
Zasilanie
Bluetooth
Mobile
Android, iOS.
zewnętrzne z baterii
Zasilanie
MindWave Windows, MacOS Podczerwień
zewnętrzne z baterii
Windows, MacOS,
Wbudowany
MindSet
Bluetooth
Android
akumulator
Windows, MacOS,
Wbudowany
MindBand
Bluetooth
Android
akumulator
Wszystkie urządzenia wykorzystują ten sam protokół komunikacyjny o nazwie ThinkGeaer Communication Protocol, dzięki czemu możliwe jest tworzenie modułów programowych kompatybilnych z każdym rozwiązaniem.
Firma dostarcza również zestawu bibliotek ThingGear SDK, ułatwiającego
tworzenie oprogramowania dla zaprezentowanych czujników dla wspieranych systemów operacyjnych. Za pomocą dedykowanych bibliotek możliwa jest współpraca z urządzeniem na wielu płaszczyznach. Producent udostępnia szereg bibliotek programowych, do których należą:
–

ThinkGear Connection – jest narzędziem uruchamianym jako usługa
systemowa i pozwala na pobieranie danych z czujników NeuroSky za
pomocą protokołu komunikacyjnego TCP/IP. Dzięki temu dane mogą
być używane przez każde oprogramowanie, które ma możliwość komunikacji poprzez mechanizm gniazd. Z poziomu tego interfejsu programistycznego można także zarządzać ustawieniami portów COM
125

Paweł Buchwald
współpracujących z urządzeniem. W ramach pakietu dostarczane jest
również narzędzie z interfejsem graficznym dla użytkownika, pomagające w skonfigurowaniu połączenia z czujnikami,
–

ThinkGear Communication Driver – jest zestawem dzielonych bibliotek w postaci plików dll, które eksponują zestaw metod do komunikowania się z urządzeniem. Komponenty przeznaczone są do tworzenia rozwiązań aplikacyjnych dla systemów operacyjnych Windows,
MacOS, windows Mobile. Ułatwiają integrację czujników z oprogramowaniem utworzonym w popularnych językach wysokiego poziomu
takich jak C/C++, c#, Java,

–

ThingGear Communication Driver for Java – zestaw bibliotek przeznaczonych do współpracy z językiem java I dostarczanym w postaci
plików jar. Przeznaczony do pracy z platformą J2ME,

–

ThinkGear Communication Protocol – pozwala na dostęp do danych
odczytanych bezpośrednio z portu COM, do którego jest podłączone
urządzenie,

–

NeuroSkyLab – interfejs pozwalający na dostęp do danych z poziomu
aplikacji w języku Mathlab.

Interfejsy mózg – komputer firmy g.tec
Firma dostarcza komplekswe rozwiązanie do prowadzenia badań elektroencefalograficznych w postaci systemu o nazwie g.BCIsys. Udostępniany
zestaw jest złożony z części sprzętowej, która pozwala na wielokanałową
akwizycję sygnału za pomocą elektrod. W zależności od wersji systemu,
dostępna liczba elektrod waha się od 8 dla wersji mobilnej do 256 dla wersji stacjonarnej. W zestawie znajduje się także urządzenie wzmacniające,
urządzenie przetwarzające sygnał w czasie rzeczywistym oraz szereg komponentów aplikacyjnych. System pozwala również na analizę zmian EEG
uzyskanych metodą impulsów wywołanych ( stosowaną w przypadku neurofeedback). Wraz z systemem dostarczone są komponenty programowe,
które umożliwiają współpracę z środowiskiem Mathlab w zakresie analizy
sygnału EEG na bieżąco podczas badania, oraz sygnałów historycznych.
Producent dostarcza pakiet gotowych komponentów dla narzędzia SIMULINK, które pozwalają na wizualizacje i zapis analizowanych danych. Wraz
z pakietem dostarczanych jest szereg przykładów aplikacyjnych umożliwiających poznanie możliwości systemu w celu budowania własnych rozwiązań.
Przykładem interfejsów człowiek – komputer, są narzędzia intendX, które
oferują użytkownikom szereg modułów aplikacyjnych pozwalających na
126

Wykorzystanie metod biometrycznych w realizacji interfejsów...
przekazywanie do aplikacji komputerowych zdarzeń za pomocą poleceń generowanych przez użytkownika poprzez interfejs EEG. W ramach produktów intendiX dostępne są następujące narzędzia: intendX Speller, intendX
SOCI, IntendX Painting.
–

IntendX Speller – jest to narzędzie przeznaczone do użytku indywidualnego, pozwalające na wprowadzanie znaków za pomocą wirtualnej
klawiatury sterowanej poprzez EEG. Metoda bazuje na potencjałach
wywołanych kałem wizyjnym (VEP/P300). Po uruchomieniu aplikacji,
na ekranie komputera pokazuje się wirtualna klawiatura w postaci mrugającej z odpowiednią częstotliwością macierzy znaków. Wybór odpowiedniego znaku odbywa się poprzez skupienie uwagi użytkownika na
wybranym elemencie macierzy. Większość użytkowników jest w stanie
korzystać z wirtualnej klawiatury po 10 minutach treningu. Szybkość
wprowadzania znaków tą metodą wynosi od 3 do 10 znaków na minutę.
Za pomocą funkcjonalności wbudowanej w oprogramowanie, możliwe
jest głosowe odczytywanie wprowadzanego tekstu przez komputer, integracja z klientem poczty elektronicznej, czy wysyłanie komend do
aplikacji zewnętrznych,

Rysunek 11. Wirtualna klawiatura wykorzystywana w systemie IntendX Speller
(źródło: dokumentcja intendX)

–

IntendX SOCI – jest oprogramowaniem pozwalającym na integrację
aplikacji komputerowych z funkcjonalnością sterowania, za pomocą
mechanizmu bazującego na metodzie potencjałów wywołanych. Interfejs graficzny komputera, jest wzbogacany o dodatkowe piktogramy
reprezentujące funkcjonalności takie jak, ruch kursora w prawo, lewo,
kliknięcie myszy itp. Każda z ikon mruga z określoną częstotliwością,
Jeśli użytkownik skupi swoją uwagę na mrugającej ikonie, zmianie ule127

Paweł Buchwald
ga obraz jego EEG. Po odczycie wyboru danej funkcjonalności przez
użytkownika, możliwe jest przypisanie wywołania odpowiedniej funkcji. System IntendiX SOCI umożliwia odczyt zdarzeń generowanych
przez użytkownika, za pomocą zmian EEG z dokładnością do 98%,
–

IntendX Painting – Działanie systemu jest podobne do IntendX Speller, jednak jego interfejs jest przystosowany do generowania graficznych obrazów, na podstawie wybranych dzięki zmianom EEG symboli. Dzięki temu systemowi, użytkownik ma możliwość korzystania
z funkcjonalności typowych aplikacji graficznych takich, jak rysowanie
kształtów, wypełnianie kolorem itp. Wybór odpowiednich funkcji odbywa się bez użycia aparatu mięśniowego. Użytkownik musi jedynie skupić swoją uwagę na wybranym symbolu graficznym, reprezentującym
daną funkcję.

7.5. Otwarty modułowy system EEG
Wzrastająca popularność interfejsów BCI (ang. Brain computer interface) przyczyniła się do powstawania inicjatyw budowy otwartych systemów monitoringu EEG do zastosowań niekomercyjnych. Jednym z nich
jest projekt modułowego systemu, opracowany przez społeczność osób
zainteresowanych wykorzystaniem tej metody biometrycznej. Szczegółowe informacje na temat projektu są zawarte na stronie openeeg.sourceforge.net. Prezentowany system jest przeznaczony do samodzielnej
konstrukcji. Koszty jego wytworzenia oscylują w granicach od 200 do
400 USD, w zależności od użytych komponentów. Sprzętowa część systemu zbudowana jest w oparciu o wzmacniacz ośmiokanałowy, oraz szereg elektrod pozwalających na dokonywanie pomiarów EEG. Na stronie
projektu umieszczono także przykłady konstrukcji elektrod wraz z dokumentacją ich wykonania, które zostały zaproponowane przez społeczność
wspierającą projekt.
Alternatywnym pomysłem, jest użycie kanału wejściowego karty dźwiękowej do analizy sygnału EEG. Ze względu na dostępność karty dźwiękowej, w dzisiejszych komputerach osobistych, byłoby to rozwiązanie
o niskich kosztach i szerokiej dostępności. Problemem w tym przypadku
jest filtrowanie na wejściu systemów audio sygnałów o częstotliwości
poniżej 20 Hz. Sygnał EEG nie może być więc przekazywany bezpośrednio tą metodą. Rozwiązaniem tego problemu może być budowa układu
wejściowego, złożonego z modulatora częstotliwości oraz wzmacniacza.
Działanie takiego systemu obrazuje schemat blokowy pokazany na rysunku 12.

128

Wykorzystanie metod biometrycznych w realizacji interfejsów...

Rysunek 12. Koncepcja wykorzystania karty dźwiękowej do odczytu sygnałów
biometrycznych

7.6. Programowa obsługa odczytywanych danych EEG
Oprogramowanie, które pozwala na interpretację odczytanych strumieni danych biometrycznych, jest zależne od komponentów programowych,
dostarczanych wraz z urządzeniami do detekcji sygnału biologicznego. Ze
względu na tąę zależność, oraz szeroki zbiór różnego rodzaju sprzętowych
systemów do detekcji fal mózgowych implementacją oprogramowania,
które mogłoby współpracować w środowisku rozproszonym z różnorakim sprzętem, jest trudnym wyzwaniem dla projektantów i programistów.
Rozwiązaniem tego problemu może być idea neuro-serwera. Polega ona na
stworzeniu dodatkowej warstwy programowej, która byłaby elementem pośredniczącym pomiędzy urządzeniami do detekcji EEG, a oprogramowaniem wykorzystującym dane. Do transmisji danych w takiej architekturze,
może być użyty popularny protokół, np. TCP/IP, który pozwala zorganizować architekturę systemu w taki sposób, aby działała ona w rozproszonym
środowisku sieciowym. Koncepcja neuro serwera zakłada ustalenie wspólnego ujednoliconego sposobu zapisu informacji o EEG, który pozwoliłby
na przechowywanie danych o odczytanych wartościach EEG, a jednocześnie dawałby pewne funkcjonalności odnośnie ich interpretacji. Oczywiście
sposób w jaki aplikacje klienckie korzystałyby z serwera pośredniczącego,
zdeterminowany jest przeznaczeniem aplikacji i wymaganiami użytkowników. Koncepcja działania neuro serwera przedstawia rysunek 13.
Jedna z realizacji neuro serwera jest otwarty projekt neuro-server prezentowany na stronie projektu Open EEG. Pozwala on na współpracę z takimi
aplikacjami bazującymi na detekcji EEG jak: BioEra, BrainBay and EEGMIR.

129

Paweł Buchwald

Rysunek 13. Koncepcja współpracy neuro-serwera z aplikacjami klienckimi

Systemy detekcji danych biometrycznych, które mogą być wykorzystane do
realizacji interfejsów człowiek – komputer, posiadają różnorodne sposoby
komunikacji z oprogramowaniem, oraz różne interfejsy API. W opracowaniu przedstawiono jedynie wybrane systemy do detekcji EEG, jednak ze
względu na ich rosnącą popularność wśród użytkowników, powstają coraz
nowsze rozwiązania, zestawienie dostępnych na rynku systemów komercyjnych, które są w zasięgu finansowym przeciętnego użytkownika i mogą
być wykorzystane do celów codziennej pracy z systemami komputerowymi
przedstawiono w tabeli 4.
Tabela 4. Zestawienie systemów pełniących role interfejsów człowiek – komputer
w oparciu o pomiary biometryczne
Urządzenie

130

Ilość
Interpretacja danych
elektrod

MindWave

1

Emotiv

14

MindSet

1

Mrugnięcie oczami, 4
rodzaje fal mózgowych
Fale mózgowe, odczyt
ekspresji twarzy,
odczyt ruchów głową
Odczyt fal
mózgowych, detekcja
mrugnięć

Dostępność SDK
TAK

Dostępność
produktu Producent
od
Marzec
NeuroSky
2011

TAK

Grudzień
2009

Emotiv
System

TAK

Marzec
2007

NeuroSky

Wykorzystanie metod biometrycznych w realizacji interfejsów...
Neural
Impulse
Acctuator

3

XWave

1

MyndPlay

1

Fale mózgowe (alfa
beta), ruchy gałek
ocznych i aktywność
mięśni twarzy
EEG interpretacja
przy użyciu chipu
NeuroSky
EEG interpretacja
przy użyciu chipu
NeuroSky

TAK

Maj
2008

OCZ
technology

TAK

Styczeń
2011

PLX
Devices

TAK

Grudzień
2012

MyndPlay

Rosnąca liczba produkowanych urządzeń tego typu i coraz szersze zastosowanie koncepcji interfejsów człowiek – maszyna, przyczyniają się do powstawania licznych aplikacji, które muszą współpracować z różnorodnymi
urządzeniami. Z tego względu opracowanie pośredniczącej warstwy programowej takiej, jak prezentowany neuro-serwer, nabiera dużego znaczenia
w pracach nad rozwojem interfejsów człowiek – komputer, oraz aplikacji,
które będą je wykorzystywały.

Zastosowania interfejsów człowiek – komputer
opartych o biometrię
Systemy detekcji EEG EMG oraz ruchów gałek ocznych, do niedawna były
stosowane jedynie w celach diagnostyki klinicznej lub w drogich urządzeniach, przystosowanych do ułatwiania życia osobom niepełnosprawnym. Do
wykorzystania urządzeń detekcji impulsów elektrobiologicznych człowieka,
przyczyniły się w dużym stopniu badania nad biologicznym sprzężeniem
zwrotnym (ang. Biofeedback). Biofeedback polega na dostarczaniu człowiekowi informacji zwrotnej na temat jego zmian fizjologicznych organizmu takich, jak zmiany fal mózgowych. Za pomocą tej metody można świadomie
wpływać na generowanie fal mózgowych, ale również wykorzystywać taką
możliwość do przekazywania danych wprost do systemu komputerowego,
z pominięciem aparatury mięśniowej człowieka. Systemy detekcji impulsów
eloktrobiologicznych, które są generowane podczas pracy ludzkiego mózgu
czy aktywności mięśniowej, mogą dostarczyć danych na temat zachowania
człowieka, jego stanu emocjonalnego, skoncentrowania a także kondycji zdrowotnej. Monitorowanie takich parametrów jest obecnie realizowane w sporcie wyczynowym, zastosowaniach wojskowych ( ekstremalne dla człowieka
warunki podczas lotów kosmicznych). Coraz bardziej dostępna aparatura do
pomiaru impulsów bioelektrycznych, przyczynia się do szerszego wykorzystania monitorowania stanu człowieka podczas jego pracy. Jest to zagadnienie
istotne szczególnie w tego typu systemach, gdzie nadrzędną kontrolę stanowi
131

Paweł Buchwald
człowiek, a od jego prawidłowych działań zależy zdrowie i życie innych ludzi
lub bezpieczeństwo i poprawna praca systemów technicznych. Pomimo dynamicznego rozwoju technologii informatycznych z dziedziny sztucznej inteligencji, systemów ekspertowych i automatyzacji pracy, człowiek jest w dalszym ciągu nadrzędnym i niezastąpionym elementem ogólnie pojętego informatycznego systemu zarządzania. Ocena prawidłowej pracy poszczególnych
elementów systemów technicznych a tym samym najważniejszego elementu,
jakim jest zasób ludzki jest ważnym zagadnieniem w kontekście zachowania
ciągłości działania systemu jako całości.
Wykorzystanie biometrii w detekcji stanu użytkowników systemów technicznych, może przyczynić się do wykrycia sytuacji, w której osoba obsługująca urządzenie, nie jest w stanie prawidłowo wykonać swojej pracy,
z powodu chwilowej niedyspozycji wynikającej ze stanu zdrowia lub przemęczenia. Tego typu zagadnienia dotyczą nie tylko pracy w szczególnych
warunkach zagrożenia zdrowia ludzkiego, ale również wielu innych dziedzin życia takich, jak zachowanie bezpieczeństwa ruchu drogowego, czy
monitorowanie kondycji zdrowotnej osób starszych.
Śledząc rozwój popularnych urządzeń do rozpoznawania sygnałów biometrycznych, pozwalających na odczyt EEG, EMG i ruchów gałek ocznych,
można zauważyć iż coraz częściej do akwizycji danych w takich systemach,
są stosowane urządzenia mobilne i bezprzewodowa transmisja sieciowa co
w połączeniu z miniaturyzacją elektrod i systemów pomiarowych, zwiększa
możliwości zastosowania wszędzie tam, gdzie wykorzystanie dużej gabarytowo i mało ergonomicznej aparatury pomiarowej nie jest dostępne.
Na rozwój interfejsów człowiek – maszyna, wpływa także dynamiczny rozwój przemysłu rozrywkowego. Wiele obecnych gadżetów do sterowania
obiektami w grach komputerowych wykorzystując mechanizmy detekcji
EEG czy EMG, z powodzeniem pozwala na przekazywanie danych o intencjach użytkownika do aplikacji komputerowych. Systemy służące do sterowania kursorem myszy lub zdarzeniami w aplikacji dostępne dla przeciętnego
użytkownika, zostały również przedstawione w powyższym opracowaniu.
Pomimo, iż mechanizmy działania metod biometrycznych, są znane od dawna, to ich wykorzystanie w technologiach informatycznych było możliwe
dopiero od lat 60 minionego stulecia. W ostatnim dziesięcioleciu powstały
tanie systemy stanowiące interfejsy człowiek – komputer, do powszechnego
użytku, które zdobywają coraz większą popularność. Z tego powodu należy
spodziewać się ich dalszego rozwoju i integracji z popularnymi narzędziami
informatycznymi.

132

Wykorzystanie metod biometrycznych w realizacji interfejsów...

Literatura
1. Agrawal G., Zikov T. and Bibian S.: “Robust and Real-Time Automatic Detection
of Suppression in EEG Signals,” Proceedings of the 2011 Annual Meeting of the
American Society of Anesthesiologists, Chicago, IL, A503, October 2011.
2. An Luo, Thomas J. Sullivan: A user-friendly SSVEP-based brain-computer
interface using a time-domain classifier; 2010.
3.. Buchwald P.: The Example of IT System with Fault Tolerance in a Small Business
Organisation in A. Kapczyński, E. Tkacz. M. Rostański; Internet – Technical
Developments and Applications 2; Wydawnictwo Springer 2011.
4. Esfahani E.T., Sundararajan V.: Using Brain-Computer Interfaces to Detect Human
Satisfaction In Human-Robot Interaction in International Journal of Humanoid
Robotics 2011.
5. Genaro Rebolledo-Mendez, Dunwell G.I.: Assessing NeuroSky’s Usability to
Detect Attention Levels in an Assessment Exercise; International Conference on
Human-Computer Interaction 2009.
6. Grzywak A., Rostański M., Pikiewicz P.: Sieci Bezprzewodowe; Wyższa Szkoła
Biznesu 2009.
7. Iancovici T.C., Osorio S., Rosario B.: Biofeedback in Virtual Reality Applications
and Gaming, University of Massachusetts Lowell. Introduction to Biosensors.
Spring 2011.
8. Katie Crowley, Aidan Sliney, Ian Pitt, Dave Murphy: Evaluating a Brain-Computer
Interface to Categorise Human Emotional Response 2010.
9. Pikiewicz P., Grzywak A., Buchwald P.: Monitoring method of computer operation
continuity in distraction systems of management and steering, Theoretical and
Applied Informatics, vol. 22, No. 4/2010.
10. Thobbi A., Kadam R., Sheng W.: Achieving Remote Presence using a Humanoid
Robot Controlled by a Non-Invasive BCI Device, ICGST International Journal
on Automation, Robotics and Autonomous Systems, Vol 10, Issue I, page 41-45,
October, 2010.
11. Wang, S., Esfahani, E., Sundararajan V.: Evaluation of SSVEP as passive feedback
for improving the performance of Brain Machine Interfaces, Proc. IDETC/CIE
2012.
12. Yosui Y.: A Brainwave Signal Measurement and Data Processing Technique for
Daily Life Applications; Journal of Physiological Anthropology 2009.

133

Paweł Buchwald
Autorzy (w kolejności alfabetycznej)
PAWEŁ BUCHWALD
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
pbuchwald@wsb.edu.pl
PAWEŁ KOSTKA
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
pkostka@wsb.edu.pl
TOMASZ PANDER
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
tpander@wsb.edu.pl
MACIEJ ROSTAŃSKI
Wyższa Szkoła Biznesu w Dąbrowie Górniczej, Katedra Informatyki
mrostanski@wsb.edu.pl

134

