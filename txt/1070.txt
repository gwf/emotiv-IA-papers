Copyright © 2013, Society of Photo-Optical Instrumentation Engineers. One print or electronic copy may be made for personal
use only. Systematic reproduction and distribution, duplication of any material in this paper for a fee or for commercial purposes,
or modification of the content of the paper are prohibited.

Emotion Scents – A Method of Representing User Emotions
on GUI Widgets
Daniel Cerneaa,b , Christopher Webera , Achim Eberta , and Andreas Kerrenb
a University

of Kaiserslautern, Computer Graphics and HCI Group
P.O. Box 3049, D-67653 Kaiserslautern, Germany;
b Linnaeus University, Computer Science Department, ISOVIS Group
Vejdes Plats 7, SE-35195 Växjö, Sweden
ABSTRACT
The world of desktop interfaces has been dominated for years by the concept of windows and standardized user
interface (UI) components. Still, while supporting the interaction and information exchange between the users
and the computer system, graphical user interface (GUI) widgets are rather one-sided, neglecting to capture
the subjective facets of the user experience. In this paper, we propose a set of design guidelines for visualizing
user emotions on standard GUI widgets (e.g., buttons, check boxes, etc.) in order to enrich the interface with a
new dimension of subjective information by adding support for emotion awareness as well as post-task analysis
and decision making. We highlight the use of an EEG headset for recording the various emotional states of
the user while he/she is interacting with the widgets of the interface. We propose a visualization approach,
called emotion scents, that allows users to view emotional reactions corresponding to diﬀerent GUI widgets
without inﬂuencing the layout or changing the positioning of these widgets. Our approach does not focus on
highlighting the emotional experience during the interaction with an entire system, but on representing the
emotional perceptions and reactions generated by the interaction with a particular UI component. Our research
is motivated by enabling emotional self-awareness and subjectivity analysis through the proposed emotionenhanced UI components for desktop interfaces. These assumptions are further supported by an evaluation of
emotion scents.
Keywords: emotion visualization, GUI widgets, user interface toolkit, information scent

1. INTRODUCTION
Chances are that almost every person who can claim to have worked with a computer in recent years would
immediately recognize the GUI of a desktop environment. But why is this the case? How come this paradigm
of a pointer, of windows, of text ﬁelds and buttons has gained such deep roots in our minds? For sure, one
of the reasons for this is the standardization of the desktop user interface. Today, almost regardless of what
operating system or development environment a user employs, he has access to the standard UI components, all
with roughly the same properties and functionality. But while this uniformity has its advantages, it also has its
pitfalls; one of which is the diﬃculty of combining these standard components with a higher level of ﬂexibility.
At the same time, this rigidity of desktop GUIs means that they tend to remain solely means for interacting and
communicating purely objective information and navigational commands between the user and the computer.
In recent years, researchers have considered enhancing the GUI with subjectivity and emotions1–4 in order
to enrich the user experience and support analysis, collaboration and awareness. However, most of the proposed
solutions aimed at modifying the design of the common desktop by adding virtual (emotional or social) agents
or by oﬀering a highly dynamic interface capable of changing based on the user’s emotions. In the following, we
propose an approach aiming at enhancing the standard UI components with minimal emotional representations.
Called emotion scents, these emotion-enhancements for common GUI widgets can encode and visualize the
Further author information: (send correspondence to D.C.)
D.C.: E-mail: cernea@cs.uni-kl.de; C.W.: E-mail: chweber@rhrk.uni-kl.de;
A.E.: E-mail: ebert@cs.uni-kl.de; A.K.: E-mail: andreas.kerren@lnu.se

Daniel Cernea, Christopher Weber, Achim Ebert, and Andreas Kerren, "Emotion Scents - A Method of Representing User Emotions on
GUI Widgets", in Visualization and Data Analysis 2013, edited by Pak Chung Wong, David L. Kao, Ming C. Hao, Chaomei Chen,
Christopher G. Healey, Mark A. Livingston, Ian Roberts, Thomas Wischgoll, Proceedings of SPIE-IS&T Electronic Imaging, SPIE Vol.
8654, 8654-14 (2013). DOI: ???

emotional states experienced by users while interacting with these widgets. As changes in system properties
can have eﬀects on the emotional states and reactions that the user experiences,5 our hope is to capture user
emotions in real-time through the use of brain-computer interfaces (BCIs) and oﬀer the user a per-component
overview of the experienced subjective states.
The motivation of this research consists in the fact that an additional level of emotional information included
in the GUI can support the user’s emotion awareness as well as, similarly to information scents,6 improve posttask analysis and decision making. Furthermore, emotion scents has the potential of oﬀering a viable alternative
for emotional awareness of users in distributed communication and collaboration, similarly to emoticons.
In the following sections, we ﬁrst oﬀer a brief presentation of research relevant to our endeavor. We then
continue by highlighting the design requirements that emotion scents need to satisfy, followed by information on
the recording and storing of user emotion data as well as a description of the visual encoding and interactions that
have been employed. Further, we highlight the functionality and possible advantages of emotion scents through
a use case and an evaluation. We conclude with a discussion on current limitations and future developments of
our research.

2. RELATED RESEARCH
GUI widgets—like buttons, lists and check boxes—have been present in desktop computing and window-based
interfaces for decades now. More importantly, most of these widgets are standardized over an entire set of
operating systems and programming languages, thus supporting aspects like portability and familiarity with the
representation and interaction paradigm. However, these advantages come at the price of dedicated functionality
and purely information and navigation driven interaction.
To overcome these aspects, many researchers have proposed extensions for the standard UI components that
could enrich their informational and functional attributes. Baudisch et al.7 present a graphical approach that
brieﬂy highlights those UI components that the user has recently interacted with in order to support improved
user awareness without introducing delays or other interference in the interaction process. The topic of awareness
is further discussed in the context of collaborative scenarios by Hill et al.,8 where UI components are augmented
with additional information about the interactions that other users executed on these widgets.
Besides awareness of interaction, another topic that is closely related to the design of the GUI is represented
by the distribution of relevant information in highly complex systems or large structured datasets. In such cases,
information scents or information cues6 have the potential for guiding the user to the areas of the system that
can oﬀer more relevant information, support his analysis process and improve his decision making. An example
in this sense is presented by Willett et al.,9 where GUI widgets are enhanced with additional visual information
cues in order to improve, among others, the information foraging process. Note that all the above mentioned
approaches aim at oﬀering new information without changing the layout or positioning of the various widgets,
thus oﬀering an enhancement and not necessarily an alternative.
The research highlighted up to this point focuses on general enhancements of the UI components and the GUI
in general. But how can the GUI be altered in order to allow the incorporation of emotional information and even
user aﬀective states? Over the years, diﬀerent approaches have been proposed to solve this issue. While some
are simple enhancements like emoticons,10 others focused on changing the desktop interface by incorporating
human-like interfaces like emotional virtual agents,1, 2 adding aﬀective widgets for emotional interaction and
relief3, 4 or designing aﬀective systems11 that can perceive and react to the user’s emotions in application and
task-speciﬁc manner.
While emotions are increasingly reliably detectable through facial expression and gestures,12 advances in
BCI-based recognition of emotions have opened new possibilities for non-invasive and systematic detection, with
applicability in evaluations,13 aﬀective modulation14 and even measurements of mental and emotional states
while working with visualizations.15, 16 Focusing on this direction of emotion-enhanced interfaces through BCI
detected emotions, Liu et al.17 employ a commercial mobile EEG headset to detect user emotions and reﬂect
them in the GUI through a 3D virtual agent. Also, Inventado et al.18 propose a system that supports the easy
emotional annotation of user interaction.

More closely related to our work, Mehrabian19 brieﬂy suggests the representation of emotions in the interface,
while Garcia et al.20, 21 analyzes the possibilities for supporting emotional awareness in collaborative systems.
One of the solutions proposed by Garcia et al.,20, 21 the emotion awareness graph, is particularly relevant to our
GUI emotion visualization, as it represents a custom UI component that consists of a histogram-like representation. The similarity is further supported by the encoded emotional facets, as both approaches use information
related to the dimensions of pleasure/valence and arousal. But while the emotion awareness graph is a custom UI component used to suggest the emotions of other users in the interaction with the system, we aim at
enhancing standardized GUI widgets with emotional representations that suggest past and current emotions of
one or multiple users in the interaction with that particular widget. In other terms, the focus of our work is
not on highlighting the emotional experience during the interaction with an entire system, but to concentrate
on the emotional perceptions and reactions generated by the interaction with a particular UI component, the
corresponding decision and the resulting highlighted information.

3. EMOTION SCENTS
The main concept behind emotion scents focuses on two aspects: enriching the GUI with user emotions and
visualizing these emotions in correlation with the most probable event (or interface component) that generated
or inﬂuenced them. Such a combination can, similarly to information scents,6 guide the user towards new
assumptions and discoveries that otherwise would not have been explored.
In terms of utility, emotion scents are aimed at oﬀering support for real-time emotion awareness and posttask decision making and analysis. The visualized emotions can also have diﬀerent sources: these can be the
emotional states of the current user that also employs the scented interface, or the emotions of other users
recorded in previous sessions. In the ﬁrst case, the goal of the scents is to oﬀer hints to the user about his
emotions and behavior. This way, the user can have a better awareness of his emotions during the interaction
with a UI component. Further, this increased awareness coupled with the correlation between the executed
event and the recorded emotion can suggest links between emotional states and user decisions (e.g., selected one
radio button or another) or reactions (e.g., clicking a button changes the amount or ﬁltering of some information
enabling the user to gather a novel insight).5 In other instances, it might be useful to visualize a temporal
distribution of user emotions for the same application or diﬀerent versions of the same application, oﬀering the
user an overview of his past experiences and emotional states when interacting with the various components of
the GUI.
But emotional self-awareness is not the only target for emotion scents. As suggested above, scented GUI
widgets could also incorporate the emotions recorded in previous sessions by other users. In such cases, the
experiences of a community of users can be detected and harvested, allowing following users to perceive the
collective emotions and reactions of previous users that had to interact with the same UI component (i.e., make
the same decisions, obtain the same data). For example, a user would need to manipulate a slider that is enhanced
by the emotions of other users recorded in previous sessions. By means of emotion scents, the current user can
now see that most people selected value towards the right end of the slider (90-100%), but at the same time,
most of them experienced negative emotions while doing so. On the other hand, those that selected slider values
around 20-30% experienced more pleasant emotional states. This information, correlated with the information
about the application type and the concrete information that the slider is manipulating, should oﬀer the current
user the ability to make a more informed decision while interacting with the application.
The fact that in certain cases emotion scents should represent previously gathered emotional states means a
decoupling of the emotion acquisition process from the visualization through scents. More precisely, in instances
where the visualized emotion scents represent real-time emotional states the user need to be subjected to certain
emotion detection methods (in our case BCI-based detection), while in cases when the user simply executes a
post-task analysis or wants to use the collective emotional experiences of previous users for decision support,
he/she can simply do so by accessing the emotional database containing the previously recorded emotion sessions.
In the following subsections, we will focus on detailing these two independent aspects of emotion scents (i.e.,
acquisition and visualization) as well as highlight the implementation details of our approach.

3.1 Design Guidelines
Before a GUI enhancement like emotion scents can be implemented, one has to consider the eﬀects it can have
on the already existing interfaces and the methods that can be employed in order to transmit the emotional
information to the user. Thus, a set of design requirements should be established that consider particular
aspects of the problem: the type of emotional information metrics, their visual encoding and their inclusion
in the (potentially preexisting) interface. The following design guidelines have been derived through a set of
preliminary studies and the personal experience of experts in the ﬁeld of GUI design, user experience and
human-computer interaction.
• The desktop GUI should suﬀer minimal changes in the process of adaptation. More precisely, if an
application interface already exists, the emotion scents should be implemented on it without disturbing
the layout of the window or changing the positions and sizes of the UI components.
• For the same type of GUI widgets or widgets that are similar in either functionality or representation, the
emotion scents need to be located in the same relative location to the widget. This aids in making
the scents more identiﬁable, while at the same time allowing users to easily establish the correspondence
between scent and UI component.
• Together with the positioning, the size and shape of the scents should allow for comparability between
the various instances in the interface. Thus, when inspecting emotion-enhanced UI components, the user
should be able to diﬀerentiate, analyze and compare their content.
• The user should have the possibility to activate/deactivate the visualization as well as customize the
emotional representation by changing data dimension priorities, employing temporal ﬁltering or selecting a custom range of users for which to represent previous emotional states.
• Due to the restrictions in available GUI area and the necessity to maintain the initial aspect of the interface,
emotion scents should oﬀer a cumulative overview of the emotions experienced by the user or users for
the corresponding UI components. As, for example, clicking a button can happen hundreds of times in a
session, or even thousands of times in a set of user sessions, it seems more important to oﬀer a high-level
view of the experienced emotions than to include clearly browsable emotion information for every event
instance.
• The emotion scents representations should be able to clearly and concisely reﬂect the various dimensions of emotional attributes (see Section 3.2) and their distribution over the user space. In other
words, while it is not the goal of emotion scents to give the ability to inspect details about every pair of
event-emotion instance in the GUI, they should still allow users to diﬀerentiate between various facets of
the experienced emotions (e.g., pleasure and frustration versus excitement and calmness).
• Starting from the previous assessment, one should avoid considering too many emotional dimensions
at the same time. As available space and the shape of the enhancements are vital to avoiding changes in
the interface, ﬁnding the right amount of emotional and statistical information to be displayed is of key
importance.

3.2 BCI and Emotion Data
The detection of user emotions is at the core of a visualization approach like emotion scents. A convenient and
rather precise way of detecting emotional states is oﬀered by modern mobile BCI devices capable of reading brain
signals and interpreting them as emotions. For the emotion scents, the Emotiv EPOC∗ EEG headset has been
employed together with the EPOC framework for easily recognizing the emotional states of the user. The headset
is a lightweight wireless device that employs 14 electrodes in order to capture the electrical signals with brain
activity data from the user. The device is further enabled by a framework that is capable of interpreting these
signals as facial expressions, cognitive commands and emotional states (e.g., excitement, frustration). While the
∗

http://www.emotiv.com

Figure 1. Russell’s circumplex model of aﬀect.22, 23 The model focuses on the distribution of aﬀect concepts based on a
two dimensional encoding of the emotion space: valence and arousal.

Figure 2. Emotion data acquisition and representation loop. Whenever the user interacts with an input GUI widget, the
system records the EEG brain signals via the EPOC headset (1). The signals are then interpreted as emotional states
and stored with a reference to the generating GUI widget (2). The emotions captured this way can be visualized on their
corresponding widget via emotion scents, in real-time for emotion awareness or post-task for analysis and decision support
(3).

framework itself is proprietary, previous research suggests that the detection of facial expressions and emotional
states through the device framework is reasonably accurate.13 However, the focus of this research is on the visual
representation of emotions on desktop GUIs and less on the detection techniques. More precisely, emotion scents
are designed to function with any technique capable of detecting user emotions in real-time (e.g., interpretation
of facial expressions).
As the emotional continuum is extremely vast, we have focused on using an emotional variance space that
would oﬀer suﬃcient information, while at the same time maintaining a low complexity and dimensionality. One
such emotional model is Russell’s circumplex model of aﬀect22, 24 (Figure 1) that focuses on mapping a large
set of emotions in a two dimensional space. The axes of the emotional space are represented in this case by the
valence of the emotion (positive or negative) and the arousal (excited or calm). One advantage of this model is
that the emotions supported by the EPOC framework are also present in the model. As the values outputted
by the framework have been validated in an evaluation,13 it is possible to use the reported emotions from the
EPOC framework as direct input for Russel’s model. At the same time, using a model like this one that maps a
large set of common emotions to a 2D space allows us to decouple the visual representation of emotions on GUI

elements from the medium that executes the actual detection. This means that emotion scents could be used
together with other techniques for emotion detection, like real-time interpretation of facial expressions or voice
analysis.
While other models are available,25 Russell’s model is widely accepted and used also in other aﬀective systems
in visualization.23 Additionally, arousal has been found to be related to valence, as ”arousal increases when
valence extends towards extremely positive of negative values. But for low values of valence, regardless of the
polarity, arousal is almost always low.”24 This interconnection has the potential to further reinforce the correct
interpretations of the detected emotions.
Figure 2 highlights the acquisition and visualization loop of emotion scents. In a ﬁrst step, a user is interacting
with the GUI widgets of an application or system. In parallel, he is wearing the EPOC device that continuously
reads his brain signals. If the user interacts with a particular UI component, e.g., a button, at that instance a
signal is sent to read the current brain signals; these signals are then interpreted as emotions by the framework
on the computer and the results are stored in a database. Depending on the selected mode, these emotions can
be then represented in real-time on the UI component the user interacted with. Alternatively, users have the
possibility of querying this database of collected emotions and show emotion scents for entire sets of emotional
experiences.
Furthermore, there are diﬀerent types of emotional triggers that have to be considered: ”triggers that result
in an action (physical response) or a reaction (emotional response)”.26 More precisely, the assumption we make
is that both emotions that the user experiences slightly before and slightly after the interaction can be related
to the action he exerted. This is due to the fact that users might experience aﬀective states on action (e.g.,
because of a previous decision) or on reaction (e.g., due to new information that has been added to the system).
As such, emotional states are recorded for a couple of seconds before, during and after the interaction. Later,
the user that displays this information has the possibility to customize the emotion scents by selecting between
the modes emotion on action and emotion on reaction.
User emotions are not the only information that are being stored in the emotion dataset of an application.
Besides the valence and arousal values that are detected when a user interacts with a UI component, the system
also stores the exact time when the interaction took place and the identiﬁer of the UI component for subsequent
correlation. Finally, in the case of UI components where the user can select multiple states (e.g., slider with
multiple values, combo boxes, lists, etc.) an additional values is stored representing the selected value. On one
hand, this is relevant because the emotion of the user should be coupled to the actual event and decision; on the
other hand, as highlighted in the next subsection, GUI widgets can have separate emotion scent representations
for each available component state.

3.3 Visual Encoding
Guided by the requirements from Section 3.1, the emotion scents (Figure 3) were designed to encode two
emotional dimensions (valence and arousal) as well as for certain UI components the user selected values. Scents
are visualized as thin, horizontal, colored bars, composed of horizontal segments of equal length x, see Figure 4.
The overall width n of the bar is constant over all the emotion scents in an application in order to allow better
comparability. As such, the size of a segment x is computed by querying the database for the number of distinct
emotional readings available for the current control. For example, if an emotion bar has the width of n = 60
and there are ten distinct emotional readings in the dataset for the current control and timeframe, the width of
a segment will be set to x = 6. This also means that the higher the number of available emotion samples for
a scent, the smoother seems the transition between the colors as segments can take widths of one pixel or less
(Figure 3c).
As each bar segment represents an actual emotional reading, the encoding of the nuance of the emotion
(positive or negative) or the overall excitement is done through colors. But a particular diﬀerence between
the two emotional channels of valence and arousal consists in the fact that while valence is perceived to be
bi-polar, arousal on it’s calm-excited scale seems to be a more uniﬁed characteristic, as suggested by our initial
discussions with GUI designers and HCI experts. Thus, we encoded the two axes as following: red for negative
valence; green for positive valence; and blue for excitement. Similar settings have been used in emotion-related

Figure 3. Diﬀerent representation of emotion scents: (a) Two buttons with their corresponding emotion scent representations for valence (top) and arousal (bottom). (b) Two check boxes with the valence and arousal emotion scents displayed
in parallel. Depending on the sort priority (i.e., based on valence intensity (top) or arousal intensity (bottom)), the
position of the segments in the second dimension are established by their correspondence to the sorted ones. (c) Two
radio buttons presenting their emotion scents encoding valence. The top bar includes over 50 separate EEG readings,
thus resulting in over 50 segments that—depending on the interface—can have a width of under one pixel, resulting in a
smooth transition between colors and suggesting a high number of emotional readings for that particular widget. For the
bottom bar the segments are more visible, as they encode only 10 EEG readings, resulting in wider segments (i.e., larger
x values).

Figure 4. The emotion scent representation. The length n of the bar is predeﬁned. Each colored segment encodes an
emotional state measured during the user’s interaction with the current GUI widget. Green segments represent positive
emotions, while red ones encode negative emotions. In this particular representation the segments are sorted based on the
valence of the emotions in order to allow a better recognition of the emotional distribution. Note that the length of one
segment x is inversely proportional to the number of emotional states stored for the current widget (x = n/nreadings ).

applications,18, 27 as using the trichromat characteristic of the human eye should improve the visibility of the
colored areas—a vital aspect for visualizations with limited display space.
Further, scents are positioned immediately over the lower-left corner of the corresponding UI component
(Figure 3). However, the user has the possibility of adjusting both the size and positioning of the scent to ﬁt
the layout and needs of the application. This is achieved through a context menu that the user can open by
right-clicking on any emotion scent in the application.
Changes executed in the context menu of an emotion scent can have both local (only this scent instance) and
global eﬀects (all emotion scents in the application). One important attribute that can be modiﬁed is the length
of the scents, cf. Figure 5. The bars are set by default to equal lengths, supporting the comparison of ratios
between positive, negative and aroused states. Still, if the user is interested in focusing on the ratio of existing
emotional readings for each UI component, he/she can switch to a view that computes a constant segment width
x and variable bar with n.
Moreover, the user has the ability to choose between representing either one emotional dimension (Figure 3a)
or both valence and arousal in the same bar (Figure 3b). In both situations, the context menu allows emotion
segments to be sorted based on diﬀerent criteria: based on the intensity of the emotional channel (Figure 3a)
or on the temporal order of detection (Figure 8). A special case appears when both valence and arousal are
displayed simultaneously. As the two emotional dimensions recorded for a particular user are closely coupled, one
should consider this connection when representing valence and arousal next to each other. Therefore, in settings
where both axes are visualized, the user has the possibility to sort only based on one emotional intensity, while
the ordering of the second dimension results from the correlations with the ﬁrst. Figure 3b shows two check

Figure 5. Emotion scents representation applied for a combo box control: (left) normalized representation where the width
of each bar is equal to the maximum width n, independently of the number of emotional readings available for each combo
box item (i.e., n is constant); such a representation enables a better ratio comparison. (Right) relative representation
where each emotion segment x has a constant width computed as x = n/max(nireadings ). This representation supports
the comparison of the emotional states quantiﬁed by the ratio of users that interacted with the corresponding items.

Figure 6. Emotion scents represented for slider widgets. The particularity of these widgets consists in the possibility of
having multiple diﬀerent values for a single GUI item. The top two sliders support the representation of emotion scents
by displaying a single colored bar that follows the slider head and encodes the cumulated emotions for the corresponding
slider values. In cases like the bottom slider, where the distance between the possible slider head positions is large enough
to accommodate the width of emotion scents, multiple bars can be displayed at the same time.

boxes enhanced with emotion scents. In the top scent, for example, the valence bar is sorted by the intensity of
the emotion (starting from very negative to very positive). The arousal segments from the blue bar below are
sorted simply by establishing the correspondence to the same emotional instance/reading. This approach can
have certain advantages, as the emotion scent for the top check box shows: while there seems to be a slightly
higher prevalence of negative emotions, the users that experienced positive feelings while interacting with the
widget have also experienced higher levels of excitement.
As mentioned in the beginning of this section, for some UI components the user has the possibility to select
a value. One such component that we considered is the slider (Figure 6). In order to maintain consistency and
avoid visual overload, for sliders only one single emotion scent is represented that follows the position of the
slider head and indicates the emotional states of the users that made the same selection. In cases with sliders
that allow selecting a high number of values (40+), the scent can be customized to consider a window region
around the current selection of the slider head and display all the emotions for users that selected values inside
this window. On the other hand, is a slider has very few selectable values (Figure 6-bottom), emotion scents can
be displayed for the entire length of the slider, encoding the emotional states of the users that selected values in
the corresponding selection windows.
In terms of implementation, the emotion scents have been implemented in Java using the Swing GUI widget
toolkit. The representation was obtained by implementing an alternative Look&Feel (or theme) that incorporates
these colored scents. After the emotion scent theme is included in an application, the user has the possibility of
enabling this interface by the simple click of a menu button. In the current version, only input or navigational

Figure 7. Emotion scents encoding the user arousal during the interaction with the SwingSet demo interface‡ . For certain
GUI widgets, one can detect a greater incidence of higher arousal values.

widgets have been enhanced with emotion bars (Figure 7): buttons, combo boxes, check boxes, radio buttons,
tabs, lists, tree views, sliders, and spinners. This is partly due to the fact that the user does not need to interact
with all GUI widgets (e.g., labels or icons), which in turn means that in some cases there is no possibility to
establish a correlation between an event and an emotion.

4. USE CASE
Figure 8 highlights a use case for the emotion scents visualization: A programmer called John starts a coding
session in his Java development environment. During a two hour period in which he is wearing a mobile BCI
and/or is recorded by a facial expression system, he extends an already existing code by making changes in
multiple code ﬁles, and compiles the application several times. At the end of his session, he goes to the view
section in the menu bar and selects another Look&Feel (i.e., theme) that displays the emotion cues inside the
application GUI (Figure 8). By right-clicking on any of the emotion scents, he can ﬁlter the time period for
which the emotion data is considered and customize their representation through a context menu. After selecting
the data for the last two hours, he notices based on the valence representation that he accessed two ﬁles very
often and that in most cases his experiences when selecting these ﬁles were positive. At the same time, John
observes that the debug button was executed quite often and that especially in the second half of the session,
debugging operations were becoming frustrating to him. A similar pattern is valid for the execution button. He
thinks back and remembers that he had to handle a set of errors that appeared due to his initial design. John
switches to the arousal visualization and notices the same pattern: higher excitement value for the second half
of his session in the case of the debug and run buttons. He now realizes that the errors he was trying to solve
put him under more stress than he initially realized. Thus, he decides that in the future he should invest a bit
more time in the design process to avoid unnecessary stress and obtain a better productivity.
A similar example, but where the emotion information of multiple users is visualized, can be given with the
following case. A company is creating a complex software solution. The developers and designers want to ﬁnd out
‡

http://java.sun.com/products/plugin/1.4/demos/plugin/jfc/SwingSet2/SwingSet2.html

Figure 8. Emotion scents displayed on the interface of a simple Java IDE. The colored bars represent the valence of the
emotion and the emotional states are in temporal order. The multiple red segments on the debug button could suggest
that the programmer experienced diﬃculties in his coding process, especially in the second part of his coding session.

which modules and corresponding interfaces of the application induce frustration to the users. For this purpose,
during the testing stage they collect—with the consent of the users—anonymous emotion data correlated with
the various GUI widgets present in all the windows of the application. With this emotion database, they can now
inspect what events produced intense emotions on user-side and analyze how these states might be inﬂuenced
by user decisions, system interaction or displayed information.
Certainly, in both examples the emotion scents visualization work only under the assumption that a method
has been employed in order to record the emotional states of the user. While emotion detection based on
facial expressions or speech is also a viable alternative, we focus on mobile BCI technologies that present a great
potential. Even if current BCI approaches can be still cumbersome by requiring wet sensors and exact positioning
on the scalp, emotion scents is aimed partly at future generations of these mobile headsets, where using a BCI
could already become as simple as putting on a hat.

5. EVALUATION
An evaluation was devised in order to capture some of the advantages that an emotion-enhanced interface can
oﬀer. For this purpose, a tool was implemented that visualizes the U.S. foreign aid over the last decades, see
Figure 9. The visualization was implemented in Java, while the interface was coded in the Swing widget toolkit;
the represented data was obtained from the ManyEyes§ website. Additionally, the standard GUI widgets have
been extended to implement emotion scents.
The evaluation included a group of 24 participants (9 women, 15 men) with ages between 19-55 years and an
average of 28.25 years. The group consisted of people with at least intermediate experience with computers and
mostly with previous experience in working with visualizations. After a brief introduction to the visualization
tool, the group was randomly subdivided in two equal sets of 12 participants. The members of both groups
§

http://www-958.ibm.com/software/data/cognos/manyeyes/

Figure 9. Visualization application for a dataset obtained from the ManyEyes website showing the distribution of U.S.
Foreign Aid in the recent decades. The GUI widgets have been enhanced with emotion scents to give another dimension
of information to the navigation and analysis process of the data.

received the task of gathering insights from the proposed visualization. More speciﬁcally, they were asked to
inspect if they could detect any patterns between armed conﬂicts that the U.S. were involved in over a period of
50 years and the distribution of foreign aids. One of the simplest possible connection that we were looking for
was the detection of an increased foreign aid to the nations with which the U.S. has been previously involved in
a conﬂict.
The members of the ﬁrst group were asked to undertake the task ﬁrst. Further, these participants were
equipped with an EEG headset in order to record their emotional states during their interaction with the
interface. However, none of them did beneﬁt from having any emotion scents displayed on the interface. Also,
each participant was asked to signal whenever he/she reached an insight. This approach would, on the one
hand, allow the recording of the emotional reaction while interacting with the visualization and the data, and
on the other hand it would oﬀer a set of time intervals that each participant required to detect the abovementioned pattern. After the task, each member of the ﬁrst group was additionally asked to ﬁll in a short
questionnaire focused on their emotional reactions. Most users reported having no emotional reactions while
interacting with the visualization. At the same time, four participants mentioned being surprised at some of
the ﬁndings they made. However, the measurements from the EEG device suggested additional events with
emotional aspects. Finally, six participants suggested that the EEG headset was uncomfortable to wear, while
four expressed concerns about possible privacy issues (”You mean someone else can now see what I’m feeling?”).
Once the members of the ﬁrst group ﬁnished the task and the questionnaire, the participants from the second
group were given the same task, this time with the emotion scents constructed based on previous user experiences
displayed on the GUI of the visualization. Again, the time interval was recorded that each group member required
in order to detect the pattern. Note that while the emotion scents can oﬀer information about the views or
GUI widgets that were more often visited, this is only a secondary purpose of the representation. Compared to
the work of Willett et al.,9 emotion scents focus on oﬀering information about the emotional states the users
experienced during or right around the period of their interaction with a certain widget. If this information
is not available or not relevant, the information about the frequency of visit is also not included (i.e., a white
emotion bar does not allow the detection of gradients).
After analyzing the results from the evaluation, we found that the ﬁrst group that did not have the aid from
the emotion scents took on average 310s to ﬁnd the pattern (AV G1 = 310, SD1 = 68). At the same time, the
second group managed to gather the same insights in 248s (AV G2 = 248, SD2 = 44). These values suggest that

representing the emotions that users had during their interaction can oﬀer new users orientational cues about
application features that were emotionally remarkable to their predecessors.
To brieﬂy inspect the potential for self-awareness, the participants from the second group were also presented
the simple Java compiler from Figure 8. After a similar scenario to the one highlighted in Section 4 was presented
to them, they were asked the utility of such a representation for self-awareness. In this case the results were
more evenly distributed, as six participants could imagine having emotion scents included in various interfaces
for enabling emotional self-awareness, while the others were more skeptical.
Finally, the members of the second group were asked to ﬁll in a questionnaire. This time, the questions
were focused in the representation of emotion scents and their utility. All participants considered that the color
coding is useful, while ﬁve participants suggested that the size of the emotion bars is rather small not allowing
them to see details. Still, after a informal inquiry, these participants agreed that they still had the possibility of
detecting the diﬀerent colors and their ratio in the bar.

6. DISCUSSION
Our ﬁndings from the evaluation of emotion scents suggest that this integrated, in-place visualization of user
emotions can enhance window-based interfaces and oﬀer additional information about the subjective impressions
and experiences of users while interacting with the GUI. Also, the supplied emotional information incorporates
two facets of the emotional space, namely valence and arousal, oﬀering an overview of the emotional reactions
of one or multiple users in previous or the current session. While inspecting the design guidelines, we can notice
that the enumerated requirements have all been considered in the current representation. In terms of integration,
the GUI has suﬀered only minimal changes, and the location of the scents allows for good comparability. More
importantly, the visualization encodes emotional information in a way that allows the detection of emotional
trends on each UI component but without overloading the interface or the emotion bars. Additionally, the
low resolution oﬀered by a representation within a small display area is also intended to reﬂect elements like:
the complexity of human emotions, a limited precision of the acquisition of emotions, and a certain level of
uncertainly in the correlation of user emotions and events (i.e., in some cases, the emotional states recorded at
the moment when an event takes place might be generated by other environmental or user-internal aspects).
Furthermore, it seems important to highlight that one should not directly interpret emotion scents as indicatives of right and wrong. Scents represent basically an additional level of information that is being collected
and reﬂected, and that capture the user’s subjective views. Having negative emotions in a certain case or when
manipulating a certain UI component might in some cases even be desired, e.g., positive feelings after saving an
image one has just modiﬁed in a photo editor, or negative feelings after exiting an application the user really
enjoys.
Another vital aspect of recording and visualizing emotions in the interface is privacy. As one would expect,
people are in general concerned about having their emotions recorded, distributed, analyzed. While some of
these issues can be solved by collecting only anonymous data and, of course, with the consent of the user, there
are other important aspects that need to be addressed. Currently, the implemented emotion scents system stores
the emotional readings in a database local to the machine on which the toolkit was installed. Therefore, the
security and distribution of emotion data is entirely under the user’s control. At the same time, in order to
support data transfer and analysis, users have the possibility of importing emotion information from external
sources and represent them on a system where emotion scents is running.

7. CONCLUSION AND FUTURE WORK
In this paper, we have introduced a solution for visualization of user emotions during his interaction with a
speciﬁc UI component. We proposed a set of design requirements that would not require a complete redesign
of the standard desktop interface paradigm, and highlighted our solution entitled emotion scents. We suggested
one approach for obtaining information about user emotions through a mobile, wireless BCI headset and showed
how emotion scents are integrated in the GUI. A set of examples and an evaluation suggest that emotion scents
can be used successfully in desktop environments for emotional awareness and decision support, as well as for
analysis of user emotions over the various GUI widgets of a system, a set of users, and/or a particular timeframe.

Aim of our future work is to improve the management of the emotion database, as well as the ﬂexibility
of the privacy settings. To this end we plan to implement a privacy management system that allows users to
view and select emotion frames/intervals or emotions related to a UI component and decide if they would like
to make these public or private. These features would than need further reﬁnements to clearly establish the
diﬀerent sublevels of privacy: Can public emotions be visualized on the same computer if another user starts the
application? Or can public emotions be transmitted to a server and used by the developer of the application to
improve the user experience and get feedback on how the application is perceived?
The current implementation only focuses on navigational and input GUI widgets as these are the only ones
where the user needs to focus on or interact with the widget. For output widgets, like UI labels, the user only
perceives the information displayed without him needing to execute mouse clicks or manipulate the component
in any way. As a result, it is diﬃcult to establish a correlation between the GUI widget and the generated
emotion. Still, the incorporation of methods like eye-tracking should support a better detection of the user’s
focus point, thus oﬀering a stronger correlation between the interface and the user experience, while at the same
time removing some of the emotional noise generated by internal (e.g., the user daydreams and looks away from
the screen) or external (e.g., someone approaches the user and starts a discussion) factors.

ACKNOWLEDGMENTS
This work was supported by the German Research Foundation (Deutsche Forschungsgemeinschaft, DFG) as part
of the International Graduate School (International Research Training Group, IRTG 1131) in Kaiserslautern,
Germany. The authors would also like to thank Kristin Suchner for her contribution to the implementation
eﬀort, as well as the participants of the user study for their time and useful remarks.

REFERENCES
[1] Masuch, M., Hartman, K., and Schuster, G., “Emotional agents for interactive environments,” in [Proceedings of the Fourth International Conference on Creating, Connecting and Collaborating through Computing],
C5 ’06, 96–102, IEEE Computer Society, Washington, DC, USA (2006).
[2] Lisetti, C. L. and Nasoz, F., “Maui: a multimodal aﬀective user interface,” in [Proceedings of the tenth ACM
international conference on Multimedia], MULTIMEDIA ’02, 161–170, ACM, New York, NY, USA (2002).
[3] Hole, L. and Millard, N., “Aﬀective widgets: supporting the daily work experiences of contact centre
advisors,” in [Proceedings of the Nordichi User Experience Workshop 2006 ], (2006).
[4] Millard, N. and Hole, L., “In the moodie: Using ’aﬀective widgets’ to help contact centre advisors ﬁght
stress.,” in [Aﬀect and Emotion in Human-Computer Interaction ], Peter, C. and Beale, R., eds., Lecture
Notes in Computer Science 4868, 186–193, Springer (2008).
[5] Thuring, M. and Mahlke, S., “Usability, aesthetics and emotions in human-technology interaction,” International Journal of Psychology 42(4), 253–264 (2007).
[6] Chawla, S. and Bedi, P., “Improving information retrieval precision by ﬁnding related queries with similar
information need using information scent,” in [Proceedings of the 2008 First International Conference on
Emerging Trends in Engineering and Technology ], ICETET ’08, 486–491, IEEE Computer Society, Washington, DC, USA (2008).
[7] Baudisch, P., Tan, D. S., Collomb, M., Robbins, D. C., Hinckley, K., Agrawala, M., Zhao, S., and Ramos,
G., “Phosphor: explaining transitions in the user interface using afterglow eﬀects.,” in [UIST ], 169–178,
ACM (2006).
[8] Hill, J. and Gutwin, C., “Awareness support in a groupware widget toolkit,” in [Proceedings of the 2003
international ACM SIGGROUP conference on Supporting group work], GROUP ’03, 258–267, ACM, New
York, NY, USA (2003).
[9] Willett, W., Heer, J., and Agrawala, M., “Scented widgets: Improving navigation cues with embedded
visualizations.,” IEEE Trans. Vis. Comput. Graph. 13(6), 1129–1136 (2007).
[10] Huang, A. H., Yen, D. C., and Zhang, X., “Exploring the potential eﬀects of emoticons,” Inf. Manage. 45,
466–473 (Nov. 2008).

[11] Akgun, M., Akilli, G. K., and Cagiltay, K., “Bringing aﬀect to human computer interaction,” in [Aﬀective
Computing and Interaction ], 308–324 (2010).
[12] Castellano, G., Kessous, L., and Caridakis, G., “Aﬀect and emotion in human-computer interaction,”
ch. Emotion Recognition through Multiple Modalities: Face, Body Gesture, Speech, 92–103, SpringerVerlag, Berlin, Heidelberg (2008).
[13] Cernea, D., Olech, P.-S., Ebert, A., and Kerren, A., “Measuring subjectivity - supporting evaluations with
the emotiv epoc neuroheadset.,” Journal for Artiﬁcial Intelligence (KI) 26(2), 177–182 (2012).
[14] Garcia Molina, G., Tsoneva, T., and Nijholt, A., “Emotional brain-computer interfaces,” in [ACII 2009:
Aﬀective Computing & Intelligent Interaction], Cohn, J., Nijholt, A., and Pantic, M., eds., 138–146, IEEE
Computer Society Press, Los Alamitos (September 2009).
[15] Cernea, D., Olech, P.-S., Ebert, A., and Kerren, A., “Controlling in-vehicle systems with a commercial eeg
headset: Performance and cognitive load,” OpenAccess Series in Informatics (OASIcs) (2012).
[16] Anderson, E. W., Potter, K. C., Matzen, L. E., Shepherd, J. F., Preston, G., and Silva, C. T., “A user
study of visualization eﬀectiveness using eeg and cognitive load.,” 791–800, Proceedings of Comput. Graph.
Forum. 2011 (2011).
[17] Liu, Y., Sourina, O., and Nguyen, M. K., “Real-time eeg-based human emotion recognition and visualization,” in [Proceedings of the 2010 International Conference on Cyberworlds ], CW ’10, 262–269, IEEE
Computer Society, Washington, DC, USA (2010).
[18] Inventado, P. S., Legaspi, R. S., Numao, M., and Suarez, M., “Observatory: A tool for recording, annotating
and reviewing emotion-related data.,” in [KSE ], 261–265, IEEE (2011).
[19] Mehrabian, A., “Framework for a comprehensive description and measurement of emotional states.,” Genetic, social, and general psychology monographs 121, 339–361 (Aug. 1995).
[20] Garcı́a, O., Favela, J., and Machorro, R., “Emotional awareness in collaborative systems,” in [Proceedings
of the String Processing and Information Retrieval Symposium & International Workshop on Groupware],
SPIRE ’99, 296–, IEEE Computer Society, Washington, DC, USA (1999).
[21] Garcı́a, O., Favela, J., Licea, G., and Machorro, R., “Extending a collaborative architecture to support
emotional awareness,” in [Emotion Based Agent Architectures (ebaa99], 46–52 (1999).
[22] Russell, J. A., “A circumplex model of aﬀect,” Journal of Personality and Social Psychology 39, 1161–1178
(1980).
[23] Stahl, A., Sundström, P., and Höök, K., “A foundation for emotional expressivity,” in [Proceedings of DUX
’05 conference on Designing for User eXperience ], (2005).
[24] Gokcay, D., “Emotional axes: Psychology, psychophysiology and neuroanatomical correlates,” in [Aﬀective
Computing and Interaction: Psychological, Cognitive and Neuroscientiﬁc Perspectives, Information Science
Publishing, IGI Global], 56–73 (2010).
[25] Dryer, C. D., “Dominance and valence: A two-factor model for emotion in hci,” in [Emotional and Intelligent:
The Tangled Knot of Cognition. ], TR FS-98-03, 75–81, AAAI Press, Menlo Park, CA, USA (1998).
[26] El-Nasr, M. S., Morie, J., and Drachen, A., “A scientiﬁc look at the design of aesthetically and emotionally
engaging interactive entertainment experiences,” in [Aﬀective Computing and Interaction: Psychological,
Cognitive and Neuroscientiﬁc Perspectives, Information Science Publishing, IGI Global ], 281–307 (2010).
[27] Ohene-Djan, J., Sammon, A., and Shipsey, R., “Colour spectrum’s of opinion: An information visualisation
interface for representing degrees of emotion in real time,” in [Proceedings of the conference on Information
Visualization ], IV ’06, 80–88, IEEE Computer Society, Washington, DC, USA (2006).

