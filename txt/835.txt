mm13-89

1

User-centered EEG-based Multimedia Quality
Assessment
Arghir-Nicolae Moldovan, Ioana Ghergulescu, Stephan Weibelzahl and Cristina Hava Muntean

Abstract—Multimedia users are becoming increasingly
quality-aware as the technological advances make ubiquitous the
creation and delivery of high-definition multimedia content.
While much research work has been conducted on multimedia
quality assessment, most of the existing solutions come with their
own limitations, with particular solutions being more suitable to
assess particular aspects related to user’s Quality of Experience
(QoE). In this context, there is an increasing need for innovative
solutions to assess user’s QoE with multimedia services. This
paper proposes the QoE-EEG-Analyser that provides a solution
to automatically assess and quantify the impact of various factors
contributing to user’s QoE with multimedia services. The
proposed approach makes use of participant’s frustration level
measured with a consumer-grade EEG system, the Emotiv
EPOC. The main advantage of QoE-EEG-Analyser is that it
enables continuous assessment of various QoE factors over the
entire testing duration, in a non-invasive way, without requiring
the user to provide input about his perceived visual quality.
Preliminary subjective results have shown that frustration can
indicate user’s perceived QoE.
Index Terms—Quality of Experience, multimedia quality
assessment, subjective assessment methods, objective quality
metrics, EEG.

M

I. INTRODUCTION

ULTIMEDIA services such as video sharing, video-ondemand, IPTV, and video conferencing are increasing
fast in popularity, with the network traffic generated by these
services being projected to increase three times between 2012
and 2016, and to reach 55% of the global Internet traffic by
2016 [1]. At the same time, users increasingly use multiple
devices such as smartphones, tablets, PCs and even smart TVs
for conducting online activities in general and for accessing
multimedia services in particular [2]. With the proliferation of
high-resolution device displays and powerful video cameras
integrated with mobile devices, of high-definition multimedia
content, as well as of high-speed wireless networks that enable
delivering this content, users’ expectations in terms of the
quality offered by multimedia services are also increasing.
There are many factors contributing to user’s perceived
quality of multimedia services, with research and industry
alike, gradually shifting from the notion of Quality of Service

Manuscript received May 9, 2013. This work was supported by Irish
Research Council’s EMBARK Initiative.
A.-N. Moldovan, I. Ghergulescu, S. Weibelzahl and C. H. Muntean, are
with the School of Computing, National College of Ireland, Mayor Street,
IFSC,
Dublin
1,
Ireland
(e-mail:
amoldovan@student.ncirl.ie;
ioana.ghergulescu@student.ncirl.ie;
sweibelzahl@ncirl.ie;
cmuntean@ncirl.ie).

User-related
Factors
- preferences
- expectations
- emotions
- purposes of use
- visual acuity
- device performance
and characteristics
- etc.

Audio Qualityrelated Factors
- clarity
- echo
- background noise
- naturalness
- speech
- pitch fluctuations
- synchronisation
- etc.

Network
Transmissionrelated Factors
- throughput
- buffering
- delay
- jitter
- loss
- etc.

Multimedia QoE
Factors
Visual Qualityrelated Factors
- blurriness
- blockiness
- sharpness
- clarity
- brightness
- colours saturation
- motion naturalness
- text readability
- etc.

Content-related
Factors
- type (e.g., news,
sports, cartoons,
movies, etc.)
- features (e.g., text,
shapes, persons,
animations, etc.)
- spatial details
- motion vectors
- colourfulness
- etc.
Media-related
Factors
- video codec
- resolution
- framerate
- video bitrate
- audio codec
- audio bitrate
- sampling rate
- etc.

Fig.1. Factors contributing to user’s QoE with multimedia services.

(QoS), to broader aspects such as Quality of Perception (QoP)
or Quality of Experience (QoE) [3],[4], [5]. Figure 1 presents
a list of multimedia QoE factors, grouped in different
categories such as: user-related, network transmission-related,
content-related, media-related, visual quality-related and audio
quality-related factors. This list is not an exhaustive one and
many other factors can contribute as well to a user’s QoE.
The high number of factors and combinations in which they
may occur, makes it very difficult to assess user’s QoE.
Although many solutions for multimedia quality assessment
have been proposed, this area is far from being fully
understood and continues to draw much attention from
researchers, industry and standardisation organizations.
Subjective assessment consisting of a number of
participants providing input (e.g., perceived multimedia
quality ratings) relative to their QoE with the multimedia
service, is usually considered the most accurate and reliable.
However, this approach presents significant limitations when
it comes to assess and quantify the contribution of multiple
QoE factors as participants would be disturbed from their
natural viewing experience due to the interaction taking place
during the subjective study. At the same time, existing
objective metrics that aim to enable automatic multimedia

mm13-89

2

quality assessment without user input, usually tend to offer
mixed performance as they exploit various content
characteristics and artifacts (e.g., blurriness, blockiness, etc.),
or have requirements such as for example strict temporalspatial synchronization that render them unsuitable in case of
adapted streams with varying characteristics (e.g., resolution,
frame rate, etc.), and/or network transmission-related
distortions (e.g. delays, frames loss, etc.) [6].
In order to overcome many of the challenges with
multimedia delivery such as the diversity of device
characteristics and the variable network conditions, adaptive
multimedia streaming has evolved from a mainly research
topic few years ago to a number of industry solutions and
standards being available nowadays [7].
In this context, this paper aims to propose a hybrid
subjective-objective solution for automatically assessing and
quantifying the impact of multiple factors contributing to
user’s QoE with adaptive multimedia services. Instead of
asking the participants to provide input, the proposed QoEEEG-Analyser uses as indicator of participant’s QoE, their
frustration level measured continuously over the testing
session using the Emotiv EPOC1 EEG system. Frustration was
previously related to user’s QoE with applications, for
example relative to user’s tolerance to delay [8].

II. RELATED WORK
A. Multimedia QoE Assessment
Various methods for assessing the multimedia quality, both
subjectively by asking a group of subjects to rate the quality or
objectively by computing metrics with the help of software
programs [9], have been proposed.
A number of subjective methods for assessing the visual
and audio-visual quality of multimedia applications were
standardized in ITU-T Rec. P.910 [10] and ITU-T Rec. P.911
[11], which provide useful guidelines regarding the subjects
and test material selection, the test environment setup, rating
scales to be used for assessment, as well as methods for
analysing the data. The main difference between the various
methods consists in the way the test sequences are presented
and rated such as: one-by-one presentation and rating of
individual sequences (e.g., Absolute Category Rating method),
rating of the difference between pairs of sequences (e.g.,
Degradation Category Rating method), or continuous quality
rating over the entire sequence duration (e.g., Single Stimulus
Continuous Quality Evaluation method).
The Mean Opinion Score (MOS) representing the average
subjective quality ratings across all participants are usually
represented on nominal scales, such as for example a 1 to 5
scale, where 1-Bad and 5-Excellent. Since one approach to
interpret QoE is as the “overall acceptability of an application
or service as perceived subjectively by the end-user” [12],
mappings between MOS and the binary measure of
acceptability have also been proposed [13].
1

Emotiv, http://www.emotiv.com/

Subjective assessment of user perceived quality (QoP) was
also extended to include user’s satisfaction with the
multimedia clip measured on a 6-level scale, together with
his/her ability to perceive, synthesise and analyse the
informational content, measured as the percentage of correct
answers provided to a series of content-related questions [3].
Another mixed approach called Open Profiling of Quality
(OPQ), which combines traditional analysis of user-provided
quality ratings with quantitative data analysis, was also
proposed [14]. By making use of various advanced statistical
procedures for data analysis, OPQ was shown to provide a
deeper understanding of user’s QoE with 3D multimedia
content by identifying important underlying factors related to
content, viewing experience, as well as visual, audio and
audiovisual quality (e.g., motion, clarity, colour, brightness,
ease of viewing, eye strain, etc.) [15].
Objective video quality assessment metrics range from fullreference metrics that compare the quality of a clip relative to
that of its reference unaffected by distortions, to reducedreference and no-reference metrics that require only some or
no information regarding the original clip [16]. Existing
objective metrics exploit various characteristics of natural
images (e.g., edge, plane, texture, etc.), or of the human visual
system (e.g., sensitivity to change in local gradient, contrast,
blockiness, etc.) [17].
B. Electroencephalography
Electroencephalography (EEG) represents the process of
recording electrical brain activity with the help of electrodes
placed on the human scalp [18]. The recorded EEG signal is
composed of multiple individual oscillations at different
frequencies, which have been grouped in five major frequency
bands or brain waves: delta, theta, alpha, beta, and gamma.
While oscillations characteristic for all of these waves may be
present in the overall EEG signal at the same time, one or
another are usually dominant during certain states of
awareness or different activities.
Various researchers have explored the possibility of using
EEG as a means for detecting or differentiating between
human basic emotions such as happiness, joy, distress,
surprise, anger, fear, disgust and sadness [19], [20], [21], or
between learning related emotions such as engagement,
boredom and frustration [22], [23], [24].
In multimedia-related research areas, EEG has been used
for automatic implicit emotional tagging of multimedia
content, as an alternative to explicit approaches that require
users to tag the clips themselves [25]. Furthermore, the
possibility of using EEG for assessing user’s perceived
multimedia quality has also been explored. For example, the
P300 EEG component consisting of a peak in the EEG signal
voltage occurring approximately 300ms after a significant
event, was shown to have the potential of being used in order
to assess whether users perceived changes in the multimedia
quality [26], [27]. The authors showed that the more degraded
a video is (i.e., due to compression), the earlier and higher the
P300 amplitude is rising.

mm13-89

3

Fig.2. Block-level architecture of QoE-EEG-Analyser.

While traditional EEG equipments have a high cost and
require a laborious setup that can take as much as one hour for
a single participant [18], more recently a number of consumergrade EEG equipments (e.g., Emotiv EPOC, NeuroSky2), have
started to be commercialised. Although these equipments
present a number of limitations, such as lower number of
sensors and functionalities, thanks to their affordable prices
and reduced intrusiveness they draw the attention of an
increasing number of researchers.
The Emotiv EPOC system in particular is a wireless brain
computer interface device that records EEG signals from 14
electrodes. Along with raw EEG signals, the system enables to
capture affective data (i.e., engagement, frustration,
excitement and meditation), as well as expressive data (e.g.,
blink, right/left wink, look right/left, eyebrow raise, smile,
laugh, etc.). Additionally, the device can be trained in order to
enable brain controlled actions (e.g., moving objects) based on
brain waves. The Emotiv EPOC was shown to provide
accurate affective data when validated against questionnaires
[28], [29].
Examples of research studies that have used the Emotiv
EPOC system range from remote controlling objects such as a
smartphone [30] or a robot arm [31], to measuring human
emotional states during educational related studies [32], [33].
In multimedia-related research, the affective data provided by
the Emotiv EPOC system was used in order to add context to
user’s tagging of multimedia clips [34].
Literature review shows that few research studies have
explored the potential of using EEG for multimedia quality
assessment. Furthermore, to the best of our knowledge no
previous research used the affective data provided by the
Emotiv EPOC system in order to enable automatic assessment
and quantifying of individual factors on user’s perceived
multimedia quality.

2

Neurosky, http://www.neurosky.com/

III. QOE- EEG-ANALYSER
A. Block Architecture
The block-level architecture of the proposed QoE-EEGAnalyser is presented in Figure 2. In order to assess and
quantify the impact of various factors on user’s QoE, a
subjective study has to be conducted. During the study a
number of participants are asked to view the test multimedia
streams affected by various adaptive strategies or artifacts
(e.g., variable resolution, bitrate, framerate, buffering, loss,
etc.), named QoE factors thorough this paper.
While viewing the streams, participants’ frustration level is
continuously computed by the EmoEngine at a granularity of
approximately one sample per second, based on the raw EEG
data sent by the Emotiv EPOC headset. The frustration values
measured on a continuous scale between 0 and 1, are retrieved
by the Frustration Logger through the Emotiv API, and saved
to the Frustration Logs Database for further analysis.
In parallel, the MM (Multimedia) Metrics Computation
component analyses the multimedia streams and computes a
number of metrics that describe how the values of the
corresponding QoE factors change over time. Metrics that are
automatically extracted from the stream includes for example
the video bitrate and resolution, if say the impact of varying
these parameters with the available network bandwidth is
analysed. The list of QoE factors under test (see Figure 1 for a
list of possible factors), is taken from the QoE Factors
Database, while the values of the metrics are stored in the MM
Metrics Logs Database.
Based on the information collected from multiple
participants, the QoE Factors Assessment component assesses
the impact of individual QoE factors on user’s frustration.
This is done by analysing how participants’ frustration
changes for various configurations of the QoE factor values
occurring over the duration of the multimedia streams. The
impact of a QoE factor is determined based on the relative
frustration change it causes, and is stored in the QoE Factors
Impact Database.

mm13-89

4

Algorithm 1: Assessment of a QoE factor
INPUT
Get frustration logs
Get MM metrics logs
PROCEDURE
begin
detect standalone configurations
for all
= detectFactorValue ( )
for all k participants
Compute Δ ,
endfor
Δ = average(Δ , )
endfor
lm = linearModel( , Δ )

.

(a)

OUTPUT
lm
B. Principle of QoE- EEG-Analyser
As previously shown in Figure 1, there are various factors
impacting user’s QoE with multimedia services. Assume that
there are N factors whose impact has to be analysed, which are
grouped in a set expressed as in (1).
,

1,

(1)

can take different values
on a
A specific QoE factor
continuous or discrete scale. For example, the QoE factor
‘content type’ can take a value in a predefined set (e.g., news,
sports, cartoons, etc.), while the QoE factor ‘video bitrate’ can
vary between a minimum and a maximum value for each
measurement unit (e.g., frame, second, segment, clip, etc.).
representing the number of independent
Having the
values for a
factor, the total number of standalone
configurations tested can be expressed as in (2). The
standalone configurations ( ) corresponding to a QoE factor
represent configurations when the value of that factor varies,
while all the other factors are maintained constant.

(b)
Fig.3. Subjective multimedia quality assessment study: (a) Graphical
illustration of the test-bed setup; (b) Lab testing environment.

IV. SUBJECTIVE CASE STUDY AND RESULTS
A subjective study was conducted in order to analyse if the
proposed QoE-EEG-Analyser, and in particular the approach
of using the EEG-based frustration measures provided in realtime by the Emotiv EPOC system can be used in order to
assess and quantify the impact of various factors on user’s
QoE. The study involved a number of participants watching
and rating the quality of multimedia clips with changing
quality levels, while wearing the Emotiv EPOC headset.

(3)

A. Test Setup
The evaluation test-bed is illustrated in Figure 3. A Google
Nexus 7 tablet mobile device running on Android 4.2
operating system was used for displaying the multimedia clips
used for testing. The device has a 7” LED-backlit IPS LCD
capacitive touchscreen with a resolution of 1280 x 800 pixels.
Furthermore it has an Nvidia Tegra 3 1.2 GHz Quad-core
ARM Cortex A9 CPU and 1 GB of RAM, being capable of
playing full-HD 1080p videos without any buffering or
playback issues.
A Dell Vostro 3350 laptop PC running Microsoft Windows
7 operating system was used to retrieve and save to log files
the EEG data provided by the Emotiv EPOC headset, through
the Emotiv API. The tablet device and the EEG recording PC
were time synchronized in order to facilitate the data analysis.

The impact of a QoE factor
, is determined as in
Algorithm 1, based on the set of relative frustration changes
Δ corresponding to the
independent values of the factor.
Linear regression is used in order to assess and quantify the
impact of the QoE factor.

B. Multimedia Clips
Four high-quality H.264 compressed educational
multimedia clips with different content characteristics were
used for the subjective study. The clips present both natural
and non-natural (e.g., text) information, being representative

2
Since the absolute frustration value (F) measured by the
Emotiv EPOC system can vary between different participants
for the same QoE factor configuration, QoE-EEG-Analyser
uses the relative frustration change Δ , which is computed for
a participant relative to his/her frustration level measured at
), as in (3).
the initial moment (
Δ

· 100 %

mm13-89

5

(a) AtomSize

(b) ArtOfBook

(c) ProjectPlanning

(d) CategoriesInNumbers

Fig. 4. Representative frames for the multimedia test clips.
TABLE I
VIDEO ENCODING CHARACTERISTICS OF THE ORIGINAL MULTIMEDIA CLIPS
H.264 Video
Compression
Profile

Resolution
[pixels]

AtomSize

High@L4.0

1920x1080

2392

ArtOfBook

Main@L3.1

1280x720

5061

29.97

ProjectPlanning

High@L3.1

1280x720

1890

29.97

CategoriesInNumbers High@L3.1

1280x720

1843

29.97

Multimedia
Clip

Video
Bitrate
[kbps]

Framerate
[fps]
25

for the broad category of educational multimedia clips. These
were selected from large collections of educational clips that
are freely available for download on iTunes U3 and Miro
Guide4. Figure 4 shows representative frames from each
multimedia clip. A short description of the four clips is
provided below.
• AtomSize – animation clip explaining the size of an atom;
the clip was published by TEDEducation5.
• ArtOfBook – documentary, presenting the University of
Iowa’s Center for the Book program, available on
University of Iowa’s iTunes U page6.
• ProjectPlanning – presentation, explaining project
management planning in five steps; the clip was produced
by ProjectManager.com7.
• CategoriesInNumbers – screencast, explaining how to view
the rows in spreadsheets by category using values in specific
rows; the clip was produced by MacMost8.
Table I presents the video encoding characteristics of the 4
original multimedia clips. As already mentioned all of them
were compressed using H.264/MPEG-4 AVC, at different
3
Apple iTunes U - Learn anything, anywhere, anytime,
http://www.apple.com/ education/itunes-u/
4
Miro Guide - Video Podcast Directory, http://www.miroguide.com/
5
Just How Small Is an Atom? - Jonathan Bergmann,
http://ed.ted.com/lessons/just-how-small-is-an-atom#watch
6
The
Art
of
The Book,
Iowa
Magazine Segments,
https://itunes.apple.com/ie/itunes-u/iowa-magazine-segments-hd/id413518586
7
5
Steps
to
Project
Management
Planning,
http://www.projectmanager.com/5-steps-to-project-management-planning.php
8
MacMost
Now
845:
Using
Categories
In
Numbers,
http://macmost.com/using-categories-in-numbers.html

profiles. The AtomSize clip has a resolution of 1920x1080
pixels and a frame rate of 25 frames per second, while the rest
of the clips have a resolution of 1280x720 pixels and a frame
rate of 29.97 frames per second. The video bitrate of the test
sequences varies between 1843 kbps in case of the
CategoriesInNumbers clip, and 5061 kbps in case of the
ArtOfBook.
The variation between source clips in terms of their
encoding settings was unavoidable due to the unavailability of
high-resolution and high-bitrate/ uncompressed educational
clips of different content types, and also of sufficiently long
duration to extract 5 minutes long test sequences required for
the scenarios adopted in this study.
C. Test Sequences and Scenarios
The AtomSize clip was used for training the participants
while ArtOfBook, ProjectPlanning and CategoriesInNumbers
multimedia clips were used for the actual subjective study.
One minute (60 seconds) long training sequence, and 5
minutes (300 seconds) long testing sequences were extracted
from the original clips and used in the study.
Table II summarises the content characteristics of the three
testing sequences in terms of their Spatial Index (SI),
Temporal Index (TI) [10] and Colorfulness (CF) [35]. The
three metrics are computed individually for each video frame,
while their measure for the entire clip is obtained either by
taking the maximum [10] or the average [36] across all
frames.
TABLE II
TESTING SCENARIOS ENCODING SETTINGS
SI

Test
Sequence

TI

CF

Max

Avg

Std Max Avg Std

Max

Avg

Std

ArtOfBook

46.4

12.5 6.5 81.9 2.5 3.7

73.7

33.1

12.0

ProjectPlanning

11.3

10.3 0.6 4.1

0.8 0.4

31.0

21.9

2.3

CategoriesInNumbers 13.1

10.6 0.7 6.9

0.1 0.3

20.1

14.0

1.2

Table III summarises the three testing scenarios that have
been considered in the subjective study. The scenarios
correspond to three different QoE factors: video bitrate,
framerate and resolution. Six video bitrates were considered
for Scenario 1, starting from 1800 kbps, and decreasing in
steps of 300 kbps. The maximum bitrate and the bitrate step
were selected based on the recommendation provided in [37],
TABLE III
TESTING SCENARIOS ENCODING SETTINGS
Testing Scenario
(QoE Factor
Analysed)

Resolution
[pixels]

Scenario 1
(Bitrate)
Scenario2
(Framerate)

1280x720

Scenario3
(Resolution)

1280x720,
854x480,
640x360,
426x240,
320x180

1280x720

Video Bitrate
[kbps]
1800, 1500, 1200,
900, 600, 300
1800

Framerate
[fps]
30
30, 25, 20,
15, 10, 5

30
Constant compression
ratio ~1.95 bits/px/sec
(i.e., 1800, 800, 450, 200,
115)

mm13-89
of 1500-1800 kbps which are typical bitrates used for H.264
compressed video streaming at 720p resolution over Wi-Fi.
Six video framerates were considered for Scenario 2,
ranging from 30 fps to 5 fps, in steps of 5 frames per second.
Five video resolutions were considered for Scenario 3,
corresponding to the 720p, 480p, 360p, 240p and 180p
common video profiles and the 16:9 aspect ratio. The video
bitrates corresponding to the four smaller resolutions were
selected so to maintain a constant compression ratio of 1.95
bits per pixel per second, as for the case of 1280x720
resolution at 1800 kbps.
A particular testing scenario consists of gradually
decreasing the corresponding QoE factor value from the
highest to the lowest value, going through all individual
values. All three scenarios were run for each multimedia test
sequence but in different order.
Long test sequences were selected in order to simulate a
more natural viewing experience and to facilitate the analysis
against the continuous measures provided by the Emotiv
EPOC system. The 5 minutes length of the sequences enables
to display each of the 17 test conditions across the three
scenarios for 15 seconds.
In order to help the participants re-adjust their opinion for
each scenario, an additional 15 seconds long segment at the
highest quality (i.e., 1800 kbps, 30 fps, 1280x720), is
displayed at the beginning of each testing scenario. However
this segment are eliminated from the analysis as recommended
for continuous quality assessment procedures in ITU-R
Recommendation BT.500-12 [38].
AviSynth9 was used for extracting the 15 seconds long
segments from the original clips and for changing the video
resolution and framerate accordingly. XMedia Recode10 media
converter software, with x26411 video encoder and FAAC12
audio encoder was used for compressing the tests segments in
.mp4 containers. 2-pass average bitrate, Main profile, with
CABAC and no B-frames, encoding settings for the H.264
codec were used for compressing the test sequences.
All videos presented audio component, which was
maintained during the subjective testing for a more natural
viewing experience, although the study has addressed only the
video component. The audio quality was maintained constant
across all test sequences and test cases (i.e., AAC-LC codec,
128 kbps, 44.1 KHz sampling rate).
D. Methodology
The Single Stimulus Continuous Quality Evaluation
(SSCQE) [38] approach was used for video quality assessment
during the multimedia playback. An Android application was
developed for displaying the test sequences and quality rating.
The quality rating was done on a continuous scale (i.e., 1 to
100), with the help of a vertical slider placed on the right hand
side of the display and controlled by the participant through
9

Avisynth - Main Page, http://avisynth.org/mediawiki/Main_Page
XMedia Recode, http://www.xmedia-recode.de/
11
VideoLAN-x264, http://www.videolan.org/developers/x264.html
12
AudioCoding - FAAC, http://www.audiocoding.com/faac.html
10

6
the touchscreen interface.
At the beginning of the testing session, each participant was
explained the testing procedure, read information
recommended by Emotiv about the EPOC system and EEG
recording, and filled in a demographic questionnaire. The
attendance was on a voluntary basis and verbal agreement on
their attendance was obtained from each participant prior
commencing the testing.
The testing session continued with setting up the Emotiv
EPOC headset, while making sure that good signal is received
from all sensors. After the training phase the actual testing was
conducted by displaying the three test sequences ArtOfBook,
ProjectPlanning and CategoriesInNumbers, in a random order
for each participant.
Constant conditions were maintained for all participants.
The multimedia test clips were played from the internal
storage of the device in order to minimize any latencies and
playback issues with buffering. The screen brightness was set
at maximum value while the audio was played through the
device speaker.
E. Results and Analysis
The performance of the proposed approach to use the
frustration measure provided continuously by the Emotiv
EPOC headset for video QuE assessment, was compared
against the time sampled SSCQE subjective ratings provided
through the on-screen slider by the participants while viewing
the multimedia test sequences.
Figure 5 presents the distribution of participants based on
their frustration change, as well as based on the change in their
subjective video quality ratings, in case of the ArtOfBook test
sequence. The results show that overall across the three testing
scenarios the EEG frustration measure offers a better
performance for detecting the decrease in video quality as
opposed to the subjective video quality ratings. The frustration
increased for 72.22% of the participants, and decreased for
27.78% of the participants. As opposed, only in case of 61%
of the participants the video quality decrease occurring for the
scenarios was confirmed by the SSCQE ratings, with the
ratings increasing for 11.11% of the participants and
remaining constant for 27.78% of the participants.
In case of Scenario 1 (video bitrate) the frustration
increased for 66.66% of the participants and decreased for
33.33% of the participants, while the subjective ratings
increased for 50% of the participants and remained constant
for the other 50% of the participants.
In case of Scenario 2 (framerate) the frustration increased
for 66.66% of the participants and decreased for 33.33% of the
participants, while the subjective ratings increased for 66.66%
of the participants, remained constant for 16.67% of the
participants, and increased for 16.67% of the participants.
In case of Scenario 3 (resolution) the frustration increased
for 83.33% of the participants and decreased for 16.67% of the
participants, while the subjective ratings increased for 66.66%
of the participants, remained constant for 16.67% of the
participants, and increased for 16.67% of the participants.
These results also indicate that for this particular

mm13-89

7

40

60

30
20

60

80

30
20
10

20

40

60

80

Time [sec]

66.66
%
83.33
%
11.11
%

27.78
%

Overall

40

Time [sec]

0

27.78
%
61.11
%

35%
65%
0%

Decreased

20

40

16.67
%

16.67
%

72.22
%

0

0

Relative Frustration Change [%]

66.66
%

0.00
%

0.00
%

10

80

Rel. Frustration Change vs. Time for
"ArtOfBook" and "Scenario 3"

16.67
%

16.67
%
0.00
%

20

Time [sec]

16.67
%

33.33
%

Scenario 3

Scenario 2

0

66.66
%

0

40
30
20

-10

0.00
%

-10

66.66
%

50.00
%

10

50.00
%

0

33.33
%

Relative Frustration Change [%]

Scenario 1

0.00
%

Rel. Frustration Change vs. Time for
"ArtOfBook" and "Scenario 2"
Relative Frustration Change [%]

Rel. Frustration Change vs. Time for
"ArtOfBook" and "Scenario 1"

SSCQE Ratings

-10

Frustration

No Change

Fig.6. Relative frustration change measures computed every 1 second
interval and averaged across all participants for ArtOfBook test sequence.

from 1280x720 to 320x180 (as well as the corresponding
video bitrate from 1800 kbps to 115 kbps to maintain a
constant compression ratio of ~1.95 bits/ pixel/ second).
Therefore the results have shown that the proposed
approach using the relative frustration change is feasible to be
used in order to automatically assess and quantify the impact
of various factors on user’s multimedia QoE, without having
to rely on subjective ratings provided directly by the user.

Increased

Fig.5. Distribution of participants based on their change in frustration and
subjective SSCQE video quality ratings for the ArtOfBook test sequence.

multimedia clip the participants are more sensitive to a
decrease in framerate and resolution as opposed to a decrease
in video bitrate.
The proposed method offers the major advantage that it
captures the impact of various QoE factors faster than
traditional methods relying on user input.
Figure 6 presents the relative frustration change measures
computed and averaged across all participants at a granularity
of 1 second, over the duration of the three testing scenarios,
for the ArtOfBook test sequence. The linear model is also
illustrated in the figure.
While the relative frustration change measure presents
oscillations in time, the results show that user’s frustration
tends to increase as the video quality decreases with the
reduction in bitrate, framerate and resolution respectively.
Estimated from the linear models, participants’ frustration
increased with ~16% when gradually reducing the video
bitrate from 1800 kbps to 300 kbps, with ~18% when
gradually reducing the video framerate from 30 fps to 5 fps,
and with ~42% when gradually reducing the video resolution

V. CONCLUSION AND FUTURE WORK
This paper proposed a user-centered solution for multimedia
quality assessment based on EEG-based frustration measures
provided in real-time by the Emotiv EPOC system. The
proposed QoE-EEG-Analyser enables assessing and
quantifying the impact of various factors on user’s QoE in a
non-invasive way, in the sense that participants do not have to
provide input on their perceived experience. The proposed
solution complements the existing subjective assessment
methods, being particularly useful in situations when
continuous assessment over the entire clip duration is
necessary, such as for example assessing the impact of
multimedia content adaptation strategies in real-like viewing
scenarios. Future work is currently undertaken that extends the
subjective analysis with higher number of clips, as well as
more complex statistical analysis necessary to better assess
and quantify the impact of different factors on user’s QoE.
REFERENCES
[1]

Cisco Systems, Inc., ‘Cisco Visual Networking Index: Forecast and
Methodology, 2011-2016’, 30-May-2012. [Online]. Available:
http://www.cisco.com/en/US/solutions/collateral/ns341/ns525/ns537/ns
705/ns827/white_paper_c11-481360.pdf. [Accessed: 15-Dec-2012].

mm13-89
[2]
[3]
[4]

[5]
[6]

[7]

[8]
[9]
[10]
[11]
[12]

[13]

[14]

[15]

[16]
[17]

[18]

[19]
[20]
[21]

[22]

Google Inc., ‘The New Multi-Screen World’, Aug-2012. [Online].
Available:
http://www.thinkwithgoogle.com/insights/featured/newmulti-screen-world-insight/. [Accessed: 15-Dec-2012].
G. Ghinea and S. Y. Chen, ‘Measuring quality of perception in
distributed multimedia: Verbalizers vs. imagers’, Computers in Human
Behavior, vol. 24, no. 4, pp. 1317–1329, Jul. 2008.
R. Trestian, A.-N. Moldovan, C. H. Muntean, O. Ormond, and G.-M.
Muntean, ‘Quality Utility modelling for multimedia applications for
Android Mobile devices’, in 7th IEEE International Symposium on
Broadband Multimedia Systems and Broadcasting (BMSB 2012), Seoul,
Korea, 2012, pp. 1 –6.
B. Ciubotaru, G.-M. Muntean, and G. Ghinea, ‘Objective Assessment of
Region of Interest-Aware Adaptive Multimedia Streaming Quality’,
IEEE Trans. on Broadcast., vol. 55, no. 2, pp. 202–212, Jun. 2009.
A.-N. Moldovan and C. H. Muntean, ‘Subjective Assessment of
BitDetect - A Mechanism for Energy-Aware Multimedia Content
Adaptation’, IEEE Transactions on Broadcasting, vol. 58, no. 3, pp.
480–492, 2012.
A.-N. Moldovan and C. H. Muntean, ‘Towards Personalised and
Adaptive Multimedia in M-learning Systems’, in Proceedings of 16th
World Conference on E-Learning in Corporate, Government,
Healthcare, and Higher Education (E-Learn 2011), Chesapeake, VA,
USA, 2011, pp. 782–791.
C. H. Muntean and J. McManis, ‘End-User Quality of Experience
Oriented Adaptive E-learning System’, Journal of Digital Information,
vol. 7, no. 1, 2006.
G.-M. Muntean, P. Perry, and L. Murphy, ‘Objective and subjective
evaluation of QOAS video streaming over broadband networks’, IEEE
Trans. Netw. Serv. Manage., vol. 2, no. 1, pp. 19–28, Nov. 2005.
ITU-T, ‘P.910: Subjective video quality assessment methods for
multimedia applications’, International Telecommunication Union,
Geneva, Switzerland, Apr. 2008.
ITU-T, ‘P.911: Subjective audiovisual quality assessment methods for
multimedia applications’, International Telecommunication Union,
Geneva, Switzerland, Dec. 1998.
ITU-T, ‘P.10/G.100 Amendment 2: New definitions for inclusion in
Recommendation ITU-T P.10/G.100 - Vocabulary for performance and
quality of service’, International Telecommunication Union, Geneva,
Switzerland, Jul. 2008.
T. C. M. de Koning, P. Veldhoven, H. Knoche, and R. E. Kooij, ‘Of
MOS and men: bridging the gap between objective and subjective
quality measurements in mobile TV’, in Proceedings of Multimedia on
Mobile Devices 2007, IS&T/SPIE Symposium on Electronic Imaging,
San Jose, CA, USA, 2007, vol. 6507, p. 11.
D. Strohmeier, S. Jumisko-Pyykkö, and K. Kunze, ‘Open Profiling of
Quality: A Mixed Method Approach to Understanding Multimodal
Quality Perception’, Advances in Multimedia, vol. 2010, pp. 1–28,
2010.
D. Strohmeier, S. Jumisko-Pyykkö, K. Kunze, and M. Bici, ‘The
Extended-OPQ Method for User-Centered Quality of Experience
Evaluation: A Study for Mobile 3D Video Broadcasting over DVB-H’,
EURASIP Journal on Image and Video Processing, vol. 2011, no. 1, p.
538294, Feb. 2011.
S. Winkler and P. Mohandas, ‘The Evolution of Video Quality
Measurement: From PSNR to Hybrid Metrics’, IEEE Transactions on
Broadcasting, vol. 54, no. 3, pp. 660–668, 2008.
S. Chikkerur, V. Sundaram, M. Reisslein, and L. J. Karam, ‘Objective
Video Quality Assessment Methods: A Classification, Review, and
Performance Comparison’, IEEE Transactions on Broadcasting, vol.
57, no. 2, pp. 165–182, Jun. 2011.
E. Harmon-Jones and D. M. Amodio, ‘Electroencephalographic
methods in psychology’, H. Cooper, P. M. Camic, D. L. Long, A. T.
Panter, D. Rindskopf, and K. J. Sher, Eds. Washington, DC, US:
American Psychological Association, 2012, pp. 503–522.
K. S. Park, H. Choi, K. J. Lee, J. Y. Lee, K. O. An, and E. J. Kim,
‘Emotion recognition based on the asymmetric left and right activation’,
pp. 201 – 209, 2011.
P. C. Petrantonakis and L. J. Hadjileontiadis, ‘Emotion Recognition
From EEG Using Higher Order Crossings’, pp. 186 –197, Mar. 2010.
D. Nie, X.-W. Wang, L.-C. Shi, and B.-L. Lu, ‘EEG-based emotion
recognition during watching movies’, presented at the 2011 5th
International IEEE/EMBS Conference on Neural Engineering (NER),
2011, pp. 667 –670.
A. Belle, R. Hobson, and K. Najarian, ‘A physiological signal
processing system for optimal engagement and attention detection’,

8

[23]
[24]

[25]

[26]

[27]

[28]
[29]
[30]

[31]

[32]

[33]
[34]

[35]
[36]
[37]
[38]

presented at the 2011 IEEE International Conference on Bioinformatics
and Biomedicine Workshops (BIBMW), 2011, pp. 555 –561.
L. Shen, M. Wang, and R. Shen, ‘Affective e-learning: Using
“emotional” data to improve learning in pervasive learning
environment’, pp. 176–189, 2009.
E. T. Mampusti, J. S. Ng, J. J. I. Quinto, G. L. Teng, M. T. C. Suarez,
and R. S. Trogo, ‘Measuring Academic Affective States of Students via
Brainwave Signals’, presented at the 2011 Third International
Conference on Knowledge and Systems Engineering (KSE), 2011, pp.
226 –231.
A. Yazdani, J.-S. Lee, and T. Ebrahimi, ‘Implicit emotional tagging of
multimedia using EEG signals and brain computer interface’, in
Proceedings of the first SIGMM workshop on Social media, New York,
NY, USA, 2009, pp. 81–88.
S. Scholler, S. Bosse, M. S. Treder, B. Blankertz, G. Curio, K.-R.
Muller, and T. Wiegand, ‘Toward a Direct Measure of Video Quality
Perception Using EEG’, IEEE Transactions on Image Processing, vol.
21, no. 5, pp. 2619 –2629, May 2012.
S. Arndt, J. Antons, R. Schleicher, S. Moller, and G. Curio, ‘Perception
of low-quality videos analyzed by means of electroencephalography’, in
2012 Fourth International Workshop on Quality of Multimedia
Experience (QoMEX), 2012, pp. 284 –289.
R. Kuber and F. P. Wright, ‘Augmenting the Instant Messaging
Experience through the Use of BCI and Gestural Technologies’, 2012.
D. Cernea, A. Kerren, and A. Ebert, ‘Detecting Insight and Emotion in
Visualization Applications with a Commercial EEG Headset’, 2011.
M. Petersen, C. Stahlhut, A. Stopczynski, J. Larsen, and L. Hansen,
‘Smartphones Get Emotional: Mind Reading Images and Reconstructing
the Neural Sources’, S. D’Mello, A. Graesser, B. Schuller, and J.-C.
Martin, Eds. Springer Berlin / Heidelberg, 2011, pp. 578–587.
G. N. Ranky and S. Adamovich, ‘Analysis of a commercial EEG device
for the control of a robot arm’, presented at the Bioengineering
Conference, Proceedings of the 2010 IEEE 36th Annual Northeast,
2010, pp. 1 –2.
J. Gonzalez-Sanchez, M. E. Chavez-Echeagaray, R. Atkinson, and W.
Burleson, ‘ABE: An Agent-Based Software Architecture for a
Multimodal Emotion Recognition Framework’, presented at the 2011
9th Working IEEE/IFIP Conference on Software Architecture
(WICSA), 2011, pp. 187 –193.
I. Ghergulescu, ‘Automatic Non-Disturbing Motivation Monitoring in
Game-based E-learning through Player Behaviour and EEG’, Ph.D.
Thesis, National College of Ireland, Dublin, Ireland, 2013.
S. Davis, E. Cheng, I. Burnett, and C. Ritz, ‘Multimedia user feedback
based on augmenting user tags with EEG emotional states’, in 2011
Third International Workshop on Quality of Multimedia Experience
(QoMEX), 2011, pp. 143 –148.
D. Hasler and S. E. Suesstrunk, ‘Measuring colorfulness in natural
images’, pp. 87–95, Jun. 2003.
S. Winkler, ‘Analysis of Public Image and Video Databases for Quality
Assessment’, IEEE Journal of Selected Topics in Signal Processing,
vol. 6, no. 6, pp. 616 –625, Oct. 2012.
A. Molnar and C. H. Muntean, ‘Cost-Oriented Adaptive Multimedia
Delivery’, IEEE Transactions on Broadcasting, vol. Early Access
Online, 2013.
ITU-R, ‘BT.500-12: Methodology for the subjective assessment of the
quality of television pictures’, International Telecommunication Union,
Geneva, Switzerland, Sep. 2009.

