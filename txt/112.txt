International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

Social Robots for Reinforcing Attention and
Forming Emotional Knowledge of Children
with Special Educational Needs
A. Lekova, T. Tanev, S. Kostova, P. Dachkinov, V. Vassileva-Aleksandrova, O. Bouattane

Abstract — Emotional child-robot interaction helps catching
quickly child attention and enhancing information perception
during learning and verbalization in children with Special
Educational Needs (SEN). This will improve the pedagogical
rehabilitation for these children and also develop their emotional
knowledge and memory in play-like activities mediated by
emotion-expressive social robots. The designed by us EEG-based
portable Brain-Computer Interface (BCI) measures and features
the brain electrical activity in real time in order to analyze the
correlated attentional or emotional states of a child. Different BCI
outputs can assist special educators in assessment of the emotional
and cognitive performance of a child or can be used directly as
inputs for robot control. BCI is a new technology for human-robot
interaction and it can evolve into technology for self-regulatory
training of attention and emotional skills via neurofeedback
exposed on a robot. Since the attention or emotional responses of
children with SEN make robots to act, these skills are naturally
reinforced in the play. Two research protocols based on this idea
are presented. They describe the procedures for conducting
custom scenarios for robotic intervention with a portable brainlistening headset. The proposed BCI is used to translate the
human brain activity, head motion and eye movement during the
process of expressing emotions into robot commands. Then, they
are wirelessly transmitted to the robot sensors, modules and
controllers by a new designed BCI-Robot framework. This study
marks a first step towards the way of utilizing the advantages of
educational theater in Social Robotics in order to transfer the
emotional talent of an actor to an emotion-expressive robot.
Index Terms— Brain-computer interface, Children with SEN,
Educational theater, Emotion understanding, Human-robot
interaction, Neurofeedback, Pedagogical Rehabilitation, Social
robots

I. INTRODUCTION

E

motions reading is a critical element of the social
intelligence because it is the way we make sense of other
people's behavior and decide on our own next moves. Emotions
play the central role in day-to-day living and emotional
regulation has an impact on the intensity, duration, and
The manuscript has been submitted for review on the 20th of January, 2019.
The research findings about brain interaction with the robots NAO and EmoSan
are supported in part by the H2020 Project CybSPEED under Grant 777720,
while the study of brain-inspired control of the robot BigFoot is supported by
National Scientific Research Fund, Project under Grant DH17/10.
A. Lekova, T. Tanev, S. Kostova and P. Dachkinov are with the Institute of
Robotics, Bulgarian Academy of Sciences, Sofia, Bulgaria (e-mails:

http://innove.org/ijist/

expression of emotional reactions [1]. Recent neuroimaging
findings have indicated that emotions have a significant
influence on the cognitive processes in humans, including
perception, attention, learning, memory, reasoning and problem
solving [2]. Attention is a key factor for human cognition and a
link between the brain and behavior. Because there is great
public interest in neuroscience, more details about the
modulation of attentional and emotional networks in the brain
are provided in Section II [3], [4]. In the same section, a new
explanation why emotional events capture attention and
enhance cognitive processes [5] is provided, too
Conclusions from our past projects such as METEMSS [6]
revealed that positive emotional events during the play with
humanoid and non-humanoid robots can unlock unexpected
potential skills in children with Special Educational Needs
(SEN), quickly and automatically capture attention and
facilitate perceptual process. Our results are consistent with
those shown in the literature, for example authors in [7] prove
that emotions facilitate language learning. However, our past
results from [6] revealed that children with SEN face
difficulties in guessing others' emotions and paying attention to
own or other people’s feelings. That is why they cannot handle
simplest social situations and an early intervention for
formation of emotional knowledge and memory through
experience is critical, and what is more practice is very
important. Some explanations from behavioral studies about the
abilities for emotion and face recognition in individuals with
autism [8], [9] describe these impairments with deficiency in
emotion processing or misinterpreting facial expression due to
different brain activity in parietal, temporal and occipital
regions in right hemisphere. Technologies can help in
recognizing changes in emotions that a child is experiencing.
They can be captured by external observation such as tracking
of body postures and facial expressions, or physiologically.
Novel approaches in emotion recognition include the use of
information technologies and specifically the use of brain aware
devices and Brain-Computer Interface (BCI) which bypass the
conventional channels of communication (i.e. muscles) and to
alekova.iser@gmail.com, tanev_tk@hotmail.com, kostovasp@yahoo.com,
pdachkinov@gmail.com).
V. Vassileva-Aleksandrova is with Educational Theater “Tsvete”, Sofia,
Bulgaria, (e-mail: vilan_7@yahoo.com).
O. Bouattane is with ENSET, University Hassan II de Casablanca,
Casablanca, Morocco (e-mail: o.bouattane@gmail.com).

26

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

provide direct communication and control between the human
brain and the physical devices. BCI translates different patterns
of brain activity into commands in real time [10]. Recently,
portable, non-invasive and affordable EEG commercial devices
for “gaming” or “well-being sustainability” have emerged [11].
They record the brain activity and measure the change in brain
pulse voltage by electrodes on the scalp. We use low resolution
devices “EPOC” or “Insight” (Fig.1.) by EMOTIV
bioinformatics company [12]. The EPOC/INSIGHT electrode
locations (INSIGHT ones are discriminated by red circles) are
shown on Fig.1.a according to the 10-20 international EEG
system ( Fig.1.b), recommended by the International Federation
of Societies for Electro-encephalography and Clinical
Neurophysiology [13]. Although both devices have low
resolution and are with a few electrodes, they provide highquality output neural signals [14]. If machine learning
techniques are implemented in BCI, the emotions can be
captured and recognized based on the spatial and temporal
resolution of the neural activity. For instance, arousal and
valence scores can be used for discriminating the current
emotions. The current version of EMOTIV hardware and
software offers three different kinds of detection algorithms,
based on extensive scientific studies in order to develop
accurate machine learning algorithms to classify and grade the
intensity of different conditions. EMOTIV measures 6 different
mental states (performance metrics) in real time: Excitement
(Arousal),
Interest
(Valence),
Stress
(Frustration),
Engagement/Boredom, Attention (Focus) and Meditation
(Relaxation).

Fig. 1. EMOTIV BCI technologies and electrode locations according to the
10-20 international EEG system.

In the current study, we have developed a BCI to be used in
daily life and out of laboratory environment for learning
through play/art in a pedagogical scenarios mediated by
socially assistive robots. BCI measures and features in real time
the brain electrical activity in order to analyze the correlated
attentional or emotional states of a child. It is used either for
monitoring, or for attention and emotional training based on
imitation or a neurofeedback. The robots are controlled by BCI
and this integration of both technologies engages the children
with SEN and reinforce their attentional or emotional
responses. We explained this with the robot intervention which
encourages exploration and play, and it is attractive for teaching
daily living skills, organizing sensorimotor system and
understanding social opportunities in communications. The

http://innove.org/ijist/

knowledge can be gained by observation, however a deeper
learning can occur when learners perform the actions
themselves [15]. The mediation aspect of the robot is the core
in social robotic studies [16] where the memory, emotion and
learning are involved in Robotic Intervention Scenarios (RIS)
[17]. Since the body is involved in the process of the “learning
by experience”, the perceived stimuli are transformed into a
more stable memory and cognitive representations because the
notion of the body includes not only the body itself but also the
senses, the mind and the brain [18].
Directed by the neuroscience implications how to strengthen
the memory traces and neuronal excitation of the brain we
include more modalities in RIS, such as touch, audio, 3D
objects, movements and the emotional impact of the
educational theater. It provides a safe environment to the
children and uses creative tools to address critical problems in
a bizarre way - very emotional, strange or unusual. In the frame
of our current project for pedagogical rehabilitation in special
education [19] the robotics researchers collaborate with the
actors from the Educational Theater “Tsvete” [20]. The actors
have long experience in puppet therapy [21] and that is why we
have attempted to transfer the emotional and social talents of
the actors into the world of robots in order to improve the
pedagogical rehabilitation for children with SEN and to develop
their emotional knowledge and memory by play-like activities
mediated by social robots. An approach for tracking the actor
behavior and transferring it to the emotion-expressive robot in
order the latter to have similar to the actor’s qualities is
presented in Section V.
Recently, the robots have appeared on the theater stages to
educate and promote social changes in behaviors [22] and to
evaluate their use in public and private settings by studding the
interactions with care robots [23]. There is also a term “robot
puppeteering” where one or several puppeteers control the
head, arms and legs of a robot, such as Loren the Robot Butler
in the movie “Teach me how to Dougie”. However, these
results cannot be used for educational purposes because the
puppeteers or human-robot interactions take place in the
theater, unaffordable for repetitive needs of learning for
children with SEN. To the best of our knowledge, we have
marked a first step towards transferring the advantages of
educational theater into the social robotics and augmenting a
socially assistive robot with the emotional and social talent of
an actor. Thus, we want to develop child-robot interactions that
catch quickly the child attention and enhance the information
perception during learning. We implemented in RIS the actors’
way for performing, understanding and responding to emotions
in order the child to experience the emotion-generative process
together with the robot and thus to develop the child’s episodic
memory.
In the Gross ‘‘modal model’’ of emotion [1], the generation
of emotions is a special sequential “situation-attentionappraisal-response” process which occurs over time. Any case
of deficiencies in these stages requires emotional regulation
strategies. Another and more intuitive emotional regulation is
through neurofeedback – a biofeedback that uses real-time
displays of brain activity (most commonly EEG). The
27

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

neurofeedback rehabilitation is effective for training attention
or emotion self-regulation of the brain function. An emotional
experience can be processed as a healthy or unhealthy one. The
emotional regulation strategies which we take into account are
“cognitive reappraisal” involving healthy changing of an
emotional response based on reinterpreting the meaning and
assigning a non-emotional meaning to a situation [24]. Since
we consider the training of the emotional experience by
cognitive reappraisal is not sufficiently studied for children
with SEN, thus we present our goal to implement the ‘‘modal
model’’ of emotion [1] in the safe world of robots and our
hypothesis to place the child’s neurofeedback in the play-like
robotic interventions. For understanding the emotions
experienced by the child and the level of attention, the play-like
activities for controlling the robots are personalized according
to the current child’s brain activity correlated to the attentional
or emotional states, which are translated into input commands
to the humanoid or non-humanoid robot. Thus, the robots
express the attention or emotions of the child and this is helpful
in the pedagogical rehabilitation, since children with SEN avoid
direct eye contact or do not understand their own emotions.
Thus, the correlated brain activity is tracking and copying on
the programmable robots in order to assist special educators in
monitoring the progress in skills learning. For understanding
the child level of attention we monitor human eye movements
based on EEG brain activity during blinking. There is a
variation in the alpha waves of the EEG spectrum in the frontal
and occipital lobes. The time of blinking and its rate can
indicate how engaged people are with what they are looking at
and how important this is for them [25]. Children with ASC
blink differently than typically developing children. In [25] the
researchers found that typically developing children blinked
less before something emotionally was expected to happen
because they anticipate it, while children with autism react to
the physical event after it has already happened because they
can’t anticipate it.
Since the research in the pedagogical rehabilitation is
conducted according to a plan (a protocol), we designed two
research protocols. They describe the procedures for
conducting custom scenarios for robotic intervention by using
a low resolution and portable brain-listening headset. The
protocols consist of the procedures for conducting the
experiments, i.e., what and how it will be done in the study. A
protocol also describes the target groups, the length of the
study, the materials and the related methods. The first protocol
describes how to reinforce children to pay attention or learn
emotional knowledge by imitation and the second protocol
describes a game with the robots where the child learns about
its own emotions based on a neurofeedback exposed on a robot.
A special attention in this study is devoted to healthy emotional
reactions of the children. The research protocols have been
submitted to the Ethics Committee for Scientific Research
(ECSR) to the Institute of Robotics of Bulgarian Academy of
Sciences (IR-BAS) for approving that emotional reactions are
helpful and they happen in the right situations. Some of our
experimental sessions, based on emotional situations proposed
in [26] and adapted by us for using in the world of robots, can
http://innove.org/ijist/

be seen in Section III (Fig.2. and Fig.3). The feeling of
satisfaction and enjoyment learned from the emotional lesson is
checked at the end of the session. The ECSR criteria for banning
a child to participate in the experimental session are presented
in details in Section V. These protocols have been approved
from the ethic committee of IR-BAS and can be tested by other
researchers. We propose an additional research protocol
describing how the emotional talent of an actor in expressing
emotions can be tracked by external (inertial data) or
physiological (neural data) observations, respectively, and
recorded.
In the proposed robotic intervention scenarios, an innovative
BCI-Robot framework containing EMOTIV brain headset for
tracking, processing and translating motion or EEG data into
robot commands has been developed. A new designed BCIRobot framework is used to transmit online and wirelessly
commands to the robot sensors, modules and controllers. Three
types of BCI-robot software frameworks are instantiated: (1)
EMOTIV-NAO software framework translating the correlated
brain signals in time to NAO eye sensor commands in scenarios
for assessing child concentration and attention, (2) EMOTIVBigFoot software framework, where the score for joint attention
of a child is translated into commands to control the movement
of the non-humanoid robot BigFoot for training in real time, (3)
EMOTIV-EmoSan software framework, where the emotions
experienced by the child switch the corresponding robot
intervention scenarios for cognitive reappraisal. A different
BCI software framework for motion capturing is proposed and
the corresponding algorithms for transferring the head and eye
movements of the actor to the robot are presented. In the present
study we prove that emotion-expressive robot fulfils the
motions of the actor’s head.
The paper is organized as follows: Section II presents related
works about neuroscience implications for pedagogical
rehabilitation and the assistive technologies used. Section III
presents the research protocols for reinforcing the attention and
emotional knowledge of children with SEN with examples for
robotic intervention scenarios. Section IV presents three
applications of the BCI-Robot wireless framework containing
an EMOTIV brain headset, as well as the BCI software
framework for motion capturing correlated with the head
movements exposed on the EmoSan robot. The approach for
tracking the actor behavior and transferring it to control system
of the emotion-expressive robot, as well as the results based on
data from two actors, are discussed in Section V. Conclusion
follows.
II. RELATED WORKS
A. Neuroscience implications for learning, memory,
cognition and behavior
At the end of the eighteenth century Franz Joseph Gall, a
German physician and neuroanatomist brought together
biological and psychological concepts in the study of human
behavior [27]. He proposed the radical new ideas that all
behavior originated from the brain, that different parts of the
brain control specific aspects of our behavior and that the center
28

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

for each mental function grew with use, such as a muscle bulks
up with exercise. Scientists assume, in the words of
neuroscientist Marvin Minsky [28] (p. 287) that “the mind is
what the brain does” and if we want to understand the mind, we
should look to neuroscience and the brain for the real answers.
Minsky [28] also writes on p. 163 that “our culture wrongly
teaches us that thoughts and feelings lie in almost separate
worlds. In fact, they are always intertwined. And really, there
are actions in the mind with which we set up thoughts and
fantasies that move our own emotions, arousing hopes and fears
through self-directed offers, bribes, and even threat”. The last
proves that there are schemes for self-control, stimulating acts
and thoughts that directly change the brain's chemical
environment. Minsky proposes to regard emotions not as
separate from thoughts in general, but as varieties or types of
thoughts, each based on a different brain-machine that
specializes in some particular domain of cognition. Emotions
are universal and cannot be felt without thought, however
reasoning processes engage cortical regions of the brain, while
brain structures linked to emotions are often subcortical, such
as the amygdala, ventral striatum, and hypothalamus.
The emotional reaction is usually caused by our thoughts (Fig.
2), however it can be triggered unconsciously (Fig.5.) without
understanding why it is happening [26]. For example, the fear
evolves to protect ourselves and the control relies on
controlling/changing our thoughts by coping statements or by
asking whether the threat is true or unreal. Emotions are
affected by culture, too. According to many theories, like
cultural syndrome [29], culture allocates attention to inner
feelings and thoughts very differently. Cultural contexts
encourage or suppress emotional expression.

Fig. 2. A person can have different thoughts about the same situation (adapted
from Fig.1 in [26])

In [30] the researchers evaluate the brain activations which
http://innove.org/ijist/

are associated with the application of two different emotional
regulation strategies - cognitive reappraisal and expressive
suppression. An EEG device “Emotiv EPOC” is used to
measure the brain electrical activity of the participants during a
session of inducing sadness in a virtual reality environment. In
the control group, authors found significant activations in
several right frontal regions that were related to the induction
of negative emotions. For the group practicing emotional
regulation strategies, they found significant activations in the
limbic, occipital, and parietal regions. A survey with similar
results obtained through clinical neuroimaging systems, which
is consistent with the findings for brain activation associated
with regulation of emotions is presented in [30]. To summarize,
the neural structures involved in emotional processing form two
systems - a ventral emotion system and a dorsal emotion
system. They have a role in the decrease of negative emotions
and form a complex network that is responsible for processing
responses to emotional events involving the ventromedial
prefrontal cortex, the dorsolateral prefrontal cortex, the
orbitofrontal cortex, amygdala, insula, hippocampus, and
cingulate cortex.
By education we enhance learning and neuroscience explains
the mental processes involved in the development, plasticity,
learning, memory, cognition and behavior [31]. Future
educational practice should be transformed taking into account
some of the key insights from neuroscience [32]: (1) Learning
outcomes are not solely determined by the environment but
biologically, i.e. biological factors play an important role in
accounting for differences in learning ability between
individuals, (2) Understanding of specific learning difficulties
(e.g. dyslexia) and uncovering why certain types of learning are
more rewarding than others, (3) The brain changes constantly
as a result of learning, and remains ‘plastic’ throughout life, (4)
Self-control in acquisition of knowledge and mastery benefit
future learning and the neuroscience has a key role in
investigating the means of enhancing the brain power by
neurofeedback, (5) Some insights from the neuroscience are
relevant for the development and use of adaptive digital
technologies that have the potential to create more learning
opportunities inside and outside the classroom.
The learning is structural changes in the brain and the key of
these changes is the neuroplasticity. Neuroplasticity can be
defined as the ability of the nervous system to respond to
intrinsic or extrinsic stimuli by reorganizing its structure,
function and connections [33]. Learning means that one has
created a strong enough memory trace to keep it and adding
more modalities, such as touch, audio, manipulation of 3D
objects and movements in robotic intervention might strengthen
the memory traces of the learners. This helps neurologically the
Emotion and Attention systems in the brain to interact, resulting
in emotional association of information from short-term
memory to long-term memory. A neuroscience explanation for
this is that the amygdala and prefrontal cortex cooperate with
the medial temporal lobe in an integrated manner that affords
(1) the amygdala modulating memory association; (2) the
prefrontal cortex mediating memory encoding and formation;
and (3) the hippocampus for successful learning and long-term
29

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

memory retention [2]. Thus, emotions could be used for
modulating the selectivity of attention, as well as motivating
action and behavior. More understanding how the cognition and
emotion are effectively integrated in the brain, how emotions
enhance perception and attention, and anatomical basis for
cognitive-emotional interactions can be seen in [4]. A new
explanation as to how emotional events easily capture our
attention is given in [5], i.e., as a novel pathway formed from
the amygdala, the brain's emotional center, and the thalamic
reticular nucleus, a key node in the brain's attentional network
in the upper surface of temporal lobe. This amygdalar pathway
formed unusual synapses with more large and efficient
terminals than pathways from the orbitofrontal cortex.
Recent studies report the role of amygdala–frontal
connectivity during regulation of emotions [30], [34].
Successful control of affect depends on the capacity to
modulate negative emotional responses through the use of
cognitive strategies, such as cognitive reappraisal. These
strategies involve frontal cortical regions in the modulation of
amygdala
reactivity.
The
authors
in
[34] use
psychophysiological interaction analyses of functional
magnetic resonance imaging data and show that brain activity
in specific areas of the frontal cortex (dorsolateral, dorsal
medial, anterior cingulate, orbital) covariates with amygdala
activity and this limbic-frontal circuitry is important for the
regulation of emotions.
B. Assistive technologies for pedagogical rehabilitation
We have chosen to use the EMOTIV EEG brain tracking
device [12] because the Emotiv technology allows to detect the
facial expression, current cognitive and mental states, and
periodic brain rhythms using the row EEG data stream. Inertial
sensors are mounted in the headset and provide motion data
streams. Four simple periodic rhythms recorded in the EEG are
identified by frequency (Hz) and amplitude – theta, alpha, beta
and gamma. EMOTIV EPOC (Fig.1.a) has 14 EEG sensors.
Eight of them are positioned around the frontal and prefrontal
lobes and they pick up signals from facial muscles and the eyes.
The sensors induce noise in EEG signals and most EEG systems
filtered or ignored artefacts when interpreting the brain signals.
The EMOTIV detection system also filters these signals out,
however, it also uses these signals to classify which muscle
groups are responsible for the artifacts. Thus, the so-called
“smart artifacts” induced in pure EEG signals are diverted and
classified to map the activation in different muscle groups and
eye movement into events that are used for detecting the facial
expression and blinking [35].
The accuracy of EMOTIV has been validated by several
independent studies presented in [12]. In one of the studies [14]
the authors compare EPOC to a research-grade Neuroscan
system and conclude that an adapted EPOC system can produce
valid auditory Event Related Potentials (ERPs) in children. It is
important to adapt the EPOC system since passive auditory
ERPs have been proved useful for considering auditory and
visual processing in attention deficit hyperactivity disorder
(ADHD) and specific language impairments. Thus these
http://innove.org/ijist/

authors encouraged us to use EMOTIV EEG devices out of
laboratory environment. Another conclusion from [14] states
that the children have “noisier” EEG (and ERP) responses,
contaminated to a greater degree by electrical noise generated
by movement and resulted in artefacts in EEG signals.
According to authors [14], the reason for this is that the children
have difficulties in keeping still during the long test sessions
and the laboratory EEG settings are intimidating for them.
Systems with BCI feedback for rehabilitation have been
reported to be effective for training attention’s self-regulation
in children with Attention Deficit Hyperactivity Disorder
(ADHD) [36], [37] or for neurofeedback training and speech
therapy in order to enhance learning and speech ability in
patients with a diagnosis of ASC [38]. ADHD affects
approximately 10% of children in the world and conventional
therapy (including medications, psychotherapy, behavior
therapy, training, etc.) and health coverage systems for ADHD
is insufficient. Novel approaches in attention training include
the use of information technologies and specifically the use of
biofeedback treatments as effective for training the attention’s
self-regulation in children with ADHD. The neurofeedback
relays on the neurophysiological self-regulation skills and
provides visual feedback to the users about their performances
in specific cognitive tasks. The EEG based systems try to train
the individual to a particular profile of EEG, not individualized
but based on group dynamics [38], [39], [40], [41] and [42]. For
example, the study [38] examines neurofeedback training and
speech therapy in order to enhance learning and speech ability
in patients with a diagnosis of autism spectrum disorder. A
single case pre- and post-intervention study is adopted. The
training incorporates video feedback in order to increase the 47Hz band (using arousal protocol) on T4-P4 (electrodes’
location can be seen in Fig.1.). The results of the formal
interview, EEG and self-reports show significant reduction in
signs and symptoms, and enhancement in performance [38].
Nowadays, the brain-controlled mobile robots have received
a great deal of attention because they can improve the quality
of life [10]. The scientific implication of robotic intervention
scenarios aims at understanding the thoughts and facial
expression by a child during the process of experiencing of
emotions. There are a lot of training and evaluating classifiers
in the literature designed for the task of detecting the emotional
state of a person by observation of her/his EEG data. Machine
learning techniques in [43] are used to classify emotional states
in high/low arousal and positive/negative valence (Fig.3). Since
EMOTIV currently measures 6 different cognitive states in real
time and two of them are Excitement (Arousal) and Interest
(Valence), emotions can be mapped out on a chart (Fig.3.)
modeling the range of arousal (high to low) and valence
(pleasure to displeasure) that is experienced during a particular
emotion. Authors monitor EEG signals in four locations in the
prefrontal cortex: AF3, AF4, F3 and F4 according to 10-20
international EEG system [13]. Beta waves are associated with
an alert or excited state of mind, whereas dominant alpha
activity is associated with brain inactivation. Consequently, the
beta/alpha ratio is a reasonable indicator of the arousal state of
a person, while the valence in [43] is estimated by computing
30

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

and comparing the alpha power a and beta power b in channels
F3 and F4, given in (1).
valence = aF4/bF4 − aF3/bF3

(1)

Other EEG features about emotions detection can be found
in [44], [45] and [46]. In [46] human emotions are accessed by
simultaneous recording of EEG and Eye-Tracker devices.

Fig. 3. Valence–arousal circumplex chart (adapted from Wikipedia)

The scientific implication of our experiments to transfer the
techniques for learning through art in RIS will be to promote
brain plasticity from stimuli based on sensorimotor modalities
that actors use but in a low-priced way. Our findings are
consistent with the results from the European Project
CREATIONS [18] which suggest that embodied learning may
lead to learning outcomes of a higher quality after
interdisciplinary connection of science with different forms of
Art. The project data were collected from thirteen theatrical
performances which were organized by secondary school
students (500 subjects). However, the educational theater is an
expensive therapy because the effective learning needs doses of
practice over time according to the neuroscience. This is the
reason why we have asked the actors from theater “Tsvete” to
record their own expressions of emotions in different
interactive robotic scenarios for emotional regulation because a
theatric interactive robotic scenario is difficult to be designed
by the robotic scientists.
III. MATERIALS AND METHODS
A. Hypothesis and relevance of the materials and methods
Our hypothesis consists of the following: 1) Feeling,
understanding and expressing emotions are enhanced in the
robotic vs human special educator conditions because the
robotic intervention engages and encourages the exploration
and play; 2) Theater techniques for expressing emotions with
exaggerated mood induction, narrating by theatrical intonation
and using more movements in showing emotions will reinforce
the attention and learning; 3) Brain neuroplasticity and neuronal
excitation of children with SEN will be increased based on
practicing daily life behaviors mediated by socially assistive
robots and integrated with BCI in order to track the correlated
attention and emotional state of the child; 4) A neural proof that
robots reduce the stress in the children during play-like
http://innove.org/ijist/

activities because they are their friends not teachers; 5) The
assistive technologies support special educator in a nonintrusive learning assessment in an entertaining and playful
environment by systematic measuring technique, based on
quantitative measures; 6) The pedagogical rehabilitation needs
time and it is repetitive, therefore if the artistic talents of the
actors are transferred on robots these limitations could be
overcame.
Materials: Assistive robots: humanoid robot NAO [47], nonhumanoid robot BigFoot [48] and emotion-expressive robot
EmoSan [49]. The NAO robot is especially designed to not
scare the kids. Our past and current projects have proven this
for children with SEN more than 3 years old. The children like
also the non-humanoid robot BigFoot very much because it has
fun movements. The intelligent sensors used for assessing the
personal behavior are: MS Kinect sensor [50] and EEG-based
brain-aware headset – EMOTIV [12]. This headset is harmless
wearable device complying with the requirements of the Low
Voltage Directive 2006/95/EC, the EMC Directive
2004/108/EC, the R&TTE Directive 1999/5/EC, and carries the
CE and C-Tick marks accordingly.
Participants: 5-14 years old children of both genders.
Recruitment of participants: Recommended by a pedagogue.
The individuals in all groups had previously been diagnosed by
a licensed clinical psychologist or doctor.
B. Methods
The experimental conditions for testing the proposed robotic
intervention scenarios are described in details in the research
protocols. Here we present the developed BCI-robot software
frameworks and the research solutions: (1) How to use the
commercially available EEG portable EMOTIV headset with
few electrodes for assessment of child attentional and emotional
states in daily life; (2) How to integrate BCI with programmable
robots in order to assist therapists in child eye-tracking; (3) A
new acquisition protocol for detecting eye blinking based on
neural oscillations in only two frequency ranges is designed and
the acquisition of neural data is robust to drying up of EEG
electrodes or head movement; (4) A BCI framework for
emotion capturing and facial expression and analysis in real
time is designed (Fig.8), (5); A new acquisition protocol for
emotional robot-driven interaction based on EMOTIV mental
and cognitive detection is designed; (6) A robot capable of
imitating the movements of the human head is designed; (7)
Tracking and transferring head movements expressing
emotions from an actor to a robot is performed; (8) A BCI-robot
software framework for self-regulatory training of attention and
emotional skills via neurofeedback exposed on the emotionexpressive robot is designed (Fig.6).
To make the assessment more consistent and safer, the
children attention and engagement is monitored by external or
internal observation of head and hand pose changes in time
through a computer vision-based approach or by inertial
sensors. Head and hand poses are key elements of human
behavior analysis. Many techniques for computing head poses
have been reported and the advantages of depth image
irrelevant to light, rotation angle and scale can be exploited. The
31

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

MS Kinect sensor we use for recording the changes in pose is
external to the child since the wearable sensors require careful
calibration and may cause discomfort for the child.
In preparing the intervention scenarios we have been directed
by the good skills presented in [24] and [26] which describe
how to distinguish the helpful emotions and how to practice
“cognitive reappraisal”. We transfer these intervention
scenarios in the safe world of robots and our modification of
Fig.2 is shown in Fig. 4. When the emotion is helpful the robot
listens to what it is telling to do. When an emotion is unhelpful
then there are some good skills that a child has to learn and
practice in the scenario. The robot changes its own thought after
a “cognitive reappraisal” and the child practices this change
correspondingly by imitation. For instance, if the robot feels
angry or sad and the emotion is not helpful then different
activity or exercising, such as gymnastics, dancing, etc., can
help. When the emotion “fear” is unhelpful in the current
situation (Fig.5), another useful skill to be practiced is
“approaching your fear accompanied by Robi – a friend”.
Experimental conditions: The children participate solely (in
individual sessions and in 5-6 minutes per game). During the
session Robi plays 5 games with the child designed for
understanding the Shapes, Emotions, Shopping, Transport,
Body Sounds/ Acoustics. Data analysis concerns brain activity
correlated with eye blinking and its rate index, and shoes how
engaged child is, what is looking at and how important this is
for it. During the game for “playing with robots through
emotions” Robi searches and shows pillows which correspond
to the emotion experienced by the child. At the same time, the
emotion-expressive robot EmoSan imitates the emotion in order
to show it to the child. The child is able to control the BigFoot
robot by head nodding/shaking in guessing what emotion Robi
understands and EmoSan experiences, respectively. Then, the
EmoSan robot is controlled via neurofeedback until the child
feels the right emotion. The child neurofeedback is always
exposed on the humanoid or non-humanoid robots under the
supervision of the special educator. Thus, the child can pay
attention to its own emotions and through the mediated robot
can understand what he/she feels and how responds. The
educational theater “Tsvete” respects the European culture,
where the expression of emotions is encouraged. This directed
our RIS towards learning from practice to handle situations
when a child or a robot feels happy, sad, scared or angry.
In the game for “playing with robots through emotions” a
child can learn emotional knowledge by imitation or by
neurofeedback (Fig. 8.). The play enhances the learning of
emotions because they are neural inputs to control of the play
scenarios and thus reinforces attentional or joyful emotional
responses of the child. The play also facilitates the assessment
of emotional state and cognitive processing of a child by the
special educators. In the Emotive Control Panel the therapist
may monitor the Performance Metrics tab containing two
graphs, which can be customized to display different
combinations of emotional states and time scales. By default,
the top chart is configured to plot 30 seconds of data for the
Engagement, Frustration, and Instantaneous Excitement
detections. The bottom chart default is a display of 5 minutes
http://innove.org/ijist/

data length for the Long-Term Excitement detection. The
plotted values are the output scores returned by the Performance
Metrics detections.
Possible emotional situations and reactions in the world of
robots are illustrated in Fig. 4. The special educator explains
how the child should change its emotional response and by
monitoring the child’s behavior physiologically- or by a visionbased sensor she/he can assess the progress in reinterpreting the
meaning of the situation. The child observes a situation with the
three robots which may cause different thoughts and emotional
reactions. Based on the emotion experienced by the child,
which is tracked by BCI, the interactive scenario is adapted in
order to personalize his/her attention or emotional knowledge.
For example, the EmoSan robot can have different thoughts
about the same situation which leads to different kinds of
changes in: 1) what the face and head do; 2) what it pays
attention to and thinks about; and 3) how EmoSan wants to act.
If EmoSan gets angry when Robi does something mean to it
(the bottom part of Fig. 4.), the anger can inform Robi not to be
mean anymore. If Robi and BigFoot notice that EmoSan is sad
without friends, the sadness can help them to see that EmoSan
needs their love and support. However, sometimes these
identical emotions can be unhelpful if they happen in the wrong
situations. Another example is as follows: if EmoSan gets angry
with Robi because it has hurt EmoSan by accident. This might
lead to deterioration of their friendship.

Fig. 4. A child can have different thoughts about the same situation. Different
thoughts can then lead to different kinds of changes in body, attention and
action. (Figure is adapted from [26] to the world of robots)

The technologies used during the experiments (sensors,
interfaces and robots) help the special educator to monitor and
record the children emotions in quantities measures. For more
consistent assessment we use Microsoft Kinect sensor for
extracting 3D poses by external observations from an image
sequence integrating RGB and depth information. Thus, the
special educators can monitor the sessions online and can be
prompted in assessing specific behavior based on the head and
hand pose changes. These data could be later analyzed offline
in order to prove whether the games mediated by humanoid and
non-humanoid robots enhance attention and emotional skills of
children with SEN. An additional approach, which helps special
32

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

educator to evaluate the child’s concentration and emotional
engagement in real time, is the usage of the led sensors in the
eyes of the Robi robot. The robot blinking duplicates the child’s
blinking by one of the BCI-robot frameworks - EMOTIV-NAO
Software Framework.

response: 0.5-43Hz. Digital notch filters at 50Hz and 60Hz are
built-in. Then, we pre-process EEG signals by using 4th order
Butterworth high pass filter with cut-off 4Hz and by FFT
transformation we extract the different brain rhythms (δ, θ, α,
β, γ). Artefacts in EEG signals arising from head movements
are categorized by automatically detecting their presence
through inspection of the gyroscope waveforms. The artefacts
in EEG signals coming from the head movement are removed
using the measurements of the EMOTIV gyroscope sensor. The
gyroscope channels, capturing the X, Y head-movements,
provide two gyroscope signals for the lateral and vertical
rotation around the neck and axis passing through ears. Each
0.5ms we put trigger markers in the EEG recordings and we
base our analysis on the imported short epochs. Any epoch with
contamination excessing an experimentally defined threshold is
excluded (Fig.7.a).

Fig. 5. A child has an emotional reaction triggered unconsciously

IV. BCI-ROBOT SOFTWARE FRAMEWORKS
Three types of BCI-Robot wireless framework for training or
assessment (Fig.6.) in the pedagogical rehabilitation of children
with SEN have been developed and deployed on the three
robots: 1) EMOTIV-NAO Software Framework (ENSF)
translates the correlated brain signals in time to NAO eye sensor
commands in scenarios for assessing child concentration and
attention; 2) EMOTIV-BigFoot software framework (EBSF),
where the child head motion is translated into commands to
control the movement of the non-humanoid robot BigFoot in
real time for training child’s attention on a specific emotion; 3)
EMOTIV-EmoSan software framework (EESF), where the
child’s current emotion directs the corresponding scenarios for
cognitive reappraisal. A BCI software framework for human
motion capturing during expressing of emotions is proposed at
the end of this section.
We have designed, developed and deployed our own
scenarios on humanoid robot NAO for training of listening,
understanding and speaking skills of children with
communicational disorders in a playful learning environment
for orientation in space, shapes, colors and emotions. In order
to understand the current child emotions and the level of
attention, the child’s brain activity is translated into commands
to humanoid robot Robi and to non-humanoid robot EmoSan.
ENSF translates the correlated brain signals in time to robot eye
sensor commands. In the EBSF the child head motion is
translated into commands to control the movement of the nonhumanoid robots BigFoot. By ENSF and EESF the child can
understand its own emotion by imitation and usage of toys that
are familiar to the child. For instance, Robi shows to the child
commercially available emotional pillows, or EmoSan
nods/plays the current child’s emotion.
The “Insight” or “EPOC” headsets (Fig.1) used in the
framework has 5, respectively 14 wireless channels, which
record human brainwaves with 128 or 250 samples/sec per
channel, resolution: 14 bits with 1 LSB = 0.51μV, frequency
http://innove.org/ijist/

Fig. 6. BCI-Robot wireless framework for training or assessment

A. Brain activity correlated with eye blinking exposed on the
NAO robot
The EMOTIV-NAO Software Framework translates the
correlated brain signals in time to robot commands for changing
led sensors mounted in the eyes. A new acquisition protocol,
describing how to extract brain activity correlated with
blinking, is experimented and appreciated by the therapists as a
useful monitoring tool for the engagement (attentional or
emotional) of the child. The different frequency bands are
individually analyzed and combined together. In order to
control the NAO led sensors, we analyze manly the electrodes
on the forehead AF3 and AF4 where the BCI is robust to
electrode drying up. Thus, we imitate the blinking on NAO in
time via wireless interface. All 5 electrodes are used when we
detect performance metrics in order to reprogram the robot to
adapt the attentional or emotional play scenarios for the current
child. Children without autism seem to be able to anticipate
what is coming next based on facial expressions, body language
and wordplay. Things are different for children with autism.
Without understanding the social context in which actions
happen, children with autism may often be reacting, after the
fact, to physical events that have already happened” [25]. The
ENSF provides a way to monitor the attention and emotional
engagement by measuring blinking on the NAO’s eyes because
children with autism avoid eye contact and measuring it on
NAO assists the special educator remotely to evaluate this
33

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

indicator.

samples (128s/s)

a) Analyzing physical and physiological signals in detecting artefacts

control Robi in recognizing and bringing the emotional pillows
in the room that corresponds to current emotional state of the
child, who wears the EMOTIV headset (Fig.8). Handling the
EMOTIV event engine_PerformanceMetricEmoStateUpdated,
we map the raw scores for excitement, relaxation, stress and
boredom to the corresponding “Naomark” (landmark) stuck on
the emotional pillow. Then, Robi explains how it feels and asks
the child to guess the emotion that Robi is experiencing. The
child responses by head nodding for “No” or by head shaking
for “Yes”. If the answer is “No” BigFoot goes forward like a
spider. Otherwise, it performs left or right funny rotations. In
case of a wrong guess, EmoSan prompts the child by playing
this emotion.

Blinking features

20
10
0
0

200000

400000

600000

samples (128s/s)

800000

b) Blinking features - ratio θAF4/aAF4
Fig. 7. Blinking activity at the right hemisphere

The proposed new acquisition protocol for blinking detection
extracts averaged signals in θ and α and evaluates output score
- a ratio between their powers in time as a classification for
close or open state. We have found the AF4 electrode at the
right side of the forehead as most promising, marked with a red
circle in Fig.1.b. After extracting the eye-blinking features for
open_state and close_state (2) we experimentally defined the
value of threshold Thr to 5. The θ and α power of blinking
activity at AF4 is shown in Fig. 6.b. This figure shows that both
θ and α powers increase, however the θ power increases slightly
faster than α at the moment of blinking and it is significantly
higher. The ratio gets bigger at the moment when the person
blinks.
open_state if Ratio = θ AF4/aAF4 < Thr
(2)
Then, the current state and its duration is mapped to eye
blinking parameters in ALLeds Python module in order to
control NAO’s eye-LEDs on the robot. The protocol has been
experimentally tested and it has proved to be feasible and not
require a training phase. The eye blinking is translated in real
time using Python scripts in order to control the intensity of the
NAO’s eye-sensors.
B. Brain activity correlated with emotions exposed on the
Robi, EmoSan and BigFoot robots
The newly designed emotional robot-driven interaction
protocol is based on EMOTIV SDK mental and cognitive
detections. Six different emotional and sub-conscious
dimensions are measured in real time [35] – Excitement
(Arousal),
Interest
(Valence),
Stress
(Frustration),
Engagement/Boredom, Attention (Focus) and Meditation
(Relaxation). In the designed play these performance metrics
http://innove.org/ijist/

Fig. 8. Brain-robot game for learning emotional knowledge by imitation or by
neurofeedback

In the designed ENSF, the performance metric scores for the
current child emotional activity over time are the inputs to the
NAO module to move the robot – ALMotion. The scores switch
the NAO module for recognizing the corresponding landmark ALLandMarkDetection. In the designed EESF, the EMOTIV
performance metric scores correlated to the child emotions over
time are the inputs to the EmoSan module in order to express
the correlated emotion and drive the corresponding lesson for
cognitive reappraisal. Monitoring the child behavior
physiologically or by vision-based sensor can help in assessing
how the child reinterprets the meaning of the situation and how
it changes its own emotional responses. The emotional
reactions presented in Fig.4. and Fig.5. involve changes in: 1)
what your body does; 2) what you pay attention to and think
about; and 3) how you want to act. It is important to pay
attention to an emotional reaction you have experienced and try
to figure out which emotion you feel and why in order to
respond to it in healthier ways. The EMOTIV gyroscope
measurements during the child head nodding or shaking are
translated to robot commands and used as inputs to the BigFoot
or EmoSan robot controllers. The captured human head motion
is exposed on the EmoSan robot. The eyes and lips of the
EmoSan robot are under development.
C. Motion capturing correlated with head movements exposed
on the EmoSan robot
The BCI software framework for motion capturing has been

34

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

designed, developed and tested. The artistic skills of the
talented actors from the theater in expressing emotions have
been utilized as their motions have been captured and recorded.
The corresponding algorithms for transferring the head motions
of the actor to the robot have been developed. The experimental
conditions for testing the communication protocol based on the
EMOTIV headset for tracking the movements of the actors
begin with a calibration of the EMOTIV and Kinect sensors.
During the session 3D coordinates of head and neck are
extracted to label the Emotiv row data. Then the actor starts
playing a sequence of head and face postures for 6 emotions:
fear, joy, surprise, sadness, disgust and anger. Here, the goal is
to transfer these motions to a socially assistive robot. We
consider the technical and artistic challenges of tracking and
translating movements expressing emotions from an actor to a
robot in order to stimulate the attention and establish emotional
contact with a child. For this reason, we propose a software
framework for motion capturing and transferring the
movements of an actor head to a robot. The technical challenge
was what kind of motion-tracking device to be used and how
the robot to be designed in order to be capable of imitating the
movements of the human head to a great extent. In this study
we use the 9-axis inertial motion sensor (3-axis gyroscope, 3axis accelerometer, 3-axis magnetometer) embedded in the
Emotiv brainwave headset. The sensors’ output signals (with
resolution 64 samples per second) are tracked, recorded and
analyzed online by a custom Emotiv SDK application. These
signals are processed and transferred to an emotion-expressive
robot head dubbed EmoSan, which we have designed and
developed. The movements of this robot are realized by a
parallel kind of mechanism based on the Gough-Stewart
platform (Fig.9). It has 6 degrees of freedom and it was proven
to fulfil the motions of the human head [49]. The robot motion
capabilities are revealed through the analysis of the workspace,
which is graphically illustrated in [49].
V. RESULTS AND DISCUSSION
The designed robotic intervention scenarios are based on a
continuous feedback from the special educators in the Day Care
Center for Children with Disabilities "Zdravets" situated in the
town of Bansko. We integrated several assistive innovative
technologies in the work practice of the special educators for
reinforcing the learning, such as intelligent sensors, robots and
brain aware devices. Our research findings are based on the
pilot experiments. The practitioners found that the BCI-NAO
software framework for exposing on the NAO robot the brain
activity correlated with the child’s eye blinking is very useful
for the assessment of the child’s activity. The director of the
center asked for more support in developing a very simple play
where the child can understand its own emotion by imitation.
We have discussed the possibilities and this has resulted in the
proposed brain-robot game for “playing with robots through
emotions” where NAO mimics the emotions by toys that are
familiar for the children in “Zdravets”, such as “emotional
pillows”. They are used by NAO to interact with the child in
assistance with the non-humanoid robots EmoSan and BigFoot
according to the research protocols described in Section III.
The developed games for “orientation in space, shapes and
http://innove.org/ijist/

colors”; “emotional reactions in the world of robots” and
“playing with robots through emotions” comply with several
criteria for ceasing the participation of a child with SEN in the
experimental session. They are as follows: 1) if a child is
reluctant to participate, the parent can decide to withdraw
his/her consent for the participation of the child during or after
the data collection; 2) the play with robots could be ceased by
the therapist; 3) the interaction between robots and children
must be safe; 4) the child communicates with each robot in the
presence of a teacher; 5) if the child is anxious, the activity will
be terminated immediately
The research protocol for tracking and recording the
movements of actors by the EMOTIV headset have been
approved by the Ethics Committee for Scientific Research, too.
According to the experimental conditions we have recorded the
data from two actors - a female and a male. We have collected
data for from 6 emotions performed by the two actors: fear, joy,
surprise, sadness, disgust and anger. Each one was repeated 3
times with duration of 30 seconds. Because the EPOC
resolution is 64 samples per second, we collected 36 files with
about 1950 records in each one. After processing the data, we
have proved that the sensors and interfaces used in the
experimental sessions help to monitor, measure and analyze the
actor’s head and face movements for expressing emotions and
these movements could be transferred to the robot actuators or
eye and lip sensors.
In order to capture the motion of the human head the data
from the 9-axis inertial motion sensor embedded in the Emotiv
brainwave headset have been used. The algorithm for
processing the raw data from the inertial sensor is beyond the
scope of this study and will be presented in a further paper. The
processed data give the orientation and position of the Emotiv
headset (i.e. the human head) with respect to the earth
coordinate system {Oe} (Fig.9). The body coordinate system
{Ob} is attached to the Emotiv headset and it moves with the
headset. The origin of the earth coordinate system {O e} is a
fixed point on the ground and its axes are also fixed. At this
stage we use the angular rate about the x, y and z axes of the
sensor frame from the tri-axis gyroscope to obtain the
orientation of the body frame, while the data from the tri-axis
accelerometer are used to determine the displacement of frame
{Ob} starting from a known position.
The obtained orientation of the Emotiv headset is given by a
3x3 orientation matrix R. Then, any point (position vector) of
the Emotiv headset (the human head, respectively) given in
body coordinate system {Ob} can be expressed in the earth
coordinate system {Oe} as follows

P e = R  p b + O e Ob ,

(3)

where P e is a position vector of a point attached to the human
head (Emotiv headset) expressed in the earth coordinate system
{Oe}; pb is the same point given in the body coordinate system
{Ob}; OeOb is the vector connecting the origins of the earth
coordinate system {Oe} and body coordinate system {Ob}.

35

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

where Bi = O r Bi and Ai = O r Ai are vectors expressed in the
base coordinate system {Or}.
Linear actuators (DC motors) are used to vary the lengths of
the legs. Therefore, the control of these motors ensures that the
desired lengths of the actuators are achieved which will
correspond to the given position and orientation of the moving
platform. Following this algorithm, the robot head will
reproduce the captured motion of the human (actor) head.

Fig. 9. Schematic arrangement of the coordinate systems of the Emotiv
brainwave headset and the robot

Thus, we can trace the orientations of the human head and
the positions of a point from the head with respect to the earth
coordinate system {Oe} during the motion. The next step is to
transfer the path (set of positions) and set of orientations to the
robot, i.e., to establish a robot control algorithm. The
dimensions of EmoSan robot are similar to the size of the
human head and neck. Then, without loss of generality, we
could set the earth coordinate system {Oe} to coincide with the
coordinate system {Or}, the latter is attached to the base of the
robot and it is fixed. Similarly, the coordinate system {O m},
attached to the moving platform of the robot, coincides with the
body coordinate system {Ob} of the Emotiv headset. The
motion of the moving platform of the robot is realized by the
length variation of the six legs which connect the two platforms
(the base and the moving platforms) of the robot. In order to
control the robot, the lengths of the six legs have to be obtained.
At every given time moment, the orientation and the position of
the moving platform correspond to a particular set of leg
lengths. Since the orientation of the robot head should
correspond to the orientation of the human head (Emotiv
headset), the orientation matrix is the same as in eq. (3). The
joints of the base and the moving platforms are arranged at
points Ai and Bi, respectively (Fig. 8). We need to obtain the
coordinates of points Bi with respect to the base coordinate
system {Or}, i.e,

Bir = R  B im + O r O m ,

(4)

where B im is a point (position vector) from the moving platform
given in the coordinate system {Om} and its coordinates are
known from the robot design; B r is the same point expressed in
i

the base coordinate system {Or}; Or Om = OeOb is the vector
connecting the origins of the coordinate system {O r} and
coordinate system {Om} ( Or Om = OeOb ).
Then, the leg lengths are as follows:

Li = Bi − A i , (i = 1..6) ,

http://innove.org/ijist/

In addition to the robot head motion, the transfer of the
movements of face muscles to the EmoSan robot eyes and lips
is to be performed and this task is still under development.
Tracking facial expression is used to transfer the actor’s
emotional features on the robot. This is important, because
children with autistic spectrum conditions (ASC) frequently
misinterpreted happy faces as neutral faces, and confused
neutral faces with negative facial expressions (sad and angry)
[9]. During our past recordings, together with head motion we
tracked the facial expression and emotional grades of an actor,
provided by Emotiv technology. The three BCI-Robot software
frameworks have been tested separately for their feasibility
during pilot experiments with typically-developing children.
They will be tested together and experiments with children with
SEN will be conducted soon.
CONCLUSION
The main contributions of the proposed study are two
research protocols for reinforcing the attention and emotional
knowledge of children with SEN by means of robotic
intervention scenarios, where an innovative BCI-Robot
framework containing EMOTIV brain headset for tracking,
processing and translating motion or EEG data into robot
commands has been developed. The first protocol describes
how to reinforce children to pay attention or learn emotional
knowledge by imitation and the second protocol describes a
game with the robots where the child learns about its own
emotions based on a neurofeedback exposed on a robot. These
protocols have been approved from the ethic committee of IRBAS and can be tested by other researchers. We propose an
additional research protocol describing how the emotional
talent of an actor in expressing emotions can be tracked by
external (inertial data) or physiological (neural data)
observations, respectively, and recorded. An approach for
tracking the actor behavior and transferring it to the emotionexpressive robot in order the latter to have similar to the actor’s
qualities is proposed. Data from two actors have been tracked,
recorded, processed and transferred to the robot control system.
Also, the developed interface could be applied for direct
transfer of the human motion to the robot in online mode. We
have proved experimentally that the proposed BCI-Robot
framework is capable to mimic the human head motion and eye
movement during the process of expressing emotions. The
proposed software frameworks, connecting the EMOTIV BCI
with the Python API (for NAO robot) or Arduino API (for
BigFoot and EmoSan robots), are sufficiently general and can
be used for different cognitive tasks and with other robots.

(5)

36

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

REFERENCES
[1]

[2]
[3]
[4]

[5]

[6]
[7]

[8]

[9]

[10]

[11]

[12]
[13]
[14]

[15]

[16]
[17]
[18]

[19]
[20]
[21]

[22]

[23]

[24]

[25]

J. Gross and R. Thompson, “Emotion Regulation: Conceptual
Foundations”. In J. Gross James (Ed.), Handbook of emotion regulation.
New York, NY, US: Guilford Press. pp. 3-24. 2007.
Ch. Tyng et al, “The Influences of Emotion on Learning and Memory”,
Frontiers in Psychology, vol.8, pp. 1-22, 2017.
S. Yamaguchi and K. Onoda, “Interaction between emotion and attention
systems”, Frontiers in Neuroscience, vol. 6, 2012.
L. Pessoa, “Cognition and emotion”. Scholarpedia, 4(1):4567, [online
document
],
2009.
Available:
http://www.scholarpedia.org/article/Cognition_and_emotion [Accessed
Jan. 20, 2019].
B. Zikopoulos and H. Barbas, “Pathways for Emotions and Attention
Converge on the Thalamic Reticular Nucleus in Primates”. Journal of
Neuroscience, vol. 32 (15), pp. 5338-5350, 2012.
METEMSS
Project,
[Online],
Available:
http://ir.bas.bg/METEMSS/en/index.html. [Accessed Jan. 20, 2019].
P. MacIntyre and T. Gregersen, “Emotions that facilitate language
learning: The positive-broadening power of the imagination”. Studies in
Second Language Learning and Teaching, vol. 2, pp. 193-213, 2012.
D. Feuerriegel, O. Churches, J. Hofmann and H. Keage, “The N170 and
face perception in psychiatric and neurological disorders: A systematic
review”. Clinical Neurophysiology, vol. 126(6), pp. 1141–1158, 2015.
J. Walsh, S. Creighton and M Rutherford, “Emotion Perception or Social
Cognitive Complexity : What Drives Face Processing Deficits in Autism
Spectrum Disorder ?”. Journal of Autism and Developmental Disorders,
vol. 46(2), pp. 615–623, 2016.
N. Krishnan et al, “Electroencephalography (EEG) Based Control in
Assistive Mobile Robots: A Review”, In Proc. IOP Conferens: Materials
Science and Engineering, 121 pp. 1-11, 2017
MindtecStore
Europe
[Online],
Available:
https://www.mindtecstore.com/BrainExpress-Products [Accessed Jan.
20, 2019].
Emotiv EEG Neuroheadset, https://www.emotiv.com/
J. Malmivuo and R. Plonsey, “Bioelectromagnetism: Principles and
Applications of Bioelectric and Biomagnetic Fields”. Oxford Univ, 1995.
N. Badcock et al.,“Validation of the Emotiv EPOC EEG system for
research quality auditory event-related potentials in children”, PeerJ vol.
3, pp. e907, 2015. [Online]. Available: https://doi.org/10.7717/peerj.90
M. Johnson-Glenberg at all, “Effects of Embodied Learning and Digital
Platform on the Retention of Physics Content: Centripetal Force”,
Frontiers in Psychology, vol. 7, pp. 1819, 2016
C. Breazeal, “Toward sociable robots”, Robotics and Autonomous
Systems vol. 42, pp. 167–175, 2003.
C. Breazeal, “Emotion and sociable humanoid robots”, International
Journal of Human-Computer Studies, vol. 59, no. 1-2, pp. 119-155, 2003.
Z. Smyrnaiou, M. Sotiriou, E. Georgakopoulou and Ο. Papadopoulou,
“Connecting Embodied Learning in educational practice to the realisation
of science educational scenarios through performing arts”, In Proc. of Int.
Conf. “Inspiring Science Education”, Athens ‘04, 2016, pp. 37-45.
H2020
Project
CybSPEED,
[Online],
Available:
https://cordis.europa.eu/project/rcn/212970_en.html
Educational Theater Tsvete, [Online], Available: http://theatretsvete.eu.
[Accessed Jan. 20, 2019].
V. Vassileva-Aleksandrova, “Puppet Therapy in Education -Rehearsal for
the
Real
Life”,
October
2018,
[Online],
Available:
http://theatretsvete.eu/?page_id=4984. [Accessed Jan. 20, 2019].
E. Jochum, J. Schultz, E. Johnson and TD. Murphey, “Robotic puppets
and the eningeering of autonomous theater”. In: Laviers A, Egerstedt M
(eds) Controls and art: inquiries at the intersection of the subjective and
the objective. Springer, New York, pp 107–128, 2014.
E. Jochum, E. Vlachos, A. Chistoffersen, S. Nielsen, I. Hameed and Z.
Tan “Using Theatre to Study Interaction with Care Robots”. International
Journal of Social Robotics vol. 8, pp. 457-470. 2016
R. Ray, K. McRae, K. Ochsner and J. Gross, “Cognitive Reappraisal of
Negative Affect: Converging Evidence From EMG and Self-Report”.
Emotion, vol. 10, pp. 587–592, 2010
S. Shultz, A. Klin, and W. Jones, “Inhibition of eye blinking reveals
subjective perceptions of stimulus salience”, Biological Sciences -

http://innove.org/ijist/

[26]

[27]

[28]
[29]
[30]

[31]
[32]

[33]
[34]

[35]

[36]

[37]

[38]

[39]

[40]
[41]

[42]

[43]

[44]

[45]

[46]

Psychological and Cognitive Sciences - Social Sciences PNAS, 108 (52),
pp. 21270-21275, 2011.
R. Smith, A. Alkozei and W. Killgore, "How do Emotions Work?".
Frontiers for young minds, 2017. [Online serial]. Available:
https://kids.frontiersin.org/article/10.3389/frym.2017.00069
F. Gall, "On the Functions of the Brain and of Each of Its parts: With
Observations on the Possibility of Determining the Instincts, Propensities,
and Talents, Or the Moral and Intellectual Dispositions of Men and
Animals, by the Configuration of the Brain and Head, vol. 1." Marsh,
Capen & Lyon. Originally published: 1835.
M. Minsky, “The Society of Mind”. Simon and Schuster, New York. 1986
.
H. Triandis, “Cross-cultural perspectives on personality”. San Diego:
Academic Press. pp. 439–464, 1997.
A. Rodríguez, B. Rey, M. Clemente, M. Wrzesien, and M. Alcañiz,
“Assessing brain activations associated with emotional regulation during
virtual reality mood induction procedures”. Expert Syst. Appl. vol. 42, 3,
pp. 1699-1709, 2015.
F. Strumwasser, “The relations between neuroscience and human
behavioral science.” J Exp Anal Behavior; vol. 61(2):307-17, 1994
The Royal Society, “Brain waves module 2: Neuroscience: implications
for education and lifelong learning”, LondonThe Royal Society. [Online],
2011.
Available:
https://royalsociety.org/~/media/royal_society_content/policy/publicatio
ns/2011/4294975733.pdf
S. Cramer et al, “Harnessing neuroplasticity for clinical applications”.
Brain 134(6), pp. 1591–1609, 2011.
S. Banks, K. Eddy, M. Angstadt, P. Nathan, and K. Phan, “Amygdala–
frontal connectivity during emotion regulation”. Social Cognitive and
Affective Neuroscience, Vol. 2, pp. 303-312, 2007.
Emotiv Brainware, “Understanding The Performance Metrics Detection
Suite”.
[Online],
Available:
https://emotiv.zendesk.com/hc/enus/articles/201444095 [Accessed Jan. 20, 2019].
C. Lim, T. Lee, C. Guan, D. Fung, Y. Zhao, S. Teng, H. Zhang and K.
Krishnan, “A brain-computer interface based attention training program
for treating attention deficit hyperactivity disorder”. PLoS One , vol. 7,
no. 10, 2012.
D. Blandón, J. Munoz, D. Lopez, and O. Gallo, “Influence of a BCI
neurofeedback videogame in children with ADHD. Quantifying the brain
activity through an EEG signal processing dedicated toolbox”. In Proc.
IEEE 11CCC ’04 2016, pp. 1-8.
M. Karimia, S. Haghshenasb, and R. Rostamic, “Neurofeedback and
autism spectrum: A case study”, Procedia - Social and Behavioral
Sciences vol. 30, pp. 1472 – 1475, 2011.
M Sterman, “Physiological origins and functional correlates of EEG
rhythmic activities: implications for self-regulation“. Biofeedback Self
Regul vol. 21, pp. 3–33, 1996.
S. Butnik, “Neurofeedback in adolescents and adults with attention deficit
hyperactivity disorder”. J Clin Psychology vol. 61, pp. 621–625, 2005.
N. Lofthouse, I. Arnold, S. Hersch, E. Hurt and R. Debeus, “A Review of
Neurofeedback Treatment for Pediatric ADHD”. Jornal Atten Disorders
vol. 16, pp. 351–72, 2011.
H. Gevensleben, A. Rothenberger, GH. Moll, and H. Heinrich,
“Neurofeedbackin children with ADHD: validation and challenges”.
Expert Rev Neurother vol. 12, pp. 447–460, 2012.
R. Ramirez and Z. Vamvakousis, “Detecting Emotion from EEG Signals
Using the Emotive Epoc Device”. In: Zanzotto F.M., Tsumoto S., Taatgen
N., Yao Y. (eds) Brain Informatics. BI 2012. Lecture Notes in Computer
Science, vol. 7670. Springer, Berlin, Heidelberg.
C. Niemic, “Studies of emotion: A theoretical and empirical review of
psychophysiological studies of emotion”. Journal of Undergraduate
Research 1, pp. 15–18, 2002.
L. Aftanas and S. Golocheikine, “Human anterior and frontal midline
theta and lower alpha reflect emotionally positive state and internalized
attention: high-resolution EEG investigation of meditation”,
Neuroscience Letters, vol. 310, no. 1, pp. 57-60, 2001.
A. Muthusamy, et al, “A Narrative Speech, Gaze and Gesture Robot
Accessing to Human Emotion and Memory by Using a Simultaneous
Recording of EEG and Eye-Tracker System”, In Proc. of 12th Int. Conf.
on Innovative Computing, Information and Control, Japan, ‘ 08 2017.

37

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019
[47] NAO
humanoid
robot.
[Online],
Available:
https://www.softbankrobotics.com/emea/en/nao. [Accessed Jan. 20,
2019].
[48] A. Lekova, I. Chavdarov, B. Naydenov, A. Krastev, S. Kostova, "Braininspired IoT Controlled Walking Robot Big-Foot", Advances in Science,
Technology and Engineering Systems Journal, vol. 4, no. 3, pp. 220-226,
2019.
[49] P. Dachkinov, T. Tanev, A. Lekova, D. Batbaatar, and H. Wagatsuma,
“Design and Motion Capabilities of an Emotion-Expressive Robot
EmoSan”, In Proc. 10th IEEE Int. Conf. on Soft Computing and Intelligent
Systems and 19th International Symposium on Advanced Intelligent
Systems SCIS&ISIS2018, Toyama, Japan ‘12, 2018, pp. 1332-1338.
[50] MS Kinect, [Online], Available: https://developer.microsoft.com/enus/windows/kinect. [Accessed Jan. 20, 2019].

A. Lekova was born in Sofia, Bulgaria
Currently she is a Professor and Head
of "Interactive Robotics and Control
Systems" Department, Institute of
Robotics (IR), Bulgarian Academy of
Sciences (BAS), Sofia, Bulgaria. As
the academic record, she was awarded
PhD in 1995 in the field of Computing
from Technical University - Sofia,
Visiting Assistant Professor, Faculty of Computing, Technical
University in 1990-1992, and Erasmus teacher in "Wireless
mobile networks, intelligent routing protocols and security" University of Portsmouth, UK, 2008-2013. In 2017 she was
selected as a full Professor in IR-BAS.
A. Lekova coordinated or contributed in more than 11 EU
and national projects, such as Project N D03-90. Methodologies
and technologies for enhancing the motor and social skills of
children with developmental problems, EEA Grant (20152016) and CybSPEED: Cyber-Physical Systems for
PEdagogical Rehabilitation in Special EDucation, Proposal
Number 777720, MSCA-RISE - (2017-2021). Most recent
publications are in the area of Fuzzy Logic, Evolving
Clustering, Interactive Robotics, EEG-based Brain-Computer
Interfaces and Internet-of-Things (IoT).

T. Tanev received the M.S. degree in
mechanical engineering from Technical
University, Sofia, Bulgaria, in 1982 and
the Ph.D. degree in robotics from Institute
of Mechatronics, Bulgarian Academy of
Sciences, Sofia, Bulgaria, in 1994.
In 1995 he was a post-doctoral researcher
in University of Salford, UK. From 1996
to 2000, he was a Research Associate with
the Central Laboratory of Mechatronics and Instrumentation Bulgarian Academy of Sciences. From 2000 to 2003, he was a
Research Fellow with the Open University, UK. From 2003 to
2015, he was an Associate Professor at the Central Laboratory
of Mechatronics and Instrumentation and Institute of Systems
Engineering and Robotics - Bulgarian Academy of Sciences.
Since 2015, he has been a professor with the Institute of
Robotics, Bulgarian Academy of Sciences, Sofia, Bulgaria. His
research interests include robot kinematics, robot singularities,
http://innove.org/ijist/

computational kinematics, robot performance, parallel robots,
medical robots, social robots, mechatronics.
He is a member of the “Computational Kinematics” Technical
Committee of the IFToMM (International Federation for the
Promotion of Mechanism and Machine Science).as born in.

S. Kostova was born in Novgrad, Ruse
region, in 1959. She received the B.S. and
M.S. degrees in Mechanical Engineering
from Technical University, Ruse in 1982,
second M.S. degree in Applied
Mathematics and Informatics from
Technical University of Sofia in 1983 and
the Ph.D. degree in Control Systems from
Bulgarian Academy of Sciences in 2002.
From 1984 to 1987, she was Assistant Professor of
Mathematics in D. A. Tsenov Academy of Economics,
Svishtov. She has been a Research Associate in Institute of
Control and System Research (ICSR) - Bulgarian Academy of
Sciences from 1995 to 2007. Since 2007 after habilitation she
is Associate Professor in Institute of Robotics (former ICSR)
with the Interactive Robotics and Control Systems department.
She is the author and coauthor of more than 50 articles in the
field of control systems, robotics, positive systems, modelling,
and sustainable engineering. Assoc. Prof. Kostova participated
and coordinated several research projects funded from H2020,
FP5, FP7, NSF of Bulgaria, DSPF and ERASMUS+. She has
won scholarships for specializations at NTNU, Trondheim,
Norway (2010, 2012); EIFER, Karlsruhe (2007), TU-Berlin
(2006), Center of Technomathematics, University of Bremen
(2004), Germany, Univ. Polytechnic of Valencia (2005), Spain,
Curtin University of Technology, Perth, Australia (2003) etc.

P. Dachkinov was born in Panagyurishte,
Bulgaria, in 1993. He received his B.S..
degree in Logistics Engineering and M.S
degree in Mechanical Engineering from the
Technical University of Sofia in 2018.
From 2016 to 2019 he was an engineer in
the Institute of Robotics, Bulgarian
Academy of Sciences. Since 2019 he has
been an Assistant in section Robotics and
Mechatronics Intelligent Systems, Institute of Robotics,
Bulgarian Academy of Sciences, Sofia, Bulgaria. He is a
coauthor in six articles. His research interests include
mechanical engineering, social robots, additive manufacturing,
3D printing and 3D modelling.

38

International Journal of Information Science & Technology – iJIST, ISSN : 2550-5114
Vol. 3 - No. 6 - October 2019

V. Vassileva - Aleksandrova is an actress,
art therapist, theatrical pedagogue, parttime lecturer at Sofia University, expert and
facilitator in many social projects.
In 1990, she graduated from the National
academy of theatre and film arts with
“Puppet theater”. In 2005, she graduated her
Master's degree at NBU with thesis on "The
Role of Forum Theatre for managing anger
in adolescence". She is currently a PhD student in "Special
Pedagogy" at Sofia University "Kliment Ohridski". The topic
of her thesis is "Prevention of violence at school through puppet
therapy".
Violina Vasileva is one of the founders of Theatre Tsvete
Association in 1993, and since 2013 she is a Chairwoman of the
Management Board of the Association. She has participated in
more than 100 projects for non-formal civic education of
adolescents and socialization of minority groups. She is
experienced in working with disadvantaged children and young
people without parents; minority groups; victims of violence;
girls-victims of trafficking; people with physical and mental
disabilities; homeless people; refugees who have suffered from
the Balkan militancy and terrorist acts in the United States. She
is an actress in dramatic and puppet productions with numerous
tours in Bulgaria and abroad. She has participated in forum
theatrical performances related to topics such as: civil rights,
domestic violence, women trafficking, drug prevention and
violence against peers. Since 2016, she has been a member of
the Independent Creative Team Playback Theater "Here &
Now" by participating in performances as an actress and a
conductor.

http://innove.org/ijist/

Omar Bouattane was born in FIGUIG
City, Morocco in 1962. He received his
PhD from the University Hassan II of
Casablanca, MOROCCO in 2001 in
Parallel
Computing
and
Image
processing. He currently serves as a full
Professor in the Department of Electrical
Engineering at “ Ecole Normale
Superieure de l’enseignement technique”
ENSET of Mohammedia. He has more than 150 scientific
publications in various domains of Computational Intelligence,
high performance computing, image processing and renewable
energy.
He has registered 6 national and PCT international Patents
regarding to the networking technology and signal synthesis.
He was awarded as the owner of the best PCT patent in
Morocco on 2011. Since 2012, He was the head of the
laboratory of Signals Distributed Systems and Artificial
Intelligence. He involved his laboratory in several partnership
activities and developed many funded projects in Morocco and
in his university. Overall, Prof. Bouattane work has received
more than 300 citations. Prof. Bouattane has been the principal
investigator and leader in 4 academic projects, funded either
publicly or privately, in the USA, Canada and France. He was
the supervisor from Morocco of a partnership program entitled
“Linkage for entrepreneurship achievement program” funded
by the USAID and HED of USA from December 2012 to
December 2014. He supervised beside his US partners a
scholarship program named “study adroad, Moroccain culture”
December 2016 to December 2018. All his academic and
research activities are in the researchgate portal at:
https://www.researchgate.net/profile/Omar_Bouattane/contrib
utions

39

