Available online at www.sciencedirect.com

Procedia - Social and Behavioral Sciences 83 (2013) 808 ‚Äì 813

2nd World Conference on Educational Technology Researches ‚Äì WCETR2012

       #    
   
   *"  "   "

  

     !  ! 

Abstract
Virtual environments and virtual characters become a part of daily life in human society. This phenomenon gives additional
advantages in such fields as distance learning or distance communication, but the same technologies can bring burden if used
without responsibility. The aim of our investigation is to establish the relation between a user and a virtual character using
feedback. The goal is to adapt/develop appropriate methods that would help to affect the emotional-state of a user by changing
visually-observed parameters of a virtual character. The experiments lead to feedback based model which allows controlling
emotional-state of a user. The initial investigation encourages performing further experiments in order to continue developing the
appropriate methods for the selection of the most effective control parameters.
¬© 2013 The Authors. Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and peer review under responsibility of Prof. Dr. Hafize Keser.
Selection and/or peer-review under responsibility of Prof. Dr. Hafize Keser Ankara University, Turkey

Keywords:           



 , feedback

1. Introduction
A virtual character or avatar already became an integral part of a majority of applications in virtual space. Either
it helps to create one‚Äôs imaginary identity in the virtual reality or it is used as virtual guide/teacher/companion. In the
first case, an avatar serves for distance communication purposes in various contexts as virtual reality games, social
networks (Zhang et al., 2010). The second case includes a large variety of virtual tours, distance learning
applications (Kort et al., 2001).
It is widely described that emotions highly affect learning (Fatahi et al., 2010). However, mostly single
investigations cover only one or another area. One of the examples can be emotions in music (Karl et al., 2007).
Zhang et al. (2010) is aiming to predict emotions of a user in social network. A number of authors have the aim to
construct emotion-based educational models (Fatahi et al., 2010, Ciceri et al., 2008). Investigations, where patients
are involved, are in progress (Mueller et al., 2012). However, only a few studies try to facilitate the learning process
of healthy volunteers by adapting the environment to the individual person and controlling the emotions to reach the
best possible result. In this paper the most informative features of avatar‚Äôs face that would help to reach the wanted

*

Corresponding Author: Egidijus Va≈°keviƒçius. Tel.: +370-37-327-900
E-mail address: e.vaskevicius@if.vdu.lt

1877-0428 ¬© 2013 The Authors. Published by Elsevier Ltd. Open access under CC BY-NC-ND license.
Selection and/or peer-review under responsibility of Prof. Dr. Hafize Keser Ankara University, Turkey
doi:10.1016/j.sbspro.2013.06.152

Egidijus Va≈°keviƒçius et al. / Procedia - Social and Behavioral Sciences 83 (2013) 808 ‚Äì 813

809

target are being explored. Different patterns of face feature parameter changes are examined and evaluated using
correlation with corresponding brain signals. An experiment of controlling the emotions using feedback is presented.
2 . I nput a nd ou
u tput s ignals for the experim ents
Three different sets of 3D woman faces were used for the experiments to explore the parameters that can be used
for human emotional-state control when reacting to the virtual character. The preprocessed EEG-based features of a
volunteer were measured using Emotiv Epoc device. The reactions to the visual input were described using
excitement, meditation, frustration and engagement/boredom parameters (Figure 1).

Figure 1. INPUT-OUTPUT scheme used in the experiments. INPUT ‚Äì virtual 3D woman faces, OUTPUT ‚Äì preprocessed EEG-based features.

Seven 3D woman faces were created using Autodesk MAYA. In continuation of previously performed
experiments (Va≈°keviƒçius et al., 2012), one 3D woman face was used as a ‚Äúneutral‚Äù one (Figure 2a). The other faces
were formed by changing face features in an extreme manner: large and small distance between eyes (Figure 2b),
wide and thin nose (Figure 2c), wide and thin chin (Figure 2d). After importing the 3D faces into Unity 3D engine,
the transitions between different faces were programmed using morphtarget's method.

Figure 2. 3D woman faces used to generate the input for the experiments.

Two kinds of tests were prepared. In the first set of tests, the features of 3D woman face were changing
continuously: from normal to extremely wide, then through normal to extremely thin, then again through normal to
extremely wide and back to normal. Only one feature was being continuously changed in a single test. The test sets
were prepared for every of three features: distance between eyes, nose width and chin width. The values of
parameters were changed from -1.75 (in small distance between eyes and thin nose/chin case) to 1.75 (in large
distance between eyes and wide nose/chin case). Neutral face had a value of 0. The time interval between
highest/smallest value and zero (normal face) was 10 s. The second set of tests was composed of the tests where
input feature values (and faces) were changed suddenly after various periods of time of constant value (Table 1).
Output parameter set includes four EEG-based features: excitement, meditation, frustration, and
engagement/boredom. The volunteers watched the prepared input test sets of 3D faces and the output features were
recorded simultaneously. Each parameter had a value varying from 0 to 1 at every time stamp. In excitement and

810

Egidijus Va≈°keviƒçius et al. / Procedia - Social and Behavioral Sciences 83 (2013) 808 ‚Äì 813

frustration parameter cases, value 0 corresponded to the lowest level of excitement and frustration and value 1
showed that the person is highly excited or frustrated. In engagement/boredom parameter case value 0 corresponds
to the largest boredom and 1 corresponds to the largest engagement. In meditation parameter case, value 1
corresponds to the largest relaxation and value 0 corresponds to the lowest relaxation. The signals were recorded in
2Hz frequency.
Table 1. Second set of tests with time intervals of constant input values
Distance between eyes
Small
Neutral
Large
Neutral
Small
Neutral
Large
Neutral

Time interval, s
10
3
7
2
3
20
5
10

Parameter value
-1.25
0
1.25
0
-1.25
0
1.25
0

Nose/chin width
Thin
Neutral
Wide
Neutral
Thin
Neutral
Wide
Neutral

Time interval, s
10
5
5
5
5
15
10
5

Parameter value
-1.25
0
1.25
0
-1.25
0
1.25
0

3 . I nput pa
a ram ett er analysis
There were two kinds of input parameters investigated: continuously changing and with constant value intervals.
The aim is to find which parameter pattern can evoke the user‚Äôs reaction more efficiently and has larger correlation
to the output signals. First of all, autocorrelation functions of input parameters were calculated (Figure 3).

2

a. u.
10

20

30

40

TYPE 2 input feature

10

20

30
time, s

40

0
-1

50

-50

50

0

50

Autocorrelation function

1

0
-2

Autocorrelation function

1

0
-2

a. u.

TYPE 1 input feature

a. u.

a. u.

2

0
-1

-50

0
time, s

50

Figure 3. Samples of input parameters (left) and corresponding auto-correlation functions (right).

Egidijus Va≈°keviƒçius et al. / Procedia - Social and Behavioral Sciences 83 (2013) 808 ‚Äì 813

811

Figure 3 shows the main patterns of parameters and their auto-correlation functions. The top pictures show the
distance-between-eyes feature (left) and its autocorrelation function (right). The function has the same pattern for
nose/chin width features in continuously changing parameter cases, because parameter values are the same for all
three features. Autocorrelation function shows that there is only one frequency in the signal. The bottom pictures
show distance-between-eyes feature and its autocorrelation function when parameter values have large jumps after
some time period of constant value. More than one frequency components can be seen in the autocorrelation
function. This means that information with more dynamics is saved in the ‚Äòtype 2‚Äô input parameter case.
4 . E xperim ents
A set of experiments was designed to the find the relation between input and output parameters when the reaction
of the user to the single parameter changes was explored. The environment was quiet and the volunteers were not
distracted by the external interferences as noise or inappropriate temperature.
4.1. Experiments for input-output parameter relation analysis
Five volunteers (three female and two male) of various ages were participating in the experiments. Four of the
volunteers were not familiar with the input before the experiment and one of them was familiar with the input data.
The difference between reactions of male and female were not investigated. Each participant was watching the input
3D face settings as follows: a) continuously changing distance-between-eyes, b) constant value intervals distancebetween-eyes, c) continuously changing nose width, d) constant value intervals nose width, e) continuously
changing chin width, f) constant value intervals chin width. Four EEG-based parameters were recorded at the same
time.
The correlation functions between input and output parameters were calculated. The largest absolute correlation
function values between input and output parameters are presented in Table 2.
Table 2. Largest absolute correlation function values between input and output parameters

Volunteer

Input

1

Eyes
Nose
Chin

2

Eyes
Nose
Chin

3

Eyes
Nose
Chin

4

Eyes
Nose
Chin

5

Eyes
Nose
Chin

Correlation with type 1 input signal
Engagement/
Meditation Frustration
boredom
 
 


  


  
  
  
 



 




  
 
 


 
  



  


  
 


  
 
  
  



  
  
  


  
  
  
 







  
  




  


  
 

Correlation with type 2 input signal
Engagement/
Meditation
Frustration
boredom




 
  
  
  
  
 
 



  
 
  




 
 
 





  


 




 
  


 



  
  
  
 
 
  
  
  
 







 
 
  
  







Excitement

Excitement

 
  
 



 
  



 
  



 
  


 
  

  







  
 

 
  
 



  



  
  



It can be noticed that output parameter correlations with ‚Äòtype 1‚Äô input signal are higher than with ‚Äòtype 2‚Äô input
signal. In correlation with ‚Äòtype 1‚Äô input signal case the largest correlation values can be observed in any output

812

Egidijus Va≈°keviƒçius et al. / Procedia - Social and Behavioral Sciences 83 (2013) 808 ‚Äì 813

parameter, depending on the different user and a kind of visual input stimulus. When analyzing correlations of
‚Äòtype 2‚Äô input signal and output signals, the results varies a lot as in the previous case. After analysis of input-output
correlation functions, a dynamic relationship between input and output is recognized. Two correlation functions are
shown in Figure 4.

Figure 4. Correlation functions of volunteer no.3 and volunteer no.5 input-output signals.

4.2. Emotional-state control using feedback
The experiment was performed to investigate if the emotional reaction of the user can be controlled by the
feedback-based method. Virtual 3D face and distance-between-eyes and nose width parameters were used. Values of
both parameters varied from 0 to 1 (from neutral face to large distance between eyes/wide nose correspondingly).
The input features were controlled using engagement/boredom and meditation parameters separately in each of 4
experiments. The control aim was to increase the engagement of the user by changing the visual 3D faces
accordingly. The aim is reached using feedback rule that is described in the following equations:
              
Where xt+1 denotes the value of input parameter at the next time stamp; xt denotes the parameter value of the
present time stamp; f is a function that depends on the control intention; yt denotes present output parameter value
and yt-œÑ denotes the value of output parameter 1.5 s ago. Values of the next input signal xt+1 varied from 0 to 1. The
results when eye-distance parameter is used for input are showed in Figure 5. The goal was reached, because the
engagement increased during the experiment and meditation parameter decreased. The results are similar when
using nose width parameter as input.
Engagement/boredom feedback and control

Meditation feedback and control
0.6

a. u.

a. u.

0.6
0.4
0.2
0
15

20

25

30
time, s

35

40

45

0.4
0.2
0
10

20

30
time, s

40

50

Figure 5. Meditation (right) and engagement/boredom (left) parameters (solid line) using feedback when controlled distance between eyes feature
(dotted line) is used for input.

5. Conclusions

Egidijus Va≈°keviƒçius et al. / Procedia - Social and Behavioral Sciences 83 (2013) 808 ‚Äì 813

813

The study was dedicated to explore input parameters that are able to characterize the 3D face and for defining
their relation with output EEG-based features. It was found that a relationship exists and according to the analysis of
correlation functions input and output signals are related dynamically. The strongest relationship is between output
and TYPE 1 input parameters. It was also found that it is possible to control the output parameters when using
feedback. The results will be used to create a feedback based psycho-emotional state control model in virtual reality.
R eferences
Ciceri, R. & Balzarotti, S. (2008). From Signals to Emotions: Applying Emotion Models to HM Affective Interactions. Affective Computing:
Emotion Modeling, Synthesis and Recognition, ITech, Education and Publishing
Fatahi, S. & Ghasem-Aghaee, N. (2010). An Effective Intelligent Educational Model Using Agent with Personality and Emotional Filters,
Proceedings of the World Congress on Engineering, Vol 1
Karl F. MacDorman, K.F, Ough, S. & Ho, C.-C. (2007), Automatic Emotion Prediction of Song Excerpts: Index Construction, Algorithm Design,
and Empirical Comparison, Journal of New Music Research, vol.36, no.4, 283‚Äì301
Kort, B., Reilly, R., & Picard R. W. (2001). An Affective Model of Interplay between Emotions and Learning: Reengineering Educational
Pedagogy - Building a Learning Companion. ICALT, 43-48.
Mueller, S.C., Hardin, M.G., Mogg, K., Benson, V., Bradley, B.P., Reinholdt-Dunne, M.L., Liversedge, S.P., Pine, D.S. & Ernst, M. (2012). The
influence of emotional stimuli on attention orienting and inhibitory control in pediatric anxiety, Journal of Child Psychology and Psychiatry
Va≈°keviƒçius, E,. Bazeviƒçius, E., Vidugirienƒó, A., Kaminskas, V. (2012). Investigation of Avatar‚Äôs Face Features Influence on User‚Äôs Emotional
State Using Neuro-Feedback, Proc. of the 7th International Conference on Electrical and Control Technologies, ECT-2012, 99-105
Zhang, Y., Tang, J., Sun, J., Chen, Y., Rao, J. (2010). MoodCast: Emotion Prediction via Dynamic Continuous Factor Graph Model, IEEE 10th
International Conference on Data Mining, 1193 - 1198

