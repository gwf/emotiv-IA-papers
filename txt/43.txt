This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identifier 10.1109/ACCESS.2017.Doi Number

Cross-Subject Multimodal Emotion Recognition
based on Hybrid Fusion
Yucel Cimtay1, Erhan Ekmekcioglu1, Seyma Caglar Ozhan2
1

Institute for Digital Technologies, Loughborough University London, UK

2

Department of Computer Education and Instructional Technology, Hacettepe University, Ankara, Turkey

Corresponding author: Erhan Ekmekcioglu (e-mail: E.Ekmekcioglu@lboro.ac.uk).

This work was supported by an Institutional Links grant, ID 352175665, under the Newton - Katip Celebi partnership between the UK and Turkey. The grant
is funded by the UK Department of Business, Energy and Industrial Strategy (BEIS) and The Scientific and Technological Research Council of Turkey
(TUBITAK) and delivered by the British Council. For further information, please visit www.newtonfund.ac.uk.

ABSTRACT Multimodal emotion recognition has gained traction in affective computing research
community to overcome the limitations posed by the processing a single form of data and to increase
recognition robustness. In this study, a novel emotion recognition system is introduced, which is based on
multiple modalities including facial expressions, galvanic skin response (GSR) and electroencephalogram
(EEG). This method follows a hybrid fusion strategy and yields a maximum one-subject-out accuracy of
81.2% and a mean accuracy of 74.2% on our bespoke multimodal emotion dataset (LUMED-2) for 3 emotion
classes: sad, neutral and happy. Similarly, our approach yields a maximum one-subject-out accuracy of 91.5%
and a mean accuracy of 53.8% on the Database for Emotion Analysis using Physiological Signals (DEAP)
for varying numbers of emotion classes, 4 in average, including angry, disgust, afraid, happy, neutral, sad
and surprised. The presented model is particularly useful in determining the correct emotional state in the
case of natural deceptive facial expressions. In terms of emotion recognition accuracy, this study is superior
to, or on par with, the reference subject-independent multimodal emotion recognition studies introduced in
the literature.
INDEX TERMS Emotion Recognition; Multimodal Emotion Recognition; Multimodal Data Fusion;
Convolutional Neural Network; Electroencephalogram; Galvanic Skin Response

I. INTRODUCTION

The study of emotion has a long history. Emotions have
been a subject of philosophy long before it was covered in
other disciplines [1]. Recognising emotions can help
understand and interpret human behaviours. The field of
affective computing emerged for emotion recognition using
various data sources and leveraging computer-based
environments [2]. Affect recognition typically requires
tracking and measuring data sources and processing them for
estimating emotions. There are various emotion recognition
studies published that use multiple sources of data (modalities)
[3-6].
The literature features different emotion models. Authors in
[7] outline a review of them considering nine different studies
and in total there are 65 different emotion categories. Emotion
can be represented by a continuous 4-D space of valence,
arousal, dominance and liking [8]. In many studies the 4-D

space is reduced to a 2-D space as valence and arousal [9] to
represent emotions. The study by Ekman et. al. [10] yielded 6
basic emotions: anger, disgust, fear, happiness, sadness, and
surprise. These emotional states are widely agreed to be
universal across cultures and races. Other emotions map on to
different points on the valence-arousal scale. A 2-D
normalised emotional circumplex model introduced in [11] is
shown in Figure 1. In [11], the valence and arousal values were
determined by analysing a large number of blog posts. The
basic emotion states are highlighted on Figure 1.
Emotion recognition can be performed by analysing the
face, body language or speech, which are the common
modalities, although they are susceptible to deliberate
deception. On the other hand, physiological signals originating
from internal bodily reactions are less affected by deceptions.
Electroencephalogram (EEG) and Functional Magnetic
Resonance Imaging (fMRI), which measure brain electrical or
1

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

blood flow activity, are two examples. Other examples that
can be measured using modern consumer-grade sensors
include heartbeat, galvanic skin response, blood volume pulse,
respiration and temperature.

FIGURE 1. 2-D valence-arousal circumplex model [11]

Emotion classification techniques typically leverage
machine learning methods, such as Support Vector Machine
(SVM), linear and nonlinear regression, decision trees and KNearest neighbour (KNN), which result in a range of
classification accuracies. More recently, deep learning models
like Convolutional Neural Networks (CNN), Long-Short
Term Memory (LSTM) and Convolutional Long-Short Term
Memory (CLSTM) have been studied in emotion
classification [3, 6]. In deep learning models, unprocessed
sensor data can be used directly by the network without preextracting features. Deep learning-based models can achieve
high accuracy recognition rates especially if there is a high
volume of accurately labelled data.
Multimodal emotion recognition aims to combine the
predictive capabilities of individual behavioural and biometric
traits for accurate classification. This bears a greater
complexity than unimodal emotion recognition systems due to
the need for jointly processing of multiple data source. Even
within multimodal approaches, there is a high degree of
variation in terms of prediction accuracy, which necessitates
the design of robust approaches. With this aim, in this paper
we propose a novel hybrid multimodal emotion recognition
model, utilising both soft- and hard-biometric data and test its
performance using existing datasets and also on a newly
created multimodal dataset.
The remainder of this paper is structured as follows: Section
II provides an overview of the relevant literature in the area of
multimodal emotion recognition. Section III gives a detailed
account of the proposed approach. Datasets used in the process
of training and testing are described in Section IV and in

Section V we provide the test results with the discussions.
Finally, Section VI concludes the paper.
II. BACKGROUND AND LITERATURE REVIEW ON
MULTIMODAL EMOTION RECOGNITION

Several unimodal and multimodal emotion recognition
studies have been reported in the literature [12-16]. Majority
of studies use non-physiological data, such as audio, video and
text [17]. One of the difficulties with non-physiological
modalities is that someone may disguise their actual emotion
and not reveal enough cues about their actual state. More
recently, researchers are showing an increasing interest in
brain signals and peripheral physiological signals due to their
nature of irrevocability [18]. However, these are prone to
measurement artefacts and noises. A multimodal emotion
recognition system reinforces the overall recognition
robustness, where temporary defects and problems in some
modalities are compensated for by other modalities.
The study in [19] proposes a fusion model based on verbal
and nonverbal information to increase the emotion recognition
capability of children companion robots. The study in [20]
created a multimodal emotion database that includes the
visual, audio, physiological, depth and pose data of 60
participants. Researchers in [21] collected participants’ facial
expressions, heart rate, pupil diameter and EEG data by
showing them specific visual stimuli. They reported that the
emotion recognition accuracy of models developed based on
multimodal features is superior to those based on single
modality.
In an application context, the study in [22] focused on two
sentiment types: semantic and contraption erudition. Authors
developed algorithms to contribute to the business perception
and consequences in product improvement. A sentiment
analysis technique based on Convolutional Neural Networks
for imagery was introduced in [23]. This study leverages
transfer leaning and the developed model performs reasonably
well under scarce training data condition. The study in [24]
reviewed the sentiment analysis approaches like dynamic
dictionary handling, lexical variations, and sarcasm sentiment
analysis. In [25] researchers applied an end-to-end deep neural
network for learning with both text and image demonstrations.
The study in [26] focuses on estimating the depression
levels. Authors use multitask modality encoders that get
individual modalities’ features as inputs and give modality
embeddings as outputs. An attention-based fusion network is
applied for the fusion of individual modalities. At the final
step, a deep network is applied. In [27], a multimodal
sentiment analysis by using textual, visual and audio features
was proposed. Authors used a CNN model to extract textual
features, and a 3D-CNN model to extract audio and visual
features. The features coming from each modality are
concatenated and for final classification, Support Vector
Machine (SVM) is applied. The study in [28] describes a
generative Unigram Mixture Model (UMM). This model
provides learning a word-emotion society lexicon by using
input from a document corpus. They compare their proposed
2

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

and state-of-the-art baseline methods on word emotion
classification and document emotion ranking. In [29], a
Multimodal Dictionary is presented to understand the
relationship between the spoken words and facial gestures
when introducing sentiment. This approach improves the
prediction accuracies in speaker independent sentiment
intensity analysis. Multiple other recent studies under the topic
of unimodal and multimodal sentiment or emotion analysis are
reported in [30-41].
Data fusion is a critical step involved in multimodal
emotion recognition for producing the estimation. The
literature about emotional data fusion involves three data
fusion techniques, which are early fusion (feature fusion) [42,
43], late fusion (decision fusion) [44, 45, 46] and hybrid
approaches [17, 47, 48].
In feature level fusion, features are extracted from each
modality and one feature set is created by combining all
modalities. This combined feature set is used for the training
of emotion recognition model. Some of the studies which use
feature fusion are [42, 43, 49]. In [42], EEG and audio-visual
feature sets are fused. It mentions that using the fusion of fullband EEG power spectrum and video audio-visual features
achieves the best recognition accuracies. It reports 96.8%
classification accuracy for positive/negative valence and
97.8% for High/Low arousal. The study in [43] integrates lowlevel audio-visual features extracted from videos and brain
functional activity measured by magnetic resonance imaging
(fMRI). For classification they use multimodal deep
Boltzmann machine. In [49], the researchers use Fisher
Criterion Score and Davies-Bouldin index feature selection
methods in order to select significant multimodal features
from physiological data. They use HMM (Hidden Markov
Model) for the classification of valence and arousal.
Feature level fusion is an imitation of human mechanism of
emotion recognition, which is based on collecting all sensory
information from different modalities and combining them.
There is no isolated emotion classification done using any
individual modality, and the classification is based on a
combined multimodal feature set. The limitation for feature
level fusion is that once a model is trained using a specific
combination of feature sets, all future test data should have
exact same feature structure with no tolerance to data loss.
Any missing modality or feature will result in a failed
classification. One possible solution is maintaining multiple
models trained using different combinations of features (or
modalities) to create backups and prevent failures resulting
from missing data. Or, missing data should be recovered
before creating test samples, as done in [50].
In [50], different missing data techniques were applied by
using early fusion with deep recurrent networks. The study in
[51] uses Deep Belief Networks (DBN) for doing a feature
level fusion of audio, face video, body video and physiological
data. Authors in [52] also apply feature level fusion of face
video and audio modalities and classify the emotional state
using a 2-Layer LSTM.

The study in [4] uses EEG and eye movement modalities to
implement a feature level fusion by using SVM and Deep
neural networks. It reports that the mean recognition accuracy
is increased combining feature fusion and deep neural network
compared to combining feature fusion with SVM. In [18],
EEG and peripheral physiological signals were used as
modalities. It applies feature fusion with SVM and bimodal
LSTM classifiers. Comparing to feature fusion with SVM, it
reports that bimodal LSTM has achieved the maximum
recognition accuracy at 93.97% on Shanghai Jiao Tong
University Emotion EEG Dataset (SEED) [46].
Decision level fusion combines the resulted emotion labels
coming from each classifier that use different modalities. In
this type of fusion, each modality leads to an independent
emotional output. These outputs can be used separately or
jointly through machine learning methods. Some studies have
implemented decision level fusion as reported in [44, 45, 49,
54, 55].
The study in [44] a decision level fusion is applied for audio
and visual data to identify emotions. The proposed method is
applied on eNTERFACE'05 database [56]. The study in [49]
uses face, voice and head movement modalities for emotion
recognition. It conducts a late classification by using a
Bayesian framework. It reports that the decision level fusion
is successful especially for detecting happiness. The fusion
strategy increases the average accuracy, from 55% to about
62% comparing to unimodal application.
In [54], the scientists propose a decision level multimodal
emotion recognition based on EGG and face modalities. They
use a stimulus which is based on a subset of clips that
correspond to four specific emotions: happiness, neutral,
sadness, and fear on the valance-arousal emotional space. For
facial expression recognition, these emotion states are detected
by a neural network classifier. For EEG data recognition, four
basic emotion states and three emotion intensity levels (strong,
ordinary, and weak) are detected by support vector machines
(SVM). They apply a sum and production rule for fusion of
the unimodal results. They report that the mean recognition
accuracy is 82% for multimodal method which is higher than
the accuracies of 74.38% and 66.88% of facial expression and
EEG modalities respectively.
The advantage of decision level fusion is that when any of
the modalities is missing, the decision can be made by using
the other modalities. However, this requires an intelligent
system, which can detect the missing modalities. One of the
earliest decision level fusion strategies uses a voting technique
[57]. In this strategy, the classification state reached by most
of the modalities was ultimately agreed on and adopted. The
drawback of this strategy is the likely tie situations. Another
approach, which avoids possible ties and increases the
accuracy, is to use the prior knowledge about each modality.
The prior knowledge includes the classification confusion
matrix for each modality and for each method used. According
to the known prior classification accuracies, for each
emotional state, a weighted voting strategy can be used.
3

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

Voting weights can be calculated from the recognition rate or
error rates that each classifier has shown with the training or
test data [58].
Beyond the use of voting schemes, the study in [59] uses a
lookup table during training to record the classification
combinations, classification output, correct labels and the
number of occurrences of combinations. The classification
confidence of combinations is measured with the number of
the occurrences. The outcome with the highest level of
confidence in the lookup table is chosen.
The study in [60] computes confidence for each modality
and weights the unimodal decisions by the measured quality
of raw signals. This operation decreases the classification error
rate and increases the quality of recognition [60]. In [61],
Relevance Vector Machines (RVM), which correspond to
embedded SVMs are used. RVMs resemble SVM but run in
an embedded Bayesian frame. It calculates the affiliation
probabilities of each emotion category across the classifiers.
These probabilities are used as classification weights for
decision level fusion.
The study in [62] uses a decision fusion strategy, which
combines the emotion outputs of visual and audio paths using
KNN or Artificial Neural Networks (ANN). The study in [63]
is one of the first examples of approaches combining audio,
face and physiological data by applying decision level fusion.
It reports the Concordance Correlation Coefficient (CCC)
metric result for arousal and valence and concludes that
multimodality increases the emotion recognition accuracy.
In [54], a decision level fusion was applied to outcomes
from EEG and face modalities. It applied SVM classification
for EEG data and Neural Network for face data. The emotional
states are grouped according to the intensity levels. According
to the labels of training data (happiness, neutral, sadness, fear)
it tried to find the optimal weights of each modality in
obtaining the final decision.
Another fusion technique is called hybrid fusion, which
typically combines both the early- and late-fusion techniques.
For instance, one classifier may deploy a feature-level fusion
for the face and body gestures modalities, while another
classifier may do so for physiological signals. Another
decision level classifier above these two classifiers can process
the results of two feature level classifiers to come up with the
final emotion label. Hence, such a system is called a hybrid
fusion system.
In [64], a simple hybrid fusion was employed where the
output of an early fusion classifier is feeding input to a
decision-level fusion system. A recent study in [48] uses a
latent space map for the fusion of audio and video modalities;
and then, by using a Dempster-Shafer (DS) theory-based
evidential fusion method, the projected features on the crossmodal space are fused with the textual modality.
Multimodal techniques have also been applied in crosssubject and subject-independent emotion recognition studies.
The study in [65] uses physiological signals with feature level
fusion. Authors trained a deep CNN model and achieved 94%

subject-independent average accuracy on BP4D+ dataset [66]
for 10 emotion classes. The study in [67] uses speech and body
motions modalities with two stages Gaussian Mixture Model
(GMM) mapping framework. It achieves a maximum
accuracy of 63%, 51%, 50% for valence, arousal and
dominance, respectively, on USC CreativeIT multimodal
emotion database [68].
The study in [69] uses a decision fusion on physiological
data including photoplethysmography (PPG), a respiratory
belt (RB) and fingertip temperature (FTT) and conducts onesubject-out test on the DEAP dataset. It achieves 72.18% and
73.08% mean accuracies on high-low valence and high-low
arousal, respectively. The study in [70] uses a three-stage
decision method for classifying four emotions on the DEAP
dataset. It achieves 77.57% average accuracy when classifying
the labels as high- and low- arousal, and 43.57% when
classifying as high- and low- valence.
A. LIMITATIONS OF THE REVIEWED MULTIMODAL
EMOTION RECOGNITION STUDIES

While the published studies report varying recognition and
classification accuracies under varying contexts, we noted
some common limitations applying to a multiple of these
studies. They are summarised below.
- Studies that exploit facial expression analysis for
emotion detection broadly assume that the participants
do not show deceptive expressions, while this is not
always the case. DEAP dataset is a good example for
this. Visual-based multimodal emotion recognition
studies tend to yield inferior accuracy results with this
dataset.
- Multimodal emotion recognition studies generally do
not discriminate between the predictive capabilities of
individual modalities. For example, GSR is effective in
predicting the arousal, but not valence. Incorporating
all modalities equally in a model could reduce the
recognition accuracy.
- The effects of emotion transitions are not reflected on
physiologic outputs with the same delay. Yet, many
reviewed studies jointly process multiple modalities
measured at co-located time windows that can limit the
achievable recognition accuracy.
B. CONTRIBUTIONS OF THIS WORK

-

-

-

We employ subsets of modalities and features derived
from them, in association with the targeted dimension,
i.e., valence or arousal. Our decision to select the
subsets of modalities is informed by the former studies.
The presented emotion recognition model is made
robust against natural deceptive facial expressions by
comparing facial-expression based dominant emotions
against emotions inferred via other modalities.
We adopt a hybrid approach composed of feature- and
decision-level fusion. Physiologic modalities measured
4

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

-

at varying time instances are jointly processed for
improved accuracy.
Our method is tested on multiple datasets, which
include instances of deceptive facial expressions.
The feature extraction capability of a pretrained CNN
model (InceptionResnetV2 [64]) is utilised to eliminate
the need for manual feature extraction.

III. PROPOSED METHOD

In this study a new multimodal emotion recognition method is
introduced, which uses face, EEG and GSR data modalities.
The overall system diagram is shown in Figure 2, which will
be described later in detail. In this study, to detect the
emotional state, the valence probability array coming from
EEG modality at time 𝒕, the arousal probability array coming
from the combined data of EEG and GSR at time instant 𝒕 +
𝑻𝟎 and, the discrete emotion probability array coming from
face modality (𝑷𝑭 ) at time 𝒕 + 𝑻𝟎 are used.
The reason we introduce a time delay 𝑻𝟎 between EEG
sample and other modalities’ samples is due to the fact that
physically the emotion signal first emerges in the brain and the
brain sends the exciter signal to trigger other modalities with
some delay. 𝑻𝟏 depends on the processing power of used
hardware. Empirically we have chosen 𝑻𝟎 as 0.35 sec. For
prediction of arousal we use EEG and GSR raw data together,
and for valence we only use EEG data. There are three separate
InceptionResnetV2 CNN models for arousal, valence and
face-based discrete emotion, respectively.

FIGURE 2. Proposed Hybrid Multimodal Emotion Recognition Method

A. FACIAL EXPRESSION ANALYSIS

The face data used in this study for training our multimodal
emotion recognition model is composed of well-known public
face imagery datasets including Cohn-Kanade+ face dataset
[72], Radboud Faces Database [73], FacesDB [74] and
AffectNet [75]. Since we get use of the spatial information
contained in the images, for the purpose of training, rather than
video, static images are preferred. For each of the datasets
used, all the facial images were manually double-checked to
identify and leave out the samples where we don’t have a
consensus with the associated emotion label. This filtering
step is applied in our work to minimise the influence of

training samples with potentially wrong or arguable labels on
the classifier performance. Table I shows the percentage of
eliminated labelled facial image samples from each dataset
following the application of the filtering step.
TABLE I
PERCENTAGE OF ELIMINATED IMAGES FROM EACH DATASET

Cohn Kanade [49]

Percentage of
Eliminated Images
17

FacesDB [51]

12

Radboud [50]

14

AffectNet [52]

22

Name of Dataset

Following the filtering step, a final dataset is obtained by
combining all labelled samples from the four datasets. The
number of face images belonging to each emotion category is
given in Table II.
TABLE II
NUMBER OF LABELLED FACE IMAGES USED IN TRAINING
Emotion Category

#Images

Angry
Disgust
Afraid
Happy
Neutral
Sad
Surprised
Total

1117
1173
784
2417
1401
874
1126
8898

It is important to note that this combined dataset features
labelled face image samples taken using different camera
settings (angle of shooting, camera zoom level, resolution,
average brightness) in order to handle different conditions. We
don’t use the face data of LUMED-2 [76] and DEAP [77]
dataset, since they are the test datasets for this study. Face
modality is used in the form of still images of faces. A pretrained state-of-the-art InceptionResnetV2 CNN model is
trained again using the previously discussed and refined face
datasets as input, and their associated discrete emotion labels
as output. The reason for choosing InceptionResnetV2 is due
to its proven capability of yielding one of the highest
recognition accuracies in the image classification context [78].
The initial network weights were adopted from ImageNet
training. Hence, it is called pre-trained. The parameters of
training are given in Table III.
This CNN model is trained with 8898 face images (see
Table II) in the emotion categories of angry, disgust, afraid,
happy, neutral, sad and surprised. Image data augmentation
and normalisation is applied on the training set prior to training
in order to handle different shifting, zooming and lighting
conditions.

5

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

TABLE III
TRAINING PARAMETERS OF FACE NETWORK
Property

Value

Base Model

InceptionResnetV2

#Images

8898

#Extra Layers

3 Dense layers with depth of 1024

Rotation range

10

Width-Shift Range

0.1

Height-Shift Range

0.1

Zoom Range

0.2

Horizontal Flip

True

Regularization

L2

Optimizer

Adam

Loss

Categorical cross entropy

#Epochs

30

Shuffle

True

Batch Size

64

#Classes

7

Environment

Win 10, Parallel GPU (2 GPUs),
TensorFlow

The outputs of trained model 𝑪𝑵𝑵𝑭 in Figure 3 are the
probability vectors of seven discrete emotion categories
(based on the Ekman’s model, and additionally the neutral
emotion state) and following the application of the argmax
function, the final emotion state is yielded. The diagram of
emotion classification based on the face data modality is
shown in Figure 3.

FIGURE 3. Face Emotion Recognition Model

B. GALVANIC SKIN RESPONSE (GSR) ANALYSIS

Second modality used in the proposed hybrid multimodal
architecture is the GSR data. When humans are exposed to an
event or stimuli, the emotion is triggered first in the brain [79].
The brain signals start to change and send exciter signals to the
other parts of the body.
For instance, face expression and voice tone change.
Another important change occurs in physiological signals.
One of them is GSR, which is a measure of change in the
electrical resistance of the skin. When one is emotionally
aroused, the electrical conductivity of the skin changes. GSR,

which is also known as EDA (Electrodermal activity) or SC
(Skin Conductance), is one the most important measures of
emotional arousal [80].
GSR measures the skin secretion that is an unconscious
process under the control of body’s sympathetic nervous
system and reflects the changes in arousal. When people are
aroused due to various effects like fear, joy or stress, skin starts
to sweat. GSR has been investigated in many studies and it is
still regarded as one of the most powerful methods for
understanding and measuring physiological arousal.
GSR has been used in a wide range of research activities,
which include but not limited to physiological research,
clinical research and psychotherapy, consumer neuroscience
and marketing, media and ad testing, usability testing and User
Experience (UX) design [81]. Under normal conditions, for a
healthy individual GSR is near stable. Once an individual is
exposed to some arousing stimuli, more frequent changes in
the GSR data start to be observed. Following to the transition
to a calmer state, GSR change activity decreases. Since GSR
is highly linked to one’s level of arousal, we use this modality
together with the EEG modality for predicting the arousal
level of individuals.
C. ELECTROENCEPHALOGRAM (EEG) ANALYSIS

One way of measuring the brain activities is using the EEG
technology. The change of the electric potential formed on the
skull is measured by using actual and reference electrodes.
Some commercially available EEG devices in the consumer
market include Emotiv, Neurosky, and Neuroelectrics [82].
These devices have various spatial and temporal resolutions.
Spatial resolution is related to number of electrodes placed on
the head and the temporal resolution is related to the number
of electric potential changes recorded in a second. Generally,
EEG has a low spatial, but a high temporal resolution
compared to other brain monitoring technologies such as
fMRI and functional near-infrared spectroscopy (fNIRS).
There are different electrode positioning standards for EEG,
such as 10-20, 10-10 and 10-5 that result in different spatial
resolutions [83].
EEG is preferred in this work due to its reliability and
portability compared to other brain monitoring technologies.
Clinical applications have traditionally been one of the
application areas of EEG. EEG has been used to investigate
the signal patterns related to epilepsy [84] and sleep [85].
Detection of the hyperactivity and consciousness related
disorders [86, 87], measurement of mental workload [88],
level of attention [89-91], mood and emotions [92-94] have
been the other application areas of EEG.
Although EEG has gained popularity in the context of
emotion recognition studies due to its resistance to deceptive
actions of humans, it exhibits varying distributions for
different people as well as for the same person at different time
instances [95, 96]. This is a problem, which decreases the
accuracy of emotion recognition for subject-independent
applications. One way of mitigating this kind of a problem is
6

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

decreasing the number of emotion classes by grouping them
and using complex techniques on feature extraction to increase
the accuracy [97]. In this study we leveraged the research
outputs of our previous study in [97]. Although the subjectindependent emotion recognition accuracy of EEG is
relatively low, when the number of emotion categories is
reduced to two (e.g., high and low valence/arousal), the
accuracy increases accordingly.
Our previous study on EEG-based emotion recognition [97]
was a leave-one-subject-out cross-validated study. This is
depicted in Figure 4a, where the outputs of EEG-based
emotion recognition system are the probabilities of high- and
low- valence (PV-H and PV-L) considering two classes.
In order to increase the overall accuracy, in the proposed
method, we do a pre-grouping on the valence and arousal
labels in order to reduce the total number of output classes. We
group valence as high- /low- valence and arousal as high/low- arousal. In this study, for estimation of arousal, we have
combined the prediction capabilities of both the GSR and EEG
modalities.
The outputs of the network that relies on the early fusion of
raw GSR and EEG are the probabilities of high- and lowarousal (PA-H and PA-L), as shown in Figure 4b. The training
parameters of EEG network in Figure 4a, and the network
combining both EEG and GSR modalities in Figure 4b are
given in Table IV. The parameters used are the same. Note
that, like in the case of the face modality, for the physiological
signals we also used the InceptionResnetV2 Convolutional
Neural Network as the underlying network architecture.

TABLE IV
TRAINING PARAMETERS OF EEG AND GSR+EEG NETWORK
Property
Base Model
Additional Layers
Regularization
Optimizer
Loss
Max. #Epochs
Batch Size
#Classes
Environment

Parameters
InceptionResnetV2
Global Average Pooling, 5
Dense Layers
L2
Adam
Categorical cross entropy
100
64
2 (H-L Valence/Arousal)
Win 10, Parallel GPU (2
GPUs),
TensorFlow

The outputs of 𝑪𝑵𝑵𝑨 in Figure 3, are the probability
vectors of high- and low- arousal. Similarly, the outputs of
𝑪𝑵𝑵𝑽 are the probability vectors of high- and low- valence.
The outputs of 𝑪𝑵𝑵𝑽 and 𝑪𝑵𝑵𝑨 are fed into a weighting unit.
This unit calculates the weighted sums of valence and arousal,
respectively. Weighted valence 𝑽𝑾 and weighted arousal 𝑨𝑾
are then sent to the distance calculator. Distance calculator
calculates the emotional distance from the weighted valencearousal to actual valence-arousal pairs, which correspond to
the seven discrete emotion states. Emotional distances are fed
to the final decision tree system. The output of 𝑪𝑵𝑵𝑭 is fed to
decision tree system at this stage too.
The decision tree system, which is explained in more detail
below, outputs the final computed emotional state at time 𝒕 +
𝑻𝟏 . Distance calculator calculates the emotional distance from
the weighted valence-arousal to actual valence-arousal pairs,
which correspond to the seven discrete emotion states.
Emotional distances are fed to the final decision tree system.
The output of 𝐶𝑁𝑁𝐹 is fed to decision tree system at this stage
too. The decision tree system, which is explained in more
detail below, outputs the final computed emotional state at
time 𝑡 + 𝑇1 . The reason we put a time delay 𝑇0 between EEG
sample and other modalities’ samples is due to the fact that
physically the emotion signal first emerges in the brain and the
brain sends the exciter signal to trigger other modalities with
a of delay.

(a)
D. DECISION TREE

(b)
FIGURE 4. (a) One-Subject-Out Valence Recognition Model, (b) OneSubject-Out Arousal Recognition Model

At the core of the proposed method lies the function of a
decision tree. Here we put the emphasis first on the probability
vector of the face modality-based emotion detection system.
We consider not only the dominant emotion (i.e., the one with
the maximum probability), but also the emotion with the
second highest probability. This is because individuals may
hide their actual emotion and the likelihood of the second most
probable emotion state being the actual emotion state
increases. Also, face modality-based emotion systems may
sometimes lead to false detections in terms of the emotional
states in the case of confusing gestures. At this point, the other
physiological modalities (i.e., GSR and EEG) help detect the
emotional state with better accuracy.
7

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

EEG is a nonstationary signal and shows different
distributions from subject to subject. Therefore, it is not very
successful in terms of predicting the discrete emotional state
or the valence and arousal when subject-independency is
sought [95], and especially when the number of output classes
is relatively high. GSR on the other hand is mostly a
measurement of arousal and doesn’t give useful information
about the valence dimension.
We set the normalised centre values of high and low as 0.75 and 0.75, respectively. These centre values for low and
high can change according to the choice of low and high
intervals. In our case, the resulted valence value is the average
of valence vector [𝑃𝑉 ] and arousal is the average of arousal
vector [𝑃𝐴 ]. We also use the associated (mapped) valence and
arousal values of the used seven discrete emotion states,
shown in Figure 1. These are empirical values previously
introduced in [11]. These values ([valence, arousal]) include:
[0,0], [0.89,0.17], [-0.81,-0.40], [-0.68,0.49], [-0.40,0.79], [0.12,0.79], [0.42,0.88] for emotion states 'Neutral', 'Happy',
'Sad', 'Disgust', 'Angry', 'Afraid', and 'Surprised', respectively.
In the decision tree, if the maximum probability in [𝑃𝐹 ]
vector coming from face modality based network is over a predefined threshold that is empirically set to 0.9, we take the
final emotion state as the one discrete emotion with maximum
probability value in [𝑃𝐹 ]. If the maximum probability is below
the threshold, then we use the distances from the weighted
valence-arousal to each valence-arousal pair of discrete
emotion states (i.e., output of distance calculator). We
compare the emotional distances of the emotion states with the
highest and second highest probabilities in the [𝑃𝐹 ] vector. We
check whether the emotional distance of the emotion state with
second highest probability in [𝑃𝐹 ] is higher than the one with
highest probability. If it is higher, then we take the one with
second highest probability in [𝑃𝐹 ] as the final emotional state.
Otherwise, we assign the discrete emotional state with the
highest probability in [𝑃𝐹 ] as the final emotional state output.

else
if 𝑑𝐸 (argsecondmax ([𝑃𝐹 ]))< 𝑑𝐸 (argmax ([𝑃𝐹 ]))
emotionstate = argsecondmax ([𝑃𝐹 ]))
else
emotionstate = argmax ([𝑃𝐹 ])
return emotionstate
//Main hybrid fusion-based emotion recognition cycle
load 𝐶𝑁𝑁𝑉 , load 𝐶𝑁𝑁𝐴 , load 𝐶𝑁𝑁𝐹 , load [𝐺𝑇𝑉 , 𝐺𝑇𝐴 ]
𝑇ℎ = some value between [0.7,1]
[𝑊𝑉 ] = first element is between [-1, -0.5], second element is
between [0.5,1]→the center value of high and low valence
[𝑊𝐴 ] = first element is between [-1, -0.5], second element is
between [0.5,1] →the center value of high and low arousal
while True
EEG = readEEGData ()→ at time 𝑇
sleep for 𝑇0 →𝑇0 is chosen as empirically,
optimally set to 0.35 sec.
GSR = readGSRData ()
Face = readFaceData ()
EEG_GSR = concatenate (EEG,GSR)
𝑃𝑉 = 𝐶𝑁𝑁𝑉 (EEG)
𝑃𝐴 = 𝐶𝑁𝑁𝐴 (EEG_GSR)
[𝑃𝐹 ] = 𝐶𝑁𝑁𝐹 (Face)
𝑉𝑊 = sum (𝑃𝑉 *[𝑊𝑉 ]), 𝐴𝑊 = sum (𝑃𝐴 *[𝑊𝐴 ])
𝑑𝐸 = DiffCalculator (𝑉𝑊 , 𝐴𝑊 , [𝐺𝑇𝑉 , 𝐺𝑇𝐴 ])
FinalState = DecisionTree ([𝑃𝐹 ], [𝑑𝐸 ] , 𝑇ℎ)
IV. DATASETS

The multimodal datasets used in the proposed emotion
recognition system include LUMED-2 [76] and DEAP [77]
multimodal emotional databases. These two datasets are open
to public access.

E. THE OVERALL HYBRID FUSION WORKFLOW

A. OVERVIEW OF DEAP DATASET

The proposed method is suitable for real-time operation. To
summarise the overall hybrid fusion based multimodal
emotion recognition technique, which uses face data, GSR and
EEG inputs, a pseudocode is provided below. In the
pseudocode, 𝐺𝑇𝑉 and 𝐺𝑇𝐴 represent the ground truth valence
and arousal pairs taken from valence-arousal circumplex
model shown in [11], and MSE stands for Mean-Square
Error. Other symbols in the pseudocode are as defined in the
previous sub-sections of Section III.

As explained in [8], DEAP is a multimodal emotion dataset
which includes peripheral physiological signals and the EEG
of 32 participants. Frontal face video of 22 participants was
also additionally recorded. To elicit emotions in the
participants, one-minute long video clips (music) were shown,
and the data was recorded in real-time. Each participant
watched 40 separate videos. Participants rated the videos in
terms of levels of arousal, valence, like or dislike, dominance,
and familiarity along a number scale between 1 and 9. A 32channel EEG device was used. The raw EEG data was down
sampled to 128 Hz. The EOG artefacts were removed and a
bandpass filter was applied whose cut-off frequencies are 4.0
Hz and 45.0 Hz. The data was segmented into 60-second
intervals and a 3 second baseline data was removed. Face
videos are also set to 60-second long segments at 50 fps and
720x576 resolution.

def DiffCalculator (𝑉𝑊 , 𝐴𝑊 , [𝐺𝑇𝑉 , 𝐺𝑇𝐴 ])
[𝑑𝐸 ] = MSE (𝑉𝑊 , 𝐴𝑊 , [𝐺𝑇𝑉 , 𝐺𝑇𝐴 ])
return 𝑑𝐸
def DecisionTree ([𝑃𝐹 ],[𝑑𝐸 ] , 𝑇ℎ)
if max ([𝑃𝐹 ]) > 𝑇ℎ
emotionstate = argmax ([𝑃𝐹 ])

8

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

B. OVERVIEW OF LUMED-2 DATASET

V. RESULTS AND DISCUSSION

Loughborough University Multimodal Emotion Database2 (LUMED-2) is a new multimodal emotion dataset that was
created by the researchers of Loughborough University, UK,
and Hacettepe University, Turkey, by collecting simultaneous
multimodal data from 13 participants (6 females and 7 males)
by showing audio-visual stimuli [76]. The total duration of all
stimuli is 8 minutes and 50 seconds, which consist of short
video clips selected from the web to elicit specific emotions.
Between each video clip, in order to let participants, have a
rest, a 20-second grey screen was showed.
Although it is anticipated that each video clip elicits a
distinct emotional state and thus can determine the label of the
resulting emotion, in reality the same content might trigger
differing emotions for different participants. Therefore, after
each session, the participants were additionally asked to label
the clips with the felt emotional state while watching them.
Three different emotions were resulted from labelling: sad,
neutral and happy. The facial expressions of the participants
were captured using a webcam at a resolution of 640x480 and
at 30 fps.
Participants’ EEG data was captured using an ENOBIO 8channels wireless EEG device, which has a temporal
resolution of 500 Hz [73]. We filtered EEG data for the
frequency range [0, 75Hz] and applied baseline subtraction for
each window. As for the peripheral physiological data, an
EMPATICA E4 Wristband [98], powered by Bluetooth, was
used to record participants’ GSR. A screenshot of the data
capturing system is shown in Figure 5. We prepared a fully
wireless setup, which facilitates a more comfortable
experience for the participants while watching the stimuli,
reducing the inherent distress induced by wired devices
surrounding their body. This is one of the important
advantages of our multimodal data collection setup.

In this work, leave-one-subject-out cross-validated
classification tests were conducted on two different
multimodal emotion datasets: DEAP and LUMED-2. For
DEAP dataset, we first generated the discrete emotion labels
using the participants’ rating of valence and arousal (already
existing in the dataset). We extracted the central valence and
arousal values of seven discrete emotion states from the 2-D
emotional circumplex model shown in Figure 1. Then the
Least Mean Squares (LMS) distance between each emotional
state and the participant ratings were calculated. The emotion
state, which gives the minimum distance and has a maximum
normalised distance of 0.2, was assigned as the discrete
emotional label for the corresponding samples in the dataset.
For LUMED-2, the discrete emotion labels are given as “sad”,
“neutral” and “happy”, i.e., three classes of emotions.
We classified the emotions under three test conditions:
using (a) face modality only, (b) physiological modalities
only, and (c) face and physiological modalities together. For
DEAP dataset’s physiological (EEG+GSR) data, we have
achieved an average cross-subject high-low valence
classification accuracy of 86.6% and an average high-low
arousal classification accuracy of 84.7%. In these two-class
classifications, we defined the interval between 7 and 9 on the
valence/arousal scale as “High”, and the interval between 1
and 3 as “Low”. Note that these are not the classification
accuracies obtained for discrete emotion states.
Table V shows the classification accuracy results for the
three test conditions using the DEAP dataset and for 10
randomly selected participants who had their face video also
taken. The number of unique emotion states are also given in
the table. It can be seen from the table that using EEG and
GSR modalities with face modality increases the recognition
accuracy in comparison to using only the face modality and/or
only the physiological modalities, for all selected participants.
The mean accuracy is increased by approximately 10%
compared to using only the face modality.
The proposed hybrid multimodal emotion recognition
method also increases the accuracy compared to using
physiological modalities only. It increases the mean accuracy
by approximately 13%. An important point to note is that the
average emotion recognition accuracies are relatively low for
the DEAP dataset. This is mainly because the participants’
faces are covered by multiple Electromyogram (EMG)
electrodes and connectors. EMG setup acts as a hindrance that
prevents participants from showing facial expressions freely.
This is regarded as deceptive facial actions in terms of our
study, which may lead to a higher percentage of false
classifications for the face modality.
An example face imagery from the DEAP dataset is shown
in Figure 6. A face-based emotion classifier yields for this
specific face image a 93% probability of “happy” state and a
4% probability of “neutral” state. On the other hand, the
generated emotion label based on the valence and arousal
values is “Angry”. It is also difficult to recognise the actual
emotion state with the proposed multimodal method when the
dominant emotion probability is over a high threshold. We
have set the threshold probability as 0.9. When the participant

FIGURE 5. LUMED-2 Data Collection Setup

9

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

starts to show the actual emotion naturally, meaning that the
maximum value of probability vector produced by face
modality drops under 0.9 and even if it is in the second place
in the face recognition probability order, our proposed method
mostly catches the actual emotion by use of EEG and GSR.
These results can be observed from Table V and VI.

FIGURE 6. An example face imagery of emotion “Angry” from
DEAP dataset.

TABLE V
“ONE-SUBJECT-OUT” PREDICTION ACCURACIES (%) FOR DEAP DATASET
Physiological
Face +
#Emotion
User
Face
(GSR+EEG)
(GSR+EEG)
states
User 1
0.03
4.51
11.45
5
User 2

31.35

24.72

47.12

4

User 3

85.50

56.89

91.51

3

User 4

29.34

21.33

39.56

4

User 5

46.31

36.42

57.88

5

User 6

45.39

43.01

58.74

4

User 7

25.04

27.49

44.78

4

User 8

25.44

28.30

40.66

5

User 9

11.88

16.48

33.91

3

User 10

18.01

20.21

37.33

4

Average
(22 users)
Std.Dev.
(22 users)

37.11

32.45

53.87

-

20.35

15.56

16.44

-

Table VI shows the accuracy results for the LUMED-2
dataset for three discrete emotion states: sad, neutral and
happy. As shown in the table, face and physiological
modalities establishes superiority to each other for different
users, however using all modalities together with proposed
method improves the accuracy for all users. The average
accuracy is 52% better than using face only case and 42.3%
better than using physiological modality. Standard deviation is
reduced by 75% and 50% compared to face only and
physiological only cases respectively.
TABLE VI
“ONE-SUBJECT-OUT” PREDICTION ACCURACIES (%) FOR LUMED-2
DATASET

User 1

70.2

Physiological
(GSR+EEG)
63.0

User 2

73.3

58.4

79.3

User 3

72.9

70.5

81.4

User

Face

Face +
(GSR+EEG)
74.2

User 4

77.9

61.1

83.6

User 5

34.2

56.2

71.3

User 6

43.1

40.1

73.3

User 7

28.6

45.7

66.0

User 8

38.3

53.3

71.9

User 9

45.5

54.2

75.5

User 10

39.3

36.6

70.0

User 11

6.0

39.0

67.4

User 12

40.5

52.2

73.1

User 13

44.3

49.1

78.2

Average

47.2

52.2

74.2

Std. Dev.

20.8

10.0

5.2

The inter time-delay has been chosen empirically. It is one
of the core parameters that impacts the recognition accuracy.
We employ different time delay values from 0 to 1 sec. and
observe the changing in the accuracies. Figure 7 shows the
accuracy change for the DEAP and LUMED-2 datasets as the
introduced time delay changes between 0 to 1s. The optimal
value for both datasets is around 0.35. So, we set the time delay
as 0.35 sec. Figure 7 shows how the value of time delay affects
the mean recognition accuracy. This proves that the biosignals do not react simultaneously and there is a time lag
between brain response and other bio-signals. This is due to
the time it takes to transmit biochemicals to other parts of the
body, after being initiated inside the brain.

FIGURE 7. Emotion recognition accuracy with respect to the Time
delay between the considered modalities

The proposed method has also been compared to other
cross-subject unimodal and multimodal emotion recognition
studies in the literature. There are not many multimodal
emotion recognition studies that do classification on discrete
emotion states basis. Most of the multimodal emotion
recognition studies implement emotion recognition on the
scales of valence and arousal.
Therefore, to be able to make a comparison, we will regard
the high/low valence and high/low arousal classification
methods as they implement 2 emotion states classification.
Table VII shows the accuracy comparison between the
proposed method and 15 other multimodal and unimodal
emotion recognition studies. The number of output emotion
classes is an important aspect, which determines the mean
10

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

classification accuracy. Note that the left-hand side of the ‘/’
symbol stands for the high-low arousal accuracy and the righthand side of it stands for the high-low valence accuracy. If
there is no ‘/’ symbol, then the comparison is done based on
either valence or arousal with maximum accuracy.
Proposed method provides the accuracy results based on
discrete emotional states. The proposed method performs the
best in terms of 2-states, although it implements a 4-state
classification. For a fair comparison, we have calculated the
mean accuracy of the classification results of users who have
2 and 3 emotion states. We put these results in Table VII too.
Based on that, when the number of states is equal to 2,
proposed study yields better mean accuracy compared to
others on the DEAP dataset.
We could find only one study which reports the accuracy
based on face modality, on DEAP. However, this study
updates the recognition model parameters according to dataset
and, they use some of the users’ face data to train their model.
However, we report the face-based subject independent
accuracy without using any users’ data from DEAP dataset.
Therefore, we foresee that if we use some users’ data to train
our model in terms of face modality, the face-based
recognition accuracy would improve.
Since Lumed-2 is a new multimodal dataset, we have been
unable to reproduce the respective methods in other studies. It
needs to be emphasized that the accuracy figures we report
here are one-subject-out figures (subject-independent results).
This should not be confused with the subject-dependent
accuracy results, which are reported in most studies in the
literature and tend to be much higher than one-subject-out
results.
TABLE VII
BENCHMARK FOR “ONE-SUBJECT-OUT” MEAN PREDICTION ACCURACIES
(%) ON THE DEAP DATASET
Number of emotion
Study
Modality
Accuracy
states
Proposed
Multimodal 53.8
4

Study-3 [109]

Multimodal

74.2/80.3

2

Study-4 [110]

EEG

68.4/78.9

2

VI. CONCLUSIONS

The objective of this paper is to classify emotional states
by using a multimodal approach. We use a hybrid fusion of
face, EEG and GSR modalities. We use feature fusion on EEG
and GSR modalities for estimating the level of arousal. In the
final step we use late fusion of EEG, GSR and face modalities.
Our proposed model has the capability of detecting the actual
emotional state when it is dominant, or it is hidden due to
natural deceptive face actions.
We present a subject independent emotion recognition
system that is suitable for real time operations, since it will not
require feature extraction. Hence, this brings flexibility and
reduces the overall processing load. Although subjectindependent recognition accuracy of EEG modality is
relatively low due to its nonstationary properties, it is effective
if the number of output classes is limited. Therefore, we
limited the number of classes to two (high and low states for
valence and arousal) to make use of its success when detecting
the actual emotional state.
Future studies should focus on subject-independent
emotion recognition for person-independent applicability and
concentrate on reducing the number of modalities for
practicality. Reported user-based emotion recognition
accuracies are higher compared to cross-subject and subjectindependent emotion recognition, but these are less practical.
In addition, since the comfort of use and non-intrusiveness is
important for end users, significant effort should be made
towards reducing the weight and form factor of sensor devices.
ACKNOWLEDGMENT

We would like to thank the creators of DEAP dataset for
openly sharing the dataset with us and the wider research
community. The authors would also like to thank all
volunteering staff and students in Loughborough University
London and Hacettepe University for participating in the
recording sessions to generate the LUMED-2 dataset and
Perihan TEKELI from Hacettepe University for her excellent
coordination activities.

Proposed

Multimodal

79.2

2 or 3

Proposed

Multimodal

82.7

2

Study-1 [70]

Multimodal

43.5 / 77.5

2/2

Study-2 [69]

Multimodal

72.1 / 73.0

2/2

FAWT [99]

EEG

79.9

2

T-RFE [100]

EEG

78.7

2

Inc.ResnetV2[97]

EEG

72.8

2

REFERENCES

ST-SBSSVM [101]

EEG

72.0

2

[1]

VMD-DNN [102]

EEG

62.5

2

[2]

MIDA [103]

EEG

48.9

2

TCA [104]

EEG

47.2

2

SA [105]

EEG

38.7

2

ITL [106]

EEG

40.5

2

GFK [107]

EEG

46.5

2

KPCA [108]

EEG

39.8

2

Study-3 [109]

Face

71.1/72.3

2

Study-3 [109]

EEG

68.8/75.3

2

[3]

[4]

[5]

A. Dąbrowski, "Emotions in Philosophy. A Short Introduction", in
Studia Humana, 2016, vol.5-3, p.8-20.
Picard R.W., "Affective Computing", MIT Media Laboratory, in
Perceptual Computing, 1995.
H. Ranganathan, S. Chakraborty and S. Panchanathan, "Multimodal
emotion recognition using deep learning architectures," in
Proceedings of IEEE Winter Conference on Applications of
Computer Vision (WACV), 2016, p. 1-9.
W. Zheng, W. Liu, Y. Lu, B. Lu and A. Cichocki, "EmotionMeter: A
Multimodal Framework for Recognizing Human Emotions," in IEEE
Transactions on Cybernetics, 2019, vol. 49, no. 3, p. 1110-1122.
K. Bahreini, R. Nadolski and W. Westera, "Data Fusion for Realtime Multimodal Emotion Recognition through Webcams and
Microphones in E-Learning", in International Journal of Human–
Computer Interaction, 2016, vol.32, p.415-430.
11

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

[6]

[7]

[8]

[9]
[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]
[18]

[19]

[20]

[21]

[22]

[23]

[24]

[25]

B. Bucur, I. Şomfelean, A. Ghiuruţan, C. Lemnaru and M.
Dînşoreanu, "An early fusion approach for multimodal emotion
recognition using deep recurrent networks," in IEEE 14th
International Conference on Intelligent Computer Communication
and Processing (ICCP), 2018.
Wang, Z., Ho, S. & Cambria, E. A review of emotion sensing:
categorization models and algorithms. Multimed Tools Appl (2020).
https://doi.org/10.1007/s11042-019-08328-z.
S. Koelstra, C. Mueh, M. Soleymani, J. Lee, A. Yazdani, T.
Ebrahimi, T. Pun, A. Nijholt, I. Patras, "DEAP: A Database for
Emotion Analysis using Physiological Signals", in IEEE
Transactions on Affective Computing, 2011, vol. 3, no. 1, p. 18-31.
J. A. Russell, "A circumplex model of affect", Journal of Personality
and Social Physiology, 1980, vol. 39, no. 6, p. 1161–1178.
P. Ekman, W.V. Friesen, M. O’Sullivan, A.I. Chan, T. Diacoyanni,
K. Heider, R. Krause, W.A. LeCompte, T. Pitcairn, and P. E. RicciBitti, "Universals and cultural differences in the judgments of facial
expressions of emotion." in Journal of Personality and Social
Physiology, 1987, vol. 53, no. 4, p. 712–717.
G. Paltoglou and M. Thelwall, "Seeing Stars of Valence and Arousal
in Blog Posts", IEEE Transactions on Affective Computing, 2013,
vol. 4, no. 1, p. 116-123.
R. Fabien & S. Björn, V. Michel, J. Shashank, M. Erik, L. Denis, C.
Roddy and P. Maja, "AV+ EC 2015--the first affect recognition
challenge bridging across audio, video, and physiological data.", 5th
International Workshop on Audio/Visual Emotion Challenge, 2015.
A. Kapoor, W. Burleson, R.W. Picard, "Automatic prediction of
frustration", in International Journal of Human-Computer Studies,
2007, vol. 65, p. 724-736.
H. Zhang, A. Jolfaei and M. Alazab, "A Face Emotion Recognition
Method Using Convolutional Neural Network and Image Edge
Computing," in IEEE Access, vol. 7, 2019, p. 159081-159089.
J. Deng, S. Frühholz, Z. Zhang and B. Schuller, "Recognizing
Emotions From Whispered Speech Based on Acoustic Feature
Transfer Learning," in IEEE Access, 2017, vol. 5, p. 5235-5246.
C. Qing, R. Qiao, X. Xu and Y. Cheng, "Interpretable Emotion
Recognition Using EEG Signals," in IEEE Access, 2019, vol. 7, p.
94160-94170.
H.A. Osman and T. H. Falk, "Multimodal Affect Recognition:
Current Approaches and Challenges", in IntechOpen, 2016.
T. Hao, L. Wei, Z. Wei-Long, L. Bao-Liang, L. Derong, X. Shengli,
L. Yuanqing, Z. Dongbin, E. El-Sayed, " Multimodal Emotion
Recognition Using Deep Neural Networks ", in International
Conference on Neural Information Processing, 2017.
J. Chen, Y. She, M. Zheng, Y. Shu, Y. Wang, and Y. Xu. A
multimodal affective computing approach for children companion
robots. In Proceedings of the Seventh International Symposium of
Chinese CHI (Chinese CHI ’19). 2019. p. 57–64.
D. Hazer-Rau, S. Meudt, A. Daucher, J. Spohrs, H. Hoffmann, F.
Schwenker, H. Traue, "The uulmMAC Database—A Multimodal
Affective Corpus for Affective Computing in Human-Computer
Interaction", Sensors 2020, 20, 2308.
K. Masui, T. Nagasawa, H. Doi, N. Tsumura; Continuous estimation
of emotional change using multimodal affective responses
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR) Workshops, 2020, pp. 290-291.
B. Singh, N. Kushwaha, O.P. Vyas, An interpretation ofsentiment
analysis for enrichment of Business Intelligence. In 2016 IEEE
Region 10 Conference (TENCON), 2016, p. 18-23.
J. Islam, Y. Zhang, Visual Sentiment Analysis for Social Images
Using Transfer Learning Approach. In 2016 IEEE International
Conferences on Big Data and Cloud Computing (BDCloud), Social
Computing and Networking (SocialCom), Sustainable Computing
and Communications (SustainCom) (BDCloudSocialComSustainCom), 2016, p. 124-130.
P. Yadav, D. Pandya, SentiReview: Sentiment analysis based on text
and emoticons. In 2017 International Conference on Innovative
Mechanisms for Industry Applications (ICIMIA), 2017.
N. Xu, W. Mao, A residual merged neutral network for multimodal
sentiment analysis. In 2017 IEEE 2nd International Conference on
Big Data Analysis (ICBDA). 2017, doi:10.1109/icbda.2017.8078794

[26] SA Qureshi et al. Multitask representation learning for multimodal
estimation of depression level. IEEE Intelligent Systems 34(5), 4552 (2019)
[27] S Poria et al. Multimodal sentiment analysis: Addressing key issues
and setting up the baselines. IEEE Intelligent Systems 33(6), 17-25
(2018)
[28] A Bandhakavi et al. Lexicon generation for emotion analysis from
text. IEEE Intelligent Systems 32(1), 102-108 (2017)
[29] A Zadeh et al. Multimodal sentiment intensity analysis in videos:
Facial gestures and verbal messages. IEEE Intelligent Systems 31(6),
82-88 (2016)
[30] E Cambria. Affective computing and sentiment analysis. IEEE
Intelligent Systems 31(2), 102-107 (2016)
[31] A Esuli et al. Cross-lingual sentiment quantification. IEEE
Intelligent Systems 35(3), 106-114 (2020)
[32] J Schuurmans and F Frasincar. Intent classification for dialogue
utterances. IEEE Intelligent Systems 35(1), 82-88 (2020)
[33] N Majumder et al. Sentiment and sarcasm classification with
multitask learning. IEEE Intelligent Systems 34(3), 38-43 (2019)
[34] QJ Yang et al. Segment-level joint topic-sentiment model for online
review analysis. IEEE Intelligent Systems 34(1), 43-50 (2019)
[35] M Dragoni et al. OntoSenticNet: A commonsense ontology for
sentiment analysis. IEEE Intelligent Systems 33(3), 77-85 (2018)
[36] E Cambria et al. Sentiment analysis is a big suitcase. IEEE
Intelligent Systems 32(6), 74-80 (2017)
[37] M Ebrahimi et al. Challenges of sentiment analysis for dynamic
events. IEEE Intelligent Systems 32(5), 70-75 (2017)
[38] A Weichselbraun et al. Aspect-based extraction and analysis of
affective knowledge from social media streams. IEEE Intelligent
Systems 32(3), 80-88 (2017)
[39] N Majumder et al. Deep learning-based document modeling for
personality detection from text. IEEE Intelligent Systems 32(2), 7479 (2017)
[40] E. Cambria, N. Howard, J. Hsu and A. Hussain, "Sentic blending:
Scalable multimodal fusion for the continuous interpretation of
semantics and sentics," 2013 IEEE Symposium on Computational
Intelligence for Human-like Intelligence (CIHLI), Singapore, 2013,
pp. 108-117, doi: 10.1109/CIHLI.2013.6613272.
[41] Soujanya Poria, Erik Cambria, Newton Howard, Guang-Bin Huang,
Amir Hussain, Fusing audio, visual and textual clues for sentiment
analysis from multimodal content, Neurocomputing, Volume 174,
Part A, 2016, Pages 50-59
[42] B. Xing et al., "Exploiting EEG Signals and Audiovisual Feature
Fusion for Video Emotion Recognition," in IEEE Access, 2019, vol.
7, p. 59844-59861.
[43] J. Han, X. Ji, X. Hu, L. Guo and T. Liu, "Arousal Recognition Using
Audio-Visual Features and FMRI-Based Brain Response," in IEEE
Transactions on Affective Computing, 2015, vol. 6, no. 4, p. 337347.
[44] S. Sahoo and A. Routray, "Emotion recognition from audio-visual
data using rule based decision level fusion," 2016 IEEE Students’
Technology Symposium (TechSym), 2016, pp. 7-12.
[45] K. Song, Y. Nho, J. Seo and D. Kwon, "Decision-Level Fusion
Method for Emotion Recognition using Multimodal Emotion
Recognition Information," 2018 15th International Conference on
Ubiquitous Robots (UR), Honolulu, HI, 2018, p. 472-476.
[46] A. Metallinou, S. Lee and S. Narayanan, "Decision level
combination of multiple modalities for recognition and analysis of
emotional expression," IEEE International Conference on Acoustics,
Speech and Signal Processing, 2010, p. 2462-2465.
[47] M. Mansoorizadeh and N. M. Charkari, "Hybrid feature and decision
level fusion of face and speech information for bimodal emotion
recognition," 2009 14th International CSI Computer Conference,
Tehran, 2009, p. 652-657.
[48] S. Nemati, R. Rohani, M. E. Basiri, M. Abdar, N. Y. Yen and V.
Makarenkov, "A Hybrid Latent Space Data Fusion Method for
Multimodal Emotion Recognition," in IEEE Access, 2019, vol. 7,
pp. 172948-172964.
[49] J. Chen, B. Hu, L. Xu, P. Moore and Y. Su, "Feature-level fusion of
multimodal physiological signals for emotion recognition" IEEE
International Conference on Bioinformatics and Biomedicine
(BIBM), 2015, p. 395-399.
12

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

[50] B. Bucur, I. Şomfelean, A. Ghiuruţan, C. Lemnaru and M.
Dînşoreanu, "An early fusion approach for multimodal emotion
recognition using deep recurrent networks," IEEE 14th International
Conference on Intelligent Computer Communication and Processing
(ICCP), 2018, p. 71-78.
[51] H. Ranganathan, S. Chakraborty and S. Panchanathan, "Multimodal
emotion recognition using deep learning architectures," IEEE Winter
Conference on Applications of Computer Vision (WACV), 2016, p.
1-9.
[52] T. Panagiotis, T. George, N. Mihalis, S.Björn, Z. Stefanos, "End-toEnd Multimodal Emotion Recognition Using Deep Neural
Networks.", IEEE Journal of Selected Topics in Signal Processing,
2017.
[53] SEED Dataset. Available online: http://bcmi.sjtu.edu.cn/~seed/
(accessed on 19/04/2020)
[54] V. Luis, H. Yongrui, Y. Jianhao, L. Pengkai, P. Jiahui, "Fusion of
Facial Expressions and EEG for Multimodal Emotion Recognition",
in Computational Intelligence and Neuroscience, 2017.
[55] B. Reuderink, M. Poel, K. Truong, R. Poppe, M. Pantic, "DecisionLevel Fusion for Audio-Visual Laughter Detection", in Machine
Learning for Multimodal Interaction. MLMI 2008. Lecture Notes in
Computer Science, 2008, vol 5237.
[56] O. Martin, I. Kotsia, B. Macq and I. Pitas, "The eNTERFACE' 05
Audio-Visual Emotion Database," 22nd International Conference on
Data Engineering Workshops (ICDEW'06), Atlanta, GA, USA,
2006, pp. 8-8, doi: 10.1109/ICDEW.2006.145.
[57] C. Y. Suen and L. Lam, “Multiple classifier combination
methodologies for different output levels,” in International workshop
on multiple classifier systems, 2000, pp. 52–66.
[58] F. Lingenfelser, J. Wagner, and E. André, “A systematic discussion
of fusion techniques for multi-modal affect recognition tasks,” in
Proceedings of the 13th international conference on multimodal
interfaces, 2011, pp. 19–26.
[59] Y. S. Huang and C. Y. Suen, “The behavior-knowledge space
method for combination of multiple classifiers,” in IEEE computer
society conference on computer vision and pattern recognition, 1993,
p. 347–347.
[60] R. Gupta, M. Khomami Abadi, J. A. Cárdenes Cabré, F. Morreale,
T. H. Falk, and N. Sebs, “A quality adaptive multimodal affect
recognition system for user-centric multimedia indexing,” in
Proceedings of the 2016 ACM on international conference on
multimedia retrieval, 2016, p. 317–320.
[61] M. E. Tipping, “Sparse Bayesian learning and the relevance vector
machine” in Journal of Machine Learning Research, 2001, vol. 1, p.
211–244.
[62] H. Miao, Y. Zhang, W. Li, H. Zhang, D. Wang and S. Feng,
"Chinese Multimodal Emotion Recognition in Deep and Traditional
Machine Leaming Approaches," 2018 First Asian Conference on
Affective Computing and Intelligent Interaction (ACII Asia),
Beijing, 2018, p. 1-6.
[63] R. Fabien, S. Björn, V. Michel, J. Shashank, M. Erik, L. Denis, C.
Roddy and P. Maja, "AV+ EC 2015--the first affect recognition
challenge bridging across audio, video, and physiological data", 5th
International Workshop on Audio/Visual Emotion Challenge
(AVEC), 2015.
[64] J. Kim, E. André, M. Rehm, T. Vogt, and J. Wagner, “Integrating
information from speech and physiological signals to achieve
emotional sensitivity,” in Proc. INTERSPEECH, 2005, pp. 809–812.
[65] A. Sharma and S. Canavan, "Multimodal Physiological-based
Emotion Recognition", 2019.
[66] Z. Zhang, J. Girard, Y. Wu, X. Zhang, P. Liu, U. Ciftci, S. Canavan,
M. Reale, A. Horowitz, H. Yang, J. Cohn, Q. Ji and L. Yin,
"Multimodal Spontaneous Emotion Corpus for Human Behavior
Analysis", IEEE International Conference on Computer Vision and
Pattern Recognition (CVPR),2016.
[67] F. Syeda, E. Engin, "Cross-Subject Continuous Emotion Recognition
Using Speech and Body Motion in Dyadic Interactions", in
Interspeech 2017, 2017, p. 1731-1735.
[68] A. Metallinou, Z. Yang, C.C. Lee, C. Busso, S. Carnicke, and S.S.
Narayanan, "The USC CreativeIT database of multimodal dyadic
interactions: from speech and full body motion capture to continuous

emotional annotations," Journal of Language Resources and
Evaluation, 2015.
[69] A. Deger, Y. Yusuf and K.Mustafa, "Emotion Recognition from
Multimodal Physiological Signals for Emotion Aware Healthcare
Systems", Journal of Medical and Biological Engineering, 2020.
[70] C.Jing, H. Bin, W. Yue, M. Philip, D. Yongqiang, F. Lei, D. Zhijie,
"Subject-independent emotion recognition based on physiological
signals: A three-stage decision method", in BMC Medical
Informatics and Decision Making, 2017.
[71] Szegedy, Christian, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A. Alemi. "Inception-v4, Inception-ResNet and the
Impact of Residual Connections on Learning." In AAAI, vol. 4, p.
12. 2017.
[72] L. Patrick, C. Jeffrey, K. Takeo, S. Jason, A. Zara, M. Iain, "The
Extended Cohn-Kanade Dataset (CK+): A complete dataset for
action unit and emotion-specified expression", IEEE Computer
Society Conference on Computer Vision and Pattern Recognition Workshops, CVPRW, 2010. p. 94 -101.
[73] O. Langner, R. Dotsch, G. Bijlstra, D.H.J. Wigboldus, S.T. Hawk, A.
van Knippenberg "Presentation and validation of the Radboud Faces
Database", in Cognition & Emotion, vol. 24(8), p.1377-1388
[74] FacesDB. Available online:
http://app.visgraf.impa.br/database/faces/(accessed on 10/04/2020)
[75] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “AffectNet: A New
Database for Facial Expression, Valence, and Arousal Computation
in the Wild”, IEEE Transactions on Affective Computing, 2017.
[76] LUMED-2 Dataset. Available online:
https://figshare.com/articles/dataset/Loughborough_University_Mult
imodal_Emotion_Dataset_-_2/12644033
[77] DEAP Dataset. Available online:
https://www.eecs.qmul.ac.uk/mmv/datasets/deap/ (accessed on
21/04/2020).
[78] Pretrained Deep Neural Networks, Available online:
https://uk.mathworks.com/help/deeplearning/ug/pretrainedconvolutional-neural-networks.html (accessed on 01/03/2020).
[79] K. A. Lindquist, T. D. Wager, H. Kober, E. Bliss-Moreau, L. F.
Barrett, “The brain basis of emotion: a meta-analytic review.” in The
Behavioral and brain sciences, 2012, vol. 35 p.121-43.
[80] M. Benedek, C. Kaernbach, "A continuous measure of phasic
electrodermal activity", Journal of Neuroscience Methods, 2010, vol.
190, p. 80-91.
[81] Imotions. Available online: https://imotions.com/blog/gsr-why-5application-trends-biometric-research/
[82] Top 14 EEG Hardware Companies: Available online:
https://imotions.com/blog/top-14-eeg-hardware-companies-ranked/
(accessed on 05/04/2020)
[83] J. Valer, T. Daisuke, D. Ippeita, "10/20, 10/10, and 10/5 systems
revisited: Their validity as relative head-surface-based positioning
systems", Journal of NeuroImage, 2007, vol. 34, p. 1600-1611.
[84] U.R. Acharya, S. V. Sree, G. Swapna, R. J. Martis, and J.S. Suri,
"Automated EEG analysis of epilepsy: a review", Journal of
Knowledge-Based Systems, 2013, vol. 45, p.147–65.
[85] K.A.I. Aboalayon, M. Faezipour, W.S. Almuhammadi, and S.
Moslehpour, "Sleep stage classification using EEG signal analysis: a
comprehensive survey and new investigation", MDPI Journal of
Entropy, 2016, vol. 18.
[86] Engemann D.A. et al, Robust EEG-based cross-site and crossprotocol classification of states of consciousness, Journal of Brain,
2018, Vol. 141, Pages 3179–92.
[87] M. Arns, C. K. Conners, and H. C. Kraemer, "A decade of EEG
theta/beta ratio research in ADHD: a meta-analysis, Journal of
Attention Disorders, 2013, vol. 17, p. 374–83.
[88] W.K.Y. So, S.W.H. Wong, J.N. Mak, R.H.M. Chan, "An evaluation
of mental workload with frontal EEG", Journal of PLOS ONE, 2017,
vol. 12(4).
[89] N.H. Liu, C.Y. Chiang, H.C. Chu, "Recognizing the degree of
human attention using EEG signals from mobile sensors", Journal of
Sensors (Basel). 2013, vol. 13(8), p. 10273–10286.
[90] Shestyuk A.Y., Kasinathan K., Karapoondinott V., Knight R.T.,
Gurumoorthy R., Individual EEG measures of attention, memory,
and motivation predict population level TV viewership and Twitter
engagement. Journal of PLOS One, 2019, vol. 14(3).
13

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3023871, IEEE Access

[91] M. Mohammadpour, and S. Mozaffari, "Classification of EEG-based
attention for brain computer interface", 3rd Iranian Conference on
Intelligent Systems and Signal Processing (ICSPIS), 2017, p. 34-37.
[92] S. Thejaswini, K.M. Ravikumar, L. Jhenkar, N. Aditya, K.K. Abhay,
"Analysis of EEG Based Emotion Detection of DEAP and SEED-IV
Databases using SVM", International Journal of Recent Technology
and Engineering (IJRTE), 2019, vol. 8, p. 207-211.
[93] J. Liu, H. Meng, A. Nandi, and M. Li, "Emotion detection from EEG
recordings", 12th International Conference on Natural Computation,
Fuzzy Systems and Knowledge Discovery (ICNC-FSKD), 2016, p.
722-1727.
[94] A. Gómez, L. Quintero, N. López, J. Castro, "An approach to
emotion recognition in single-channel EEG signals: a mother child
interaction", Journal of Physics: Conference Series, 2016, vol. 705.
[95] Zhang W., Wang F., Jiang Y., Xu Z., Wu S., Zhang Y. (2019),
"Cross-Subject EEG-Based Emotion Recognition with Deep Domain
Confusion", In: Intelligent Robotics and Applications. ICIRA 2019.
Lecture Notes in Computer Science, vol 11740. Springer, Cham.
[96] Yin Z, Wang Y, Liu L, Zhang W, Zhang J. Cross-Subject EEG
Feature Selection for Emotion Recognition Using Transfer
Recursive Feature Elimination. Front Neurorobot. 2017;11:19.
Published 2017 Apr 10. doi:10.3389/fnbot.2017.00019.
[97] Y. Cimtay, E. Ekmekcioglu, "Investigating the Use of Pretrained
Convolutional Neural Network on Cross-Subject and Cross-Dataset
EEG Emotion Recognition", Sensors, 2020, vol.20.
[98] Empatica E4. Available online: https://www.empatica.com/enint/research/e4/ (accessed on 09/04/2020).
[99] V. Gupta, M.D. Chopda, R. B. Pachori, "Cross-Subject Emotion
Recognition Using Flexible Analytic Wavelet Transform From EEG
Signals", IEEE Sens. J. 2018, 19, 2266–2274.
[100] Z. Yin, Y. Wang, L. Liu, W. Zhang, J. Zhang, "Cross-Subject EEG
Feature Selection for Emotion Recognition Using Transfer
Recursive Feature Elimination", Front. Neurorobot. 2017, 11, 200.
[101] F. Yang, X. Zhao, W. Jiang, P. Gao, G. Liu, "Multi-method Fusion
of Cross-Subject Emotion Recognition Based on High-Dimensional
EEG Features", Front. Comput. Neurosci. 2019, 13, 53.
[102] P. Pandey, K. Seeja, "Subject independent emotion recognition from
EEG using VMD and deep learning", J. King Saud Univ. Comput.
Inf. Sci. 2019, 53–58.
[103] K. Yan, L. Kou, D. Zhang, "Learning Domain-Invariant Subspace
Using Domain Features and Independence Maximization", IEEE
Trans. Cybern. 2018, 48, 288–299.
[104] S.J. Pan, I. W. Tsang, J. T. Kwok, Q. Yang, "Domain Adaptation via
Transfer Component Analysis", IEEE Trans. Neural Netw. 2010, 22,
199–210.
[105] B. Fernando, A. Habrard, M. Sebban, T. Tuytelaars, "Unsupervised
Visual Domain Adaptation Using Subspace Alignment", Proc. of the
2013 IEEE International Conference on Computer Vision, Sydney,
Australia, 1–8 December 2013; pp. 2960–2967
[106] Y. Shi, F. Sha, "Information-Theoretical Learning of Discriminative
Clusters for Unsupervised Domain Adaptation", Proc. of the 2012
International Conference on Machine Learning (ICML), Edinburgh,
Scotland, 26 June–1 July 2012; pp. 1275–1282.
[107] B. Gong, Y. Shi, F. Sha, K. Grauman, "Geodesic flow kernel for
unsupervised domain adaptation", Proc. of the 2012 IEEE
Conference on Computer Vision and Pattern Recognition,
Providence, RI, USA, 16–21 June 2012; pp. 2066–2073.
[108] B. Schölkopf, A. Smola, K. -R. Müller, "Nonlinear Component
Analysis as a Kernel Eigenvalue Problem", Neural Comput. 1998,
10, 1299–1319
[109] Y. Huang, J. Yang, S. Liu, J. Pan, "Combining Facial Expressions
and Electroencephalography to Enhance Emotion
Recognition", Future Internet 2019, 11, 105.
[110] V. Rozgi´c, S. N. Vitaladevuni, R. Prasad, "Robust EEG emotion
classification using segment level decision fusion", Proc of the 2013
IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), Vancouver, BC, Canada, 2013; pp. 1286–
1290

YUCEL CIMTAY received the Ph.D. degree
from Ankara University, Electrical and
Electronics Engineering, Turkey. He is
currently a postdoctoral research associate in
the Institute for Digital Technologies,
Loughborough University London. His
research interests include signal and image
processing, data science and machine
learning.

ERHAN EKMEKCIOGLU received his Ph.D.
degree from the University of Surrey, U.K., in
2010, where he was a Post-Doctoral Researcher
until 2014. Since 2014, he has been with the
Institute for Digital Technologies at
Loughborough University London, U.K, where
he is currently a Senior Lecturer and the Chief
Director for post-graduate taught programmes.
His current research interests include affective
computing, immersive media, multimedia
processing and applied machine learning. He is a Guest Editor for IEEE
Multimedia Communications Technical Committee publications, and a
regular reviewer of IEEE Transactions, including Circuits and Systems for
Video Technology, Multimedia, and Image Processing.
SEYMA CAGLAR-OZHAN received her
master’s degree from Hacettepe University in
2017 and continues her doctorate education at
Computer Education and Instructional
Technology department of Hacettepe
University, Turkey. Her research interests
include cognitive and emotional processes in
e-learning environments, human-computer
interaction and cognitive science.

14

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

