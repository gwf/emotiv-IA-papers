Downloaded from orbit.dtu.dk on: Mar 23, 2020

Mind Controlled Drone: An Innovative Multiclass SSVEP based Brain Computer
Interface

Chiuzbaian, Andrei; Jakobsen, Jakob; Puthusserypady, Sadasivan
Published in:
Proceedings of 2019 7th International Winter Conference on Brain-Computer Interface
Link to article, DOI:
10.1109/IWW-BCI.2019.8737327
Publication date:
2019
Document Version
Peer reviewed version
Link back to DTU Orbit

Citation (APA):
Chiuzbaian, A., Jakobsen, J., & Puthusserypady, S. (2019). Mind Controlled Drone: An Innovative Multiclass
SSVEP based Brain Computer Interface. In Proceedings of 2019 7th International Winter Conference on BrainComputer Interface IEEE. https://doi.org/10.1109/IWW-BCI.2019.8737327

General rights
Copyright and moral rights for the publications made accessible in the public portal are retained by the authors and/or other copyright
owners and it is a condition of accessing publications that users recognise and abide by the legal requirements associated with these rights.
 Users may download and print one copy of any publication from the public portal for the purpose of private study or research.
 You may not further distribute the material or use it for any profit-making activity or commercial gain
 You may freely distribute the URL identifying the publication in the public portal
If you believe that this document breaches copyright please contact us providing details, and we will remove access to the work immediately
and investigate your claim.

Mind Controlled Drone: An Innovative Multiclass
SSVEP based Brain Computer Interface
Andrei Chiuzbaian

Jakob Jakobsen

Sadasivan Puthusserypady

Dept. of Electrical Engineering
Technical University of Denmark
Lyngby, Denmark
andrei.chiuzbaian@gmail.com

Dept. of Space Research & Space Technology
Technical University of Denmark
Lyngby, Denmark
jj@space.dtu.dk

Dept. of Electrical Engineering
Technical University of Denmark
Lyngby, Denmark
spu@elektro.dtu.dk

Abstract—A crucial element lost in the context of a neurodegenerative disease is the possibility to freely explore and interact
with the world around us. The work presented in this paper is
focused on developing a brain-controlled Assistive Device (AD)
to aid individuals in exploring the world around them with the
help of a computer and their thoughts. By using the potential
of a noninvasive Steady-State Visual Evoked Potential (SSVEP)based Brain Computer Interface (BCI) system, the users can
control a flying robot (also known as UAV or drone) in 3D
physical space. From a video stream received from a video
camera mounted on the drone, users can experience a degree of
freedom while controlling the drone in 3D. The system proposed
in this study uses a consumer-oriented headset, known as Emotiv
Epoch in order to record the electroencephalogram (EEG) data.
The system was tested on ten able-bodied subjects where four
distinctive SSVEPs (5.3 Hz, 7 Hz, 9.4 Hz and 13.5 Hz) were
detected and used as control signals for actuating the drone. A
highly customizable visual interface was developed in order to
elicit each SSVEP. The data recorded was filtered with an 8th
order Butterworth bandpass filter and a fast Fourier transform
(FFT) spectral analysis of the signal was applied in other to detect
and classify each SSVEP. The proposed BCI system resulted in
an average Information Transfer Rate (ITR) of 10 bits/min and
a Positive Predictive Value (PPV) of 92.5%. The final conducted
tests have demonstrated that the system proposed in this paper
can easily control a drone in 3D space.
Index Terms—Brain Computer Interface (BCI), Electroencephalogram (EEG), Steady State Visual Evoked Potential
(SSVEP), Drone Control, Assistive Device (AD), Information
Transfer Rate (ITR).

I. I NTRODUCTION
“Disability should never be a barrier in one‘s
success.” - Stephen Hawking.
Worldwide, one person out of five suffers from some form
of disability according to the 2011 WHOs world report on
disability [1]. This global estimation is on the rise due to
aging population and a rapid spread of chronic diseases. Many
of these people suffer from neuromuscular disorders such
as amyotrophic lateral sclerosis (ALS), spinal cord injury,
brainstem stroke and many other disorders responsible for
causing the loss of voluntary muscle control. Such people are
often locked in a wheelchair or on a bed unable to move their
limbs or go anywhere they would like to go by themselves.
They have to face great barriers in the modern society due
to their disabilities and deprivation of common activities like

interacting or playing games with other people, activities that
are crucial for personal development and have a significant
impact on the quality of life. Those with a lack of motor
skills would benefit enormously from devices that can augment
their mobility. Over the last few years, the state of the art
technology known as the Brain-Computer Interface (BCI) has
become more and more accessible to the wider public and it
is our moral responsibility to use such technologies in order
to lift these barriers and give disabled people a chance to
regain a normal life. There has been a lot of work in this
direction during the past few years where researchers have
tried innovative solutions for developing a user-friendly and
easy to use assistive systems for controlling a drone. In 2012,
Yipeng et al designed a BCI system that was using motor
imagery (MI) signals acquired from thinking left, thinking
right and thinking push combined with the artifact signals
from eye blinking and tooth clenching in order to control an
AR drone [2]. A different setup was suggested by Byung et al,
where a hybrid interface was used [3]. In their study, the drone
was controlled by using a low-cost electroencephalographic
(EEG) headset together with an eye-tracking device. Although
the BCI systems presented by previous authors come as
affordable solutions for those who want to control a drone
with their minds, the same studies have confirmed that BCI
systems based on motor imagery commands are susceptible to
artifacts like inappropriate eye blinking or muscle activity.
The novelty of the work presented in this study is in
achieving a user-friendly, fully independent multi-class BCI
system based on the Steady State Visual Evoked Potential
(SSVEP) paradigm that allow users to control a drone in 3D
physical space only by using their EEG signals. In addition,
the system we propose is ready-to-go which means users do
not require any previous training or experience in order to
actuate the drone.
II. S YSTEM A RCHITECTURE AND I MPLEMENTATION
The BCI system architecture can be seen in Fig.1. The
main components of the system are: a custom-made visual
interface, EEG headset, a laptop used for signal acquisition
and processing, and a wireless drone.

B. EEG Headset
For the BCI system proposed in this paper, a consumeroriented EEG headset (shown in Fig.3) was used as it is
non-invasive, affordable and most important, portable. The
solution was to use the hardware unit from an Emotiv EPOC
neuroheadset connected to the EEG-recording cap made by
Easycap.

Fig. 1. The proposed BCI system architecture.

A. Visual Interface
The BCI system developed in this paper is based on a
SSVEP paradigm. In order to stimulate the SSVEPs, a visual
interface (Fig.2) was designed. The interface was developed in
C# and implemented in Visual Studio 2017. The application
runs on a 24-inch computer screen with a refresh rate of 120
Hz. It consists of four arrow-like stimuli placed equidistant at
the top, bottom, left and right sides of the screen. Each stimulus represents a movement command that actuates the drone
in a specific direction. A high contrast color (black/white)
scheme was chosen for our application and plain textures for
the visual stimuli in order to increase the chances of evoking
SSVEPs [4] [5] [6]. Furthermore, the luminance contrast level
was increased by adjusting the contrast and brightness levels
on the digital display [7].

Fig. 3. The EEG headset with the two main electrodes used for recording
the encephalic activity.

The cap comes equipped with 14 recording electrodes and
two reference electrodes (CMS and DRL). All 16 electrodes
are AgCl electrodes able to provide superior EEG recording
quality over the gold-plated electrodes in the Emotiv EPOC’s
original design. The EEG headset is used to extract the
encephalic activity from the visual cortex in the posterior
pole of the occipital lobe. The raw EEG signals recorded
from channels O1 and O2 are sampled by the Emotiv EPOC’s
hardware with a sampling rate of 128 Hz and a 50 Hz digital
notch filter is applied over the data in order to remove any EM
interference emitted by the power lines. The data collected by
the EEG headset is sent via Bluetooth to a laptop with a USB
dongle.
C. Signal Processing

Fig. 2. The visual interface used for evoking SSVEPs.

The stimulation frequencies (f U , f D , f L , and f R ) for evoking each SSVEP together with the corresponding movement
command for the drone can be seen in Table I. A combination
of turn right and then go forward commands was used in order
to compensate for the lack of left-right strafe movements.
TABLE I
T HE DRONE CONTROL COMMANDS AND THEIR CORRESPONDING
STIMULATION FREQUENCIES .
Arrow
Up
Down
Left
Right

Stimulus frequency [Hz]
fU
fD
fL
fR

Command
Take-off
Land
Forward
Right turn

In order to process the EEG signals, several software tools
were used. For signal acquisition and for decryption of the raw
EEG data, a script was developed and implemented in Python.
The signal processing, feature extraction, and classification
algorithms were performed in Matlab. In order to send the
EEG data from Python to Matlab, a software tool known as
Lab Streaming Layer (LSL) was used. The analysis of the
EEG data is performed in real time over a succession of time
lapses. The time lapses were divided into two-second intervals
and processed individually. The data was filtered afterwards
by using an 8th order Butterworth bandpass (2-15Hz) filter
and normalized by using zero-mean and unit variance techniques. A fast Fourier transform (FFT) algorithm was used
for the feature detection of the SSVEPs. For classifying each
SSVEP, a threshold technique was used and specific threshold
levels were applied on both frequency and magnitude axis.
If the peak value of one of the SSVEPs is evoked within its

specific detection area during the execution of the program,
the program will generate an output command. The generated
output command will be translated into one of the four control
commands that will be used for flying the drone. The FFT
spectral analysis performed on the EEG data is illustrated in
Fig.4.

Fig. 4. The frequency and magnitude levels of each SSVEP and their
detection.

In this FFT spectral analysis, each of the four SSVEPs is
marked with a different color and their maximum frequency
and magnitude values are displayed in a text box next to their
maxima point. The frequency threshold levels for the minimum
(Fmin. ) and maximum (Fmax. ) values for each SSVEP is marked
with a vertical dashed line and the magnitude threshold level
is marked with a horizontal dashed line. The Fmin. and Fmax.
threshold values for each SSVEP can be seen in Table II.
From the tests performed during the project, we noticed that
the background noise in the EEG signals was peaking at about
2000 µV2 on the magnitude axis and therefore we set the
max. threshold value to 2200 µV2 to avoid any false SSVEP
detection.

Biomedical Engineering Department. The provided room was
partly phonically isolated. The experiments were performed
during daytime in a fully lighted room. In total, two studies were carried out. The first study consisted of a set of
experiments performed on three volunteers in order to test
different SSVEP responses. In the second study, a total of
ten volunteers participated in the tests in order to evaluate
the performance of the real-time BCI system. All participants
in the experiments were healthy individuals with normal or
corrected to normal vision (wearing contact lenses), and none
of them suffered from epilepsy. All subjects were provided
verbal informed consent prior to their participation in the
experiments, which were approved by the Regional Committee
on Health Research Ethics for the Capital Region of Denmark
(reference H-3-2013-004) and carried out in accordance with
the corresponding guidelines and relevant regulations on the
use of human subjects for health-related scientific research.
Each individual was seated comfortably in an armchair
with their forearms placed flat at 90◦ on the armrests and
their feet resting flat on the ground. During the recordings,
the computer screen running the visual interface was placed
at approximately 1m above the ground and 0.6m from the
subject’s eyes. The EEG cap was placed on the subject’s head
and secured with a velcro strap across the chin. A highly
conductive electrolyte gel had to be applied in between the
subject skin and the electrodes in order to increase the signal
conductivity. As the experiments had to be conducted indoor,
due to safety reasons, the drone had to be replaced with a
feedback circuit. The feedback circuit consisted of four LEDs
controlled by an Arduino board. Each LED was turned on
depending on the SSVEP detected by the BCI system. For
example, when the subject was gazing at the left arrow on
the screen the corresponding 13.5 Hz SSVEP was elicited and
converted into a control signal that turned on the left LED.
The experimental setup and the LED circuit are illustrated in
Fig.5.

TABLE II
T HE FREQUENCY AND MAGNITUDE THRESHOLD VALUES SET FOR EACH
SSVEP.
SSVEP [Hz]
fR
fU
fD
fL

Fmin. [Hz]
5.0
6.6
8.0
10.9

Fmax. [Hz]
5.3
6.8
8.5
11.3

Magnitude [µV2 ]
2200
2200
2200
2200

III. E XPERIMENTAL PROTOCOL
All the experiments performed during this project were
conducted at the Technical University of Denmark in the

Fig. 5. The experimental setup (left) and the LED circuit (right).

IV. R ESULTS
In order to assess the performance of the BCI system
proposed in this paper, a scenario was created where each
subject had to control the drone on a predefined path. The
command sequence used during the test trials can be seen in
Table III.

The subjects were instructed to gaze at one stimulus at a
time. Once the detection of the SSVEP occurred, the subject
had to change their gaze to the next stimulus in the sequence.
According to the feedback received from the BCI system the
commands were classified as Correct (C), Incorrect (I) or Not
Detected (N D). For evaluating the performance of the BCI
system, the Information Transfer Rate (IT R) was used as a
measure. The IT R was calculated (Eq.(1)) as suggested in
[8].



pe
, (1)
IT R = p0µ log2 N + p0e log2 p0e + pe log2
N −1
where N is the number of stimuli (in this case N=4), p0µ = 1−
pµ , and p0e = 1 − pe . pµ and pe here represents the probability
of undefined cases and the probability of incorrect detected
cases, respectively and are defined as:
pµ

=

pe

=

ND
,
C + I + ND
I
.
C + I + ND

(2)
(3)

The ITR is expressed in bits/command. In order to obtain the result in bits/min, the value had to be multiplied
with the detection speed of the BCI system which was 20
commands/min. The ITR achieved by each subject during the
evaluation process of the BCI system can be seen in Fig.6.

represents the proportion of SSVEPs that are correctly detected
and it can be obtained by applying the following formula:
TP
,
(4)
TP + FP
where TP stands for all the true positive commands and
FP for all the false positive commands send by the BCI
system to the drone when an SSVEP was detected. A TP
command represents a successfull detection of an SSVEP by
the BCI system while the FP command is an erroneous signal
interpreted by the BCI system as an SSVEP.
In this study, the BCI system achieved a mean PPV of
92.5±8.29% across ten subjects. The results obtained form
the subjects who participated in the evaluation stage of the
BCI system is illustrated in Table IV. The table presents the
elapsed time, the number of output commands sent by the BCI
system to the drone, and the ITR and PPV achieved by each
subject. As can be seen from the table, 5 out of 10 subjects
achieved a PPV of 100%.
PPV =

TABLE IV
T HE RESULTS OBTAINED BY THE SUBJECTS DURING THE EVALUATION
PROCESS OF THE BCI SYSTEM .

Subject
1
2
3
4
5
6
7
8
9
10
Mean
Std. dev.

Time
(s)
62
60
124
118
100
78
156
140
84
92
101
30.76

C
8
8
7
7
8
7
8
6
8
7
7.4
0.66

Commands
I
ND
0
22
0
22
1
54
1
51
0
42
1
31
0
70
2
62
0
34
1
38
0.6
42.6
0.66
15.51

ITR
(bits/min.)
16.20
16.20
7.23
7.76
9.60
11.24
6.00
5.82
11.40
9.24
10.069
3.57

PPV
(%)
100.0
100.0
87.5
87.5
100.0
87.5
100.0
75.0
100.0
87.5
92.5
8.29

V. D ISCUSSION
Fig. 6. The ITR achieved by each subject and the mean value achieved by
the BCI system.

Another metric used for evaluating the performance of
the BCI system is the Positive Predictive Value (PPV). It

TABLE III
T HE STIMULI SEQUENCE USED DURING THE EVALUATION PROCESS OF
THE BCI SYSTEM .
Sequence
1
2
3
4
5
6
7
8

Command
Take off
Forward
Turn right
Decrease altitude
Forward
Increase altitude
Turn right
Land

Stimulus
UP
LEFT
RIGHT
DOWN
LEFT
UP
UP
DOWN

The results from the study presented in this paper have
demonstrated that people are able to control a drone in 3D
space with the state of the art technology, known as the BCI
[9] [10] [11] [12]. By presenting visual stimuli in front of
user’s retina, evoked brain signals (SSVEP) can be recorded
from the occipital region of the brain using EEG techniques
[13] [14] [15] [16]. Through signal processing, these signals
can be translated into control commands for actuating different
electronic devices such as wheelchairs [17] [18] [19] [20],
robotic arms [21] [22] or, like in our study, a drone [23]. The
BCI system presented in this paper is based on a four-class
SSVEP model. In order to improve the control scheme of the
drone in 3D, the system can be extended to a six-class SSVEP
model in which six distinct SSVEP will be used to actuate the
drone. The BCI system presented in this work has achieved a
moderate ITR of 10 bits/min. which means the system will be
able to output in average 10 control commands every minute,
making the whole system relatively slow in comparison with

controlling the drone from a remote controller. For future
improvements, the authors recommend the use of a premium
EEG device like a clinical or research-oriented neuroheadset.
It is worth mentioning that the experiments done during this
project have been conducted in a fully-lighted room during
daytime which may have affected the quality of the SSVEP
detection. The authors would recommend that the future work
done on SSVEP-based BCIs to be performed in dark rooms
which can result in a better SSVEP detection.
VI. C ONCLUSION
The goal of this work was to develop a user-friendly, fullyindependent BCI system that will allow impaired people to
control a drone in 3D space by using only their brain. The
application intends to enhance the quality of life allowing
users to interact with the environment around them. The
system is based on a 4-class SSVEP model able to move the
drone up/down, left/right, forward/backward. The BCI system
was tested on ten able-bodied control subjects and by using
standard metrics it was able to achieve an average accuracy of
92.5% and an average ITR of 10 bits/min. A video with the
BCI system controlling the drone in real-time can be found
on the following link [23].
ACKNOWLEDGEMENTS
The authors would like to thank the support from DTU
Space for allowing us to access to the drone which was
developed in their laboratory. The EEG equipment used in
this study was provided by the Department of Computer Engineering and Department of Electrical Engineering at DTU. We
acknowledge these departments for their support. The authors
would like to thank all the volunteers who participated in the
experiments and give special thanks to all the colleagues in the
Bio-medical Engineering group (DTU) who contributed with
insightful ideas and feedback.
R EFERENCES
[1] J. R.Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M.
Vaughan, “Brain-computer interfaces for communication and control”,
Clinical Neurophysiology, Vol. 113, No.6, pp. 767–791, June 2002.
[2] Y. Yu, D. He, W. Hua, S. Li, Y. Qi, Y. Wang, and G. Pan, “Flyingbuddy2:
A brain-controlled assistant for the handicapped”, In Proceedings of the
2012 ACM Conference on Ubiquitous Computing, Pittsburgh, PA, USA,
pp. 669–670, 05–08 September, 2012.
[3] J.-S. Lin, and Z. Y. Jiang, “Implementing remote presence using quadcopter control by a non-invasive BCI device”, Computer Science and
Information Technology, ISSN 1820-0214, Vol. 3, No.4, pp. 122–126,
May 2015.
[4] J. Bieger, G. G.-Molina, and D. Zhu, “Effects of stimulation properties in
steady-state visual evoked potential based brain-computer interfaces”, In
Proceedings of the 34th IEEE International Conference of IEEE EMBS,
Buenos Aires, Argentina, pp. 3345–3348, August 2010.
[5] T. Cao, F. Wan, P. U. Mak, P.-I. Mak, M. I. Vai, and Y. Hu, “Flashing
color on the performance of ssvep-based brain-computer interfaces”, In
Proceeding of Annual International Conference of the IEEE EMBS, San
Diego, CA, USA, pp. 1819–1822, 28 August–1 September 2012.
[6] B. Z. Allison, I. Sugiarto, B. Graimann, and A. G. P. Graser, “Display optimization in SSVEP BCI”, IEEE International Conference on
Computer Engineering and Technology, at Singapore, Singapore, 22–24
January 2009.

[7] A. Duszyk, M. Bierzynska, Z. Radzikowska, P. Milanowski, R. Kus,
P. Suffczynski, M. Michalska, M. Labecki, P. Zwolinski, and P. Durka,
“Towards an optimization of stimulus parameters for brain-computer
interfaces based on steady state visual evoked potentials”, Plos One,
Vol. 9, No.11, November 2014.
[8] P. F. Diez, S. M. T. Muller, V. A. Mut, E. Laciar, E. Avila, T. F. B.Filho, and M. S.-Filho, “Commanding a robotic wheelchair with a highfrequency steady-state visual evoked potential based brain-computer
interface”, Medical Engineering Physics, Vol. 35, No.8, pp. 1155–1164,
August 2013.
[9] K. LaFleur, K. Cassady, A. Doud, K. Shades, E. Rogin, and B. He,
“Quadcopter control in three-dimensional space using a noninvasive
motor imagery-based brain- computer interface”, Journal of Neural
Engineering, Vol. 10, No.4, June 2013.
[10] B.H. Kim, M. Kim, and S. Jo, “Quadcopter flight control using lowcost hybrid interface with EEG-based classification and eye tracking”, Computers in Biology and Medicine, Vol. 51, pp. 82–92, doi:
10.1016/j.compbiomed.2014.04.020, August 2014.
[11] M. Al-Ayyoub, A.-H. A. A.-Rahman, Y. Qadoumi, M. B. Hani, M.
Masadeh, R. A. A.-Asal, and M. B. Aboud, “Mind-Controlling Green
Unmaned Vehicles through the Cloud: Building a Prototype”, Journal of
Advances in Informational Technology, ISSN: 1798-2340, Vol. 6, No.1,
February 2015.
[12] N. Kosmyna, F. T.-Bernard and B. Rivet, “Towards Brain Computer
Interfaces for Recreational Activities: Piloting a Drone”, In Proceedings
of the 15th International Federation for Information Processing, Part I,
ISSN 0302-9743, pp. 506–522, Bamberg, Germany, September 14–18,
2015.
[13] G. R. M.-Putz, R. Scherer, C. Brauneis, and G. Pfurtscheller, “Steadystate visual evoked potential (SSVEP)-based communication: impact of
harmonic frequency components”, Journal of Neural Engineering, vol.
2, No.4, pp. 123–130, December 2005.
[14] K. B. Ng, A. P. Bradley, R. Cunnington, “Stimulus specificity of
a steady-state visual-evoked potential-based brain-computer interface”,
Journal of Neural Engineering, Vol. 9(3), May 2012.
[15] Z. Wu, Y. Lai, Y. Xia, D. Wu, and D. Yao, “Stimulator selection in
SSVEP-based BCI”, Medical Engineering & Physics, Vol 30, No.8, pp.
1079-1088, October 2008.
[16] B. Allison, T. Luth, D. Valbuena, A. Teymourian, I. Volosyak, and A.
Graser, “BCI Demographics: How Many (and What Kinds of) People
Can Use an SSVEP BCI?”, IEEE Transactions on Neural Systems and
Rehabilitation Engineering, vol. 18, No.2, April 2010.
[17] C. Mandel, T. Luth, T. Laue, T. Rofer, A. Graser, and B. K-Bruckner,
“Navigating a Smart Wheelchair with a Brain-Computer Interface Interpreting Steady-State Visual Evoked Potentials”, In Proceedings of the
International Conference on Intelligent Robots and Systems, St. Louis,
MO, USA, 10–15 October 2009.
[18] J. Philips, J. R. Millan, G. Vanacker, E. Lew, F. Galan, P. W. Ferrez, H.
V. Brussel, and M. Nuttin, “Adaptive Shared Control of a Brain-Actuated
Simulated Wheelchair”, In Proceedings of the IEEE 10th International
Conference on Rehabilitation Robotics, at Noordwijk, Netherlands, 13–
15 June 2007.
[19] S. M. T. Muller, T. F. Bastos, and M. S. Filho, “Proposal of a
SSVEP-BCI to Command a Robotic Wheelchair”, Journal of Control,
Automation and Electrical Systems, Vol. 24, No.1-2, pp. 97–105, April
2013.
[20] M. Palankar, K. J. D. Laurentis, R. Alqasemi, E. Veras, R. Dubey,
Y. Arbel, and E. Donchin, “Control of a 9-DoF Wheelchair-Mounted
Robotic Arm System Using a P300 Brain Computer Interface: Initial
Experiments”, In Proceedings of the IEEE International Conference on
Robotics and Biomimetics, at Bangkok, Thailand, 22–25 February 2009.
[21] G. R. M.-Putz, and G. Pfurtscheller, “Control of an Electrical Prosthesis
With an SSVEP-Based BCI”, IEEE Transactions on Biomedical Engineering Vol. 55, No.1, pp. 361-364, 1–4 February 2008.
[22] E. Hortal, D. Planelles, A. Costa, E. Ianez, A. Ubeda, J.M Azorin, and
E. Fernandez, “SVM-based Brain-Machine Interface for controlling a
robot arm through four mental tasks”, Neurocomputing, Vol. 151, part
1, pp. 116–121, January 2014.
[23] Website: https://www.youtube.com/watch?v=zvjwaTDsEVQ

