Proc. EECSI 2017, Yogyakarta, Indonesia, 19-21 September 2017

EEG Based Emotion Monitoring Using Wavelet and
Learning Vector Quantization
Esmeralda C. Djamal and Poppi Lodaya
Department of Informatics
Universitas Jenderal Achmad Yani
Cimahi – Indonesia
e-mail: esmeralda.contessa@lecture.unjani.ac.id
Abstract— Emotional identification is necessary for example in
Brain Computer Interface (BCI) application and when emotional
therapy and medical rehabilitation take place. Some emotional
states can be characterized in the frequency of EEG signal, such
excited, relax and sad. The signal extracted in certain frequency
useful to distinguish the three emotional state. The classification of
the EEG signal in real time depends on extraction methods to
increase class distinction, and identification methods with fast
computing. This paper proposed human emotion monitoring in
real time using Wavelet and Learning Vector Quantization (LVQ).
The process was done before the machine learning using training
data from the 10 subjects, 10 trial, 3 classes and 16 segments (equal
to 480 sets of data). Each data set processed in 10 seconds and
extracted into Alpha, Beta, and Theta waves using Wavelet. Then
they become input for the identification system using LVQ three
emotional state that is excited, relax, and sad. The results showed
that by using wavelet we can improve the accuracy of 72% to 87%
and number of training data variation increased the accuracy. The
system was integrated with wireless EEG to monitor emotion state
in real time with change each 10 seconds. It takes 0.44 second, was
not significant toward 10 seconds.

EEG signals are the response of sound stimulation [19], after
watching movies [20], watching ads [21], listening to music
[11], playing video game [12] , and watching videos [22], [23].
Moreover, wireless EEG provide comfort so emotional
identification of brain signals can be an intermediate device in
the development of Brain Computer Interface (BCI) [24] [25].

Keywords—emotion monitoring; EEG signal; Wavelet; LVQ;
Brain Computer Interface

This research focuses on identifying emotion state. Prior
research on EEG based emotion identification using statistical
[38], Support Vector Machine (SVM) [39], Multilayer
Perceptron [40] and Learning Vector Quantization (LVQ) [23].
Although there are varieties of emotional states to describe the
human’s feelings, until now only limited types of emotions can
be recognized using EEG. They are time constrains, accuracy,
number of electrodes, number of the recognized emotions, and
benchmark EEG affective databases [37].

EEG signal transform becomes a model and by analyzing it
provides an effective way to classify the EEG signal. In general,
EEG signal consists of wave components, differentiated by their
frequency regions. They are alpha waves (8-13 Hz), very often
appears when people are in conscious and relaxed conditions;
beta wave (14-30 Hz), often occurs when people are in thinking;
theta wave (4-7 Hz), usually happens when people take a nap,
feel sleepy, or suffer emotional stress; and delta wave (0.5-3 Hz),
very often appears when people are in deep sleep. As a
consequence, a lot of researches concerning EEG signal analysis
represent the signal into frequency domain. It can be used like
Power Spectral Density [26] [27] [24], Wavelet [18], [28]–[31]
[2], [8], [32] [33]. Besides, EEG model used Autoregressive [5],
[34]–[36], and Fractal Dimension [37] [10].

I. INTRODUCTION
Emotion is a physiological process triggered by conscious
and/or unconscious perception of an object or situation and is
often associated with mood, temperament, personality,
disposition, and motivation [1]. Emotion plays an important role
in human communication that can be expressed through gesture,
facial expression, text or speech. Emotion is a feeling and a
particular thought in human, such as pleasure, anger, sadness,
passion, and disappointment. Emotion can be divided into
positive and negative emotion. Emotional control is very
important, but some people have problems for it so that therapy
is needed. When emotional therapy is performed, it is necessary
to identify and monitor the impact of therapy on emotional
change to be positive. One instrument that can identify the
emotion state in real time is the Electroencephalogram (EEG).

This research developed emotional response identification
system in real time every 10 seconds. It used Wavelet
transformation and LVQ. One of the emotional identification
information based on the emotional identification can be
informed the presence of Alpha, Beta, Theta, and Delta waves.
Some research introduced Gamma wave can be represented Beta
wave. Therefore using Wavelet for the signal extraction in the
frequency area is very useful to improve the accuracy [9], [19],
[29]. While the ability LVQ in machine learning to identify
signals having fast computation [2], and quite accurately, so it is
suitable for use as an identification system in real time.

EEG signals involve a great deal of information about the
function of the brain, which may reflect a state of mind, such as
level of attention [2], relax condition [3], mental activity [4],
human grasping [5] [6], human attention [7], alertness level [8],
or emotional conditions [9], [1], [10], and [11], [12]–[18].
Several studies on identification of emotional states through

978-1-5386-0549-3/17/$31.00 ©2017 IEEE

Real time monitoring system was developed with 10 subject
each 15 trials. Then it was segmented each 10 seconds and
extracted into Alpha, Beta, and Theta waves using Wavelet. The

98
.

Proc. EECSI 2017, Yogyakarta, Indonesia, 19-21 September 2017

three waves of four channels was identified using LVQ into
three emotion i.e. excited, relax, and sad.

identification of the condition of emotion EEG signals
constructed based on wavelet extraction as the input of LVQ.
Experiments performed as illustrated by Fig. 2.

The system's ability to identify emotions is highly dependent
on training data that precisely identify emotional state before.
One way to obtain certain emotion state was by stimulation with
music that can generate waves related to the emotional state [19].
Some music evoke Beta waves related to excited emotional
state, slow music generate theta wave representing sad emotion,
and some music evokes alpha waves for a relaxed emotional
class. The system implemented in software to identify three
emotional states in real time that can be used for monitoring
emotional therapy.
II. MATERIAL AND METHODS
Monitoring system of emotion state as illustrated by Fig. 1.

Fig. 2. EEG recording

Each subject is recorded with three different emotional
states, namely excited, relax and sad. Recording begins with a
stimulus of instrumental songs that can evoke beta waves or
excited state. After that subject given about 30 minutes to rest
and then proceed to record with sound stimulation that generate
alpha waves or relaxed emotional state. Finally subjects given a
sound stimulation that can generate Theta waves to evoke sad
emotions.
B. Wavelet Extraction
EEG signals contain 0-128 Hz frequencies. Using Wavelet
transform, EEG signal was extracted into needed frequencies (532 Hz) which contained Alpha, Beta, and Theta waves. Discrete
wavelet transforms against x(n) signal is described as (1):
Fig. 1. Emotion state monitoring using Wavelet and LVQ

C ( j, k ) =

1
2

The system first learns by using the training data so that it
can be obtained generalization in the form of weights which
stored in the database. To improve recognition accuracy then the
EEG signal is first extracted in the frequency range 5-32 Hz
using Wavelet as Fig. 1

j

 x(n)ψ * (2 − j n − k ) (1)
n

is a wavelet base function, with and is a scale factor and shift.
Some of the wavelet functions are convolutions as well as
down sampling which filtered the resulting signal value from
each wave. Using wavelet transform, signal may be composed
of signal approximation and detail. For example signal with 128
Hz sampling rate contains 0-64 Hz frequency band. It can be
extracted into 5-32 Hz frequency band after five steps
decomposition as Fig.3.

A. EEG Data Acquisition
EEG data is recorded with Emotiv Insight wireless EEG
from 10 subjects for machine learning. In order to minimize
other variables electrical activity in the brain, we selected the
subject from a 20-25 years old, healthy and declared willingness
as subject in this research. We used wireless EEG recording by
placing electrodes on the four channels, namely AF3, T7, T8 and
AF4 and with 128 Hz sampling frequency. It was recorded three
sessions: in the morning, noon, and night. Before recording, the
subject was asked to get enough sleep between 7-9 hours, then
starting light sport and not hungry. The recording took place in
a dim lighting level and not noisy. Each subject was recorded sit
and open eyes. Everything were set to minimize the influence of
other variables than emotional. The recording is performed for
three minutes which were segmented every 10 seconds per
channel, thus resulting in a set of 128 x 10 = 1280 points in a
data set. EEG signal recording was used by 10 people for
training data and 10 people tested offline test data before the
system was used.
The face video was recorded to compare the result. Subjects
performed three emotion state, excited, relax, and sad. System

Fig. 3. Wavelet extraction into 5-32 Hz

99

Proc. EECSI 2017, Yogyakarta, Indonesia, 19-21 September 2017

Approximation is a resulted signal generated from
convolution process of original signal with low-pass filter while
detail is a signal generated from convolution process of original
signal with high-pass filter. Approximation and detail described
in (2) and (3):

reference vectors or weights compared training data each class.
We are looking at based on (6).
=∑

−

(6)

Approximation signal = yhigh k
(2)
=∑ x n ∗g n−k
Detail signal
= ylow k
= ∑ x n .h n − k
(3)
where
( ) = original signal
( ) = low-pass filter coefficient
h(n) = high-pass filter coefficient
k, n = index 1 − till lenght of signal
Wavelet with Symlet2 function, contain four low pass filter
coefficient (detoned gn) and four high-pass filter coefficient
(denoted hn) (4) and (5).
Scale function coefficient (low-pass filter)
ο=

√
√

,

1

=

√
√

,

2

√

=

√

,

3

=

√

Fig. 4. LVQ Architecture

(4)

√

System identification was based on the results of the wavelet
extraction, which is consecutive waves of theta, alpha, and beta
on every 10 seconds and every channels. Considering the
emotion state factor related to the asymmetric of symmetric
channel, the information is also included in the identification
system.

Wavelet function coefficient (high-pass filter)
ο=

√
√

,

1

=−

√
√

,

2

=

√
√

,

3

=−

√
√

(5)

Follows is the series of input identification system:

t1−8, s =1, AF 3 , a1−12, s =1, AF 3 , b1−36, s =1, AF 3 ,,



t1−8, s =10, AF 3 , a1−12, s =10, AF 3 , b1−36, s =10, AF 3 ,

t
, a1−12, s =1,T 8 , b1−36, s =1,T 8 ,,
1
−
8
,
s
=
1
,
T
8



t1−8, s =10,T 8 , a1−12, s =10,T 8 , b1−36, s =10,T 8 ,


x(n) = et1−8, s =1, AF 3− AF 4 , ea1−12, s =1, AF 3− AF 4 , eb1−36, s =1, AF 3− AF 4 ,,


et1−8, s =10, AF 3− AF 4 , ea1−12, s =10, AF 3− AF 4 , eb1−36, s =10, AF 3− AF 4 



et1−8, s =1,T 7 −T 8 , ea1−12, s =1,T 7 −T 8 , eb1−36, s =1,T 7 −T 8 ,,



et1−8, s =10,T 7 −T 8 , ea1−12, s =10,T 7 −T 8 , eb1−36, s =10,T 7 −T 8



Previous research were using Wavelet Symlet2 function in
order to extract beta power [41]. Using Symlet2 in calculation
actually almost the same with Daubechies4 calculation in
determining values of alpha, beta, and theta waves from EEG
signal. However, while the number of coefficients contained in
Symlet2 and Daubechies4 are equal, their values are different.
Symlet2 forms noted for asymmetric signals.
The extraction of EEG signals into theta waves was
conducted in five steps by approaching the frequency range 5-8
Hz that yielding 8 points. Then, the signal was extracted in alpha
waves 9-14 Hz with five steps for getting into 12 points. Beta
wave extraction was after four steps (15-32), gave 36 points.
After wavelet filtering, each second, using wavelet extraction we
reduced 128 points into 56 points data. If the signal is 10
seconds, we got 560 points.

Where t: theta wave, a: alpha wave, b: beta wave, et:
equilibrium of theta wave, ea: equilibrium of alpha wave, eb:
equilibrium of beta wave. Thus we get 560+560+560 = 1680
data input. Input of LVQ is 1680 or number of n.

C. Monitoring System Using Learning Vector Quantization
Learning Vector Quantization (LVQ) is a supervised
version of vector quantization that can be used when we have
each input data with class label. This learning technique uses the
class information to reposition the Voronoi vectors slightly as to
improve the quality of the classifier decision regions, which
adapted from Kohenen Map. A two stage process of LVQ shown
Fig. 4.

The first step is feature selection – the unsupervised
identification of a reasonably small set of features in which the
essential information content of the input data is concentrated.
The second step is the classification where the feature domains
are assigned to individual classes.
The LVQ algorithm attempted to correct winning weight wi
which minimum D. The correction was by shifting:
1.

Each of the classes used select one set of input vectors from
the training data called weights. A competitive layer will learn
to classify input vectors. The classes obtained as a result of this
competitive layer depend on the Euclidean distance between

2.

100

If the input xi and wining wi have the same class label, then
move them closer together by ∆ ( ) = ( )( −
)
If the input xi and wining wi have a different class label,
then move them apart together by ∆ ( ) = − ( )( −
)

Proc. EECSI 2017, Yogyakarta, Indonesia, 19-21 September 2017

3.

TABLE II.

Voronoi vectors/weights wj corresponding to other input
regions are left unchanged with ∆wi (t) = 0.

USING WAVELET IN ACCURACY OF RECOGNITION SYSTEM
Accuracy of (%)

Where β(t) is a learning rate that decreases with the number
of epochs of training. In this way we get better classification than
by the SOM alone.
III. RESULT AND DISCUSSION
Wavelet extraction used Symlet2 by Wavelet extraction of
5-32 Hz reduced DC frequency and noise of signal as illustrated
by Fig 5.

State

Without
Wavelet

With 3 wave of
Wavelet

Exited
Relax
Sad
Average

73
74
70
72

84
88
79
84

With 3 wave and
asymmetric of
Wavelet
88
90
84
87

This research result that using wavelet filtering improved
accuracy from 72% into 87%. While using asymmetric of pair
symmetric channel improved accuracy of 84 to 87%.
C. System Monitoring of Emotion State
Recognition system tested toward training data. One of the
advantages of a LVQ is revision of all weights but winner weight
as the ANN method. Testing of all training data about 100%
accuracy. The training processed took only under 10 minutes
from all training data.
Monitoring system toward attention has implemented in
integration software with wireless EEG. Each testing took 0.44
second to identify of 10 seconds. So it was not a signification lag
time.

Fig. 5. EEG signal compared wavelet filtering in a second

TABLE III.

Fig. 5 appears that the EEG signal was extracted to eliminate
the signal details outside the frequency of 5-32Hz with half of
data points

Subject
1
2
3
4
5
6
7
8
Average

A. Optimizing of LVQ Parameter
Monitoring system for emotional condition needs to be
tested against optimization of training parameters to find
parameters that produce weight and accuracy that provide the
best accuracy as shown in Table I Tests using the constant
reduction of fixed learning rate, namely 0.1.
TABLE I.
α

Epoch

Time

1
2
3
4
5

0.05
0.04
0.03
0.02
0.01

59
57
55
51
44

706
670
666
666
666

Excited
88
87
83
86
92
89
86
89
88

Accuracy of (%)
Relax
Sad
89
84
93
82
83
76
84
87
96
89
97
87
88
84
88
86
90
84

Average
87
87
81
86
92
91
86
88
87

The emotional state monitoring system that has been
implemented in software is shown in Fig. 6. It can give
emotional state in real time.

OPTIMIZED OF TRAINING PARAMETER

No

SYSTEM ACCURACY OF TESTING DATA

Accuracy (%)
Training
New
Data
Data
100
87
100
84
100
83
100
84
100
85

Optimized training parameters of LVQ are learning rate α
0.01 to 0.05 with 0.001 of learning rate reduction and the
maximum epoch of 10000. We got that learning rate gave best
accuracy 0.05. The next in the system.
B. Using Wavelet Filtering
Furthermore, the system tested the effect of using Wavelet
extraction on accuracy as shown in Table II. It can be seen that
72% without extraction to 87% using extraction with
asymmetric wave of pair symmetric channel and with 84%
without asymmetric wave. So the identification process will be
very good if it passes the stage of extraction first and using
symmetric of wave.

Fig. 6. Real time monitoring system of emotional state

IV. CONCLUSION
This research has developed an EEG signal recognition
system on the attention state using wavelet extraction and LVQ.
Using wavelet extraction could improve system accuracy from
72% to 87%. Using wavelet transform can also be the series

101

Proc. EECSI 2017, Yogyakarta, Indonesia, 19-21 September 2017

configuration theta wave, alpha and beta. Using asymmetric
channel improved accuracy of 84% to 87%.

2011.
[13] N. Kumar, K. Khaund, and S. M. Hazarika, “Bispectral Analysis of EEG
for Emotion Recognition,” Procedia Comput. Sci., vol. 84, pp. 31–35,
2016.
[14] J. Atkinson et al., “Improving BCI-based emotion recognition by
combining EEG feature selection and kernel classifier,” Expert Syst. Appl.
Elsevier, vol. 47, no. 3, pp. 35–41, 2016.
[15] M. Othman, A. Wahab, I. Karim, M. A. Dzulkifli, and I. F. T. Alshaikli,
“EEG Emotion Recognition Based on the Dimensional Models of
Emotions,” Procedia - Soc. Behav. Sci., vol. 97, pp. 30–37, 2013.
[16] Y. Zhanga, X. Jia, and S. Zhanga, “An approach to EEG-based emotion
recognition using combined feature extraction method,” Neurosci. Lett.,
vol. 633, pp. 152–157, 2016.
[17] Y. P. Lin, C. H. Wang, T. L. Wu, S. K. Jeng, and J. H. Chen, “Support
vector machine for EEG signal classification during listening to emotional
music,” in Proceedings of the 2008 IEEE 10th Workshop on Multimedia
Signal Processing, MMSP 2008, 2008, pp. 127–130.
[18] Y. Liu, O. Sourina, and M. K. Nguyen, “Real-time EEG-based Human
Emotion Recognition and Visualization,” in 2010 International
Conference on Cyberworlds, 2010, pp. 256–277.
[19] E. C. Djamal and Suprijanto, “Recognition of Electroencephalogram
Signal Pattern against Sound Stimulation using Spectral of Wavelet,” in
Tencon 2011, 2011, pp. 767–771.
[20] D. Nie, X. Wang, L. Shi, and B. L. S. Member, “EEG-based Emotion
Recognition during Watching Movies,” in Proceedings of the 5th
International IEEE EMBS Conference on Neural Engineering, 2011, pp.
667–670.
[21] T. Nomura and Y. Mitsukura, “Extraction of unconscious emotions while
watching TV commercials,” in IECON 2015 - 41st Annual Conference of
the IEEE Industrial Electronics Society, 2016, pp. 368–373.
[22] M. Soleymani, M. Pantic, and T. Pun, “Multimodal emotion recognition in
response to videos,” IEEE Trans. Affect. Comput., vol. 3, no. 2, pp. 211–
223, 2012.
[23] A. Marzuki, L. Dioren Rumpa, A. D. Wibawa, and M. H. Purnomo,
“Classification of human state emotion from physiological signal pattern
using pulse sensor based on learning vector quantization,” in 2016
International Seminar on Intelligent Technology and Its Application, 2016,
no. May 2017, pp. 129–134.
[24] E. C. Djamal, M. Y. Abdullah, and F. Renaldi, “Brain Computer Interface
Game Controlling Using Fast Fourier Transform and Learning Vector
Quantization,” J. Telecommun. Electron. Comput. Eng., 2017.
[25] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M.
Vaughan, “Brain Computer Interfaces for communication and control,”
Front. Neurosci., vol. 4, no. 113, pp. 767–791, 2002.
[26] N. H. Liu, C. Y. Chiang, and H. C. Chu, “Recognizing the degree of human
attention using EEG signals from mobile sensors.,” Sensors (Basel)., vol.
13, no. 8, pp. 10273–10286, 2013.
[27] M. Murugappan and S. Murugappan, “Human emotion recognition through
short time Electroencephalogram (EEG) signals using Fast Fourier
Transform (FFT),” in 2013 IEEE 9th International Colloquium on Signal
Processing and its Applications (CSPA), 2013, pp. 289–294.
[28] M. Murugappan, M. Rizon, R. Nagarajan, S. Yaacob, D. Hazry, and I.
Zunaidi, “Time-Frequency Analysis of EEG Signals for Human Emotion
Detection,” in 4th Kuala Lumpur International Conference on Biomedical
Engineering 2008 IFMBE, 2008, pp. 262–265.

Using LVQ could reduce the computation time to be under a
minute without accuracy loss. Obtaining generalization of data
in LVQ training such was relatively faster and more stable than
using Multilayer Perceptron. Each testing took 0.44 second to
identify 10 seconds signals data. Therefore it was not a
significant lag time.
ACKNOWLEDGMENT
This research was funded by Penelitian Unggulan Perguruan
Tinggi Kementerian Riset, Teknologi, dan Pendidikan Tinggi,
Republik Indonesia in 2017.
REFERENCES
[1] S. Koelstra et al., “DEAP: A database for emotion analysis; Using
physiological signals,” IEEE Trans. Affect. Comput., vol. 3, no. 1, pp. 18–
31, 2012.
[2] E. C. Djamal, D. P. Pangestu, and D. A. Dewi, “EEG-Based Recognition
of Attention State Using Wavelet and Support Vector Machine,” in 2016
International Seminar on Intelligent Technology and Its Applications
(ISITIA), 2016, pp. 139–144.
[3] R. Ekayama, E. C. Djamal, and A. Komarudin, “Identifikasi Kondisi Rileks
Dari Sinyal Eeg Menggunakan Wevalet Dan Learning Vector
Quantization,” in Seminar Nasional Sains dan Teknologi (SNST), 2016, pp.
150–155.
[4] A. Turnip and K. S. Hong, “Classifying mental activities from EEG-P300
signals using adaptive neural networks,” Int. J. Innov. Comput. Inf.
Control, vol. 8, no. 9, pp. 6429–6443, 2012.
[5] E. C. Djamal, Surijanto, and S. J. Setiadi, “Classification of EEG-Based
Hand Grasping Imagination Using Autoregressive and Neural Networks,”
J. Teknol., vol. 78, no. 6–6, pp. 105–110, 2016.
[6] V. Gaveau et al., “Automatic online control of motor adjustments in
reaching and grasping,” Neuropsychologia, vol. 55, no. 1, pp. 25–40, 2014.
[7] M. Gola, M. Magnuski, I. Szumska, and A. Wróbel, “EEG beta band
activity is related to attention and attentional deficits in the visual
performance of elderly subjects,” Int. J. Psychophysiol., vol. 89, no. 3, pp.
334–341, 2013.
[8] E. C. Djamal, Suprijanto, and A. Arif, “Identification of Alertness State
Based on EEG Signal Using Wavelet Extraction and Neural Networks,” in
2014 International Conference on Computer, Control, Informatics and Its
Applications (IC3INA), 2014, pp. 175–180.
[9] M. Murugappan, N. Ramachandran, and Y. Sazali, “Classification of
human emotion from EEG using discrete wavelet transform,” J. Biomed.
Sci. Eng., vol. 3, no. 4, pp. 390–396, 2010.
[10] S. Sengupta, S. Biswas, and S. Nag, “Emotion Specification from Musical
Stimuli : An EEG Study with AFA and DFA,” in 4th International
Conference on Signal Processing and Integrated Networks (SPIN) 2017,
2017.
[11] R. Duan, X.-W. Wang, and B.-L. Lu, “EEG-Based Emotion Recognition
in Listening Music by Using Support Vector Machine and Linear Dynamic
System,” in International Conference on Neural Information Processing,
2012, pp. 468–475.
[12] D. Giakoumis, D. Tzovaras, K. Moustakas, and G. Hassapis, “Automatic
recognition of boredom in video games using novel biosignal momentbased features,” IEEE Trans. Affect. Comput., vol. 2, no. 3, pp. 119–133,

102

Proc. EECSI 2017, Yogyakarta, Indonesia, 19-21 September 2017

[36] X. D. Zhang and H. R. Choi, “Pattern Recognition of Human Grasping
Operations Based on EEG,” pp. 592–600, 2006.
[37] Y. Liu, O. Sourina, and M. K. Nguyen, “Real-time EEG-based Emotion
Recognition and its Applications,” Trans. Comput. Sci. XII, vol. 6670, pp.
256–277, 2011.
[38] M. Islam, T. Ahmed, S. S. Mostafa, M. S. U. Yusuf, and M. Ahmad,
“Human emotion recognition using frequency amp; statistical measures of
EEG signal,” Informatics, Electron. Vis. (ICIEV), 2013 Int. Conf., pp. 1–6,
2013.
[39] A. Bhardwaj, A. Gupta, P. Jain, A. Rani, and J. Yadav, “Classification of
human emotions from EEG signals using SVM and LDA Classifiers,” in
2015 2nd International Conference on Signal Processing and Integrated
Networks (SPIN), 2015, pp. 180–185.
[40] Y. P. Lin, C. H. Wang, T. L. Wu, S. K. Jeng, and J. H. Chen, “Multilayer
perceptron for EEG signal classification during listening to emotional
music,” in IEEE Region 10 Annual International Conference,
Proceedings/TENCON, 2007.
[41] Z. Mahmoodin, N. S. Jalalludin, W. Mansor, K. Y. Lee, and N. B.
Mohamad, “Selection of Symlets wavelet function order for EEG signal
feature extraction in children with dyslexia,” in ISSBES 2015 - IEEE
Student Symposium in Biomedical Engineering and Sciences: By the
Student for the Student, 2016, pp. 113–117.

[29] V. Bajaj and R. B. Pachori, “Human Emotion Classification from EEG
Signals Using Multiwavelet Transform,” 2014 Int. Conf. Med. Biometrics,
no. Md, pp. 125–130, 2014.
[30] M. Murugappan, R. Nagarajan, and S. Yaacob, “Comparison of different
wavelet features from EEG signals for classifying human emotions,” 2009
IEEE Symp. Ind. Electron. Appl. ISIEA 2009 - Proc., vol. 2, no. Isiea, pp.
836–841, 2009.
[31] D. Iacoviello, A. Petracca, M. Spezialetti, and G. Placidi, “A real-time
classification algorithm for EEG-based BCI driven by self-induced
emotions,” Comput. Methods Programs Biomed., vol. 122, no. 3, pp. 293–
303, 2015.
[32] M. K. Kiymik, M. Akin, and A. Subasi, “Automatic recognition of
alertness level by using wavelet transform and artificial neural network,”
J. Neurosci. Methods, vol. 139, no. 2, pp. 231–240, 2004.
[33] A. B. M. A. Hossain, W. Rahman, and M. A. Riheen, “Left and Right Hand
Movements EEG Signals Classification Using Wavelet Transform and
Probabilistic Neural Network,” vol. 5, no. 1, pp. 92–101, 2015.
[34] A. Zabidi, W. Mansor, K. Y. Lee, and C. W. N. F. Che Wan Fadzal,
“Classification of EEG signal from imagined writing using a combined
Autoregressive model and multi-layer perceptron,” 2012 IEEE-EMBS
Conf. Biomed. Eng. Sci. IECBES 2012, no. December, pp. 964–968, 2012.
[35] E. Yulianto et al., “An analysis of EEG signal generated from grasping and
writing,” Neuropsychologia, vol. 9, no. 1, pp. 51–57, 2015.

103

