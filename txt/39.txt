This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.
Digital Object Identifier 10.1109/ACCESS.2017.DOI

A Time-Frequency Distribution-based
Approach for Decoding Visually
Imagined Objects using EEG Signals
RAMI ALAZRAI1 , AMAL AL-SAQQAF1 , FERAS AL-HAWARI1 , HISHAM ALWANNI2 , AND
MOHAMMAD I. DAOUD1
1
2

Department of Computer Engineering, School of Electrical Engineering and Information Technology, German Jordanian University, Amman 11180, Jordan
Faculty of Engineering, University of Freiburg, Freiburg 79098, Germany

Corresponding author: Rami Alazrai (e-mail: rami.azrai@gju.edu.jo).
This work was supported by the Seed-Grant program at the German Jordanian University (SEEIT 02/2020) and partially supported by the
Scientific Research Support Fund - Jordan (grant No. ENG/1/9/2015).

ABSTRACT This paper investigates the possibility of using visual imagery tasks, which are mental
imagery tasks that involve imagining the images of objects perceptually without seeing them, as a control
paradigm that can increase the control’s dimensionality of electroencephalography (EEG)-based braincomputer interfaces. Specifically, we propose an EEG-based approach for decoding visually imagined
objects by using the Choi-Williams time-frequency distribution to analyze the EEG signals in the joint timefrequency domain and extract a set of twelve time-frequency features (TFFs). The extracted TFFs are used
to construct a multi-class support vector machine classifier to decode various visually imagined objects. To
validate the performance of our proposed approach, we have recorded an EEG dataset for 16 healthy subjects
while imagining objects that belong to four different categories, namely nature (fruits and animals), decimal
digits, English alphabet (capital letters), and arrow shapes (arrows with different colors and orientations).
Moreover, we have designed two performance evaluation analyses, namely the channel-based analysis and
feature-based analysis, to quantify the impact of utilizing different groups of EEG electrodes that cover
various regions on the scalp and the effect of reducing the dimensionality of the extracted TFFs on the
performance of our proposed approach in decoding the imagined objects within each of the four categories.
The experimental results demonstrate the efficacy of our proposed approach in decoding visually imagined
objects. Particularly, the average decoding accuracies obtained for each of the four categories were as high
as 96.67%, 93.64%, 88.95%, and 92.68%.
INDEX TERMS Visual imagery, Electroencephalography (EEG), Time-frequency distribution, Braincomputer interface (BCI), Support vector machine (SVM), Pattern recognition

I. INTRODUCTION

A brain-computer interface (BCI) is a system that analyzes
the electrophysiological signals of the brain and produces
commands that reflect the user’s mental activities [1]–[4].
Over the past two decades, researchers have developed BCI
systems for different application domains, such as assisting individuals with severe motor-impairments [1], [5]–[7],
recognizing emotional states [8], and detecting pain [9]–
[11]. In this regard, various types of neuroimaging modalities
have been utilized to record brain activities and design BCI
systems [4], such as the functional magnetic resonance imaging (fMRI), positron emission tomography (PET), electroencephalography (EEG), and electrocorticography (ECoG).

Among these different neuroimaging modalities, the EEG
neuroimaging modality, which records the electrical activities of the brain that are measured at the scalp [4], has
been widely employed to design BCI systems in various
application domains [4], [12]. This can be attributed to the
following factors [2], [4], [12]: (1) the noninvasive nature of
the EEG modality, (2) the high temporal resolution of the
acquired EEG signals, and (3) the relatively low cost and high
portability of the EEG acquisition devices compared with the
acquisition devices of other neuroimaging modalities.
Recent clinical studies have indicated that mental imagery
tasks can be employed to design BCI systems that can facilitate the treatment of several mental disorders [13]. In this
1

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

regard, the literature indicates that the motor imagery (MI)
task, which refers to the process of imagining the movement
of a particular body-part (e.g. the hand or the foot) without
actually moving it [14], is by far the most commonly used
mental task to design EEG-based BCI systems [2], [15]–[17].
In fact, the majority of the existing MI EEG-based BCI systems are focused on developing signal processing and analysis techniques to distinguish between MI tasks associated
with a limited number of body-parts [18]–[25], including the
right hand, left hand, right foot, left foot, and tongue. Despite
the promising results attained by the existing MI EEG-based
BCI systems in decoding MI tasks that are associated with
different body parts, these systems are capable of producing
a relatively limited control’s dimensionality. The control’s
dimensionality of a BCI system is defined as the number of
control signals that can be generated from the BCI system,
where the number of these control signals is related to the
number of imagery tasks considered by the BCI system. This
in turn can significantly reduce the potential of utilizing these
BCI systems to support real-world applications that usually
require a high number of control signals [6], [7].
To increase the control’s dimensions of the BCI systems,
researchers have investigated the possibility of decoding
other mental imagery tasks, namely the visual imagery of
objects [2], [12], [26]–[29]. The visual imagery of an object is
a mental process that involves regenerating and maintaining
the image of the object perceptually without seeing it [30]–
[32]. This mental task has been mainly investigated using
two neuroimaging modalities [28], [33]–[39], namely fMRI
and PET. Nonetheless, several limitations, such as the high
acquisition cost, low portability, and the requirement of the
subject to sit or lay down in the scanner, might restrict the
use of these neuroimaging modalities to be only in clinics
and research labs. To alleviate these limitations, researchers
have recently started to study the possibility of using the
EEG neuroimaging modality to decode visually imagined
objects [2], [12], [40], [41]. However, decoding the visually
imagined objects by analyzing the EEG signals is a difficult
task. This is due to the inter- and intra-personal variations in
the generated brain activities during the imagination of similar and different objects [42], [43]. Moreover, the EEG signals are nonstationary signals that have time-varying spectral
characteristics, which implies that analyzing the EEG signals
in the time-domain only or the frequency-domain only might
not be sufficient. Therefore, a joint time-frequency analysis is
required to capture the time-varying spectral characteristics
of the EEG signals [44].
In this study, we propose an EEG-based approach for
decoding a large number of visually imagined objects using a
set of time-frequency features (TFFs) that are extracted from
the Choi-Williams time-frequency distribution. Specifically,
we utilize the Choi-Williams distribution to analyze the EEG
signals in the joint time-frequency domain and capture the
time-varying spectral characteristics of the EEG signals. To
reduce the high dimensionality of the Choi-Williams distribution computed for the EEG signals, a set of twelve TFFs

are extended from the time-domain and frequency-domain
to the joint time-frequency domain and extracted from the
Choi-Williams distribution computed for the EEG signals.
These TFFs are used to construct a subject-specific multiclass support vector machine classifier to decode the visually
imagined objects.
In order to validate the performance of our proposed approach, we have recorded a large visual imagery EEG dataset
that is collected from 16 healthy subjects while imagining
objects that belong to four different categories, namely nature
(fruits and animals), decimal digits, English alphabet (capital
letters), and arrow shapes (arrows with different colors and
orientations). For each subject, the number of imagined objects, number of trials, and approximate total EEG recording
time are 56 objects, 440 trials, and 2.2 hours, respectively.
Moreover, we have developed two performance evaluation
analyses to assess the performance of the proposed approach
in decoding the visually imagined objects of each of the four
categories. The first performance evaluation analysis, which
we refer to as the channel-based analysis, quantifies the effect
of utilizing different groups of EEG electrodes, which cover
various regions of the brain, on the ability of our proposed
approach to distinguish between the EEG signals associated
with the visually imagined objects within each of the four categories. The second performance evaluation analysis, which
we refer to as the feature-based analysis, quantifies the effect
of utilizing different subsets of the extracted TFFs on the
ability of our proposed approach to distinguish between the
EEG signals associated with the visually imagined objects
within each of the four categories. The results presented in
this study illustrate the capability of the proposed approach
to accurately decode the visually imagined objects within
each of the four categories in our EEG dataset. In addition,
the results obtained using the proposed approach outperform
those reported in several existing state-of-the-art approaches
in terms of the number of decoded visually imagined objects
and the achieved classification performance. These results
suggest the feasibility of using our proposed approach to
increase the control’s dimensions of EEG-based BCI systems
to better assist individuals with mental disorders. To the best
of our knowledge, this is the first study that explores the
use of the Choi-Williams distribution to analyze EEG signals
in the joint time-frequency domain for decoding visually
imagined objects.
The rest of this paper is organized as follows: In Section
II, we describe the acquired EEG dataset, time-frequency
analysis of the recorded EEG signals, TFFs extracted from
the Choi-Williams distribution, decoding visually imagined
objects, and performance evaluation analyses and metrics.
In Sections III and IV, we present and discuss the results
obtained for each evaluation analysis, and we compare the
performance of our proposed approach with the performance
of other existing approaches. Finally, the conclusion is provided in Section V.

2

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

II. MATERIALS AND METHODS
A. SUBJECTS

Sixteen healthy subjects (two females, 14 males; age range of
18 − 26 years; mean ± standard deviation age of 20.4 ± 2.1
years) with normal vision and no neurological disorders
volunteered to participate in this study. We refer to the sixteen
subjects as S1 to S16 . Before participating in the study, each
subject received a thorough written and oral explanation of
the experimental procedure and signed a written consent
form. The experimental procedure of this study was approved
by the Research Ethics Committee at the German Jordanian
University and conducted according to the principles presented in the Declaration of Helsinki.
B. STIMULI

The objects to be imagined are grouped into four categories.
The first category consists of visual stimuli related to nature.
Specifically, the first category comprises four objects, namely
a green apple, a yellow banana, a brown elephant, and a
blue bird. For each object in the first category, an image
of size 9 cm × 9 cm, resolution 2000 × 2000 pixels, and
white background was used as a visual stimulus (Fig. 1A).
These images were acquired from the Bank of Standardized
Stimuli (BOSS) [45]. The second category comprises visual
stimuli of the decimal digits (0 − 9). An image of size 6.4
cm × 3.5 cm and white background was created and used as
a visual stimulus for each decimal digit. The decimal digit
within each image was typed using a black, bold, Arial font
of size 150 pt (Fig. 1B). The third category comprises visual
stimuli of the English alphabet (A − Z). An image of size
6.4 cm × 4.3 cm and white background was created and
used as a visual stimulus for each letter. The letter within
each image was typed in an uppercase format using a black,
bold, Arial font of size 150 pt (Fig. 1C). Finally, the fourth
category consists of visual stimuli for 16 arrows that have
four orientations, including upward, rightward, downward,
and leftward, and four colors, including white, blue, red, and
green. An image of size 5 cm × 7 cm and white background
was created and used as a visual stimulus for each of the 16
arrows (Fig. 1D). Figure 1 describes the objects within each
of the four categories and shows the image used as a visual
stimulus during the imagination of each object.
C. EXPERIMENTAL PROTOCOL

At the beginning of the experiment, each subject was seated
on a comfortable chair at a viewing distance of approximately
100 cm from a computer monitor placed on top of a desk. The
computer monitor was employed to display visual stimuli
associated with the objects presented in Fig. 1, where each
visual stimulus notifies the subject to imagine a particular
object. During the experiment, the experimenter, EEG acquisition system, and recording station were located behind the
subject.
For each trial, a white screen is displayed on the monitor
for five seconds, during which the subject is asked to relax,
keep her/his eyes open, and rest her/his hands on the desk

located in front of her/him. After the relaxing period, a visual
stimulus is presented in the center of the monitor against
a white background for five seconds. During the visual
stimulus period, the subject is asked to carefully observe
the picture of the object presented on the screen located in
front of her/him. After that, a black screen is displayed on
the monitor to indicate the beginning of the visual imagery
period. During the visual imagery period, the subject is asked
to imagine the object presented during the visual stimulus
period while keeping her/his eyes closed [39], [46]–[49].
The duration of the visual imagery period is eight seconds.
Finally, a beep sound is used to alert the subject about the
end of the trial and to open her/his eyes. Figure 2 shows the
timing diagram for each recorded trial.
In this study, the number of trials recorded for each subject
while imagining each individual object in categories 1, 2,
and 3 is equal to 7 trials. Furthermore, the number of trials
recorded for each subject while imagining each object in category 4 is equal to 10 trials. Hence, for each subject, the total
number of recorded trials is equal to 440 trials (i.e., (7 trials ×
4 objects in category 1) + (7 trials × 10 objects in category 2)
+ (7 trials × 26 objects in category 3) + (10 trials × 16 objects
in category 4)). The average duration of the experiments for
each subject was approximately 6 hours divided into 3 to 4
separate recording sessions that were conducted on different
days. The duration of each recording session was within the
range of 1.5 − 2 hours, including the time required to mount
the EEG cap and electrodes on the scalp of the subject,
the time needed to perform the experiments, and the break
times granted to the subject upon her/his request during the
recording session. During each recording session, each subject was asked to immediately notify the experimenter about
any fatigue or uncomfortable feeling so that the experimenter
will immediately stop the recording, delete the last recorded
trial, and reschedule another recording session for the subject
to continue the experiments.
D. EEG SIGNALS ACQUISITION AND PREPROCESSING

The Biosemi ActiveTwo EEG recording system (Biosemi
B.V., Amsterdam, Netherlands) was employed to record the
EEG signals at a sampling frequency of 2048 Hz. In particular, 16 Ag/AgCL active electrodes were mounted in an
elastic cap according to the international 10 − 20 electrode
placement system. The utilized electrodes are mounted on
the scalp at the following locations: F p1 , F p2 , F3 , Fz , F4 ,
T7 , T8 , C3 , Cz , C4 , P3 , Pz , P4 , O1 , Oz , and O2 . These EEG
electrodes are referenced to the common mode sense (CMS)/
driven right leg (DRL) at the C1 and C2 locations. During the
recording of each trial, the offset of each electrode was kept
below 40 mV.
The recorded EEG signals were pre-processed by applying a bandpass filter with a bandwidth of 1 − 45 Hz,
re-referencing all EEG electrodes to the EEG electrode at
position Cz [50], correcting the baseline using the last 500 ms
of the relax period within each trial [41], downsampling the
EEG signals to 256 Hz, and reducing the muscular and ocular
3

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

FIGURE 1: The four categories of the imagined objects. (A) The first category consists of four objects, namely a green apple, a
yellow banana, a brown elephant, and a blue bird. (B) The second category comprises the decimal digits (0 − 9). (C) The third
category contains the English alphabet (capital letters (A − Z)). (D) The fourth category comprises 16 arrows that have four
orientations and four colors.

artifacts from the acquired EEG signals using the automatic
artifact rejection (AAR) open-source MATLAB toolbox [51].
The EEG electrode at position Cz , which was used for rereferencing, was eliminated from the following analysis [41],

[50].

4

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

FIGURE 2: The timing diagram of each recorded trial.

E. TIME-FREQUENCY ANALYSIS OF THE EEG SIGNALS

The EEG signals have shown to possess time-varying spectral
characteristics (i.e., non-stationary signals) [52], [53]. This
entails that analyzing the EEG signals in the time-only or
frequency-only domains may not be sufficient to capture
the non-stationary characteristics of the EEG signals [54].
Indeed, recent studies have indicated that the performance
of the EEG signal classification approaches that use features
that are extracted from a joint time-frequency domain outperform the performance of the approaches that utilize features that are extracted from the time-only or frequency-only
domains [8], [44], [52]. In light of this, we propose to use
the Choi-Williams distribution (CWD) [55] to analyze the
EEG signals in the joint time-frequency domain. The CWD
is a time-frequency distribution that transforms the signal
from the time-domain into a joint time-frequency domain that
describes the distribution of the energy comprised within the
signal over the time and frequency domains [55], [56].
To compute the CWD of the EEG signals, we have
implemented a sliding window to divided the EEG signal
associated with each EEG channel into a set of overlapped
segments. In particular, the size of the sliding window is
set to 256 samples, which represents the length of each
EEG segment, and the overlap between any two consecutive
window positions is set to 128 samples. Then, the CWD of
an EEG segment, z(t), denoted as CW Dz , can be computed
as follows [44], [55]:

CW Dz (t, f ) =

R∞ R∞
−∞ −∞

Γ(θ, η)Ay(t) (θ, η)e−j2πf η−j2πtθ ∂η ∂θ ,

(1)
where y(t), Ay(t) (θ, η), and Γ(θ, η) represent the analytic
signal computed for the EEG segment z(t), the ambiguity
function computed for y(t), and the time-frequency smoothing kernel of the CWD, respectively. Specifically, the analytic
signal y(t) can be computed as follows:
y(t) = z(t) + jH{z(t)},

(2)

where H{·} is the Hilbert transform [57]. The ambiguity
function Ay(t) (θ, η) is defined as the Fourier transform of

the auto-correlation function computed for y(t), which can
be expressed as follows [58], [59]:
Z ∞
η
η
Ay(t) (θ, η) =
y(t + )y ∗ (t − )ej2πθt ∂t, (3)
2
2
−∞
where y ∗ (·) is the complex conjugate of y(·). Finally, the
time-frequency smoothing kernel of the CWD can be expressed as follows [55]:
 θ2 η2 
Γ(θ, η) = exp − 2 ,
(4)
r
where r > 0 is a parameter that controls the amount of
smoothing applied to the cross-terms comprised in the computed CW Dz and its value was experimentally set to 0.5.
In this study, we have utilized the HOSA toolbox [60]
to compute the CWD of the EEG segments. Specifically,
for each EEG segment, the dimensionality of the computed
CWD is equal to τ = 256 × ω = 512, where τ and ω represent the number of samples along the time and frequency
axes, respectively. Figure 3 presents the CWD computed for
four EEG segments that were extracted from four different
trials that were recorded for the second subject recruited in
this study while imagining the following four objects: a green
apple, digit zero, capital letter A, and Arrow1.
F. FEATURE EXTRACTION

The CWD computed for an EEG segment provides a highresolution time-frequency description of the spectral variations in the EEG signals over time. Nonetheless, the number
of samples comprised within the CWD computed for each
EEG segment is larger than the number of samples within
the EEG segment. Moreover, the CWD computed for each
EEG segment contains a large number of points in the timefrequency plane with values that are equal or very close to
zero. Therefore, it is important to extract highly discriminatory features from the CWD computed for each EEG segment
to reduce the dimensionality of the CWD and build classifiers
that can accurately distinguish between the EEG segments
that are associated with different visually imagined objects.
In this study, we extract twelve time-frequency features
(TFFs) from the CWD computed for each EEG segment.
These features are: the sum of the logarithmic amplitudes
(F1 ), median absolute deviation (F2 ), root mean square value
5

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

FIGURE 3: Illustration of the computed CWD for different EEG segments. (A)-(D) represent the CWD computed for EEG
segments that were extracted from the EEG electrode mounted at position F p2 on the scalp of the second subject while
imagining a green apple, digit zero, capital letter A, and Arrow1, respectively.

(F3 ), inter-quartile range (F4 ), mean (F5 ), variance (F6 ),
skewness (F7 ), kurtosis (F8 ), flatness (F9 ), flux (F10 ), normalized Renyi entropy (F11 ), and energy concentration (F12 )
of the CWD computed for an EEG segment. Table 1 provides
a mathematical description for each of the twelve TFFs
extracted from the CWD computed for each EEG segment.
The features F1 − F8 provide statistical measurements that
are extracted from the amplitude of the points in the CWD
computed for each EEG segment, while the features F8 −F12
provide spectral measurements that quantify the spread of the
energy in the time-frequency plane of the CWD computed for
each EEG segment. Further details regarding the computation
procedure and interpretation of each of the aforementioned
twelve features can be found in our pervious work [5], [8].
The number of EEG segments that are extracted at each
position of the sliding window is equal to the number of the
utilized EEG channels (i.e., 15 EEG channels). Therefore, the
total number of features that are extracted at each position of
the sliding window is equal to 180 features (i.e., 15 EEG segments × 12 features per EEG segment). These TFFs, which
are extracted from the EEG segments that are associated with
a particular position of the sliding window, are grouped to
construct a feature vector.

G. CLASSIFICATION OF VISUALLY IMAGINED OBJECTS

Previous studies have indicated that different subjects have
different abilities to perform imagery tasks [12], [18], [53],
[61]. This suggests the construction of subject-specific classifiers to distinguish between the EEG signals associated with
various imagined objects [2], [6], [8], [40]. In this study,
we have implemented five classification scenarios for each
subject. The first, second, third, fourth, and fifth classification
scenarios aim to construct classifiers that can distinguish
between the EEG signals associated with the objects in category 1, category 2, category 3, category 4, and all combined
categories, respectively. In particular, we utilized the feature
vectors that are extracted from the trials of each subject to
build and train a multi-class support vector machine (SVM)
classifier with the Gaussian radial basis function (RBF) kernel [62] for each one of the five classification scenarios.
The reasons behind using a multi-class SVM classifier are
summarized in two folds: (1) The majority of the previous
studies that investigated the decoding of visually imagined
objects by analyzing the EEG signals, such as the studies
presented in [2], [12], [40], [41], have utilized SVM classifiers to decode the visually imagined objects. Hence, the use
of the SVM classifier can facilitate the comparison between
the performance of our proposed approach and the perfor-

6

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

TABLE 1: The twelve TFFs extracted from the CWD computed for each EEG segment.
Feature

Mathematical description

F1 =

Sum of the logarithmic amplitudes (F1 )

ω
τ X
X



log |CW Dz (t, f )|

(5)

t=1 f =1

Median absolute deviation (F2 )

F2 =

τ
ω
τ
ω
1 XX
1 XX
CW Dz (t, f ) −
CW Dz (t, f )
τ × ω t=1 f =1
τ × ω t=1 f =1

v
u
u
F3 = t

Root mean square value (F3 )

τ X
ω 
X

!
2
/(τ × ω)
CW Dz (t, f )

(6)

(7)

t=1 f =1

Inter-quartile range (F4 )

F4 =

ω
1 X
ω f =1

CW Dz

F5 =

Mean (F5 )

F7 =

Skewness (F7 )

(9)

(10)

τ X
ω 
3
X
1
CW Dz (t, f ) − F5
τ × ω × (F6 )3/2 t=1 f =1

(11)

τ X
ω 
4
X
1
CW Dz (t, f ) − F5
τ × ω × (F6 )2 t=1 f =1

(12)

F8 =

Kurtosis (F8 )

τ
ω
1 XX
CW Dz (t, f )
τ × ω t=1 f =1

(8)

τ
ω
2
1 XX
CW Dz (t, f ) − F5
τ × ω t=1 f =1

F6 =

Variance (F6 )

!
 3(τ + 1) 
 (τ + 1) 
, f − CW Dz
,f
4
4

Qτ

Qω
1/τ ×ω
t=1
f =1 (|CW Dz (t, f )|)
Pτ Pω
D
(t,
f
)|)
(|CW
z
t=1
f =1

(13)


CW Dz (t + l, f + k) − CW Dz (t, f ) , l = k = 1

(14)

F9 = τ × ω ×

Flatness (F9 )

Flux (F10 )

F10 =

τ
−l ω−k
X
X
t=1 f =1

Normalized Renyi entropy (F11 )

F11

Energy concentration (F12 )

τ X
ω
X
1
log2
=
1−ν
t=1 f =1

F12 =

CW Dz (t, f )
Pτ Pω
t=1
f =1 CW Dz (t, f )

τ X
ω p
X
|CW Dz (t, f )|

!ν
,ν = 3

(15)

!2
(16)

t=1 f =1

mance reported in these previous studies as described in
section IV. (2) Previous studies have shown that utilizing the
SVM classifier with RBF kernel can be more effective than
generative models for supervised learning problems [63].
Moreover, using the SVM classifier with RBF kernel can
achieve better performance and generalization compared
with other state-of-the-art classifiers, such as Naive Bayes,
k-nearest neighbors (k-NN), and neural networks [52], [64].
In addition, many previous EEG-based studies that were
focused on decoding imagery tasks have indicated that the

classification performance obtained using the SVM classifier
outperforms the classification performance achieved by other
classifiers, such as linear discriminant analysis (LDA) and
linear regression (LR) [65], [66].
A one-against-one voting scheme [67], [68] is employed
to implement the multi-class SVM classifier for each subject,
in which a set of n(n − 1)/2 binary SVM classifiers are
trained to construct a n-class SVM classifier. In this study,
n is equal to 4, 10, 26, 16, and 56 for the first, second,
third, fourth, and fifth classification scenarios, respectively.
7

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

Moreover, a grid-based search is performed to tune the
parameters of the constructed multi-class SVM classifiers
for each subject and each classification scenario, including
the regularization parameter (C) and the kernel parameter
(γ). In particular, we perform a grid-based search along
two directions to determine the values of γ and C for
each multi-class SVM classifier. In the first direction, we
varied the value of the parameter γ to the following range:
{0.0001, 0.001, 0.01, 0.1, 0.5, 1, 1.5, 5, 10}. Furthermore, in
the second direction, we varied the value of the parameter C
in the following range: {0.01, 0.5, 1, 5, 10, 50, 70}. For each
subject and each classification scenario, the tuned parameters
of the SVM model are selected as the values of γ and C that
maximize the average classification accuracy [69].
A ten-fold cross-validation procedure is employed to train
and test the multi-class SVM classifiers which are constructed for each subject and each classification scenario [8],
[66], [70]. Specifically, for a particular subject and classification scenario, 90% of the feature vectors associated with the
objects are randomly selected and used for training, while
the remaining 10% of the feature vectors are used for testing.
The aforementioned ten-fold cross-validation procedure is
repeated ten times, and the overall classification performance
is computed for each subject and each classification scenario
by averaging the results obtained from each repetition [5],
[6].

segments at each position of the sliding window, on the
accuracy of decoding the different visually imagined objects.
In this analysis, we utilize the minimum redundancy maximum relevance (mRMR) feature selection approach [71] to
reduce the dimensionality of the feature vectors extracted
from the group of EEG electrodes that achieved the best
classification performance in the CBA. The mRMR feature
selection approach uses the mutual information to select
a subset of the TFFs that have the maximum correlation
(i.e., relevance) with a particular imagined object and the
minimum correlation (i.e., redundancy) between the selected
features. The selected TFFs are ranked in descending order
according to the minimum-redundancy maximum-relevance
basis [71].
To implement the FBA, the mRMR approach was applied
for each subject and each classification scenario to generate
five different subsets of feature vectors, namely subset 1,
subset 2, subset 3, subset 4, and subset 5. In particular, the
first, second, third, fourth, and fifth subsets of feature vectors
comprise the selected highest-ranking 5%, 10%, 25%, 50%,
and 75% of the TFFs extracted from the EEG electrodes of
G10 , respectively. Finally, the performance of each classification scenario is assessed using each of the five subsets
of feature vectors to determine the impact of reducing the
number of features on the accuracy of decoding the visually
imagined objects.

H. PERFORMANCE EVALUATION ANALYSES AND
METRICS

3) Performance evaluation metrics

In this section, we describe the implementation of two performance evaluation analyses, namely the channel- and featurebased analyses, that investigate the effect of the utilized
EEG channels and TFFs on the performance of each of the
five classification scenarios. We also provide a description
of the performance evaluation metrics used to quantify the
classification performance obtained based on each evaluation
analyses for each of the five classification scenarios.
1) Channel-based performance evaluation analysis (CBA)

The objective of the CBA is to investigate the effect of the
utilized EEG electrodes on the accuracy of distinguishing
between the EEG signals that are associated with different
visually imagined objects. To implement this analysis, we
have constructed ten different groups of EEG electrodes that
cover various regions of the brain. Then, for each subject,
we have utilized the EEG electrodes comprised within each
of the ten groups to extract the feature vectors from the
EEG segments at each window position and perform the five
classification scenarios described in subsection II-G. Table 2
describes each of the ten groups of electrodes along with the
dimensionality of the feature vectors extracted from the EEG
electrodes comprised within each group.
2) Feature-based performance evaluation analysis (FBA)

The objective of the FBA is to study the effect of reducing
the number of TFFs, which are extracted from the EEG

For each performance evaluation analysis, the average classification performance is computed across the ten train-test
repetitions (i.e., the ten repetitions of the ten-fold crossvalidation procedure) in terms of two standard evaluation
metrics, namely the average classification accuracy (CA)
and average F1 − Score (F1 ). In particular, the average
CA and F1 values are computed for each subject and each
classification scenario across the ten train-test repetitions as
follows [72]:
!
PL
PK
1
l=1 tpl
CA = K k=1 no. of testing samples in the fold k × 100%, (17)

F1 =

1
K

1
L

PL 
l=1

2×

(REl,k ×P Rl,k )
(REl,k +P Rl,k )



!
× 100%, (18)

where K = 10 represents the number of repetitions of the
ten-fold cross-validation procedure, L represents the number
of objects associated with a particular classification scenario,
tpl represents the number of true positive samples that belong
to object l within a specific classification scenario, REl,k and
P Rl,k are the recall and precision of class l computed for the
testing fold k, respectively. After that, for each classification
scenario, we compute the mean ± standard deviation values
of the average CAs and F1 − Scores over the 16 subjects
and report these results in the next section.

8

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

TABLE 2: The groups of the EEG electrodes considered in the CBA.
Group

Description

EEG electrodes

Dimensionality of the extracted feature vector
from the EEG segments at each position of the
sliding window

Group 1 (G1 )

This group consists of six different subgroups of
EEG electrodes that cover six different regions
of the brain. These subgroups are:
1. Subgroup 1 covers the central region.
2. Subgroup 2 covers the frontal pole region.
3. Subgroup 3 covers the frontal region.
4. Subgroup 4 covers the temporal region.
5. Subgroup 5 covers the parietal region.
6. Subgroup 6 covers the occipital region.

C3 and C4
F p1 and F p2
F3 , Fz , and F4
T7 and T8
P3 , Pz , and P4
O1 , Oz , and O2

2 electrodes × 12 features = 24
2 electrodes × 12 features = 24
3 electrodes × 12 features = 36
2 electrodes × 12 features = 24
3 electrodes × 12 features = 36
3 electrodes × 12 features = 36

Group 2 (G2 )

This group includes the EEG electrodes in the
frontal pole and frontal regions.

F p1 , F p2 , F3 , Fz , and F4

5 electrodes × 12 features = 60

Group 3 (G3 )

This group includes the EEG electrodes in the
temporal and occipital regions.

T7 , T8 , O1 , Oz , and O2

5 electrodes × 12 features = 60

Group 4 (G4 )

This group includes the EEG electrodes in the
frontal pole, frontal, and parietal regions.

F p1 , F p2 , F3 , Fz , F4 ,
P3 , Pz , and P4

8 electrodes × 12 features = 96

Group 5 (G5 )

This group includes the EEG electrodes in the
temporal, parietal, and occipital regions.

T7 , T8 , P3 , Pz , P4 ,
O1 , Oz , and O2

8 electrodes × 12 features = 96

Group 6 (G6 )

This group includes the EEG electrodes in the
frontal pole, frontal, temporal, and parietal
regions.

F p1 , F p2 , F3 , Fz , F4 ,
T7 , T8 , P3 , Pz , and P4

10 electrodes × 12 features = 120

Group 7 (G7 )

This group includes the EEG electrodes in the
central, temporal, parietal, and occipital regions.

C3 , C4 , T7 , T8 , P3 ,
Pz , P4 , O1 , Oz , and O2

10 electrodes × 12 features = 120

This group includes the EEG electrodes in the
frontal pole, frontal, parietal, and occipital
regions.
This group includes the EEG electrodes in the
frontal pole, frontal, temporal, parietal, and
occipital regions.

F p1 , F p2 , F3 , Fz , F4 ,
P3 , Pz , P4 , O1 , Oz ,
and O2
F p1 , F p2 , F3 , Fz , F4 ,
T7 , T8 , P3 , Pz , P4 ,
O1 , Oz , and O2
C3 , C4 , F p1 , F p2 , F3 ,
Fz , F4 , T7 , T8 , P3 ,
Pz , P4 , O1 , Oz , and O2

Group 8 (G8 )
Group 9 (G9 )
Group 10 (G10 )

This group includes all the EEG electrodes.

III. EXPERIMENTAL RESULTS
A. RESULTS OF THE CBA

Table 3 shows the average CA and F1 values computed
for each of the five classification scenarios and each of
the ten different groups of EEG electrodes, which are depicted in 2. The CA and F1 values presented in Table 3
are computed over all subjects and all objects within each
classification scenario. In particular, for G1 , the average CA
and F1 values computed for each of the five classification
scenarios based on the TFFs extracted from the EEG electrodes comprised in subgroup 2, which cover the frontal
pole region of the brain, are substantially higher than the
average CA and F1 values obtained based on the TFFs
extracted from the EEG electrodes comprised within each of
the other five different subgroups of G1 . Specifically, using
subgroup 2, the average CA/F1 values achieved for the
first, second, third, fourth, and fifth classification scenarios
were 90.06%/89.60%, 77.44%/76.94%, 66.87%/66.01%,
71.83%/71.32%, and 59.22%/58.16%, respectively. Moreover, the results presented in Table 3 show that the average
CA and F1 values computed for each of the five classification
scenarios based on the TFFs extracted from the EEG electrodes of subgroup 1, subgroup 3, and subgroup 4 are rela-

11 electrodes × 12 features = 132
13 electrodes × 12 features = 156
15 electrodes × 12 features = 180

tively close to each other and are higher than the average CA
and F1 values obtained based on using the TFFs extracted
from the EEG electrodes of subgroup 5 and subgroup 6.
The average CA and F1 values computed for each of
the five classification scenarios based on the TFFs extracted
from the EEG electrodes comprised in G2 , which covers the frontal pole and frontal regions of the brain, are
higher than the average CA and F1 values obtained based
on the TFFs extracted from the EEG electrodes comprised
within each of the six different subgroups of G1 . Specifically, using G2 , the average CA/F1 values achieved for the
first, second, third, fourth, and fifth classification scenarios
were 92.80%/92.33%, 84.08%/83.56%, 78.69%/77.96%,
83.81%/83.47%, and 74.44%/73.44%, respectively. Similarly, for each of the five classification scenarios, using the
TFFs extracted from the EEG electrodes in G3 , which cover
the temporal and occipital regions of the brain, achieved
an average CA and F1 values that are substantially higher
than the average CA and F1 values achieved using the
TFFs extracted from the EEG electrodes of subgroup 4 and
subgroup 6, which cover the temporal region of the brain and
the occipital region of the brain, respectively.
Table 3 also shows that using the TFFs extracted from the
9

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

EEG electrodes of G4 , which cover the frontal pole, frontal,
and parietal regions of the brain, achieved an average CA
and F1 values that are slightly higher than the average CA
and F1 values achieved using the TFFs extracted from the
EEG electrodes of G2 , which cover only the frontal pole
and frontal regions of the brain. Furthermore, using the TFFs
extracted from the EEG electrodes of G5 , which cover the
temporal, parietal, and occipital regions of the brain, achieved
an average CA and F1 values that are higher than the average
CA and F1 values achieved using the TFFs extracted from
the EEG electrodes of G3 , which cover the temporal and
occipital regions of the brain. In addition, the average CA
and F1 values obtained for each classification scenario using
the TFFs extracted from the EEG electrodes of G6 , which
cover the frontal pole, frontal, temporal, and parietal regions
of the brain, are substantially higher than the average CA
and F1 values obtained using the TFFs extracted from the
EEG electrodes of G5 , which cover the temporal, parietal,
and occipital regions of the brain. Moreover, the average CA
and F1 values obtained for each classification scenario using
the TFFs extracted from the EEG electrodes of G7 , which
cover the central, temporal, parietal, and occipital regions of
the brain, outperform the average CA and F1 values obtained
using the TFFs extracted from the EEG electrodes of G5 ,
which cover the temporal, parietal, and occipital regions of
the brain. At the same time, the average CA and F1 values
obtained using the TFFs extracted from the EEG electrodes
of G6 outperform the average CA and F1 values obtained
using the TFFs extracted from the EEG electrodes of G7 .
The average CA and F1 values computed for each classification scenario based on the TFFs extracted from the
EEG electrodes of G8 , which cover the frontal pole, frontal,
parietal, and occipital regions of the brain, are slightly lower
than the average CA and F1 values obtained for each classification scenario using the TFFs extracted from the EEG
electrodes of G6 , which cover the frontal pole, frontal, temporal, and parietal regions of the brain. In addition, for each
classification scenario, the average CA and F1 values obtained using the TFFs extracted from the EEG electrodes of
G9 , which cover the frontal pole, frontal, temporal, parietal,
and occipital regions of the brain, outperform the average CA
and F1 values obtained using G7 , which overs the central,
temporal, parietal, and occipital regions of the brain, and the
average CA and F1 values obtained using G8 , which covers
the frontal pole, frontal, parietal, and occipital regions of the
brain.
Finally, the best classification performance was achieved
using the TFFs extracted from the EEG electrodes of G10 ,
which cover the central, frontal pole, frontal, temporal,
parietal, and occipital regions of the brain. In particular,
using G10 , the average CA/F1 values achieved for the
first, second, third, fourth, and fifth classification scenarios
were 95.73%/95.57%, 88.21%/87.64%, 83.59%/82.95%,
87.68%/87.42%, and 81.57%/80.82%, respectively.
To compare the performance results of the proposed approach obtained using G10 and the performance results ob-

tained using the other groups of EEG electrodes, we have
conducted paired t-tests with significance level of 0.05 that
compare the classification accuracies obtained for the five
classification scenarios using the TFFs extracted from the
EEG electrodes of G10 with the classification accuracies
obtained for the five classification scenarios using the TFFs
extracted from the EEG electrodes of G9 , G8 , G7 , G6 ,
G5 , G4 , G3 , G2 , and subgroup 2 in G1 , respectively. The
computed p values for G10 versus G9 , G8 , G7 , G6 , G5 ,
G4 , G3 , G2 , and subgroup 2 in G1 were 0.0029, 0.0013,
0.0024, 0.0022, 0.0085, 00099, 0.0036, 0.0056, and 0.0073,
respectively. The calculated p values demonstrate that the
classification performance obtained using the EEG electrodes
of G1 0 outperforms significantly the classification performance obtained using the other groups of EEG electrodes.

B. RESULTS OF THE FBA

The results of the CBA, which are shown in Table 3, indicate
that using all the TFFs that are extracted from the EEG electrodes of G10 achieved the best average CA and F1 values for
all the classification scenarios. Therefore, in this subsection,
we have selected G10 , which comprises all the EEG electrodes, to perform the FBA. The dimensionality of the feature
vectors comprised in the first, second, third, fourth, and fifth
subsets of feature vectors are equal to 9, 18, 45, 90, and 135,
respectively. Table 4 presents the average CA and F1 values
computed for each of the five classification scenarios and
each of the five different subsets of feature vectors that are
extracted from the EEG electrodes of G10 .
The results presented in Table 4 show that, for all the
classification scenarios, the highest average CA and F1
values were achieved using subset 3, which consists of the
highest-ranking 25% of the TFFs. Specifically, using the
highest-ranking 25% of the TFFs, the average CA/F1 values
achieved for the first, second, third, fourth, and fifth classification scenarios were 96.67%/96.37%, 93.64%/93.31%,
88.95%/88.41%, 92.68%/92.28% and 85.22%/84.52%, respectively. Table 5 shows the selected values of the parameters γ and C, which are obtained using the grid-based search
described in section II-G, for the SVM classifiers that are
constructed for each subject and each classification scenario
based on using the TFFs of subset 3.
To compare the classification results of the proposed
approach that are obtained using subset 3 and the results
obtained using the other subsets of TFFs, we have conducted
paired t-tests with significance level of 0.05 to compare the
classification accuracies obtained for the five classification
scenarios using subset 3 of the TFFs with the classification
accuracies obtained for the five classification scenarios using
the TFFs of subset 5, subset 4, subset 2, and subset 1,
respectively. The computed p values for subset 3 versus
subset 5, subset 4, subset 2, and subset 1 were 0.0040,
0.0301, 0.0174, and 0.00026, respectively. The calculated p
values demonstrate that the classification performance obtained using subset 3 of the TFFs outperforms significantly

10

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

TABLE 3: The results of the CBA. The highest average CA and F1 values obtained for each classification scenario are
highlighted using bold font. STD represents the standard deviation.
Groups of EEG
electrodes
Subgroup 1
Subgroup 2
Subgroup 3
Subgroup 4
Subgroup 5
Subgroup 6
G2
G3
G4
G5
G6
G7
G8
G9
G10

G1

CA
70.38
90.06
71.08
69.83
59.23
65.38
92.80
80.84
93.00
83.56
94.14
86.92
94.07
95.18
95.73

Classification
scenario 1
STD
F1
14.94
69.58
6.43
89.60
12.27
70.02
13.53
68.91
14.02
58.13
12.07
64.13
3.17
92.33
6.17
80.20
3.46
92.68
5.96
83.04
2.94
93.75
5.11
86.45
3.39
93.60
2.61
94.85
2.07
95.57

STD
15.16
6.67
12.58
13.71
14.12
12.20
3.41
6.25
3.49
6.08
3.00
5.15
3.45
2.72
2.17

CA
46.61
77.44
53.90
46.22
40.81
40.80
84.08
58.58
85.33
66.65
86.96
71.08
85.18
86.82
88.21

Classification
scenario 2
STD
F1
16.67
45.47
13.30
76.94
16.73
52.92
16.34
45.32
14.41
39.54
17.49
39.74
4.85
83.56
9.40
57.64
4.91
84.78
8.68
65.69
4.63
86.37
7.64
70.34
4.42
84.63
4.47
86.33
3.86
87.64

STD
16.93
13.40
16.95
16.52
14.58
17.54
4.98
9.61
5.02
8.86
4.78
7.79
4.65
4.62
4.08

Classification scenarios
Classification
scenario 3
CA
STD
F1
STD
37.37
12.56
36.35
12.36
66.87
18.28
66.01
18.35
44.27
13.75
43.28
13.60
34.88
10.39
33.87
10.38
30.30
12.40
29.45
12.42
29.75
15.21
29.09
15.20
78.69
7.56
77.96
7.67
49.10
8.11
48.28
8.10
79.29
7.30
78.61
7.46
52.04
7.96
51.01
8.06
82.24
7.08
81.52
7.24
65.51
7.51
64.70
7.56
79.16
6.52
78.56
6.69
81.79
6.22
81.10
6.27
83.59
6.32
82.95
6.39

CA
49.40
71.83
52.83
46.15
41.51
40.83
83.81
59.36
83.98
67.80
86.62
74.31
84.70
86.43
87.68

Classification
scenario 4
STD
F1
15.92
48.86
13.27
71.32
12.95
52.25
12.57
45.36
13.85
40.97
14.30
40.29
5.21
83.47
7.97
58.64
5.04
83.52
8.11
67.21
5.20
86.36
7.97
73.81
4.54
84.36
4.76
86.10
4.91
87.42

STD
15.94
13.31
12.94
12.61
13.84
14.27
5.28
7.95
5.15
8.19
5.26
8.03
4.63
4.87
4.96

CA
36.41
59.22
40.51
32.78
28.75
28.16
74.44
48.84
75.69
57.19
78.61
64.33
76.89
79.50
81.57

Classification
scenario 5
STD
F1
11.72
35.03
14.95
58.16
11.88
39.14
10.17
31.47
10.83
27.56
12.52
26.97
6.26
73.44
7.68
47.55
6.32
74.83
7.45
55.92
5.82
77.72
7.25
63.12
5.62
75.93
5.55
78.71
5.31
80.82

STD
11.48
14.93
11.88
10.13
10.78
12.53
6.36
7.68
6.50
7.48
5.98
7.29
5.75
5.69
5.50

TABLE 4: The results of the FBA. The highest average CA and F1 values obtained for each classification scenario are
highlighted using bold font. STD represents the standard deviation.
Subset of feature
vectors
Subset 1
Subset 2
Subset 3
Subset 4
Subset 5

CA
90.98
94.64
96.67
95.90
95.88

Classification
scenario 1
STD
F1
3.57
90.38
2.54
94.28
2.61
96.37
2.54
95.73
2.31
95.87

STD
3.74
2.68
2.55
2.61
2.38

CA
84.16
90.97
93.64
93.10
90.88

Classification
scenario 2
STD
F1
5.54
83.47
3.40
90.56
3.62
93.31
2.73
92.78
3.51
90.60

STD
5.67
3.55
3.69
2.85
3.65

Classification scenarios
Classification
scenario 3
CA
STD
F1
STD
79.76
7.45
79.08
7.58
86.41
6.62
86.07
6.63
88.95
4.50
88.41
4.65
88.50
5.23
88.11
5.35
86.65
5.77
86.10
5.87

the classification performance obtained using the other four
subsets of TFFs.
Figure 4 shows the average CA values and the corresponding standard deviation values computed for each subject and
each classification scenario using subset 3. Table 6 shows the
lowest and highest CA values computed for each of the five
classification scenarios using subset 3 along with the subjects
associated with these CA values. The results presented in
Fig. 4 and Table 6 indicate that the CA values computed
for each subject using subset 3 are substantially higher than
the random classification rates associated with the five classification scenarios, where the random classification rates
of the first, second, third, fourth, and fifth classification
scenarios are equal to 25%, 10%, 3.8%, 6.25%, and 1.78%,
respectively.
TABLE 5: The selected values of the parameters γ and C
for the SVM classifiers constructed for each subject and each
classification scenario by using subset 3.
Subject

S1
S2
S3
S4
S5
S6
S7
S8
S9
S10
S11
S12
S13
S14
S15
S16

Classification
scenario 1
γ
C
1.00
10.00
0.50
50.00
0.10
70.00
0.10
10.00
0.10
50.00
1.00
10.00
0.50
10.00
0.50
50.00
1.00
10.00
0.50
50.00
0.10
70.00
0.10
70.00
0.50
50.00
1.00
50.00
0.10
50.00
0.50
10.00

Classification scenarios
Classification
Classification
Classification
scenario 2
scenario 3
scenario 4
γ
C
γ
C
γ
C
0.50
70.00
1.50
50.00
0.50
50.00
1.00
50.00
0.50
50.00
1.50
10.00
0.10
70.00
0.50
50.00
1.00
10.00
1.00
5.00
1.00
50.00
0.50
50.00
0.50
10.00
1.00
10.00
1.00
10.00
1.00
10.00
1.00
50.00
1.00
50.00
1.00
50.00
0.50
50.00
1.00
5.00
1.50
5.00
0.50
50.00
0.50
70.00
1.00
10.00
0.50
70.00
1.00
10.00
0.50
70.00
1.50
10.00
0.50
50.00
0.50
50.00
1.00
10.00
1.50
50.00
1.00
10.00
0.50
50.00
1.50
50.00
1.00
50.00
1.00
10.00
0.50
50.00
0.50
50.00
1.50
5.00
0.50
50.00
1.00
10.00
0.50
70.00
1.00
50.00
0.10
70.00
0.50
10.00
1.50
10.00

Classification
scenario 5
γ
C
0.50
50.00
0.50
50.00
0.50
50.00
1.00
10.00
1.00
50.00
1.00
50.00
0.50
50.00
0.50
70.00
0.50
50.00
1.00
70.00
0.50
50.00
0.50
70.00
0.50
70.00
0.50
50.00
0.50
50.00
0.50
50.00

CA
84.51
91.50
92.68
91.44
90.54

Classification
scenario 4
STD
F1
5.68
84.13
3.77
91.30
3.59
92.28
3.72
91.22
4.12
90.40

STD
5.76
3.83
3.67
3.75
4.16

CA
76.35
83.92
85.22
83.06
82.45

Classification
scenario 5
STD
F1
7.44
75.54
5.43
83.36
3.74
84.52
5.07
82.34
5.16
81.72

STD
7.56
5.64
3.91
5.22
5.28

To assess the importance of each of the twelve TFFs, for
each subject, we have computed the ratio between the number
of times a particular TFF was selected for inclusion in each
of the five subsets of feature vectors to the dimensionality of
the feature vectors comprised within each of the five subsets
of feature vectors. Then, for each classification scenario and
each subset of feature vectors, the computed selection ratios
of each TFF are averaged over all subjects. Table 7 provides the average selection ratio of each of the twelve TFFs
computed for each classification scenario and each subset of
feature vectors. The results presented in Table 7 show that,
for each classification scenario, the use of subset 3 yielded an
average selection ratio that is greater than zero for each of the
twelve TFFs. In fact, subset 3 consists of the highest-ranking
25% of the TFFs extracted from the EEG electrodes in G10 .
Therefore, having an average selection ratio that is strictly
larger than zero for each of the twelve TFFs implies that each
of the twelve TFFs, which is extracted from at least one of
the EEG electrodes in G10 , is included in subset 3. These
results indicate that different TFFs, which are extracted from
various EEG electrodes, can capture different characteristics
of various visually imagined objects.
Moreover, using subset 3, the average selection ratios computed for each classification scenario, which are provided
in Table 7, explicate that the TFFs F1 , F3 , and F11 have
the highest average selection ratios compared with the other
TFFs. This is due to the important characteristics captured
by these three TFFs. Specifically, F1 , which is the sum of the
logarithmic amplitudes, is a spectral moment-related feature
that can quantify the nonlinearity of the energy distribution
in the time-frequency plane of the CWD computed for an
EEG segment [5], [8], [73]. In addition, F3 , which is the root
11

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

FIGURE 4: The CA values computed for each subject and each classification scenario using subset 3 of the feature vectors,
which consists of the highest-ranking 25% of the TFFs extracted from the EEG electrodes in G10 . The black vertical bars
represent the standard deviation in the CA values.
TABLE 6: The lowest and highest CA values computed for each of the five classification scenarios using subset 3 along with
the subjects associated with these CA values.

(Lowest CA (%), Subject)
(Highest CA (%), Subject)

Classification
scenario 1
(89.17, S10 )
(98.92, S9 )

Classification
scenario 2
(86.53, S2 )
(97.22, S12 )

mean square value, quantifies the variations of the energy
distribution in the time-frequency plane of the CWD computed for an EEG segment [5], [8]. Finally, F11 , which is
the normalized Renyi entropy, characterizes the regularity of
the energy spread in the time-frequency plane of the CWD
computed for an EEG segment [5], [8], [44], [74].
C. RUNTIME OF THE PROPOSED APPROACH

The proposed approach was implemented on a workstation
with a 3.5-GHz Intel Xeon Processor (Intel Corporation,
Santa Clara, CA, USA) and 8 GB memory. We provide
the runtime of our proposed approach computed for the

Classification
scenario 3
(80.8, S9 )
(90.73, S16 )

Classification
scenario 4
(83.33, S10 )
(95.78, S15 )

Classification
scenario 5
(72.72, S10 )
(86.35, S5 )

configuration that yielded the best classification results. In
particular, the best classification results were obtained using
the EEG electrodes comprised in G10 , which contains 15
EEG electrodes, and subset 3 of the TFFs, which contains
the highest-ranking 25% of the features (i.e., 45 TFFs). The
average ± standard deviation time required to train the multiclass SVM classifier constructed for the first, second, third,
fourth, and fifth classification scenarios computed across the
16 subjects is equal to 12.79 ± 1.36 s, 88.56 ± 6.38 s,
1048.31 ± 135.06 s, 488.79 ± 26.03 s, and 5298.74 ± 653.25
s, respectively.
Despite the relatively long time required during the train-

12

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

TABLE 7: The average selection ratio (%) of each of the twelve TFFs computed for each classification scenario and each subset
of feature vectors.
Classification
scenario
Classification
scenario 1

Classification
scenario 2

Classification
scenario 3

Classification
scenario 4

Classification
scenario 5

Subset of feature
vectors
Subset 1
Subset 2
Subset 3
Subset 4
Subset 5
Subset 1
Subset 2
Subset 3
Subset 4
Subset 5
Subset 1
Subset 2
Subset 3
Subset 4
Subset 5
Subset 1
Subset 2
Subset 3
Subset 4
Subset 5
Subset 1
Subset 2
Subset 3
Subset 4
Subset 5

F1
28.47
23.26
15.83
11.74
9.26
40.28
34.72
20.28
13.33
10.23
59.03
42.71
27.22
16.18
11.11
63.89
50.00
28.89
16.25
11.06
76.39
59.03
30.42
16.39
11.11

F2
2.08
4.51
6.67
7.50
7.69
2.78
3.82
7.50
7.92
7.92
0.00
1.39
3.89
6.25
7.78
1.39
1.39
3.33
5.97
7.31
0.00
0.35
1.53
3.89
7.59

F3
11.11
10.76
10.33
7.85
7.45
11.81
10.07
9.89
8.47
8.61
12.50
13.89
14.44
12.22
9.77
8.33
11.11
11.67
9.93
9.21
6.94
7.99
11.67
12.85
10.19

F4
0.00
0.00
0.56
1.81
6.57
1.39
1.04
0.97
1.25
2.87
0.00
0.69
1.67
1.60
3.52
0.00
1.04
2.22
3.68
4.72
1.39
4.86
5.94
7.57
6.99

ing process, we only need to learn the multi-class SVM
models offline and then identify the testing samples online.
Specifically, the average ± standard deviation time required
to compute the CWD of the EEG signal associated with
a particular EEG electrode at a particular window position
computed across the 16 subjects is equal to 4.55 ± 1.8
ms. Hence, the average time required to compute the CWD
for the EEG signals associated with the 15 EEG electrodes
comprised within G10 is equal to 68.25 ms (i.e., 4.55 ms ×
15 electrodes). Moreover, the average ± standard deviation
time required to compute the 45 TFFs within subset 3 for
the EEG signals at a particular window position computed
across the 16 subjects is equal to 118.3 ± 3.62 ms. Finally,
the average ± standard deviation time required to classify
each feature vector computed across the 16 subjects is equal
to 0.23±0.04 ms. In light of this, the average time required to
compute the CWD, extract the TFFs, and recognize the class
of the imagined object is equal to 186.78 ms, which is less
than the duration of the employed sliding window (i.e., 1000
ms). This suggests the feasibility of utilizing our proposed
approach to develop real-world BCI systems.
IV. DISCUSSION

In this study, we explored the potential of decoding visually
imagined objects using TFFs that are extracted from the
CWD computed from the EEG signals. The performance
of our proposed approach was evaluated using EEG signals
that were recorded for 16 healthy subjects while imagining
56 different objects that are arranged into four categories,
as described in Fig. 1. The results presented in section III

Time-frequency features
F5
F6
F7
F8
3.47
3.47
5.56
6.94
4.51
6.94
6.60
5.56
6.25
9.06
7.50
7.22
7.85
9.24
8.40
8.19
7.96
9.77
7.96
8.84
5.56
5.56
1.39
6.25
7.64
5.90
2.78
5.90
7.08
5.69
5.97
6.81
8.19
6.53
7.92
8.13
7.82
8.19
8.66
8.80
1.39
0.00
2.08
2.78
2.08
0.35
3.47
5.90
4.44
1.94
6.94
9.17
6.81
3.40
9.44
9.24
8.24
5.88
9.12
8.98
2.08
2.08
1.39
3.47
2.43
1.04
1.74
3.47
4.44
1.81
5.69
7.50
6.46
3.89
7.22
8.19
7.82
6.62
7.92
7.92
0.00
0.00
0.00
3.47
0.35
0.00
0.35
8.33
2.08
2.40
8.19
10.81
4.44
0.90
11.32
12.08
7.73
3.61
10.00
10.05

F9
10.42
10.42
9.33
12.43
9.91
2.08
2.08
7.22
11.39
10.28
0.00
0.69
3.33
6.53
9.03
0.00
3.13
4.86
8.68
9.86
0.00
0.00
1.14
4.03
6.57

F10
2.78
6.25
7.92
7.99
8.33
5.56
5.90
8.03
8.13
8.89
1.39
4.17
5.83
7.36
8.24
1.39
3.13
5.14
7.71
8.56
0.00
0.69
1.94
4.38
7.41

F11
16.67
12.85
11.56
8.40
8.61
11.81
11.11
10.83
10.42
9.07
16.67
18.40
14.17
12.36
9.81
11.11
13.89
14.03
12.15
10.09
10.42
14.93
19.83
15.14
11.06

F12
9.03
8.33
7.78
8.61
7.64
5.56
9.03
9.72
8.33
8.66
4.17
6.25
6.94
8.61
8.52
4.86
7.64
10.42
9.86
8.89
1.39
3.13
4.04
7.01
7.69

demonstrate the capability of our proposed CWD-based approach to accurately decode the visually imagined objects
comprised in each of the four categories.
A. RESULTS OF THE CBA AND FBA

The results of the CBA, which are presented in section III-A,
show that the CA and F1 values computed for the EEG
groups comprising EEG electrodes that cover the frontal
pole, frontal, temporal, and parietal regions of the brain, such
as G6 , G9 , and G10 , are higher than the CA and F1 values
computed for the other EEG groups. Moreover, Table 3
shows that the EEG groups containing EEG electrodes that
cover the occipital region in addition to the frontal pole,
frontal, temporal, and parietal regions of the brain, such as
G9 and G10 , achieved CA and F1 values that are higher than
the other EEG groups. In addition, the increase in the classification performance obtained for G7 and G10 in comparison
with the classification performance obtained for G5 and G9 ,
respectively, suggests that the presence of the central region
can slightly increase the classification performance. This
can be attributed to the influence of the volume conductor
on the EEG signals [75], in which the electrical potentials
generated within a particular region of the brain are spatially
disseminated to other regions and recorded by the sparsely
distributed electrodes on the scalp [5], [75].
It is worth to mention that the results of the CBA, which
are presented in section III-A, are compliant with the findings
reported in previous studies related to visual imagery [37],
[76] which have shown that the frontal pole, frontal, temporal, parietal and occipital regions are highly active during
13

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

the performance of visual imagery task. This in turn justifies
the high CA values achieved for each classification scenario
using G10 compared with the CA values obtained using the
other EEG groups which cover subsets of the brain regions
in G10 . Specifically, Table 3 shows that the highest CA
and F1 values computed for each of the five classification
scenarios were archived using G10 , which consists of 15
EEG electrodes that cover the central, frontal pole, frontal,
temporal, parietal, and occipital regions of the brain.
The results of the FBA, which are provided in Table 4,
indicate that using a subset of the TFFs extracted from the
EEG electrodes in G10 can substantially enhance the classification performance obtained for each classification scenario
compared with the classification performance obtained using
all the TFFs extracted from the EEG electrodes in G10 , which
are presented in Table 3. In particular, Table 3 and Table 4
show that the CA values obtained for each classification
scenario using subset 3, which consists of the highest-ranking
25% of the TFFs extracted from the EEG electrodes in G10 ,
are higher than the CA values obtained for each classification
scenario using all the TFFs extracted from the EEG electrodes in G10 . This can be attributed to the fact that several
TFFs that are extracted from different EEG electrodes may
comprise redundant and unrelated information [8]. This in
turn can increase the similarity between the feature vectors
that are extracted from the EEG signals associated with
various visually imagined objects, which can degrade the
classification performance. Moreover, the CA values presented in Fig. 4, which were obtained using the third subset of
feature vectors, show a variation in the computed CA values
among different subjects for the same classification scenario.
In fact, this variation in the performance achieved by different
subjects is widely known in the literature and commonly
referred to as “BCI literacy” [77], [78]. Such a variation can
be attributed to the fact that different subjects have different
capabilities of performing various mental imagery tasks [18],
[53], [61].
B. COMPARISON WITH OTHER EXISTING
APPROACHES

Literature reveals that the visual imagery of objects has been
mainly investigated using two neuroimaging modalities [28],
[33]–[39], namely fMRI and PET. Several limitations, such
as the high acquisition cost, low portability, and the requirement of the subject to sit or lay down in the scanner, might
limit the wide use of these neuroimaging modalities in visual
imagery research labs. Recently, several studies have demonstrated the possibility of decoding visually imagined objects
using the EEG neuroimaging modality. For example, Bovrov
et al. [40] recorded the EEG signals using two different EEG
acquisition systems, namely the Emotiv (14 electrodes) and
ActiCap (32 electrodes) systems, for seven healthy subjects
while imagining two types of objects, namely faces and
houses. For each subject, the acquired EEG signals were
used to construct a Bayesian classifier to decode the type
of the imagined object. The average CA values computed

across all subjects based on using the Emotiv and ActiCap
EEG acquisition systems were 48% and 54%, respectively.
In another study, Esfahani and Sundararajan [12] acquired
the EEG signals using the Emotiv EEG acquisition system
(14 electrodes) for ten healthy subjects while imagining the
shape of five geometric objects. The recorded EEG signals
were analyzed using the independent component analysis
(ICA) and Hilbert-Huang transform (HHT) to extract a set
of spectra-based features from the EEG signals. For each
subject, the extracted features were used to construct five
pair-wise linear discriminant classifiers, where each classifier
is associated with one of the five objects. The average CA
value computed across all subjects and all objects was 44.6%.
Kosmyna et al. [2] recorded the EEG signals using the 2g.tec
USBAmp EEG acquisition system (36 electrodes) for 26
healthy subjects while imagining two objects: a hammer and
a daisy flower. For each subject, the acquired EEG signals
were used to construct a spectrally weighted common spatial
patterns classifier to decode the imagined object. The average
CA value computed across all subjects was 55.9%. In a
recent study by Wang et al. [41], EEG signals were recorded
using the actiCHamp Brain Products EEG acquisition system
(17 electrodes) for 14 healthy subjects while observing five
lowercase letters, including “a”, “e”, “i”, “o”, and “t”. For
each subject, the phase and power data were extracted from
the EEG signals and used to construct a multi-class SVM
classier to decode the observed letters. The highest average
CA value computed across all subjects and all letters was
46.61%.
The work presented in the current study provides three
improvements over the approaches described in [2], [12],
[40], [41]. First, our proposed approach used a multi-class
SVM classifier to discriminate between the visually imagined
objects in each of the five classification scenarios, while
the approaches presented in [2], [12], [40] utilize binary
SVM classifiers to distinguish between individual pairs of
visually imagined objects. In this vein, researchers have
indicated that decoding various mental tasks using multiclass classification methods is more difficult that decoding
a pair of mental tasks [7]. Moreover, the use of multi-class
classification methods to decode visually imagined objects
can be exploited to advance the BCI technology by using
visual imagery as a control paradigm to increase the control
dimensionality of EEG-based BCI systems. Additionally, the
use of multi-class classification methods to decode the EEG
signals recorded for the subjects while imagining digits and
letters can facilitate the design of human-computer interfaces
that can assist individuals with severe motor and/or lingual
impairments. Second, the CA values presented in our study
were obtained using 15 EEG electrodes compared with the
best CA values reported in the studies [2], [40], [41] which
were obtained using 32, 36, and 17 electrodes, respectively.
This illustrates the effectiveness of the TFFs, which are
extracted from the CWD computed for EEG segments, in
characterizing object-related information to achieve high CA
values. Third, to the best of our knowledge, this is the first

14

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

study that investigates the feasibility of classifying 56 different visually imagined objects. The large number of objects
considered in the current study increases the difficulty of
designing accurate classifiers compared with the aforementioned approaches which considered the problem of classifying two objects, as described in [2], [40], and five objects, as
described in [12], [41]. Therefore, our proposed approach has
the potential to increase the control dimensionality of EEGbased BCI systems.
C. LIMITATIONS AND FUTURE WORK

Despite the promising results achieved using our proposed
approach in distinguishing between the EEG signals associated with various visually imagined objects, we are planning
to investigate several research directions that can enhance
the accuracy and applicability of our proposed approach.
Particularly, we are planning to explore the following future
research directions:
(1) Although the runtime of our proposed approach is less
than the time range of the employed sliding window, we
believe that there is still a room to improve the runtime of
the proposed approach using parallel computing technology,
which allows the utilization of our approach in various realtime BCI applications.
(2) Our proposed approach utilizes 12 handcrafted TFFs
that are extracted from the CWD computed for the EEG
signals. In spite of the promising results achieved using
these TFFs, the process of designing salient features that
can be used to decode different imagery tasks is considered
challenging. Motivated by the recent promising results attained in analyzing physiological signals using deep learning
approaches [79], [80], we are planning in the future to explore
the use of deep learning methods to learn latent features from
the CWD computed for the EEG signals to alleviate the need
to manually design the features and to enhance the accuracy
of decoding visually imagined objects.
(3) The CWD computed for the EEG signals comprises many
values that are equal or close to zero. As a consequence, this
increases the dimensionality of the computed CWD-based
time-frequency representation of the EEG signals, which in
turn can increase the computational time of the proposed
approach. Recently, researchers have investigated the possibility of using sparse representation classification (SRC)
methods to concisely represent multi-dimensional data, such
as the CWD computed for the EEG signals, and extract
valuable information from the data [81]. For example, Jin
et al. [82] proposed a sparse Bayesian extreme learning
machine classification method to efficiently decode MI tasks.
Motivated by the promising results depicted in [82], [83],
we intend to investigate the possibility of employing SRC
methods to reduce the complexity of our proposed CWDbased approach and enhance the decoding accuracy of visually imagined objects.
(4) As described in section II-B, the size of the images,
which are used as visual stimuli, are varied across the four
categories. However, for each group, all images were config-

ured to have the same size, where the selected size ensures
that the object in the image appears clearly. As part of
our future studies, we plan to investigate the impact of the
size of the images used as visual stimuli on the conducted
visual imagery experiments and the performance of decoding
visually imagined objects.
(5) The EEG dataset employed in the current study comprises
four object categories. To further investigate the relationship
between the performance of the proposed approach and the
number of classes and categories, we plan to acquire a largescale EEG dataset that comprises more object categories and
more objects within each category. In addition, we plan to
investigate the potential of using hierarchical classification
schemes [5], [6] to identify the category of the object at one
level of the classification hierarchy and the class of the object
at another level. Such a hierarchical classification scheme
can simplify the classification problem and enable accurate
classification between a higher number of categories and
more objects inside each category.
(6) The groups of EEG channels that are used in the CBA,
which are depicted in Table 2, were manually created based
on neurophysiologic knowledge. Recently, researchers have
investigated methodologies to automatically select the most
relevant channels to improve classification performance [84],
[85]. Inspired by promising results reported in these studies,
we plan in the future to investigate the possibility of applying
various channel selection algorithms to eliminate redundant
information encapsulated within the EEG channels, reduce
the complexity of the extracted TFFs, and improve the accuracy of decoding visually imagined objects.
V. CONCLUSION

This study explored the feasibility of using the visual imagery
task as a control paradigm to increase the control’s dimensions of EEG-based BCI systems. In particular, we have
introduced a new EEG-based approach for decoding visually
imagined objects that utilizes the CWD to analyze the EEG
signals in the joint time-frequency domain and extract a set
of twelve novel TFFs. The extracted TFFs were used to
build a subject-specific multi-class SVM classifier to decode
visually imagined objects. To validate the performance of the
proposed approach, we have recorded a large EEG dataset
that comprises a wide range of visually imagined objects.
The recorded EEG dataset was employed to conduct two
performance evaluation analyses to evaluate the capability of
our proposed approach to decode different visually imagined
objects. The experimental results presented in the current
study demonstrate the capability of our proposed approach to
accurately decode visually imagined objects based on EEG
signal analysis.
REFERENCES
[1] G. Pfurtscheller, C. Guger, G. Müller, G. Krausz, and C. Neuper, “Brain
oscillations control hand orthosis in a tetraplegic,” Neuroscience letters,
vol. 292, no. 3, pp. 211–214, 2000.
15

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

[2] N. Kosmyna, J. T. Lindgren, and A. Lécuyer, “Attending to visual stimuli
versus performing visual imagery as a control strategy for eeg-based braincomputer interfaces,” Scientific reports, vol. 8, no. 1, pp. 1–14, 2018.
[3] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M.
Vaughan, “Brain–computer interfaces for communication and control,”
Clinical neurophysiology, vol. 113, no. 6, pp. 767–791, 2002.
[4] L. F. Nicolas-Alonso and J. Gomez-Gil, “Brain computer interfaces, a
review,” Sensors, vol. 12, no. 2, pp. 1211–1279, 2012.
[5] R. Alazrai, H. Alwanni, Y. Baslan, N. Alnuman, and M. I. Daoud, “Eegbased brain-computer interface for decoding motor imagery tasks within
the same hand using choi-williams time-frequency distribution,” Sensors,
vol. 17, no. 9, p. 1937, 2017.
[6] R. Alazrai, H. Alwanni, and M. I. Daoud, “Eeg-based bci system for
decoding finger movements within the same hand,” Neuroscience letters,
vol. 698, pp. 113–120, 2019.
[7] K. Liao, R. Xiao, J. Gonzalez, and L. Ding, “Decoding individual finger
movements from one hand using human eeg signals,” PLoS One, vol. 9,
no. 1, p. e85192, 2014.
[8] R. Alazrai, R. Homoud, H. Alwanni, and M. I. Daoud, “Eeg-based emotion
recognition using quadratic time-frequency distribution,” Sensors, vol. 18,
no. 8, p. 2739, 2018.
[9] R. Alazrai, M. Momani, H. A. Khudair, and M. I. Daoud, “Eeg-based tonic
cold pain recognition system using wavelet transform,” Neural Computing
and Applications, Oct. 2017.
[10] R. Alazrai, S. AL-Rawi, H. Alwanni, and M. I. Daoud, “Tonic cold pain
detection using choi–williams time-frequency distribution analysis of eeg
signals: A feasibility study,” Applied Sciences, vol. 9, no. 16, p. 3433,
2019.
[11] L. J. Hadjileontiadis, “Eeg-based tonic cold pain characterization using
wavelet higher order spectral features,” IEEE Transactions on Biomedical
Engineering, vol. 62, no. 8, pp. 1981–1991, Aug. 2015.
[12] E. T. Esfahani and V. Sundararajan, “Classification of primitive shapes using brain–computer interfaces,” Computer-Aided Design, vol. 44, no. 10,
pp. 1011–1019, 2012.
[13] J. Pearson, T. Naselaris, E. A. Holmes, and S. M. Kosslyn, “Mental
imagery: functional mechanisms and clinical applications,” Trends in
cognitive sciences, vol. 19, no. 10, pp. 590–602, 2015.
[14] M. Jeannerod, “Mental imagery in the motor context,” Neuropsychologia,
vol. 33, no. 11, pp. 1419–1432, 1995.
[15] C. Brunner, M. Naeem, R. Leeb, B. Graimann, and G. Pfurtscheller,
“Spatial filtering and selection of optimized components in four class
motor imagery eeg data using independent components analysis,” Pattern
recognition letters, vol. 28, no. 8, pp. 957–964, 2007.
[16] G. E. Fabiani, D. J. McFarland, J. R. Wolpaw, and G. Pfurtscheller,
“Conversion of eeg activity into cursor movement by a brain-computer
interface (bci),” IEEE transactions on neural systems and rehabilitation
engineering, vol. 12, no. 3, pp. 331–338, 2004.
[17] A. Kübler, B. Kotchoubey, J. Kaiser, J. R. Wolpaw, and N. Birbaumer,
“Brain–computer communication: Unlocking the locked in.” Psychological bulletin, vol. 127, no. 3, p. 358, 2001.
[18] Y. R. Tabar and U. Halici, “A novel deep learning approach for classification of eeg motor imagery signals,” Journal of neural engineering, vol. 14,
no. 1, p. 016003, 2016.
[19] G. Pfurtscheller and C. Neuper, “Motor imagery and direct brain-computer
communication,” Proceedings of the IEEE, vol. 89, no. 7, pp. 1123–1134,
2001.
[20] V. J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,
and B. J. Lance, “Eegnet: a compact convolutional neural network for eegbased brain–computer interfaces,” Journal of neural engineering, vol. 15,
no. 5, p. 056013, 2018.
[21] R. Schirrmeister, J. Springenberg, L. Fiederer, M. Glasstetter,
K. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,
“Deep learning with convolutional neural networks for brain mapping and
decoding of movement-related information from the human eeg. arxiv,
2017,” arXiv preprint arXiv:1703.05051.
[22] H. Dose, J. S. Møller, H. K. Iversen, and S. Puthusserypady, “An end-toend deep learning approach to mi-eeg signal classification for bcis,” Expert
Systems with Applications, vol. 114, pp. 532–542, 2018.
[23] A. J. Doud, J. P. Lucas, M. T. Pisansky, and B. He, “Continuous threedimensional control of a virtual helicopter using a motor imagery based
brain-computer interface,” PloS one, vol. 6, no. 10, p. e26322, 2011.
[24] A. S. Royer, A. J. Doud, M. L. Rose, and B. He, “Eeg control of a
virtual helicopter in 3-dimensional space using intelligent control strate-

[25]

[26]

[27]

[28]

[29]

[30]
[31]
[32]

[33]

[34]

[35]

[36]

[37]

[38]

[39]

[40]

[41]
[42]

[43]

[44]
[45]

[46]

[47]
[48]

gies,” IEEE Transactions on neural systems and rehabilitation engineering,
vol. 18, no. 6, pp. 581–589, 2010.
K. LaFleur, K. Cassady, A. Doud, K. Shades, E. Rogin, and B. He,
“Quadcopter control in three-dimensional space using a noninvasive motor
imagery-based brain–computer interface,” Journal of neural engineering,
vol. 10, no. 4, p. 046003, 2013.
M. Taghizadeh-Sarabi, M. R. Daliri, and K. S. Niksirat, “Decoding objects
of basic categories from electroencephalographic signals using wavelet
transform and support vector machines,” Brain topography, vol. 28, no. 1,
pp. 33–46, 2015.
M. G. Philiastides and P. Sajda, “Temporal characterization of the neural
correlates of perceptual decision making in the human brain,” Cerebral
cortex, vol. 16, no. 4, pp. 509–518, 2006.
M. Misaki, Y. Kim, P. A. Bandettini, and N. Kriegeskorte, “Comparison of multivariate classifiers and response normalizations for patterninformation fmri,” Neuroimage, vol. 53, no. 1, pp. 103–118, 2010.
I. Simanova, M. Van Gerven, R. Oostenveld, and P. Hagoort, “Identifying
object categories from event-related eeg: toward decoding of conceptual
representations,” PloS one, vol. 5, no. 12, 2010.
N. J. Thomas, “Mental imagery, philosophical issues about,” Encyclopedia
of cognitive science, 2006.
S. M. Kosslyn and D. N. Osherson, An invitation to cognitive science:
Visual cognition. MIT Press, 1995, vol. 2.
M. Knauff, J. Kassubek, T. Mulack, and M. W. Greenlee, “Cortical activation evoked by visual mental imagery as measured by fmri,” Neuroreport,
vol. 11, no. 18, pp. 3957–3962, 2000.
E. Mellet, N. Tzourio, M. Denis, and B. Mazoyer, “A positron emission
tomography study of visual and mental spatial exploration,” Journal of
Cognitive Neuroscience, vol. 7, no. 4, pp. 433–445, 1995.
D. Pearson, R. De Beni, and C. Cornoldi, “The generation, maintenance,
and transformation of visuo-spatial mental images,” in Imagery, language
and visuo-spatial thinking. Psychology Press, 2012, pp. 17–44.
M. D’Esposito, J. A. Detre, G. K. Aguirre, M. Stallcup, D. C. Alsop,
L. J. Tippet, and M. J. Farah, “A functional mri study of mental image
generation,” Neuropsychologia, vol. 35, no. 5, pp. 725–730, 1997.
J. Fulford, F. Milton, D. Salas, A. Smith, A. Simler, C. Winlove, and
A. Zeman, “The neural correlates of visual imagery vividness–an fmri
study and literature review,” Cortex, vol. 105, pp. 26–40, 2018.
S. D. Slotnick, W. L. Thompson, and S. M. Kosslyn, “Visual memory and
visual mental imagery recruit common control and sensory regions of the
brain,” Cognitive neuroscience, vol. 3, no. 1, pp. 14–20, 2012.
A. M. Albers, P. Kok, I. Toni, H. C. Dijkerman, and F. P. De Lange, “Shared
representations for working memory and mental imagery in early visual
cortex,” Current Biology, vol. 23, no. 15, pp. 1427–1431, 2013.
T. Horikawa and Y. Kamitani, “Generic decoding of seen and imagined
objects using hierarchical visual features,” Nature communications, vol. 8,
no. 1, pp. 1–15, 2017.
P. Bobrov, A. Frolov, C. Cantor, I. Fedulova, M. Bakhnyan, and A. Zhavoronkov, “Brain-computer interface based on generation of visual images,” PloS one, vol. 6, no. 6, 2011.
Y. Wang, P. Wang, and Y. Yu, “Decoding english alphabet letters using eeg
phase information,” Frontiers in neuroscience, vol. 12, p. 62, 2018.
K. N. Kay, T. Naselaris, R. J. Prenger, and J. L. Gallant, “Identifying
natural images from human brain activity,” Nature, vol. 452, no. 7185, pp.
352–355, 2008.
K. A. Norman, S. M. Polyn, G. J. Detre, and J. V. Haxby, “Beyond mindreading: multi-voxel pattern analysis of fmri data,” Trends in cognitive
sciences, vol. 10, no. 9, pp. 424–430, 2006.
B. Boashash, Time-frequency signal analysis and processing: a comprehensive reference. Academic Press, 2015.
M. B. Brodeur, E. Dionne-Dostie, T. Montreuil, and M. Lepage, “The bank
of standardized stimuli (boss), a new set of 480 normative photos of objects
to be used as visual stimuli in cognitive research,” PloS one, vol. 5, no. 5,
2010.
P. Roland and B. Gulyás, “Visual memory, visual imagery, and visual
recognition of large field patterns by the human brain: functional anatomy
by positron emission tomography,” Cerebral Cortex, vol. 5, no. 1, pp. 79–
93, 1995.
S. M. Kosslyn, G. Ganis, and W. L. Thompson, “Neural foundations of
imagery,” Nature reviews neuroscience, vol. 2, no. 9, pp. 635–642, 2001.
T. Horikawa and Y. Kamitani, “Hierarchical neural representation of
dreamed objects revealed by brain decoding with deep neural network
features,” Frontiers in computational neuroscience, vol. 11, p. 4, 2017.

16

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

[49] J. Shin, K.-R. Müller, and H.-J. Hwang, “Eyes-closed hybrid braincomputer interface employing frontal brain activation,” PloS one, vol. 13,
no. 5, 2018.
[50] M. X. Cohen, Analyzing neural time series data: theory and practice. MIT
press, 2014.
[51] G. Gómez-Herrero, W. De Clercq, H. Anwar, O. Kara, K. Egiazarian,
S. Van Huffel, and W. Van Paesschen, “Automatic removal of ocular
artifacts in the eeg without an eog reference channel,” in Proceedings of
the 7th IEEE Nordic Signal Processing Symposium, 2006, pp. 130–133.
[52] B. Boashash and S. Ouelha, “Automatic signal abnormality detection using
time-frequency features and machine learning: A newborn eeg seizure case
study,” Knowledge-Based Systems, vol. 106, pp. 38–50, 2016.
[53] R. Alazrai, M. Abuhijleh, H. Alwanni, and M. I. Daoud, “A deep learning
framework for decoding motor imagery tasks of the same hand using eeg
signals,” IEEE Access, vol. 7, pp. 1–1, 2019.
[54] J. M. O. Toole, “Discrete quadratic time-frequency distributions: definition, computation, and a newborn electroencephalogram application,”
Ph.D. dissertation, School of Medicine, The University of Queensland,
2009.
[55] H.-I. Choi and W. J. Williams, “Improved time-frequency representation
of multicomponent signals using exponential kernels,” IEEE Transactions
on Acoustics, Speech, and Signal Processing, vol. 37, no. 6, pp. 862–871,
1989.
[56] P. Castiglioni, Choi–Williams Distribution. John Wiley & Sons, Ltd,
2005.
[57] S. L. Hahn, Hilbert transforms in signal processing. Artech House Boston,
1996, vol. 2.
[58] L. Cohen, “Time-frequency distributions-a review,” Proceedings of the
IEEE, vol. 77, no. 7, pp. 941–981, Jul. 1989.
[59] ——, Time-frequency analysis. Prentice Hall PTR Englewood Cliffs, NJ,
1995, vol. 778.
[60] A. Swami, J. Mendel, and C. Nikias, “Higher-order spectra analysis (hosa)
toolbox, version 2.0. 3,” 2000.
[61] M. Ahn and S. C. Jun, “Performance variation in motor imagery
brain–computer interface: A brief review,” Journal of Neuroscience Methods, vol. 243, pp. 103–110, 2015.
[62] V. Vapnik, “The support vector method of function estimation,” in Nonlinear Modeling. Springer, 1998, pp. 55–85.
[63] A. Y. Ng and M. I. Jordan, “On discriminative vs. generative classifiers:
A comparison of logistic regression and naive bayes,” Advances in neural
information processing systems, vol. 2, pp. 841–848, 2002.
[64] H. Qian, Y. Mao, W. Xiang, and Z. Wang, “Recognition of human activities
using svm multi-class classifier,” Pattern Recognition Letters, vol. 31,
no. 2, pp. 100–111, 2010.
[65] M. Tavakolan, Z. Frehlick, X. Yong, and C. Menon, “Classifying three
imaginary states of the same upper extremity using time-domain features,”
PloS one, vol. 12, no. 3, 2017.
[66] X. Yong and C. Menon, “Eeg classification of different imaginary movements within the same limb,” PloS one, vol. 10, no. 4, 2015.
[67] U. H.-G. Kreßel, “Pairwise classification and support vector machines,” in
Advances in kernel methods. MIT Press, 1999, pp. 255–268.
[68] C.-W. Hsu and C.-J. Lin, “A comparison of methods for multiclass support
vector machines,” IEEE Trans. Neural Networks, vol. 13, no. 2, pp. 415–
425, 2002.
[69] C.-W. Hsu, C.-C. Chang, C.-J. Lin et al., “A practical guide to support
vector classification,” Department of Computer Science, National Taiwan
University, Taipei, Taiwan, Tech. Rep., 2003.
[70] Z. Qiu, B. Z. Allison, J. Jin, Y. Zhang, X. Wang, W. Li, and A. Cichocki,
“Optimized motor imagery paradigm based on imagining chinese characters writing movement,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. PP,
no. 99, pp. 1–1, 2017.
[71] H. Peng, F. Long, and C. Ding, “Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy,”
IEEE Transactions on pattern analysis and machine intelligence, vol. 27,
no. 8, pp. 1226–1238, 2005.
[72] D. M. Powers, “Evaluation: from precision, recall and f-measure to roc,
informedness, markedness and correlation,” 2011.
[73] S.-M. Zhou, J. Q. Gan, and F. Sepulveda, “Classifying mental tasks based
on features of higher-order statistics from eeg signals in brain–computer
interface,” Information Sciences, vol. 178, no. 6, pp. 1629–1640, 2008.
[74] U. R. Acharya, H. Fujita, V. K. Sudarshan, S. Bhat, and J. E. Koh,
“Application of entropies for automated diagnosis of epilepsy using eeg
signals: a review,” Knowledge-Based Systems, vol. 88, pp. 85–96, 2015.

[75] S. P. van den Broek, F. Reinders, M. Donderwinkel, and M. Peters,
“Volume conduction effects in eeg and meg,” Electroencephalography and
clinical neurophysiology, vol. 106, no. 6, pp. 522–534, 1998.
[76] G. Ganis, W. L. Thompson, and S. M. Kosslyn, “Brain areas underlying
visual mental imagery and visual perception: an fmri study,” Cognitive
Brain Research, vol. 20, no. 2, pp. 226–241, 2004.
[77] B. Blankertz, C. Sannelli, S. Halder, E. M. Hammer, A. Kübler, K.-R.
Müller, G. Curio, and T. Dickhaus, “Neurophysiological predictor of smrbased bci performance,” Neuroimage, vol. 51, no. 4, pp. 1303–1309, 2010.
[78] M. Ahn, H. Cho, S. Ahn, and S. C. Jun, “High theta and low alpha powers
may be indicative of bci-illiteracy in motor imagery,” PloS one, vol. 8,
no. 11, 2013.
[79] X. Zhang, L. Yao, X. Wang, J. Monaghan, D. Mcalpine, and Y. Zhang, “A
survey on deep learning based brain computer interface: Recent advances
and new frontiers,” arXiv preprint arXiv:1905.04149, 2019.
[80] O. Faust, Y. Hagiwara, T. J. Hong, O. S. Lih, and U. R. Acharya, “Deep
learning for healthcare applications based on physiological signals: A
review,” Computer Methods and Programs in Biomedicine, vol. 161, pp.
1–13, 2018.
[81] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust
face recognition via sparse representation,” IEEE transactions on pattern
analysis and machine intelligence, vol. 31, no. 2, pp. 210–227, 2008.
[82] Z. Jin, G. Zhou, D. Gao, and Y. Zhang, “Eeg classification using sparse
bayesian extreme learning machine for brain–computer interface,” Neural
Computing and Applications, pp. 1–9, 2018.
[83] D. Wen, P. Jia, Q. Lian, Y. Zhou, and C. Lu, “Review of sparse
representation-based classification methods on eeg signal processing for
epilepsy detection, brain-computer interface and cognitive impairment,”
Frontiers in aging neuroscience, vol. 8, p. 172, 2016.
[84] J. Jin, Y. Miao, I. Daly, C. Zuo, D. Hu, and A. Cichocki, “Correlationbased channel selection and regularized feature optimization for mi-based
bci,” Neural Networks, vol. 118, pp. 262–270, 2019.
[85] H. Chang and J. Yang, “Automated selection of a channel subset based
on the genetic algorithm in a motor imagery brain-computer interface
system,” IEEE Access, vol. 7, pp. 154 180–154 191, 2019.

RAMI ALAZRAI received the Ph.D. degree in
electrical and computer engineering from Purdue
University, West Lafayette, IN, USA, in 2013.
In June 2013, he joined the school of electrical
engineering and information technology at the
German Jordanian University (GJU), where he
is currently an associate professor. His research
interests include machine learning, biomedical image and signal processing, brain-computer interfaces, decoding actual and imaginary motor tasks
using EEG signals, decoding visually imagined objects, EEG-based emotion
identification, EEG-based pain analysis, semantic video analysis, human
activity recognition, emotion recognition, human-to-human interaction representation and analysis, and elderly fall detection.

AMAL AL-SAQQAF received her B.S. degree in
software engineering from Taiz University, Taiz,
Yemen, in 2012, and worked as a teaching assistant in the Software Engineering Department at the
same university. She is currently completing her
M.S. degree in computer engineering at the German Jordanian University, Amman, Jordan. Her
research interests include machine learning, EEG
signal processing, and brain-computer interfaces.

17

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

This article has been accepted for publication in a future issue of this journal, but has not been fully edited. Content may change prior to final publication. Citation information: DOI
10.1109/ACCESS.2020.3012918, IEEE Access

HISHAM ALWANNI received the B.S. degree in
mechatronics engineering from the German Jordanian University, Amman, Jordan, in 2016. He
is currently pursuing his master’s degree with
the faculty of engineering, University of Freiburg,
Germany. His research interests include EEG signal processing, brain-computer interfaces, deep
learning in medical image analysis and virtual
reality.

FERAS AL-HAWARI received the B.S. degree
in ECE from Jordan University of Science and
Technology, Irbid, Jordan, in 1993, the M.S. degree in CE from Florida Institute of Technology,
Melbourne, USA, in 1995, and the Ph.D. degree
in EE from Northeastern University, Boston, USA,
in 2007. He is currently an Associate Professor
of Computer Engineering and Director of the Information Systems and Technology Center in the
German Jordanian University. He is the founder of
the software development division at the center. Previously, he worked for
Cadence Design Systems Inc., Chelmsford, USA, from 1995 to 2012. He
was a Senior Member of the Consulting Staff as well as a Technical Lead
for the circuit simulation and signal integrity analysis technologies in the
Allegro Printed Circuit Board (PCB) SI R&D Group. Dr. Al-Hawari has
seven USA patents and several other publications. He is an associate editor
of the IEEE Transactions on Components, Packaging and Manufacturing
Technologies (TCPMT). His research interests include software engineering,
application security, ERP tools development, signal integrity, PCB design,
circuit simulation, IoT, and Big Data.

MOHAMMAD I. DAOUD received the Ph.D.
degree in electrical and computer engineering
from the University of Western Ontario, London,
Canada, in 2009. During his graduate studies, he
was awarded an NSERC PGS-D scholarship and
served as a Scholar of the Canadian Institutes
of Health Research/University of Western Ontario
Strategic Training Initiative in Cancer Research
and Technology Transfer. After finishing his graduate studies, he worked in the department of electrical and computer engineering at the University of British Columbia, Vancouver, Canada as a Postdoctoral Research Fellow, where he held an NSERC
Postdoctoral Fellowship. In September 2011, he joined the department of
computer engineering at the German Jordanian University, Amman, Jordan,
where he is currently an associate professor. His research interests include
medical image and signal processing, 3D ultrasound imaging, image-guided
surgery, computer-aided diagnosis systems, machine learning, deep learning,
and parallel computing.

18

VOLUME 4, 2016

This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/.

