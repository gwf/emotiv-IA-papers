A Wearable Remote Brain Machine Interface using Smartphones and the
Mobile Network
Prashanth Shyamkumar1, a, Sechang Oh1,b , Nilanjan Banerjee2,c , Vijay K.
Varadan1,d
1

Dept. of Electrical Engineering, University of Arkansas, Fayetteville, AR-72701, USA.

2

Dept. of Computer Science and Computer Engineering, University of Arkansas, Fayetteville, AR –
72701,USA
a

pshyamku@uark.edu, bsxo005@uark.edu, cnilanb@uark.edu, dvjvesm@uark.edu

Keywords: Smartphones, EEG, EOG, Brain machine interface, smart textiles, cloud computing,

home automation, assisted living.
Abstract: A Remote Brain Machine Interface (RBMI) can be defined as a means to control a
machine that is in a different geographical location than the user. Thus far, simulations for such
interfaces using multiple channels of non-invasive EEG signals acquired through tethered systems
have been used for control of vehicles in military and exploratory applications, and for ongoing
research on RBMI controlled robotic surgery. However, simple applications of RBMI in home
automation for the elderly, low cost assistive devices for the disabled, home security etc can be built
using fewer and more portable sensor systems. As a case study, we have implemented such an
interface using a smartphone for the RBMI. The system consists of a wearable Bluetooth-enabled
head band with dry electrodes for EEG and EOG signals, a smartphone to collect and relay the data,
a laptop with internet connectivity at a remote location to retrieve the data and generate control
commands. In this paper, we describe the information architecture, the design of the wearable
nanosensors and algorithms for control command generation based on EEG and EOG. A selected
demonstration will be shown.
Introduction
Approximately 10% of the world’s population, an estimated 650 million people, live with various
forms of disability such as visual, auditory, cognitive or mobility impairment. The rate of disability
has also been rising during the past few decades. Studies have shown that there is a direct
correlation between the rate of disability, and age. Rates of disabilities among the ageing population
between 80 and 89 years is the fastest growing demographic in the world and is projected to account
for 20% of the disabled global population 60 years or older, by 2050 [1]. There is growing
awareness of this situation and research into such areas as wearable computers [2], context aware
software applications [3] and low cost sensors and systems for assistive technologies has progressed
rapidly over the last two decades. However, all of the research thus far has focused on individual
design paradigms like mobility, access to computers or communication tools. In reality, disability is
an evolving concept, the requirements or means by which disabled individuals interact with their
environment evolves on a daily basis. For example:- a blind person may find himself/herself having
to use a computer that does not have a screen-reading software. The design paradigm thus boils
down into an evolving human machine interaction scenario where the user, the disabled individual,
should be able to interact with his/her environment, as desired, using an assistive device/machine
with no perceivable limitations.
Automatic inference of a disabled individual’s intent, to control devices and home appliances
obviates the need for the intervention of a care provider in the performance of day-to-day activities
and provides a higher level of independence for them. The means to achieve this is through a brain
machine interface. The present state-of-the-art brain machine interfaces suffer from several
problems. Firstly, these systems depend on intrusive sensors that cannot be worn by individuals

continuously. For instance, wet EEG electrodes use electrolytic gels to improve contact impedance--the drying of this gel can cause skin irritation, infection, and lead to noisy data. Secondly, state-ofthe-art systems do not allow remote control of devices. For example, if the disabled individual is
outside his/her home, he/she cannot control the home security system. Third, most RMBI systems
do not reliably capture human intent from noisy data from EEG/EOG sensors. To mitigate the above
challenges, this paper presents a remote brain machine interface system that can control devices
remotely from any geographic location. The system uses highly non-intrusive and sensitive
wearable electrodes for EEG and EOG monitoring. Our system uses smart phones as a data relay for
EEG/EOG data analysis. The smartphone also acts as a computational platform to infer human
intent. The inferred intent is registered as a human gesture or action at a backend server. The server
can be used to control devices at any geographic location. We have designed, implemented, and
evaluated our fully functional RBM interface and demonstrate it efficacy in remotely controlling
robotic elements.
A Case for RBMI
In addition to home automation for the disabled,
injured, and elderly, there are several other applications
that can leverage RBM intefaces. For instance, a
plausible scenario where such interfaces can be useful is
firefighting---the use of wearable sensors controlling
robotic elements can mitigate the need to carry heavy
instrumentation inside a burning building. Such a
system can also be used in a military scenario where
vehicles can be controlled while averting the need to
explicitly carry a remote control instrument. RMBIs can
also be used to control devices inside a car when a
driver is on a highway where it is important to focus on
Figure 1: Architecture for the RBMI System
driving. Additionally, remote brain machine interfaces
can help control appliances that are not within range of
short range radios such as Bluetooth or Infra-red. To facilitate wide area control of devices using
remote brain machine interfaces, our architecture combines sensors, smartphones, and backend
servers. The architecture is illustrated in Figure 1.
Related Work
Our work builds on previous work in the areas of wearable computing, use of smartphones for
remote health diagnostics, and human computer interaction interfaces with focus on assistive
technology. For example, applications such as the personal social assistant [5], establish the
potential use of smartphones as a surrogate to deliver assistive technology for people with special
needs. Wireless control of tongue-driven assistive technologies has been designed using
smartphones [6]. A multimodal human computer interaction approach implemented through the
INREDIS platform can help disabled people use ATMs [7]. A demonstrative application for brain
smartphone interaction using an iPhone and a commercially available electroencephalogram (EEG)
headset (Emotiv EPOC EEG headset), dubbed the NeuroPhone, has been developed [8]. While
these efforts have attempted to solve the RMBI problem, they fail to provide important modalities
of non-intrusiveness and remote device control. Most of the aforementioned systems use off-theshelf sensor components and have focused on data analysis. The sensors unfortunately cannot be
used for continuous use by individuals. Additionally, these systems rely on short range wireless
technology such as Bluetooth and cannot provide truly remote control of appliances, vehicles, or
devices. Our proposed system combines non-intrusive nanostructured sensors, smartphones, and
backend servers to capture user gestures to control devices that can be located anywhere.

Proposed System
In this paper, we demonstrate our remote brain machine interface through a prototype system that
consists of a simple headband with gold nanostructured electrodes. These electrodes are used to
acquire signals reflecting user intent through EEG/EOG. A smartphone is used to relay this
information to a remote server where a server resident application generates commands based on the
signal. The commands are used to control a robotic car that is not co-located with the individual.

Figure 2(a): (left) topview of the sonsor module snapped on the headband (right) snap button male and female
between the module and the headband (b) Gold nanowire electrodes on male button connected to the headband
[9] (c) Conductive textile electrode sensors on the headband.

The robotic device is controlled by a personal computer that polls the server for commands once
every 500 milliseconds. We have chosen to use a Brain Machine Interface (BMI) implementation
because of its wide ranging applications in both assistive technologies as well as for personnel in
high risk situations like combat or fire fighting.
System Implementation and Preliminary Evaluation. We have implemented a fully functional
RMBI system. The system consists of three distinct components: (1) electrodes and sensors module
with analog conditioning and embedded software; (2) a frame-based digital signal processing
decision algorithm that infers users intent from EEG/EOG data; and (3) a smartphone application
with the inference algorithm and a backend server that acts as a surrogate for relaying commands to
remote devices. Here we describe each component of the system in detail.
Sensors. The signals used to assess user intent are bio-potential signals. The signal is acquired as
the potential difference between two electrodes placed across the forehead. This electrode
placement is not a conventional placement meant for Electrooculogram (EOG) measurements. It
was chosen because the signal response to left and right movement of the eyes was strong enough to
use as a command signal. Two types of electrodes were used in the evaluation and development of
this system, namely - gold nanowire electrodes and conductive textile electrodes made of silver
coated nylon. The use of these two types of dry electrodes for the acquisition of biopotential signals
(ECG) is discussed in detail in our previous work [9]. The electrodes used are dry, i.e., they do not
require application of a gel and minimal to no skin preparation to acquire good quality biopotential
signals. Given their non-intrusiveness, they might be best suited for wearable sensor systems. The
wearable system including the headband, sensors and sensor module attachment is depicted in the
Figure 2.

Sensor Module. The sensor module consists of three components – a high gain biopotential
amplifier to enhance the weak microvolt scale biopotential signals, a microcontroller (Atmega328P)
that performs the analog to digital conversion at a sampling frequency of 100 Hz and a Bluetooth
module (SPBT2532). The power source is a 250 mAh poly-lithium battery. The bio-potential
amplifier is a three stage amplifier with an overall gain of 65.44 dB and a 3dB bandwidth of 1.45Hz
to 40 Hz set using active RC and butterworth filters [10].
Data Inferencing Algorithm. We have developed a simple and robust inferencing algorithm that
is light weight for implementation on a resource constrained smartphone or a home-resident
personal computer. The inferencing algorithm is used to analyze the data received from the sensors
to extract intent. The algorithm is frame based---data collected over the previous 2 seconds is used
to recognize patterns that qualify as command signals. There are principally three commands that
are generated to control our robotic car- Left turn, right turn, and forward speed.
The turn detection algorithm consists of the following five steps. (1) We use a band pass filter
with a second order digital butterworth filter with a pass band of 5Hz to 15 Hz. (2) The filtered
signal is smoothed using an integration function that performs windowed averaging of the previous
three data points and assigns the value to the current data point. (3) Consequently, an adaptive
threshold is applied to the signal. The threshold is calculated as a sum of fractions of the signals
local noise peak and local signal peak. This method was adapted from previous work [11]. (4) A
fixed threshold is then applied at 0.5 times the maximum. (5) Finally, the data points that surpass
the positive threshold are assigned the values 1. Five is assigned to the data points that surpass the
negative threshold. All other points are assigned 0. The distinction between right and left signals is
made by finding the sequence of occurrences of these square waves.
The data points in the signal that surpass both the negative and positive thresholds are indicative
of deliberate eye movements intended as gestures. The progression of two consecutive commands
of left and two of right, the corresponding signals recorded and the various steps of the decision
algorithm are depicted below in Fig 3. The amplitudes for these signals are treated as arbitrary
dimensions and bare no relevance to the functioning of the algorithm as the amplitude thresholds
are adaptive. However, the amplitudes in the original signals Fig 3 (a) and (f) are 10-bit analog to
digital conversion values that range from 0 to 1024.

Figure 3(a) through (e) are figures depicting the steps of the algorithm for detection of right signal, (f) through
(j), for left signal.

The forward speed determination algorithm - The ratio of power in the alpha wave (of EEG)
frequency band of 8-12 Hz to the power in the beta wave (of EEG) frequency band of 12-30Hz is
used to compute the forward speed for the toy car when no turn signals are detected. The
determination and validation of these two algorithms was performed in our previous study [10].

Software Infrastructure. The software infrastructure for the system consists of three components
(1) smartphone relaying software: an application that collects the data from the sensor through
Bluetooth and processes the signals. (2) server software: an application that relays the commands.
(3) target machine software: an application on a personal computer that reads and relays the desired
commands from the server to the target machine to be controlled using RBMI. We use a PC in this
demonstration assuming that the eventual target machine will have a 3G or other mobile network
compatible modem to directly receive the commands. The smartphone used for the application was
the HTC pure with a Windows Mobile 6.5 operating system. While the decision algorithm runs
locally on the smartphone, it could be resident at the backend server, if required. The target
machine used in this RBMI system was a PC. The application on the PC was written and developed
using MATLAB. The application simply associated with the server and performs a URL read
operation every 500 msecs and relays the command retrieved to a Lego Mindstorms NXT Brick,
used to make a toy car.
We have performed preliminary latency measurements for processing and inferring the data on
the HTC smartphone. Our measurements show that the average latency for processing 118 seconds
of data with 22 occurrence of eye movement is 4.3 seconds. Therefore, the computational overhead
in terms of time for 1 second of data is 36.4 msecs. Additionally, the system has 3 false positives,
that makes the accuracy 86.36%.
Future Enhancements
In our application, the real-time data relay does not require significant bandwidth because we
transmit a signal of unit dimension. We recognize that this may be an impediment to the deployment
of future systems based on multiple sensors. There are two solutions to this problem. First, data
compression before transmission based on the type of sensor data will lower the amount of data to
be transmitted. Second, running part of the decision algorithm on the smartphone, applying
computationally simple noise rejection algorithms and sending only the significant extracted
features to the server will reduce the data transferred. These features can then be statistically
matched with a cloud database.
Acknowledgments
This work was supported by a research endowment for smart textiles and nanosensors from
Global Institute of Nanotechnology in Engineering and Medicine Inc (700 Research Center Blvd,
Fayetteville, AR, 72703). This work is also supported in part by NSF awards CNS-1018112, CNS1055061, CNS-1115728, and IIP-1158759. Any opinions, findings, and conclusions expressed in
this work are those of the authors and do not necessarily reflect the views of the NSF.
References
[1] WHO – World Report on Disability (2011)
[2] D.A. Ross, Implementing assistive technology on wearable computers, IEEE Intelligent Systems,
16, 3, (2001) 47- 53.
[3] J. Vales-Alonso, E. Egea-Lopez, J.P. Muoz-Gea, J.Garcia-Haro, F. Belzunce-Arcos, M.A.
Esparza-Garcia, J.M. Perez-Maogil, R. Martinez-Alvarez, F. Gil-Castieira, F.J. Gonzalez-Castao,
UCare: Context-Aware Services for Disabled Users in Urban Environments. In Ubicomm (2008)
197-205.
[4] W. Song, X. Su, Review of Mobile cloud computing, IEEE 3rd International Conference on
Communication Software and Networks (ICCSN), (2011) 1-4.
[5] S. Verstockt, D. Decoo, D.Van Nieuwenhuyse, F.De Pauw, R.Van de Walle, Assistive
smartphone for people with special needs : The Personal Social Assistant, In HSI 2009.

[6] J. Kim, X. Huo, J. Minocha, J. Holbrook, A. Laumann, M.Ghovanloo, Evaluation of a
Smartphone Platform as a Wireless Interface Between Tongue Drive System and Electric-Powered
Wheelchairs, IEEE Transactions on Biomedical Engineering,59, 6, (2012) 1787-1796
[7] M.Pous, C.Serra-Vallmitjana, R.Gimenez, M.Torrent-Moreno, D.Boix, Enhancing accessibility:
Mobile to ATM case study, IEEE Consumer Communications and Networking Conference (CCNC),
(2012) 404-408.
[8] A.Campbell, T.Choudhury, S.Hu, H.Lu, M.K.Mukerjee, M.Rabbi, R.D.S.Raizada, NeuroPhone:
Brain-mobile phone interface using a wireless EEG headset, In Proceedings of MobiHeld (2010) 38.
[9] V. K. Varadan, P. S. Kumar, S. Oh, H. Kwon, P.Rai, N. Banerjee, R. E. Harbaugh, e-Nanoflex
Sensor System: Smartphone-Based Roaming Health Monitor, J. Nanotechnol. Eng. Med.2,1 (2011)
011016 -011027.
[10] S. Oh, P. S. Kumar, H. Kwon, V. K. Varadan, Wireless brain-machine interface using EEG and
EOG: brain wave classification and robot control, Proc. SPIE - Int. Soc. Opt. Eng.8344, (2012)
83440U
[11] J. Pan, W. J. Tompkins, A Real-Time QRS Detection Algorithm, IEEE Transactions on
Biomedical Engineering, 32, 3, (1985) 230-236.

