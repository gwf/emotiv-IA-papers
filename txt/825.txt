The Role of Personalization and Multiple EEG and Sound Features
Selection in Real Time Sonification for Neurofeedback
S. Mealla1 , A. Oliveira1 , X. Marimon2 , T. Steffert3 , S. Jordà1 and A. Väljamäe4,5
1 Music

Technology Group (MTG), Department of Information and Communication Technologies (DTIC), Universitat
Pompeu Fabra, Barcelona
2 BarcelonaTECH, LASSIE Lab, Automatic Control Department, Universitat Politecnica de Catalunya, Barcelona
3 Centre for Research in Computing,The Open University, Milton Keynes, UK
4 Department of Higher Nervous Activity, St. Petersburg State University, St. Petersburg, Russia
5 Department of Behavioural Sciences and Learning, Linköping University, Linköping, Sweden
{sebastian.mealla, sergi.jorda}@upf.edu, {aluiziob.oliveira, xavier.marimon.serra}@gmail.com, tony@qeeg.co.uk,
aleksander.valjmae@liu.se

Keywords:

Sonification, EEG, alpha/theta neurofeedback, Physiological Computing, Pure Data, Sound, Real Time

Abstract:

The field of physiology-based interaction and monitoring is developing at a fast pace. Emerging applications
like fatigue monitoring often use sound to convey complex dynamics of biological signals and to provide an
alternative, non-visual information channel. Most Physiology-to-Sound mappings in such auditory displays
do not allow customization by the end-users. We designed a new sonification system that can be used for
extracting, processing and displaying Electroencephalography data (EEG) with different sonification strategies. The system was validated with four user groups performing alpha/theta neurofeedback training (a/t) for
relaxation that varied in feedback personalization (Personalized/Fixed) and a number of sonified EEG features (Single/Multiple). The groups with personalized feedback performed significantly better in their training
than fixed mappings groups, as shown by both subjective ratings and physiological indices. Additionally, the
higher number of sonified EEG features resulted in deeper relaxation than when training with single feature
feedback. Our results demonstrate the importance of adaptation and personaliziation of EEG sonification according to particular applications, in our case, to a/t neurofeedback. Our experimental approach shows how
user performance can be used for validating different sonification strategies.

1

INTRODUCTION

Recent advances in human physiological sensors
and signal processing techniques have fostered the
development of computer systems for online control, training and monitoring of various cognitive
and affective states (Allanson and Fairclough, 2004).
In addition, an increasing number of consumer ofthe-shelf devices and open source platforms transform physiology-based interaction into an important
part of the Human Computer Interaction (HCI) field.
Systems like the Emotiv EPOC (Emotiv, 2013), or
Starlab’s Enobio (Neuroelectrics, 2013) now allow a
low cost and wearable electroencephalography (EEG)
sensing in out-of-the-lab, real life conditions, increasing “external validity” of conducted research.
Sonification techniques have been widely used
to display EEG activity and are often an integral
part of physiological computing. Besides being ide-

ally suited for encoding complex dynamic structures
due to auditory perception characteristics (Guttman
et al., 2005), sound provides a complementary sensory channel complementing visual displays. Hence,
EEG sonification is a growing field of auditory displays that embrace several application domains including HCI, Brain-Computer Interfaces (BCI), EEGbased music, neurofeedback (NF) and EEG monitoring (see (Väljamäe et al., 2013) for a review).
The selection of functional EEG-to-Sound transformations is a difficult task involving perceptual and
aesthetical trade-offs depending on their application
and goals (Hermann et al., 2002). On the one hand,
simple EEG-to-sound mappings allow direct perception of changes in the EEG, producing almost reversible signals, e.g., EEG time compression with the
purpose of pre-screening specific brain activity events
(Khamis et al., 2012). However, such sonifications
tend to be unnatural and not well suited for hearing

out multiple EEG events. On the other hand, more indirect transformations provide more naturalistic, rich
and perceptually pleasant sonifications. The trade-off
here is an arbitrary mapping, as, for example, with
EEG driven musical content in (Mullen et al., 2011)
that hides original physiological features from listeners. In this context, end-users impression of the designed sonifications becomes a crucial factor for the
application success.
Regrettably, most of the available EEG sonifications are working with a constant and predefined number of EEG features, and also apply fixed, predefined
EEG-to-Sound mappings that cannot be changed by
end users. De Campo and colleagues conducted one
of the few studies addressing the personalization of
EEG sonification, i.e. adjustment of display parameters by end users (De Campo et al., 2007). Tests were
done with medical specialists performing evaluation
of EEG data containing epileptic events and seizures
and showed the importance of sonification parameters
customization.
To study both varying content (richness) and personalization aspects of auditory display for physiological data, we created a real-time sonification system that accounts for a flexible multi-parametric EEG
signal transformation into sound, also allowing users
to personalize EEG-to-Sound mappings in real time.
We tested our EEG sonification system in an empirical study based on well-established alpha/theta
(a/t) neurofeedback training paradigm (see (Gruzelier,
2009) for a review). During this training, users try to
relax with their eyes being closed. The most important moment is the so-called theta/alpha “crossover”
when alpha activity slowly subsides accompanying
sleep onset and theta activity becomes more dominant
(Egner et al., 2002). The increase of t/a power ratio
with eyes closed is a well known accompaniment of
states of deep relaxation such as stage 1 of sleep and
meditation (Gruzelier, 2009).
As the a/t NF training is typically based on auditory feedback, it is a good candidate to validate the
effectiveness of personalization and sonification richness of our auditory display. We used our sonification system to present a pleasant soundscape contingent to the production of theta, alpha, and/or t/a ratio. Specifically, we hypothesized that a/t NF training
would have:
• Hypothesis 1 - More impact for participants that
are using personalized sonification mappings than
using pre-defined, fixed sound mappings;
• Hypothesis 2- More impact for participants using multiple EEG feature sonification compared
to one only based on t/a ratio;

Figure 1: Components and data streams of the sonification
system: EEG signal acquisition, signal extraction and processing, and sonification engine.

2

METHODS

2.1

Equipment and Instrumentation

The system is composed of three main blocks: (1)
EEG signal acquisition, (2) signal extraction and processing, and (3) the sonification engine. It allows
designers to build customized and versatile mapping
definitions. Figure 1 shows the architecture of the
sonification system.
2.1.1

EEG signal acquisition

EEG data is acquired using an Emotiv EPOC, a wireless, non-invasive 14-channel EEG headset (Emotiv,
2013). The EPOC is one of the leaders in low-cost
consumer EEG sensing devices. Although it can not
be reliably used for most BCI applications since its
signal-to-noise levels are lower than in medical devices (Duvinage et al., 2013), it has been used successfully for user states monitoring (Rodrı́guez et al.,
2013). The EPOC’s electrodes require a saline solution to improve conductivity, and their placement is
related to the 10-20 system. EPOCs data is sampled
at 128 Hz and low- and high-pass filtered internally at
85 Hz and 0.16 Hz respectively. This semi-raw data is
accessed through the EPOC SDK (Emotiv, 2013) and
then accessed by Matlab/Simulink model.
2.1.2

Signals extractions and processing

We designed a toolbox for Matlab/Simulink to process EPOCs EEG data, both online and offline, and to
feed the sonification engine. A number of processing
blocks described below allow extracting EEG features
in a modular way, adapting to different neurofeedback
or monitoring strategies.
• Emotiv2Simulink, based on a Mex S-function and
on the drivers provided by the manufacturer, it allows the access to the raw data of the EPOC.
• BP filters, Butterworth IIR discrete band-pass
(BP) filters that can be tuned to any given custom

frequency.
• Envelope, is a block that squares the input signal
and then applies a FIR low-pass filter and downsampling to estimate its envelope.
• MaxMin, is a block based on a configurable window of size L; it estimates the maximum, minimum and median value of a signal in the temporal
and frequency domain.
• Hjorth Descriptors, calculate the Hjorth parameters for activity, mobility, and complexity (Liang
et al., 2013)(Hjorth, 1970).
• OSCSend, allows direct communication via UDP
with virtually any modern real-time sound synthesis environment through the Open Sound Control
(OSC) protocol(Wright, 2005).
2.1.3

Sonification engine

A versatile sonification engine was developed using
PureData (Pd) (Puckette et al., 1996). The engine has
been built following a modular approach that allows
sonification designers to choose among different EEG
features (processed in the Simulink toolbox) and their
number, and to define a certain EEG-to-Sound mapping. Although the system has been tested with only
3 EEG features feeding 5 sound modules in parallel,
there are no restrictions on the number of modules
to be displayed, other than hardware limitations (i.e.
processing power). Multiple EEG streams also can
feed a single sound module. Importantly, the engine
also allows further adjustment by end users via MIDI
controllers.
Below we provide a description of the different
modules of the sonification engine. For flexibility and
easiness, all inputs of the sonification engine modules
are normalized between 0 and 100.
Pedal module. A fixed pitch tonic sound is initialized on loading (D2, MIDI note no 38). A tremolo
effect is applied and phase shifted to each of the even
harmonics, giving a slowly moving chorus-like timbre to the drone. Inputs for this module are able to
control tremolo, panning and gain.
Melody module. The melody is constructed using a major scale stemming from five semitones (one
fourth) bellow the central tonic to sixteen semitones
above it (major third). The slope of the incoming signal controls note triggering speed and pitch, whereas
the volume is controlled by a linear function of the
input signal.
Rewarding module. This module triggers short
duration nature sounds (birds, owls, and crickets)
when the input signal goes over a predefined threshold
value. Therefore, every time the input signal is over

the threshold set, the user is rewarded with a more
varying yet still relaxing soundscape. A dynamic amplitude panning allows use of spatial audio.
Wind model. Based on a procedural audio, this
module use a series of white noise generators to
generate a wind soundscape. The use of procedural audio (in contrast to a sample-based approach)
gives a complete control over sound parameters (e.g.,
wind speed). Therefore, the perceptual quality of
the windy scene dynamically changes according to
mapped EEG features.
Rain model. Based on a sample-based approach,
this module preserves soundscape fidelity using dynamic cross fading between different rain excerpts. It
also allows a comparison with the procedural audio
approach of the Wind Module. Input signal modulates the amount of excerpts in the resulting composite Rain density, which varies from a light rain to a
small thunderstorm.
Graphic User Interface and Personalization. A
graphic user interface (GUI) was created to simplify
the patching between different sound modules and
EEG features. It also contains time and frequency
signal monitoring scopes for both input (EEG) and
output (sound) data. The GUI also displays tools for
users’ personalization via MIDI controller, allowing
them to adjust their mapping “on the fly” while listening to the sonification results in real-time.
Users are able to personalize, i.e. adjust, the mappings as follows. One EEG feature is associated
to one sound element (e.g. alpha relative power is
mapped to Rain density). The user is then allowed
to change the mapping in a continuous scale from -1
to +1. If the potentiometer is dragged to -1, the mapping is inversed, i.e. a greater value of the data feature
yields a smaller value for the sound parameter (more
alpha would lead to less rain). If it is positioned at 1,
the mapping is positive (more alpha yields more rain).
In the middle (zero), the sound is fixed and not influenced by the data (a fixed amount of rain equivalent
to 50% of its maximum value, independently of the
alpha power).
2.1.4

Settings used in the experiment

Figure 2 shows the system configuration for our evaluation study.
The EEG toolbox has been configured to extract
three main EEG features, according to the a/t NF protocol (Gruzelier, 2009). In accordance with previous NF studies (Egner et al., 2002), alpha relative
power (8 − 13 Hz range) is calculated from the occipital area where this type of activity occurs during
close-eye conditions (Kropotov, 2010). The second
feature, theta relative power (4 − 7.5 Hz range) is cal-

Table 1: Configuration of the Sound Engine for each group, defined by type of mapping (Personalized or Fixed) and number
of EEG features (Single or Multiple). EEG features include t/a ratio, and relative power of Alpha and Theta). “ Personalized”
stands for adjusted mappings driven by EEG, “Fixed” stands for fixed mapping driven by EEG, and “Constant” stands for
sound feature held constant.

Group
F-S
P-S
F-M
P-M

Foreground
t/a ratio (Fixed)
t/a ratio (Personalized)
t/a ratio (Fixed)
t/a ratio (Personalized)

Rewarding
t/a ratio
t/a ratio
t/a ratio
t/a ratio

Wind
Constant
Constant
theta (Fixed)
theta (Personalized)

Rain
Constant
Constant
alpha (Fixed)
alpha (Personalized)

change yield ascending pitches while negative
ones leads to descending pitches. Higher rate
of change yields bigger jumps in melody (more
semitones between two consecutive notes). The
note volume is a linear function of t/a ratio.
• Rewarding module: Threshold is set at 50% of the
calibrated maximum value of t/a ratio recorded
prior to the main training session.
• Wind model: Theta relative power controls wind
speed of the modeled sound object - the higher is
the input, the faster the wind will “blow”.
• Rain model: Alpha relative power is used to control Rain density.

Figure 2: Processing stages of EEG features and corresponding sound mappings

culated from the activity of all 14 channels, as cortical theta rhythms are small and diffuse when picked
up by scalp electrode, arising almost entirely from the
cerebral cortex (Kropotov, 2010). These two relative
powers are obtained by dividing the band power by
the overall signal power. In this manner, the output
signal is kept within a range of 0 to 1. Finally, the
third feature, t/a power ratio, is estimated as the main
measure for the a/t NF procedure. A spatial filtering is
applied to give more weight to the occipital channels
in the calculation of the envelope of the alpha band
and alpha-theta ratio. For the calculation of the envelope of theta, equal weight for all channels is used.
The envelope is estimated for all three EEG features
using the envelope block based on FIR-based filter
(order 35).
The sound engine has been configured as follows
(see also table 1):

The experiment was conducted in a room isolated from external noise. Participants were seated
on a swivel chair equally distant to four loudspeakers
(Roland active Loudspeakers, Model MA15-D, and
a M-Audio sound card, FastTrack Pro). Previous research has shown that spatial rendering increases affective impact of sound (Vastfjall, 2003). The lights
were dimmed down during the personalization and
pre-relaxation sessions (see Procedure section) and
turned off during the a/t NF session. A 42 inches
screen (not visible to the for participants) was used
to monitor the EEG signal quality and levels.

2.2

Experimental procedure

Two hypotheses were tested using a betweensubjects design. Thirty one participants, mean age
27.81(SD = 5.18), 15 females, took part in the experiment. The study was conducted in accordance with
the Declaration of Helsinki. Participants were randomly distributed among four experimental groups:
• F-S group: Fixed EEG-to-Sound mappings, Single EEG feature sonified (M = 26, SD = 2);

• Pedal Module: tremolo, panning and gain are linearly mapped to t/a ratio.

• P-S group: Personalized mappings, Single EEG
feature sonified (M = 27.6, SD = 5);

• Melody module: note triggering speed and pitch
are functions of the t/a ratio where its positive

• F-M group: Fixed mappings, Multiple EEG features (M = 30, SD = 8);

• P-M group: Personalized mappings, Multiple
EEG features sonified (M = 27.5, SD = 4).
All groups listened to soundscapes of a comparable complexity, but these varied in the number of EEG
features feeding the system, and in the sonification
mappings applied. Table 1 shows the configuration
of the sound engine for each group. Each experiment
lasted around 50 minutes, according to the following
protocol:
• Information and consent form: The participant
was explained each stage of the experiment, and
the relation between brain activity and the created
soundscape; the consent form was signed.
• Initial emotional state self-assessment: Subjective measures of emotional valence and arousal
were collected in paper through a 9-point SelfAssessment Manikin (SAM) after (Bradley and
Lang, 1994).
• Sensor placement and baseline state recording:
The participant was seated in a swivel chair, and
the Emotiv EPOC was mounted. The baseline
EEG activity was recorded.
• Relaxation induction: The participant was asked
to close her eyes and listen to a 5-minute sound
of sea waves. EEG activity was recorded, and
the thresholds for NF session were calculated by
looking at the maxima and minima values.
• Pre-test emotional state self-assessment: The participant filled-in again two SAM scales.
• Mapping adjustment: EEG features were sonified
online. The participant was asked to personalize
the mappings till reaching the most relaxing sound
possible, using a MIDI interface with potentiometers placed in front of the chair. There was no time
limit set to define the mappings.
• NF training session: The participant was asked to
close her eyes and to turn the chair facing away
from the experiment leader. Participants were
asked to relax and to listen to the soundscape (different for each group) for 15 minutes. Participants
were instructed to raise their hand if feeling uncomfortable or falling asleep.
• Post-test emotional state self-assessment: participants filled-in two SAM scales again.
• Headset removal and debriefing: the EEG headset was removed. Participants were debriefed,
thanked and received candies as a small reward.

2.3

Data Analysis

For analysis purposes, the raw EEG data with sampling at 128 Hz was first filtered (0.5 − 30 Hz). Using

visual inspection and thresholding (over 3 σ) data regions with artefacts were marked for removal in subsequent analyses. Closely following the design of the
NF training protocol (see section 2.1.4), we analyzed
data from O1 and O2 electrodes. Signals were BP filtered to obtain alpha (8−13 Hz) and theta (4−7.5 Hz)
components. The 15 minute data recordings were
split into 10 s epochs and for each of them relative
alpha, relative theta and t/a ratio where calculated.
Next, we averaged the means from individual epochs
for five 3 − min periods (18 epochs each) excluding
the epochs marked as containing artifacts. This was
done for O1 and O2 channels separately. Then we
used the values averaged across both channels and
used the means from the first and the last 3 − min period of experimental procedure for analysis. In other
words, we compared maximal changes caused by 15
minute training in relative alpha, relative theta and t/a
ratio.
Both subjective (SAM ratings of pre and post assessment of emotional state) and objective measures
(EEG features) of two relaxation periods were subjected to 3-way MANOVA. Therefore within-subjects
factor was Relaxation periods (1-3 min vs. 12-15
min period), whereas between-subjects factors were
Number of sonified EEG features (Single vs. Multiple) and Feedback personalization (Personalized vs.
Fixed mapping). Alpha level was fixed at 0.05 for all
statistical tests. Greenhouse-Geisser correction was
used to correct for unequal variances. For multivariate
analysis Wilks0 Lambda Λ was used as the multivariate criterion. All variables were normally distributed
according to the Kolmogorov-Smirnov test.

3

RESULTS

In accordance with the relaxing nature of the experimental procedure, the overall MANOVA effect
of Relaxation period was significant with F(5, 23) =
6.89, p < 0.001, Λ = 0.4, η̂2P = 0.6. Split by measures,
this effect reached significance for: subjective arousal
ratings, F(1, 27) = 26.06, p < 0.001, η̂2P = 0.49; for
relative alpha power, F(1, 27) = 5.81, p < 0.05, η̂2P =
0.18; for relative theta power, F(1, 27) = 10.4, p <
0.005, η̂2P = 0.28; and for t/a ratio, F(1, 27) =
5.45, p < 0.05, η̂2P = 0.17. This shows that participants in all four groups reached a certain level of
relaxation as compared with the initial 3 minute period of the experiment. More importantly, the interaction between the Relaxation period and betweengroup factors of Number of sonified EEG features and
Feedback personalization also showed significance.
Confirming our Hypothesis 1, the over-

−1

*

−1.5

**

−2

−2.5

1

0.5

0

Relative theta difference, (period 5 − period 1)

−0.5

**

F−M P−M

0.04

0.035

*
0.03

0.025

**

0.02

0.015

0.01

0.005

0.25
0.2
0.15
0.1
0.05
0
−0.05
−0.1
−0.15
−0.2
−0.25

0

−0.5

F−S P−S

0.045

Relative alpha difference, (period 5 − period 1)

0.05

1.5

t/a ratio difference, (period 5 − period 1)

Arousal reduction, SAM (1..9), (period 5 − period 1)

0

F−S P−S

F−S P−S

F−M P−M

F−M P−M

F−S P−S

F−M P−M

Figure 3: The means of the difference between the first
and the final 3-min period of the 15-minute session. Subjective arousal (left panel) and t/a ratios (right panel) are
shown for four experimental groups: F-S (Fixed mapping/Single feature), P-S (Personalized mapping/Single feature), F-M (Fixed mapping/Multiple features), P-M (Personalized mapping/Multiple features). Error bars represent
standard error values. Bonferroni-corrected significant difference from 0 at p < 0.05 (*), and at p < 0.01 (**) levels.

Figure 4: The means of the difference between the first and
the final 3-minute period of the 15-minute session. Relative theta power (left panel) and relative alpha power (right
panel) are shown for four experimental groups: F-S (Fixed
mapping/Single feature), P-S (Personalized mapping/Single
feature), F-M (Fixed mapping/Multiple features), P-M (Personalized mapping/Multiple features). Error bars represent
standard error values. Bonferroni-corrected significant difference from 0 at p < 0.05 (*), and at p < 0.01 (**) levels.

all MANOVA effect of Relaxation period ×
Feedback personalization was significant with
F(5, 23) = 2.58, p < 0.05, Λ = 0.64, η̂2P = 0.36.
This effect reached significance both for t/a ratio
at F(1, 27) = 13.14, p < 0.001, η̂2P = 0.33; and
for relative alpha power, F(1, 27) = 10.08, p <
0.005, η̂2P = 0.27. Changes in relative theta power
did not reach significance. For the groups with
personalized feedback, t/a ratios increased from
0.73 (SE = 0.2) to 1.42 (SE = 0.2), while for the
groups with fixed mappings such change could not be
found; the period means 0.93 (SE = 0.2) for the first
and 0.79 (SE = 0.2) for the fifth one (see also Figure
3, right panel). Similar pattern occurred for relative
alpha levels, with the means for personalized groups
dropping from 0.29 (SE = 0.04) to 0.15 (SE = 0.03),
and steady level means for groups with fixed mappings of 0.2 (SE = 0.04). This effect can be also see
in Figure 4 (right panel) where differences between
the last and the first period are plotted.
In line with our Hypothesis 2, the overall
MANOVA effect of Relaxation period × Number of
sonified EEG features was significant with F(5, 23) =
5.09, p < 0.005, Λ = 0.48, η̂2P = 0.53. This effect
reached significance only for t/a ratio at F(1, 27) =
8.94, p < 0.01, η̂2P = 0.25. Here, the higher number of
sonified features in the feedback resulted in a bigger
t/a ratio increase from the mean of 0.83 (SE = 0.2) at
the initial experiment stage to 1.45 (SE = 0.2) at the
final 3-min period. In comparison, groups with single
feature based feedback had no improvement, going

from the mean of 0.84 (SE = 0.2) to 0.76 (SE = 0.2),
see also Figure 3 (right panel).
The triple interaction between Relaxation period
× Feedback personalization × Number of sonified
features could be also observed. It reached significance only for the relative alpha, F(1, 27) = 5.13, p <
0.05, η̂2P = 0.16. This effect can be better seen in Figure 4 (right panel), where differences between two
periods are plotted for each of participants0 group.
While not significant when comparing the period differences within each group, this interaction is reflected in bigger difference between F-S and P-S
groups as compared to the difference between F-M
and P-M groups.

4

DISCUSSION

The results from our between-group validation of
sonifications using a/t neurofeedback training confirmed the two initial hypotheses, both for subjective
and objective data. The significant differences between initial and final relaxation periods for P-S and
P-M groups using personalized feedback support our
first hypothesis that personalized EEG-to-Sound mappings will be more instrumental than the fixed ones. A
significant increase in t/a ratio and decrease in alpha
relative power were observed after 15-min NF session. This trend was not repeated for F-S and F-M
user groups training with fixed sonification mappings.
Our second hypothesis, stating that multiple EEG

feature sonifications will be more efficient than those
relying on a single EEG feature, was also confirmed.
A significant increase of t/a ratio and a significantly
lower reported subjective arousal after the NF session
was observed only for P-M and F-M groups with multiple feature feedback. As expected, the training effect
was smaller for P-S and F-S groups training with single feature mappings.
Finally, changes in relative alpha power showed
an interaction between a number of sonified EEG
features and their personalization. Here, the differences between the P-S and F-S groups that were using fixed and personalized sonifications of a single
feature are bigger than between P-M and F-M groups
whose training was based on multiple EEG features.
In other words, personalization became less instrumental when multiple features were sonified. Here,
the decrease in attentional resources could explain
this effect. As a number of sound objects to attend becomes larger, it is more likely that “change deafness”
might reduce sonification efficiency (see a recent review by (Shinn-Cunningham, 2008) on auditory spatial attention). Certainly, the complexity of auditory
feedback is an object of a separate study.
Significant between-group differences in subjective and physiological data show that the observed results are not obtained merely due to relaxing nature of
presented soundscapes (wind, water, etc.). While we
did not aim to study specifically the neurofeedback
effects, the full procedure of a/t NF training with realtime feedback differed from simple relaxation. Undeniably, more training sessions, larger number of participants, and more robust EEG equipment should be
used to study brain dynamics during a/t training (e.g.
changes between training sessions, learning curves
within each NF session, individual differences etc.).
However, the observed results that varied significantly
between four test groups support the full functionality
of the conducted NF training. Therefore, we can argue that both sonification richness (a number of EEG
features to be displayed) and end-user personalization
play an important role in the effectiveness of real-time
EEG sonification for neurofeedback.
Our study did not include Sham group with fake
neurofeedback or Control group doing 15-minunte relaxation to compare with our results. While such control would be necessary to quantify the depth of relaxation with and without neurofeedback, this question
was not in scope of our study. As predicted, and also
as shown by the pre- and post-training differences, the
participants group F-S with fixed sonification of the
single EEG feature can be seen as a strongest baseline for the three other groups.
Several observations can be made when compar-

ing our results with previous works on sonification
richness and personalization. The transparency of the
controls for sound adjustment was an important factor, as in the case of sonification-based EEG diagnostic by expert users (De Campo et al., 2007). The positive effect of personalization in our studies demonstrates that the task of sonification adjustment was
clear and understandable for naive end-users. However, generation and direct evaluation of intuitive interfaces for both end-users and experts should be further explored. In this regard, we plan to integrate
our sonification system into a tabletop, tangible interface. Our previous work shows that such multimodal
Brain-Tangible User Interfaces (BTUI) foster performance and motivational aspects of interaction, allowing for multi-user setups with shared interface control for training and collaborative work (Mealla et al.,
2011).
Another issue is the problem of validation in sonification studies as pointed out by (Väljamäe et al.,
2013). While diagnostic and neurofeedback applications are expected to have a rigorous assessment,
most of EEG sonification studies lack a proper validation, making difficult to determine the efficiency of
a particular EEG sonification strategy. Certainly, this
lack of quantitative assessment reflects the ongoing
development in this research field. In this context, we
contributed to the methodologies for sonification validation through a combination of objective and subjective measures using a well-known neurofeedback
scenario. Some discrepancy between subjective ratings and physiological indices observed in our study
should encourage the use of multimodal measures.
Finally, it is important to note a number of advantages offer by the presented sonification system. First,
model-based sonification provides a wide range of
richness and manipulation, versatile EEG-to-Sound
mapping, and realistic dynamic sounds. Here, several
EEG features can be used as an input to one model,
e.g. rain model, where its several parameters can vary
(amount, loudness, sound characteristics). Second,
the use of Pd permits a modular approach, allowing
the combination of different sound objects and the
addition of new ones. Special interest here present
sounds that are directly related to user emotional state
like breathing or heart beat (Tajadura-Jimenez et al.,
2008). Other future work include testing different NF
protocols, integrating and evaluating the system with
other EEG devices (Mealla et al., 2011), and studying user preferences for sound mappings and their effectiveness for medical or training applications. Finally, future work regarding the a/t training protocol
will imply tests with several sessions, different sound
models and other EEG equipment.

5

CONCLUSIONS

A real-time sonification system was used to design flexible, multi-parametric EEG data sonifications, that were later adjusted in real time by endusers to personalize the mappings. Four settings varying in feedback richness and personalization were
evaluated using 15-min a/t neurofeedback training
and between-subjects design. Comparing both subjective and physiological data from pre- and posttraining showed significant relaxation in groups with
personalized feedback, while not in groups training
with fixed sonification mappings. In addition, a larger
number of sonified EEG features resulted in higher
t/a ratio increase. These results demonstrate that both
sonification richness (a number of EEG features to
be displayed) and end-user personalization play an
important role in the effectiveness of real-time EEG
sonification for neurofeedback.

ACKNOWLEDGEMENTS
The last author of this paper received funding
from Marie Curie Actions of the European Union’s
Seventh Framework Programme (FP7/2007-2013)
under REA GA-303172.

REFERENCES
Allanson, J. and Fairclough, S. (2004). A research agenda
for physiological computing. Interacting with Computers, 16(5):857–878.
Bradley, M. M. and Lang, P. J. (1994). Measuring emotion:
The self-assessment manikin and the semantic differential. Journal of Behavior Therapy and Experimental
Psychiatry, 25(1):49 – 59.
De Campo, A., Hoeldrich, R., Eckel, G., and Wallisch, A.
(2007). New Sonification Tools For Eeg Data Screening And Monitoring. In Proceedings of the 13th International Conference on Auditory Display, volume
67(2009)90, pages 536–542.
Duvinage, M., Castermans, T., Petieau, M., Hoellinger, T.,
Cheron, G., and Dutoit, T. (2013). Performance of
the emotiv epoc headset for p300-based applications.
Biomed Eng Online, 12:56.
Egner, T., Strawson, E., and Gruzelier, J. (2002). Eeg signature and phenomenology of alpha/theta neurofeedback training versus mock feedback. Applied Psychophysiology and Biofeedback, 27(4):261–270.
Emotiv (2013). Emotiv epoc. http://www.emotiv.com.
Gruzelier, J. (2009). A theory of alpha/theta neurofeedback, creative performance enhancement, long distance functional connectivity and psychological integration. Cognitive Processing, 10(1):101–109.

Guttman, S. E., Gilroy, L. A., and Blake, R. (2005). Hearing
what the eyes see: Auditory encoding of visual temporal sequences. Psychological Science, 16(3):228–235.
Hermann, T., Meinicke, P., and Bekel, H. (2002). Sonifications for EEG data analysis. In Proceedings of the
International Conference on Auditory Display (ICAD
2002), pages 3–7, Kyoto.
Hjorth, B. (1970). Eeg analysis based on time domain properties. Electroencephalography and clinical neurophysiology, 29(3):306–310.
Khamis, H., Mohamed, A., Simpson, S., and McEwan, A.
(2012). Detection of temporal lobe seizures and identification of lateralisation from audified EEG. Clinical
Neurophysiology, 123(9):1714–20.
Kropotov, J. (2010). Quantitative EEG, event-related potentials and neurotherapy. Elsevier.
Liang, S.-F., Chen, Y.-C., Wang, Y.-L., Chen, P.-T., Yang,
C.-H., and Chiueh, H. (2013). A hierarchical approach for online temporal lobe seizure detection in
long-term intracranial eeg recordings. Journal of neural engineering, 10(4):045004.
Mealla, S., Väljamäe, A., Bosi, M., and Jordà, S. (2011).
Listening to your brain: Implicit interaction in collaborative music performances. In Proc. of NIME, pages
149–154.
Mullen, T., Luther, M., Way, K., and Jansch, A. (2011).
Minding the (Transatlantic) Gap: An Internet-Enabled
Acoustic Brain-Computer Music Interface followed
throughout the next decade by a number of artists. In
Proc. NIME’11.
Neuroelectrics (2013).
Neuroelectrics enobio.
http://neuroelectrics.com/enobio.
Puckette, M. et al. (1996). Pure data: another integrated
computer music environment. Proceedings of the Second Intercollege Computer Music Concerts, pages 37–
41.
Rodrı́guez, A., Rey, B., and Alcañiz, M. (2013). Validation
of a low-cost eeg device for mood induction studies.
Studies in health technology and informatics, 191:43–
47.
Shinn-Cunningham, B. G. (2008). Object-based auditory
and visual attention. Trends Cogn Sci, 12(5):182–186.
Tajadura-Jimenez, A., Valjamae, A., and Vastfjall, D.
(2008). Self-representation in mediated environments:
the experience of emotions modulated by auditoryvibrotactile heartbeat. Cyberpsychology and Behavior, 11(1):33–38.
Väljamäe, A., Mealla, S., Steffert, T., Holland, S., Marimon, X., Benitez, R., and et al. (2013). A Review
Of Real-time EEG Sonification Research. In The 19th
International Conference on Auditory Display (ICAD2013), Lodz, Poland.
Vastfjall, D. (2003). The subjective sense of presence, emotion recognition, and experienced emotions in auditory virtual environments. Cyberpsycholy and Behavior, 6(2):181–188.
Wright, M. (2005). Open sound control: an enabling technology for musical networking. Organised Sound,
10(03):193–200.

